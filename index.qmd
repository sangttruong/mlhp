---
title: Introduction
format: html
filters:
  - pyodide
---

::: {.content-visible when-format="html"}

<iframe
  src="https://web.stanford.edu/class/cs329h/slides/1.introduction/#/"
  style="width:50%; height:250px;"
></iframe>
[Fullscreen Slide](https://web.stanford.edu/class/cs329h/slides/1.introduction/#/){.btn .btn-outline-primary .btn role="button"}

:::

Machine learning is increasingly shaping various aspects of our lives, from education and healthcare to scientific discovery. A key challenge in developing trustworthy intelligent systems is ensuring they align with human preferences. Learning from human feedback offers a promising solution to this challenge. This book introduces the foundations and practical applications of machine learning from human preferences. Instead of manually predefining the learning goal, the book presents preference-based learning that incorporates human feedback to guide the learning process, drawing insights from related fields such as economics, psychology, and human-computer interaction. By the end of this book, readers will be equipped with the key concepts and tools needed to design systems that effectively align with human preferences. 

The book is intended for researchers, practitioners, and students who are interested in intergrating machine learning with human-centered application. We assume some basic knowledge of probability and statistics, but provides sufficient background and references for the readers to follow the main ideas. The book also provides illustrative program examples and datasets. The field of machine learning from human preference is a vibrant area of research and practice with many open challenges and opportunities, and we hope that this book will inspire readers to further explore and advance this exciting field. The book is divided into 4 chapters:

-   **Chapter 1** lays the foundation for probabilistic modeling of preferences and decisions. It covers key model assumptions (e.g., bounded rationality), along with methods for preference data collection (e.g., pairwise comparisons), and models to interpret the data (e.g., Bradley-Terry model). The chapter also explore preference aggregation through the lense of social choice theory.

-   **Chapter 2** examines various functional form and learning of reward modeling via examples from the field of language modeling and robotics. We discuss the challenges in learning multimodal rewards, meta-reward learning, human biases in reward models, and strategies to leverage both rational and irrational behavior.

-   **Chapter 3** focused on a process where the end goal is to elicit the utility function. Since human is involved in the data collection, we concentrate on active learning methods that elicit the most preference information with a minimal amount of human query. Various strategies are explored, including reducing the learner's variance, exploiting ambiguity and domain knowledge in ranking, with examples from robotics and machine learning systems.

-   **Chapter 4** focuses on process where preference is a signal guiding decision. We discuss dueling bandits, an algorithm for decision making from pairwise preferences, as well as reinforcement learning from human feedback (RLHF) to align language models. Moreover, addressing the challenges of modeling uncertainty in reward models emerges as a crucial area for improvement. We also discuss decision making setting beyond a single reward frameworks to accommodate the oversight of diverse objectives. Embracing multi-objective is pivotal to representing the multifaceted goals of varied stakeholders within AI systems.

-   **Chapter 5** analyzes principles from human-computer interaction (HCI) for systems that learn from humans, like cognitive constraints and user experience. We discuss ethical issues that arise in interaction models and approaches for designing preference elicitation systems considering fairness, privacy, and other socio-technical factors. We tackle challenges around aligning learned models with values from diverse expert and non-expert stakeholders. Ethical considerations in learning from human preference are addressed. We discuss the potential benefits and risks of learning from human preferences and how to address them responsibly and fairly. We also raise questions about the selection and protection of human participants and the possible consequences of exploiting or manipulating human responses.

This book covers various topics, from the statistical foundations and strategies for interactively querying humans to applications for eliciting preference. We review the relevant foundations in microeconomics, psychology, marketing, statistics, and other disciplines and explore their applications to various domains, such as language, robotics, logistics, and more. We adopt the machine learning perspective for modeling, estimating, and evaluating the learning processes. This book is used at Stanford University for a quarter-long class CS329H: Machine Learning from Human Preferences. We include the lecture slides and homework at the begining and end of each chapter, respectively.

<!-- 

Ensuring truthfulness and fairness in eliciting human preferences is crucial, and the book discusses mechanism design principles and techniques to incentivize truthful feedback from humans. The integration of human computing techniques in learning from human preference is also explored, highlighting the use of human intelligence to solve complex problems and improve machine learning algorithms.

**Chapter 7** discusses adaptive user interfaces, which aim to provide personalized experiences by learning individual user preferences from interactions. It presents the design of adaptive interfaces as involving modeling users, collecting user traces, learning models from the data, and applying the models to adapt recommendations. The applications of adaptive interfaces mentioned include route advisors, destination selection assistants, and scheduling tools, with the goal of improving systems through intelligent personalization.

-   **Chapter 1** is an introductory chapter providing an overview of the field and motivations and outlines what will be covered.

-   **Chapter 12** focuses on mechanism design theory and how it can be applied to develop protocols and systems for truthfully learning preferences at scale.

-   **Chapter 13** looks at how human computation frameworks can enable large-scale preference elicitation by crowd-sourcing tasks to many individuals.

-   **Chapter 14** presents applications of inverse reinforcement learning using human feedback for robotics, such as learning helicopter control policies from demonstrations.

Human feedback -- whether from individuals' preferences, judgments, ratings, or other responses -- plays a pivotal role in guiding AI systems
through their learning across various tasks. The following three examples show the importance of human feedback to different systems. First, human feedback could guide AI agents in creating appealing and diverse content by assessing the quality, originality, and relevance of the content. Second, human feedback could also ensure that AI agents align with human needs and values by rectifying potential biases, errors, and harm caused by agents. Third, human feedback could help AI agents learn better policies in complex environments, such as those with sparse or noisy rewards, by encouraging agents to explore and learn in those environments.

Returning to human feedback, their integration can occur at various stages of the learning process, spanning from data collection and labeling to model selection, training, and evaluation. Incorporating these feedbacks enables fine-tuning the model to align with their hidden insights. The utilization of human feedback is crucial due to its ability to offer valuable signals that are challenging to acquire or delineate through other means, such as data or predefined cost functions. There are many ways to update models based on human feedback, depending on the type and level of feedback and the objective and structure of the model. A general taxonomy of feedback-update interactions can be divided into six categories:

-   Observation-level & Active data collection & Asking humans for feedback on specific features, such as collecting expert labels on features.

-   Observation-level & Constraint elicitation & Inferring optimization constraints from insights extracted from feedback.

-   Observation-level & Feature modification & Adding, removing, or preprocessing features of training/finetuning datasets.

-   Domain-level & Dataset modification & Generating synthetic data that satisfy certain constraints specified by human feedback, such as fairness or weak supervision.

-   Domain-level & Constraint specification & Modifying the loss function for optimizing the model based on human feedback, such as imposing fairness, interpretability, or resource constraints.

-   Domain-level & Model editing & Changing the rules or weights of the model based on human feedback, such as incorporating domain knowledge or preferences.

These ways of updating models based on human feedback can help to improve the performance, behavior, and alignment of the models with human values and goals. However, they pose challenges and risks, such as communication barriers, feedback quality, and ethical issues. Therefore, designing and evaluating the feedback-update interactions carefully and responsibly is important.

-->