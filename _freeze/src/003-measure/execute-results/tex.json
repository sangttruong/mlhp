{
  "hash": "918c48785f79f84d7f0dd216831114da",
  "result": {
    "engine": "jupyter",
    "markdown": "# Model-Based Preference Optimization {#ch-model-based}\n\n::: {.content-visible when-format=\"html\"}\n\n<iframe\n  src=\"https://web.stanford.edu/class/cs329h/slides/3.1.active_learning/#/\"\n  style=\"width:45%; height:225px;\"\n></iframe>\n<iframe\n  src=\"https://web.stanford.edu/class/cs329h/slides/3.2.metric_elicitation/#/\"\n  style=\"width:45%; height:225px;\"\n></iframe>\n[Fullscreen - AL](https://web.stanford.edu/class/cs329h/slides/3.1.active_learning/#/){.btn .btn-outline-primary .btn role=\"button\"}\n[Fullscreen - ME](https://web.stanford.edu/class/cs329h/slides/3.2.metric_elicitation/#/){.btn .btn-outline-primary .btn role=\"button\"}\n\n:::\n\n## Active Preference Learning\n### Introduction to Active Learning\n\nIn real-world scenarios, data is often scarce, and acquiring labeled data can be expensive. \nActive learning is a machine learning paradigm that aims to reduce the amount of labeled data \nrequired to train a model to achieve high accuracy. Active learning algorithms iteratively \nselect an input datapoint for an oracle (e.g., a human annotator) to label such that when the \nlabel is observed, the model improves the most. The goal of AL algorithms is to minimize the number of labels\nrequired to achieve a desired level of performance. This technique is particularly useful in\nsituations where labeling data is expensive, time-consuming, or requires domain expertise.\n\nThere are two primary setups in active learning:\n\n-   **Pool-based:** The model selects samples from a large unlabeled pool of data. \n  For example, a model for text classification selects the most uncertain texts from a large pool \n  to ask a human annotator to label.\n\n-   **Stream-based:** The model receives samples sequentially (one sample at a time) and decides \n  whether to label them. The data is gone if the decision maker decides not to label it. \n  For example, a system monitoring sensor data decides on-the-fly whether new sensor readings are \n  valuable enough to label.\n\nA common AL process is shown in @fig-schema:\n\n-   Current model trained on current dataset $\\mathcal{D}$, potential points $\\tilde{x}_1 \\dots \\tilde{x}_m$ \nare being investigated. AL will choose one of them to add to the dataset. \n\n-   Relative to the model, a proxy highlights the relative value of each point to model improvement \n$(v(\\tilde{x}_1) \\dots v(\\tilde{x}_m) )$. A naive proxy is the model's uncertainty about the point.\n\n-   The cycle repeats until we collect enough data or the model is good enough.\n\n![The current model is trained on the current data set $\\mathcal{D}$. \nPotential points $\\tilde{x}_1, \\ldots, \\tilde{x}_m$ are being investigated, \nand one of them will be chosen and added to the data set. A proxy highlights \nthe relative value of each point in terms of improving the model, denoted by \n$v(\\tilde{x}_1), \\ldots, v(\\tilde{x}_m)$. The point with the highest value is \nselected and added to $\\mathcal{D}$. This cycle repeats until enough data has \nbeen collected or the model is good enough.](Figures/active_learning_schema.png){#fig-schema width=\"40%\"}\n\nActive learning has been successfully applied to various domains to enhance real-world systems, \nincluding computer vision, natural language processing, and recommender systems. For example, active \nlearning can improve the computer vision models used in autonomous vehicles [@AL_app_autonomous], \nhere driving scenes can take infinitely many forms, making it impossible to gather an exhaustive dataset. \nInstead, probing a model to understand what type of data it would benefit from is more practical. \nIn robotics, autonomous agents may query humans when unsure how to act or when facing new situations \n[@AL_app_robotics]. In this field, collecting data often incurs significant financial and time costs: \nthe robot must act in real-time in the real world, and while parallelization is possible, being strategic \nabout which examples to collect can best benefit the model. In meteorology, active learning can help decide \nwhere to place additional sensors for weather predictions [@AL_app_sensors]. Sensor placement involves \ndeploying teams to remote locations and expensive construction for an extra data point. Choosing these \nlocations and allocating resources wisely is of interest to governments and businesses. Active learning \ncould also be employed to select data for fine-tuning large language models (LLMs) for specific downstream \ntasks [@AL_app_LLMs]. In this context, it might be difficult to fully describe an NLP task one might want an \nLLM to solve. Often, instead of defining a task via a dataset of examples, it may be easier for a human to \ninteract with the LLM for a specific use case, identify gaps in the model, and address those using active learning.\n\n\n### Introduction to Active Preference Learning\n\nConsider the scenario where a robot is being trained to assist individuals with feeding. How can such a robot be \neffectively taught to perform necessary tasks, such as determining the appropriate distance to reach, detecting the \nlocation of a person's mouth, and, most importantly, understanding human preferences? Typically, robots learn by \nobserving human demonstrations, replicating the ways a person performs the task. However, this method poses \nsignificant challenges. Expert demonstrations are often limited, and training a supervised learning model would \nrequire vast amounts of demonstration data, which is difficult to obtain at scale. Moreover, demonstrations tend \nto be variable, reflecting the actions of individual humans, making the data collection process inconsistent. To \naddress these limitations, alternative approaches have been proposed, such as using pairwise comparisons, where \nhumans evaluate two action trajectories to determine the superior one, or employing physical corrections, in which \nreward functions are learned through human-robot interactions, with humans guiding the robotâ€™s actions during the task.\n\nActive learning algorithms can be employed in preference learning tasks, such as the previously mentioned example, \nwhere the objective is to develop a model that aligns with human preferences while minimizing the need for extensive \nlabeled data or reducing the high cost of annotations. This chapter will explore the theoretical foundations of \npairwise comparisons and active preference learning, along with extensions to these methods that address known \nlimitations. Practical examples where these approaches prove beneficial will also be discussed. Additionally, we \nwill examine the role of LLMs in assisting robots through corrective feedback and highlight the applications of these techniques.\n\n\n### Uncertainty Qualification\n\n**Problem Setup**: In this section, we consider a binary classification problem. The model is trained on a small \nlabeled dataset $\\mathcal{D} = \\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$, where $x_i$ represents the input data and $y_i$ \nis the corresponding label. The model is uncertain about the class labels of some data points and can query an oracle \nto obtain the true labels of these data points. The goal is to minimize the number of queries to the oracle while \nmaximizing the model's performance.\n\nUncertainty quantification (UQ) is a critical aspect of active learning that allows models to evaluate the informativeness \nof new data points. In machine learning (ML), two primary types of uncertainty are often considered: epistemic and \naleatoric uncertainty. **Epistemic uncertainty**, or model uncertainty, arises from a lack of knowledge and can be \nreduced by acquiring more data. This type of uncertainty is especially significant when the model lacks confidence \ndue to insufficient or incomplete information in its training set. On the other hand, **aleatoric uncertainty**, or \ndata uncertainty, stems from the inherent randomness within the data itself. Unlike epistemic uncertainty, aleatoric \nuncertainty cannot be reduced, even with additional data, as it reflects noise or unpredictability in the real \ndata-generating process. Several approaches exist to quantify uncertainty in active learning, each with its strengths and limitations. \n\n**Bayesian methods**, such as Bayesian Neural Networks (BNNs) and Gaussian Processes (GPs), offer a principled way \nof estimating uncertainty by incorporating prior knowledge into the model. These approaches can generate meaningful \nuncertainty estimates that aid in choosing informative samples for labeling. However, they can become computationally \nprohibitive, especially for large and complex models, limiting their applicability in some practical scenarios.\n\nAnother common technique for uncertainty quantification is the use of **ensemble methods**, such as Random Forests or \nGradient Boosting Machines. These methods involve training multiple models and combining their predictions to provide \nan estimate of uncertainty. Ensemble methods are relatively easy to implement and can give valuable insights into model \nuncertainty. However, they can be computationally expensive and may not always produce well-calibrated uncertainty \nestimates. Moreover, they do not integrate prior knowledge, which can be a disadvantage in certain applications.\n\n**Conformal prediction methods** also provide a framework for estimating uncertainty by offering a measure of confidence \nin predictions based on the conformity of a given instance with the training data. While these methods are useful in \nsome contexts, this book focuses primarily on the Bayesian approach due to its theoretical robustness and capacity to \nquantify uncertainty in a more comprehensive manner.\n\n\n### Acquisition Function\n\nUncertainty quantification plays a vital role in **acquisition functions**, which are central to active learning strategies. \nThese functions determine which samples are most valuable to label by evaluating their utility based on the model's \ncurrent uncertainty estimates. Common acquisition functions include **uncertainty sampling** [@AL_uncertainty], \nwhich selects samples the model is least confident about, **query-by-committee** [@AL_committee], which utilizes a set \nof models to choose the most uncertain samples, and **Bayesian Active Learning by Disagreement (BALD)** [@AL_BALD], \nwhich selects samples that maximize information gain by reducing model uncertainty. Through careful uncertainty quantification, \nacquisition functions guide the active learning process, improving the model's efficiency in learning from limited data. \nOther acquisition functions that can be employed include:\n\n-   **Active Thompson Sampling** [@AL_exploreexploit]: This method leverages the Thompson Sampling algorithm to select a \n    posterior sample from the model's distribution and compute the expected utility of labeling using that sample. By doing so, \n    the algorithm balances exploration and exploitation, leading to effective active learning.\n\n-   **Expected model change** [@AL_expmodelchange]: This approach focuses on labeling points that would have the most \n    impact on changing the current model parameters.\n\n-   **Expected error reduction** [@AL_experrorredn]: Points that would most effectively reduce the model's generalization \n    error are labeled using this strategy.\n\n-   **Variance reduction** [@AL_variance]: This approach labels points that would minimize output variance, which is one \n    component of error. By selecting points that reduce variability in the model's predictions, it aims to improve overall performance.\n\n-   **User Centered Labeling Strategies** [@AL_usercentered]: This approach involves actively involving the user in \n    the labeling process by visualizing data through dimensionality reduction techniques. The user then provides labels \n    for the compiled data based on their domain expertise and preferences. This strategy leverages user input to improve \n    the quality and relevance of the labeled data.\n\n-   **Querying from diverse subspaces or partitions** [@AL_partition]: When using a forest of trees as the underlying model, \n    the leaf nodes can represent overlapping partitions of the feature space. This strategy selects instances from non-overlapping \n    or minimally overlapping partitions for labeling.\n\n-   **Conformal prediction** [@AL_conformal]: This method predicts that a new data point will have a label similar to old \n    data points in some specified way. The degree of similarity within the old examples is used to estimate the confidence \n    in the prediction.\n\n-   **Mismatch-first farthest-traversal** [@AL_mismatch]: This strategy first prioritizes data points that are wrongly \n    predicted by the current model compared to the nearest-neighbor prediction. The second criterion is the distance to \n    previously selected data, with preference given to those that are farthest away. The goal is to optimize both the \n    correction of mispredictions and the diversity of the selected data.\n\n#### Uncertainty Sampling {.unnumbered}\nUncertainty sampling [@AL_uncertainty] is a widely used acquisition function in active learning that selects data \npoints for which the model exhibits the greatest uncertainty. This method aims to improve model performance by focusing \nlabeling efforts on ambiguous samples, where additional information is likely to yield the greatest benefit. Let $x$ \nrepresent the input, and $p(y|x)$ the probability distribution of the output $y$ given $x$. Several acquisition strategies \nfall under uncertainty sampling, including **entropy sampling**, **margin sampling**, and **least confidence sampling**, \neach providing a unique measure of uncertainty.\n\n-   **Entropy sampling** measures uncertainty by calculating the entropy of the predicted probability distribution. \n    The acquisition function is given by $\\alpha(x) = - \\sum_{y} p(y|x) \\log p(y|x)$, with higher entropy values indicating \n    higher uncertainty.\n\n-   **Margin sampling** focuses on the difference between the two highest predicted probabilities for a sample. The \n    acquisition function is given by $\\alpha(x) = p(y_1|x) - p(y_2|x)$, where $y_1$ and $y_2$ are two most likely classes. \n    Smaller margins signify greater uncertainty.\n\n-   **Least confidence sampling** measures uncertainty by identifying the sample with the lowest predicted probability \n    for its most likely class. The acquisition function is $\\alpha(x) = 1 - p(y_{\\text{max}}|x)$, where $y_{\\text{max}}$ \n    is the class with the highest probability.\n\n**Example:** Consider a binary classification problem with two classes $y_1$ and $y_2$. \nWe have three samples $x_1, x_2, x_3$ and the corresponding predictive distributions are as follows:\n$$\\begin{aligned}\np(y_1|x_1) &= 0.6, \\quad p(y_2|x_1) = 0.4\\\\\np(y_1|x_2) &= 0.3, \\quad p(y_2|x_2) = 0.7\\\\\np(y_1|x_3) &= 0.8, \\quad p(y_2|x_3) = 0.2\n\\end{aligned}$$ {#eq-eq3.1}\n\n-   **Entropy Sampling** \n    +   $\\alpha(x_1) = -0.6 \\log (0.6) - 0.4 \\log (0.4) = 0.29$\n    +   $\\alpha(x_2) = -0.3 \\log (0.3) - 0.7 \\log (0.7) = 0.27$\n    +   $\\alpha(x_3) = -0.8 \\log (0.8) - 0.2 \\log (0.2) = 0.22$\n\nWe would select $x_1$ for labeling as it has the highest entropy, indicating the model is most uncertain about its prediction at $x_1$.\n\n-   **Margin Sampling** \n    +   $\\alpha(x_1) = 0.6 - 0.4 = 0.2$\n    +   $\\alpha(x_2) = 0.7 - 0.3 = 0.4$\n    +   $\\alpha(x_3) = 0.8 - 0.2 = 0.6$\n\nWe would select $x_1$ for labeling as it has the smallest margin, indicating the model is most uncertain about the prediction at $x_1$.\n\n-   **Least Confidence Sampling** \n    +   $\\alpha(x_1) = 1 - 0.6 = 0.4$\n    +   $\\alpha(x_2) = 1 - 0.7 = 0.3$\n    +   $\\alpha(x_3) = 1 - 0.8 = 0.2$\n\nWe would select $x_1$ for labeling as it has the lowest confidence, indicating the model is most uncertain about the prediction at $x_1$.\n\nIn summary, uncertainty sampling methods, whether based on entropy, margin, or least confidence, help prioritize data \npoints that the model struggles with the most. By focusing on these uncertain samples, the model can more efficiently \nimprove its performance, making uncertainty sampling a key tool in active learning.\n\n#### Query-by-Committee {.unnumbered}\nQuery-by-Committee [@AL_committee] is an active learning strategy where a committee of models selects samples for \nlabeling based on the level of disagreement among the committee members. Several acquisition functions can be employed \nunder this framework to quantify the disagreement:\n\n-   **Vote Entropy:** The vote entropy measures the uncertainty based on how often the committee members vote for each class. \n    The acquisition function is defined as $\\alpha(x) = \\mathbb{H}\\left[\\frac{V(y)}{C}\\right]$, where $V(y)$ is the number of \n    votes for class $y$ and $C$ is the number of committee members.\n\n-   **Consensus Entropy:** This acquisition function measures the entropy of the average probability distribution across committee \n    members. It is given by $\\alpha(x) = \\mathbb{H}[P_C(y|x)]$, where $P_C(y|x)$ is the average probability distribution for sample \n    $x$ across all committee members.\n\n-   **KL Divergence:** The KL divergence quantifies the disagreement by comparing the probability distribution of each \n    committee member to the average distribution. The acquisition function is given by \n    $\\alpha(x) = \\frac{1}{C} \\sum_{c=1}^{C} D_{KL}[P_c(y|x) || P_C(y|x)]$, \n    where $P_c(y|x)$ is the probability distribution of committee member $c$ and $P_C(y|x)$ is the average distribution \n    across the committee.\n\n**Example:** Consider a binary classification problem with two classes $y_1$ and $y_2$. We have three committee members and three samples: $x_1$, $x_2$, and $x_3$. The predictive distributions for each committee member are given below:\n\n|   $x$ | $p_1(y_1 \\vert \\cdot)$  | $p_1(y_2 \\vert \\cdot)$ | $p_2(y_1 \\vert \\cdot)$ | $p_2(y_2 \\vert \\cdot)$ | $p_3(y_1 \\vert \\cdot)$ | $p_3(y_2 \\vert \\cdot)$ |\n| ----- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- |\n| $x_1$ | 0.6                   | 0.4                   | 0.7                   | 0.3                   | 0.3                   | 0.7                   |\n| $x_2$ | 0.3                   | 0.7                   | 0.4                   | 0.6                   | 0.4                   | 0.6                   |\n| $x_3$ | 0.8                   | 0.2                   | 0.9                   | 0.1                   | 0.7                   | 0.3                   |\n\n**Query-by-Committee: Vote Entropy**\n\n-   For sample $x_1$, the votes for $y_1$ and $y_2$ are $V(y_1) = 2$ and $V(y_2) = 1$. The vote entropy is $\\alpha(x_1) = - \\frac{2}{3} \\log (\\frac{2}{3}) - \\frac{1}{3} \\log (\\frac{1}{3}) = 0.28$.\n-   For sample $x_2$, the votes are $V(y_1) = 0$ and $V(y_2) = 3$, resulting in vote entropy $\\alpha(x_2) = 0$.\n-   For sample $x_3$, the votes are $V(y_1) = 3$ and $V(y_2) = 0$, resulting in vote entropy $\\alpha(x_3) = 0$.\n\nThus, sample $x_1$ would be selected for labeling as it has the highest vote entropy, indicating the greatest disagreement among the committee members.\n\n**Query-by-Committee: Consensus Entropy**\n\nThe first step is to compute the consensus probability of each class for each sample:\n\n-   For $x_1$, $p_c(y_1|x_1) = \\frac{0.6 + 0.7 + 0.3}{3} = 0.53$ and $p_c(y_2|x_1) = \\frac{0.4 + 0.3 + 0.7}{3} = 0.47$.\n-   For $x_2$, $p_c(y_1|x_2) = \\frac{0.3 + 0.4 + 0.4}{3} = 0.37$ and $p_c(y_2|x_2) = \\frac{0.7 + 0.6 + 0.6}{3} = 0.63$.\n-   For $x_3$, $p_c(y_1|x_3) = \\frac{0.8 + 0.9 + 0.7}{3} = 0.8$ and $p_c(y_2|x_3) = \\frac{0.2 + 0.1 + 0.3}{3} = 0.2$.\n\nNext, we compute the entropy of these consensus probabilities:\n\n-   For $x_1$, $\\mathbb{H}[p_c(y|x_1)] = -0.53 \\log (0.53) - 0.47 \\log (0.47) = 0.30$.\n-   For $x_2$, $\\mathbb{H}[p_c(y|x_2)] = -0.37 \\log (0.37) - 0.63 \\log (0.63) = 0.29$.\n-   For $x_3$, $\\mathbb{H}[p_c(y|x_3)] = -0.8 \\log (0.8) - 0.2 \\log (0.2) = 0.22$.\n\nThus, $x_1$ would be selected for labeling as it has the highest consensus entropy, indicating the highest level of disagreement among the committee members.\n\n#### Bayesian Active Learning by Disagreement {.unnumbered}\n\nBayesian Active Learning by Disagreement (BALD) [@AL_BALD] selects the samples for which the model expects to gain the most Shannon information when corresponding labels are observed:\n\n$$\n\\mathbb{I}(\\theta; y|x, \\mathcal{D}) = \\mathbb{H}[p(y|x, \\mathcal{D})] - \\mathbb{E}_{p(\\theta | \\mathcal{D})} [\\mathbb{H}[p(y|x, \\theta, \\mathcal{D})]]\n$$ {#eq-eq3.2}\n\nwhere $\\mathbb{H}[\\cdot]$ denotes entropy. When there is significant disagreement among models, the predictive entropy (the first term) will be large, while the expected entropy (the second term) will be smaller. This difference represents the degree to which the models disagree. BALD selects points where this disagreement is maximized.\n\n\n-   To compute the first term, we can derive the following expression:\n\n$$\\begin{aligned}\n\\mathbb{H}[p(y|x, \\mathcal{D})] &= \\mathbb{H}\\left[\\int_{\\theta} p(y|x, \\theta, \\mathcal{D}) p(\\theta | \\mathcal{D}) d\\theta\\right]\\\\\n&\\approx \\mathbb{H}\\left[\\frac{1}{N}\\sum_{i=1}^{N} p(y|x, \\theta_i, \\mathcal{D})\\right]\\\\\n&= \\mathbb{H}\\left[\\overline{p}(y|x, \\mathcal{D})\\right]\n\\end{aligned}$$ {#eq-eq3.3}\n\n-   To compute the second term, we can derive the following expression:\n\n$$\\begin{aligned}\n\\mathbb{E}_{p(\\theta|\\mathcal{D})} [\\mathbb{H}[p(y|x, \\theta, \\mathcal{D})]] &= \\mathbb{E}_{p(\\theta|\\mathcal{D})} \\left[ - \\sum_{y} p(y|x, \\theta, \\mathcal{D}) \\log p(y|x, \\theta, \\mathcal{D}) \\right] \\\\\n&\\approx - \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\sum_{y} p(y|x, \\theta_i, \\mathcal{D}) \\log p(y|x, \\theta_i, \\mathcal{D}) \\right)\n\\end{aligned}$$ {#eq-eq3.4}\n\n\n**Example:** Consider a binary classification problem with two classes, $y_1$ and $y_2$. We have two samples, $x_1$ and $x_2$, and the modelâ€™s predictive distributions are as follows:\n\n- **First-time inference** (with $\\theta_1 \\sim p(\\theta | \\mathcal{D})$):\n  $$\n  p(y_1|x_1, \\theta_1, \\mathcal{D}) = 0.6, \\quad p(y_2|x_1, \\theta_1, \\mathcal{D}) = 0.4\n  $$ {#eq-eq3.5}\n  $$\n  p(y_1|x_2, \\theta_1, \\mathcal{D}) = 0.4, \\quad p(y_2|x_2, \\theta_1, \\mathcal{D}) = 0.6\n  $$ {#eq-eq3.6}\n\n- **Second-time inference** (with $\\theta_2 \\sim p(\\theta | \\mathcal{D})$):\n  $$\n  p(y_1|x_1, \\theta_2, \\mathcal{D}) = 0.8, \\quad p(y_2|x_1, \\theta_2, \\mathcal{D}) = 0.2\n  $$ {#eq-eq3.7}\n  $$\n  p(y_1|x_2, \\theta_2, \\mathcal{D}) = 0.5, \\quad p(y_2|x_2, \\theta_2, \\mathcal{D}) = 0.5\n  $$ {#eq-eq3.8}\n\n**Solution:**\n\n**Step 1:** Compute the entropy of the model's predictive distribution for each sample:\n\n-   $\\overline{p}_{\\theta}(y_1|x_1, \\theta, \\mathcal{D}) = 0.7$\n-   $\\overline{p}_{\\theta}(y_2|x_1, \\theta, \\mathcal{D}) = 0.3$\n-   $\\overline{p}_{\\theta}(y_1|x_2, \\theta, \\mathcal{D}) = 0.45$\n-   $\\overline{p}_{\\theta}(y_2|x_2, \\theta, \\mathcal{D}) = 0.55$\n\nNow, we compute the entropy for each sample using the formula:\n\n$$\n\\mathbb{H}[p(y|x, \\mathcal{D})] = - p(y_1|x, \\mathcal{D}) \\log(p(y_1|x, \\mathcal{D})) - p(y_2|x, \\mathcal{D}) \\log(p(y_2|x, \\mathcal{D}))\n$$ {#eq-eq3.9}\n\nFor $x_1$:\n\n$$\n\\mathbb{H}[p(y|x_1, \\mathcal{D})] = - 0.7 \\log(0.7) - 0.3 \\log(0.3) = 0.27\n$$ {#eq-eq3.10}\n\nFor $x_2$:\n\n$$\n\\mathbb{H}[p(y|x_2, \\mathcal{D})] = - 0.45 \\log(0.45) - 0.55 \\log(0.55) = 0.30\n$$ {#eq-eq3.11}\n\n**Step 2:** Compute the expected entropy of the model's predictive distribution for each sample.\n\nFor $x_1$:\n\n-   $\\mathbb{H}_{\\theta_1}[p(y|x_1, \\theta, \\mathcal{D})] = -0.6 \\log(0.6) - 0.4 \\log(0.4) = 0.29$\n-   $\\mathbb{H}_{\\theta_2}[p(y|x_1, \\theta, \\mathcal{D})] = -0.8 \\log(0.8) - 0.2 \\log(0.2) = 0.22$\n\nAverage the results:\n\n$$\n\\mathbb{E}_{p(\\theta|\\mathcal{D})}[\\mathbb{H}[p(y|x_1, \\theta, \\mathcal{D})]] \\approx \\frac{0.29 + 0.22}{2} = 0.255\n$$ {#eq-eq3.12}\n\nFor $x_2$:\n\n-   $\\mathbb{H}_{\\theta_1}[p(y|x_2, \\theta, \\mathcal{D})] = -0.4 \\log(0.4) - 0.6 \\log(0.6) = 0.29$\n-   $\\mathbb{H}_{\\theta_2}[p(y|x_2, \\theta, \\mathcal{D})] = -0.5 \\log(0.5) - 0.5 \\log(0.5) = 0.30$\n\nAverage the results:\n\n$$\n\\mathbb{E}_{p(\\theta|\\mathcal{D})}[\\mathbb{H}[p(y|x_2, \\theta, \\mathcal{D})]] \\approx \\frac{0.29 + 0.30}{2} = 0.295\n$$ {#eq-eq3.13}\n\n**Step 3:** Compute the BALD score for each sample.\n\nThe BALD score $\\alpha(x)$ is the difference between the predictive entropy and the expected entropy:\n\nFor $x_1$:\n\n$$\n\\alpha(x_1) = \\mathbb{H}[p(y|x_1, \\mathcal{D})] - \\mathbb{E}_{p(\\theta|\\mathcal{D})}[\\mathbb{H}[p(y|x_1, \\theta, \\mathcal{D})]] = 0.27 - 0.255 = 0.015\n$$ {#eq-eq3.14}\n\nFor $x_2$:\n\n$$\n\\alpha(x_2) = \\mathbb{H}[p(y|x_2, \\mathcal{D})] - \\mathbb{E}_{p(\\theta|\\mathcal{D})}[\\mathbb{H}[p(y|x_2, \\theta, \\mathcal{D})]] = 0.30 - 0.295 = 0.005\n$$ {#eq-eq3.15}\n\nWe would select $x_1$ for labeling since it has the highest BALD score, indicating that labeling $x_1$ will provide the most information gain for the model.\n\n### Active Learning by Variance Reduction\n\nActive Learning by Variance Reduction [@AL_variance] is an algorithm designed to select the next data point for labeling \nbased on the anticipated reduction in the model's variance. The objective is to identify the point $\\tilde{x} \\sim p(x)$ \nthat, when labeled ($y(\\tilde{x})$), will most effectively decrease the model's variance. To quantify the expected error \nat a given input $x$, we can mathematically express it as follows:\n\n$$\n\\mathbb{E}_{\\hat{y} \\sim p(\\hat{y} | \\mathcal{D}; x), y \\sim p(y|x)} (\\hat{y} - y)^2\n$$ {#eq-eq3.16}\n\nIn @eq-eq3.16, $\\hat{y}$ represents the model's prediction, while $y$ denotes the true label corresponding \nto the input $x$. This formulation captures the average squared difference between the predicted and actual values, \nproviding a measure of the model's accuracy. Utilizing concepts from bias-variance decomposition as outlined in the \nliterature [@bias_variance_orig_paper], we can expand the expected error term. The expansion is given by:\n\n$$\\begin{aligned}\n\\mathbb{E}_{\\hat{y} \\sim p(\\hat{y} | \\mathcal{D}; x), y \\sim p(y|x)} (\\hat{y} - y)^2 &= \\mathbb{E}_{\\hat{y}, y}[(\\hat{y} - \\mathbb{E}[y|x]) + (\\mathbb{E}[y|x] - y)]^2 \\\\\n&= \\mathbb{E}_{\\hat{y}, y} [(y - \\mathbb{E}[y|x])^2]\\\\\n&+ 2\\mathbb{E}_{\\hat{y}, y} [(\\hat{y} - \\mathbb{E}[y|x])(\\mathbb{E}[y|x] - y)]\\\\\n&+ \\mathbb{E}_{\\hat{y}, y}(\\hat{y} - \\mathbb{E}[y|x])^2\n\\end{aligned}$$ {#eq-eq3.17}\n\nIn @eq-eq3.17, the first term represents the variance of the true label $y$, the second term evaluates to zero, \nand the third term accounts for the variance of the model's prediction $\\hat{y}$. \nTo clarify why the second term is zero, we note that:\n\n$$\n\\mathbb{E}_{\\hat{y}, y}[\\mathbb{E}[y|x] - y] = 0\n$$ {#eq-eq3.18}\n\nThis indicates that the expected deviation of the true label from its conditional mean is null, \nas $\\mathbb{E}[y|x]$ is, by definition, the average of $y$ given $x$. Focusing on the third term, we derive it as follows:\n\n$$\\begin{aligned}\n\\mathbb{E}_{\\hat{y}, y}(\\hat{y} - \\mathbb{E}[y|x])^2 &= \\mathbb{E}_{\\hat{y}, y}[(\\hat{y} - \\mathbb{E}_{\\hat{y}}[\\hat{y}] + \\mathbb{E}_{\\hat{y}}[\\hat{y}] - \\mathbb{E}[y|x])^2] \\\\\n&= \\mathbb{E}_{\\hat{y}, y}[(\\hat{y} - \\mathbb{E}_{\\hat{y}}[\\hat{y}])^2] + (\\mathbb{E}_{\\hat{y}}[\\hat{y}] - \\mathbb{E}[y|x])^2\n\\end{aligned}$$ {#eq-eq3.19}\n\nHere, $\\mathbb{E}_{\\hat{y}}[\\hat{y}]$ represents the expected model prediction conditioned on the \ndata $\\mathcal{D}$ and input $x$. Combining the results of our analysis, we arrive at the total expected error as:\n\n$$\n\\mathbb{E}_{y} [(y - \\mathbb{E}[y|x])^2] + (\\mathbb{E}_{\\hat{y}} [\\hat{y} - \\mathbb{E}[y|x]] )^2 + \\mathbb{E}_{\\hat{y}} [(\\hat{y} - \\mathbb{E}_{\\hat{y}}[\\hat{y}])^2]\n$$ {#eq-eq3.20}\n\nIn this equation, the first term signifies the variance of the true label, which remains constant for \na given $x$. The second term captures the bias of the model, reflecting how much the average model prediction \ndeviates from the expected true label. The third term quantifies the model's uncertainty concerning the selected input $x$.\n\nReferring to [@AL_variance], we can denote the uncertainty term as:\n\n$$\n\\sigma^2_{\\hat{y}} (x | \\mathcal{D}) = \\mathbb{E}_{\\hat{y}} [(\\hat{y} - \\mathbb{E}_{\\hat{y}}[\\hat{y}])^2]\n$$ {#eq-eq3.21}\n\nThis term explicitly represents the variance of the model predictions at the input $x$ given the dataset \n$\\mathcal{D}$. More explicitly, it can be expressed as:\n\n$$\n\\sigma^2_{\\hat{y}} (x | \\mathcal{D}) =  \\mathbb{E}_{\\hat{y} \\sim p(\\hat{y} | \\mathcal{D}; x)} [(\\hat{y} - \\mathbb{E}_{\\hat{y} \\sim p(\\hat{y} | \\mathcal{D}; x)}[\\hat{y}])^2]\n$$ {#eq-eq3.22}\n\nThis formulation emphasizes the variability of the model's predictions around their mean, providing \ninsights into the model's reliability in its estimations. The active learning by variance reduction algorithm \ncan be summarized as follows:\n\n1. **Sampling Candidates**: Sample candidate points $\\tilde{x}_1, \\dots, \\tilde{x}_m$ from $p(x)$.\n2. **Compute Expected Variance Reduction**: For each candidate $\\tilde{x}_i$, compute:\n$$\n   \\mathbb{E}_{p(x)} [\\sigma^2_{\\hat{y}} (x | \\tilde{\\mathcal{D}})]\n$$ {#eq-eq3.23}\n3.  **Select the Best Candidate**: Choose the point that minimizes expected variance reduction:\n$$\n   \\tilde{x}^* = \\arg\\min_{\\tilde{x}_i} \\mathbb{E}_{p(x)} [\\sigma^2_{\\hat{y}} (x | \\tilde{\\mathcal{D}})]\n$$ {#eq-eq3.24}\n4. **Update Model**: Incorporate the newly labeled data and repeat the process.\n\nWhile there is no general recipe for the number of iterations to\nperform, one could imagine relying on some empirical measure like a loss\non left-out labelled data to gauge model improvement (as seen in [@fig-empirical:gauss; @fig-empirical:regress]). \nIntuitively, the size of the data\nset and its relationship to the loss is intimately tied to the model\ncomplexity which impacts its data-thirstiness.\n\nWe note to the reader that $P(X=x)$ is a distribution with\npotentially-infinite support and the authors do not compute this\nintegral exactly. Instead, the computational estimate of that integral\nconsists of sampling several points $x \\sim P(X=x)$ and averaging the\nquantity inside the integral over these points until convergence using\nMonte-Carlo sampling approaches (see [@monte-carlo]).\n\n![Two models were empirically explored. These two models lead to closed-form,\naccurately and efficiently-computed expected learner variance which can\nbe plugged into the algorithm.](Figures/1_two_models.png){#fig-two_models\nwidth=\"100%\"}\n\nArm2D (@fig-arm2D) is a kinematics problem where learner has to\npredict the tip position of a robotic arm given a set of joint angles\n$\\mathbf{\\theta_1}, \\mathbf{\\theta_2}$. In this analysis, the two models\nseen in @fig-two_models, namely the Gaussian mixture model and\nlocally-weighted regression (LOESS).\n\n![The arm kinematics problem. The learner attempts to predict tip\nposition given a set of joint angles\n$\\mathbf{\\theta_1}, \\mathbf{\\theta_2}$](Figures/1_experiment_setup.png){#fig-arm2D\nwidth=\"40%\"}\n\n![Arm2D domain. Dotted lines denote standard error for average of\n10 runs, each started with one initial random\nexample.](Figures/1_experiment_results_gaussian.png){#fig-empirical:gauss\nwidth=\"100%\"}\n\nThe results shown in [@fig-empirical:gauss; @fig-empirical:regress] are intriguing. \nAs expected, the variance of the learner decreases because the authors selected points to minimize \nexpected variance. Additionally, we observe a related decrease in the mean square error (MSE) of both \nmodels as the dataset size increases. This is a notable outcome because the expected learner variance \nfor these models can be computed accurately and efficiently relative to a new point. When integrated \ninto the general active learning loop ([@fig-schema]), this significantly enhances model performance.\n\nIn the case of the locally-weighted regression model ([@fig-empirical:regress]), it is surprising \nthat if points were chosen randomly, the MSE would be highly unstable, with sharp fluctuations. \nHowever, when active learning by variance reduction is applied, using expected learner variance \nas a proxy, the MSE decreases almost smoothly, aside from some initial instabilities.\n\n![Variance and MSE learning curves for LOESS model trained on the Arm2D\ndomain. Dotted lines denote standard error for average of 60 runs, each\nstarted with a single initial random\nexample.](Figures/1_experiment_results_regression.png){#fig-empirical:regress\nwidth=\"100%\"}\n\n### Active Learning in Ranking and Comparison\n\nMany researchers have shown that making comparisons is easier and more convenient \nfor users than assigning a specific score to each item. Individual comparisons yield a \ncomplete ranking over a set of $n$ objects $\\Theta = (\\theta_1, \\theta_2, \\cdots, \\theta_n)$. \nThis ranking is defined as a mapping $\\sigma : \\{1, \\cdots, n\\} \\rightarrow \\{1,\\cdots, n\\}$ \nthat orders the set of objects $\\Theta$. Specifically, for a single $\\sigma$, \n$\\sigma(\\Theta) = \\theta_{\\sigma(1)} < \\theta_{\\sigma(2)} < \\cdots < \\theta_{\\sigma(n-1)} < \\theta_{\\sigma(n)}$, \nwhere $\\theta_{i} < \\theta_{j}$ means that $\\theta_{i}$ is rated lower than $\\theta_{j}$.\n\nFor any $n$ elements to be ranked, there are $n!$ possible orderings that can result \nin the correct complete ranking. Given that a lower bound on sorting is $n\\log n$, \nobtaining a guaranteed true rating over $n$ objects requires $n\\log n$ pairwise \ncomparisons if those comparisons are chosen at random. This number can be quite high \nand costly in many applications, especially since most ranking information comes from \nhumans. The more comparisons they have to make, the more money and time is spent. This \nprocess can also be inefficient, as some comparisons provide more value to the learning \nprocess than others, making some comparisons a waste. This inefficiency can be detrimental \nin fields like psychology and market research, where comparisons are heavily utilized, and \na faster process could offer significant benefits.\n\nThe reason the lower bound on the number of comparisons is $n\\log n$ is that it assumes no \nprior information about the underlying space and field, so comparisons are chosen at random. \nHowever, leveraging the structures within the comparison space can provide more information \nabout which comparisons are most valuable. For example, [@geo_paper] discusses how eye doctors \nhave a wide range of options when assigning prescriptions for glasses, yet patients do not see \nthem making many comparisons before deciding on the best option. This is because eye doctors \nincorporate domain knowledge into the process and only ask clients for comparisons when necessary. \nApplying similar knowledge in the ranking field leads to an active learning approach that \nselects data based on the relevance of a comparison query toward finding the final $\\sigma(\\Theta)$.\n\n#### Geometric Approach to Comparisons {.unnumbered}\n\nIn this section, we will review the paper [@geo_paper], which explores active learning within data that can be embedded in a multi-dimensional space. In this context, comparisons between two different objects divide the space into halves, with one object being superior in each half. By leveraging such spatial information, the paper develops a geometric approach to ranking and active learning. This spatial information serves as the domain knowledge that informs which comparisons to perform to achieve the ranking.\n\nFor this application, the following terms are defined:\n\n1. **$R^d$**: The space in which objects can be embedded.\n\n2. **$\\theta_1, \\cdots,\\theta_n$**: The objects, now representing their locations in $R^d$.\n\n3. For each ranking $\\sigma$, there is a reference point $r_{\\sigma} \\in R^d$, such that if, \naccording to ranking $\\sigma$, $\\theta_{i} < \\theta_{j}$ (object $i$ is worse than $j$),\n then $||\\theta_i - r_{\\sigma}|| < ||\\theta_j - r_{\\sigma}||$. In other words, object $i$ is \n closer to the reference point $r_{\\sigma}$ of the ranking than object $j$.\n\n4. **$\\Sigma_{n,d}$**: The set of all possible rankings of the $n$ objects that satisfy the\n embedding distances in the space $R^d$ as defined above. Note that not all possible rankings \n will satisfy the embedding conditions, but multiple rankings might satisfy all those conditions.\n\n5. For every ranking $\\sigma$, there is $M_n(\\sigma)$, the number of pairwise comparisons \nneeded to identify the ranking. When comparisons are done at random, $E[M_n(\\sigma)] = n\\log n$. \nThe paper [@geo_paper] examines this quantity to demonstrate that it can be reduced by incorporating \nspatial knowledge.\n\n6. **$q_{i,j}$**: The query of comparison between objects $i$ and $j$.\n\n#### Embedding Space {.unnumbered}\n\n![Objects $\\theta_1, \\theta_2, \\theta_3$ and queries in $R^2$. The\n$r_\\theta$ lies in the shaded region which represents\n$\\Sigma_{n,2}$(consistent with the labels of\n$q_{1,2}, q_{1,3}, q_{2,3}$). The dotted (dashed) lines represent new\nqueries whose labels are (are not)\nambiguous.](Figures/SPACE.png){#fig-dim-space width=\"40%\"}\n\nTo properly understand how to select the most \nvaluable queries, it is essential to examine the space where the objects exist and how \nthe queries divide that space to determine the proper rankings. For this example, \nin @fig-dim-space, the paper [@geo_paper] operates in $R^2$ space with three objects: \n$\\theta_1$, $\\theta_2$, and $\\theta_3$. There are pairwise queries $q_{1,3}$, $q_{2,3}$, \nand $q_{1,2}$ between them, denoted by solid lines equidistant from the two objects they compare. \nThese lines split the $R^2$ space into halves, with each half closer to one of the two objects. \nThe paper colors the side of the worse object for each query in dark grey and takes the intersection \nof these halves, resulting in the dark grey region in the image. This region indicates \n$\\Sigma_{n,2}$ since all points follow the embedding conditions. Specifically, for every point \n$r$ in the dark grey area, $||\\theta_3 - r|| < ||\\theta_2 - r|| < ||\\theta_1 - r||$, \nmeaning $\\theta_3 < \\theta_2 < \\theta_1$. Thus, every point $r$ is one of the $r_\\sigma$ representing \ntheir respective rankings $\\sigma \\in \\Sigma_{n,2}$. In other words, the paper aims to have the \nreference points and dark grey region closest to the worst object and furthest from the best object.\n\nThe authors also denote the label for each query $q_{i,j}$, such as label $y_{i,j} = 1\\{q_{i,j}\\}$ \n(for example, $y_{1,2} = 0, y_{3,2} = 1$). This allows for deciding how to label new queries represented \nby dashed and dotted lines, depending on which objects each query compares. Focusing on the dotted line, \ncalled $q_{i,4}$, where $i={1,2,3}$, and considering potential locations of $\\theta_4$, the line must be \nequidistant from one of the three objects in the picture and $\\theta_4$, meaning $\\theta_4$ can be placed \nin three different locations. If the query performed is $q_{2,4}$, then $\\theta_4$ will be closer to the \ndark grey area than $\\theta_2$, thus $y_{2,4} = 0$. However, if $q_{1,4}$ or $q_{3,4}$ are performed, \n$\\theta_4$ will be further from the dark grey area than $\\theta_1$ or $\\theta_3$, meaning $y_{1,4} = y_{3,4} = 1$. \nIn this case, the labels are contradictory and depend on which object they are compared with, making such \na query $q_{i,4}$ ambiguous.\n\nIn contrast, the authors analyze the dashed line, called $q_{i,5}$, where $i={1,2,3}$, and consider \npotential locations of $\\theta_5$. Since the line must be equidistant from one of the three objects in the \npicture and $\\theta_5$, it can be placed in three different locations. If one of the three potential \nqueries is performed, $\\theta_5$ will be closer to the dark grey area than $\\theta_1$, $\\theta_2$, and \n$\\theta_3$, meaning $y_{1,5} = y_{2,5} = y_{3,5} = 0$. In this case, all labels are the same regardless \nof which object is used, meaning such a query will not be contradictory, as all agree on the label.\n\nThe goal is to perform as many ambiguous queries as possible and skip non-ambiguous queries to decrease \nthe total $M_n(\\sigma)$. Intuitively, if there is contradictory information about a query, it needs to be \nerformed so that a human can clarify its direction. Conversely, if all sources of information from the domain \nspace agree on the query's label, that information can be used without asking a human, incorporating the \nknowledge of the embedding distances.\n\nLastly, to consider the general case of the $R^d$ space, rather than discussing halves of the image, it \nis essential to discuss half-spaces. Similarly, consider the half-space that assigns a label of $1$ to \nthe query and the half-space assigning a label of $0$. If both half-spaces exist, they have conflicting \ninformation on the query, making the query ambiguous. However, if one of the half-spaces does not exist, \nit means the other is the full space, representing consistency in the label assignment and a non-ambiguous query.\n\n##### Algorithms for Ambiguous Query Selection {.unnumbered}\n\n```pseudocode\n#| label: alg-qsa\n\\begin{algorithm}\n    \\caption{Query Selection Algorithm}\n    \\begin{algorithmic}\n        \\State \\textbf{input:} $n$ objects in $\\mathbb{R}^d$\n        \\State \\textbf{initialize:} objects $\\theta_1, \\dots, \\theta_n$ in uniformly random order\n        \\For{$j=2, \\dots, n$}\n            \\For{$i=1, \\dots, j-1$}\n                \\If{$q_{i,j}$ is ambiguous}\n                    \\State request $q_{i,j}$'s label from reference\n                \\Else\n                    \\State impute $q_{i,j}$'s label from previously labeled queries\n                \\EndIf\n            \\EndFor\n        \\EndFor\n        \\State \\textbf{output:} ranking of $n$ objects\n    \\end{algorithmic}\n\\end{algorithm}\n```\n\nThe standard algorithm in @alg-qsa requests labels for $q_{i,j}$ if those queries \nare ambiguous; otherwise, it infers the information from prior comparisons and their labels.\n\nIt is important to demonstrate that the number of comparisons decreases. Specifically, \n[@geo_paper] shows that this algorithm has $E[M_n(\\sigma)] = O(d\\log n)$, where $d$ is the \ndimension of the space and $d < n$, which improves on the $O(n\\log n)$ baseline. The proof \ncan be studied in detail in the paper itself, but at a high level, it starts by reasoning \nabout the probability of a query being ambiguous and a comparison being requested from a \nhuman, thus representing $M_n = \\Sigma_{k=1}^{n-1}\\Sigma_{i=1}^k 1\\{Requestq_{i,k+1}\\}$. \nFor that, the authors define $Q(i,j)$, which represents the number of different rankings \nthat exist for $i$ elements in $j$-dimensional space (e.g., $Q(1,d) = 1, Q(n,0) = 1, Q(n,1) = n!$). \nIn that case, $|\\Sigma_{n,d}| = Q(n,d)$. Further, using recurrence relations for $Q(i,j)$, the \nauthors derive that $|\\Sigma_{n,d}| = Q(n,d) = O(n^{2d})$, which is omitted here. Analogously, \nthe authors define $P(i,j)$, which represents the number of rankings in $\\Sigma_{n,d}$ that will \nstill be possible with the addition of a new element $i+1$ to the ranking objects. Referring back \nto @fig-dim-space, $P(i,j)$ estimates how much of the dark grey area will still exist after making \na query for $i+1$. As indicated there, the dotted line ambiguous query did not change the dark grey a\nrea at all ($P(n,d) = Q(n,d)$), whereas the dashed non-ambiguous query would cut a piece from it \n($P(n,d) < Q(n,d)$). Thus, $Request q_{i,k+1} = P(k,d) / Q(k,d)$, so a higher value indicates \nmore possible rankings and an ambiguous query that needs to be requested to obtain more useful \ninformation. With this in mind, the authors derive that $E[M_n(\\sigma)] = O(d\\log n)$, showing \nthat fewer queries are needed for effective ranking.\n\nThe issue with this algorithm is that only one human provides the answers to the requested queries, \nwhich means it does not account for their biases. An alternative approach is a Robust Query Selection \nAlgorithm (RQSA) [@geo_paper], which uses majority voting for every query to indicate the ground \ntruth of the query's label. However, the authors consider that a group of people can still give \nincorrect or divided responses. If the votes for each answer are almost equal in number, the authors \npush that query to the end of the algorithm to see if it can become a non-ambiguous query with more \ninformation learned. If it does not, an odd number of voters is used to determine the final ranking.\n\n##### Performance Analysis {#sec-QSA .unnumbered}\n\n![Mean and standard deviation of requested queries (solid) in the\nnoiseless case for $n = 100$; $\\log_2|\\Sigma_{n,d}|$ is a lower bound\n(dashed).](Figures/Dim:query_graph.png){#fig-rand_n width=\"60%\"}\n\n::: {#tbl-geo_acc}\n  Dimension                           2      3\n  --------------- ----------------- ------ ------\n  \\% of queries         mean         14.5   18.5\n                        std          5.3     6\n  Average error    $d(\\bar{y}, y)$   0.23   0.21\n                   $d(\\bar{y}, y)$   0.31   0.29\n\n  : Statistics for the Robust Query Selection Algorithm (RQSA)\n  [@geo_paper] discussed at the end of @sec-QSA and the baseline of conducting all\n  comparisons. $y$ serves as a noisy ground truth, $\\tilde{y}$ is the\n  result of all comparisons, and $\\hat{y}$ is the output of the RQSA.\n:::\n\n@fig-rand_n shows that the number of comparisons fits within the expected bounds, as\n$\\log|\\Sigma_{n,d}| = \\log(n^d) = d\\log n$. To derive that graph,\nauthors [@geo_paper] sampled 100 random data points in a $R^d$ space,\nwhere $d$ took on 10 different values as indicated on the graph. Each\ndimension's experiments were repeated 25 times for consistency.\n\nWith regard to the accuracy and performance of the method, the authors\ndid a ranking experiment on 100 different audio signals, results of\nwhich can be seen in @tbl-geo_acc. The ground truth labels came from humans,\nindicated by $y$ in the table. That resulted in the existence of noise\nand potential errors in the ground truth, which could influence the\nperformance of both the baseline algorithm that does all comparisons\n($\\tilde{y}$) and the Robust Query Selection Algorithm (RQSA) proposed\nin @sec-QSA ($\\hat{y}$). As can be seen in both 2 and\n3-dimensional spaces RQSA performed worse by $8\\%$ compared to the\nbaseline, which indicates that active learning that uses the domain\ninformation can still be erroneous due to the inference of certain\ncomparisons that sometimes may not be entirely correct. However, as can\nbe seen by the upper part of @tbl-geo_acc,\nsignificantly less queries were requested compared to the baseline,\nwhich means that the approach can have a significant benefit at a cost\nof slight loss in accuracy.\n\n#### User Information as Domain Knowledge for Active Learning {#sec-geo_app .unnumbered}\n\nAn alternative source of domain knowledge could be users themselves, who\ncan indicate their uncertainty when it comes to comparing two objects.\nPrior studies have shown [@unnoisy_humans] that when presented with only\ntwo options when selecting which object is better, but not being able to\nproperly decide, users would get frustrated and tend to respond more\nfaultyly, creating noise and incorrect responses in the data. Through\nfeedback and other studies [@noisy_humans] it was determined that\npresenting users with an option of indifference between the two objects\ncan remove those problems. Moreover, in connection to active learning,\nthe authors show that such an option helps to select more informative\nqueries since it provides more domain knowledge that can be used,\nresulting in a decrease in the number of queries required.\n\nFor this problem, the following terms are defined:\n\n1.  $c$ - a cost function that represents user preferences, and the\n    result the model has to determine at the end of training. The\n    preferred items will have lower costs, and less preferred ones will\n    have higher costs. The goal is to determine this function with the\n    fewest possible number of queries using active learning.\n\n2.  $H$ - a set of hypotheses over the possible cost functions, where\n    for each $h \\in H$ there is a cost function $c_h$ associated with\n    it.\n\n3.  $h^*$ - a true hypothesis that the model needs to determine, which\n    has cost $c_{h^*}$ associated with it\n\n4.  $t(x,y)$ - a test performed to compare items $x$ and $y$ (the user\n    is being asked to provide a response to which item is better). Those\n    tests result in changes and adjustments to $H$ as more information\n    is learned.\n\n5.  $o(x,y)$ - observation or result of $t(x,y)$, where\n    $o(x,y) \\in \\{x<y, x>y\\}$\n\n6.  $S = \\{(t_1, o_1), (t_2, o_2),...,(t_m, o_m)\\}$ - a sequence of $m$\n    pairs of tests and observations\n\n7.  $w(H|S)$ - probability mass of all hypotheses that are still\n    consistent with the observations (similar to the dark grey area from\n    @fig-dim-space and $Q(i,j)$ discussed in @sec-QSA. This means that if $h \\in H$ is\n    inconsistent with user responses received, it is removed from $H$.\n\nWith the key terms defined, let's consider the noiseless base setting\nwhere users only have two options for response. Those components will\nalso later be translated to the setting with the third option so the\ntrue cost function can be determined there. $w(H|S)$ is the sum of the\nweights of all hypotheses that are still consistent with the evidence.\n$$\\begin{aligned}\n    w(H|S) = \\sum_{h \\in H} w(h | S)\\\\\n\\end{aligned}$$  {#eq-eq3.25}\nEach $w(h|S)$ is a probability of the evidence's\nexistence given such hypothesis: \n$$\\begin{aligned}\n    w(h|S) = p(S|h)\n\\end{aligned}$$  {#eq-eq3.26}\nSuch probability comes from the test-observation pairs\nsince they compose the set $S$. Moreover, each test is independent of\nother tests, which gives:\n $$\\begin{aligned}\n    p(S|h) = \\prod_{(t,o) \\in S} p((t,o) | h)\n\\end{aligned}$$  {#eq-eq3.27}\nIn the noiseless setting, users will select an option\nthat minimizes their cost function (selecting more preferred items),\nmathematically defined as: \n$$\\begin{aligned}\n    p((t, o = x) | h) = \n    \\begin{cases}\n        1 & c_h(x) < c_h(y)\\\\\n        0 & else\n    \\end{cases}\n\\end{aligned}$$ {#eq-prob_base}\n\n**6.3.3.1 User Noise Modeling**\n\nAs has been discussed, users are not perfect evaluators and even get\nfrustrated if unable to select the better option. Prior work\n[@unnoisy_humans] has shown that treating users as perfect can lead to\npoor performance. That gave rise to accounting for noise in users'\nresponses, but a majority of such work applies the same noise to all\nqueries and all responses. While those led to great performance results\n[@noisy_humans], they don't accurately reflect the real world, which\ngave rise to the idea of creating query-based noise.\n\nEffectively, for some of the queries it is important to incorporate the\nfact that the user is unsure and noisy, but for others, if the user is\nconfident, noise in the response is not needed at all. For\ncomparison-based learning, this means that the noise is related to the\ncosts of the two items compared. Specifically for items $x$ and $y$, if\n$c_{h^*}(x) \\simeq c_{h^*}(y)$ then the items are hard to distinguish\nfor the user, so here it is preferred to incorporate user uncertainty\nand noise. But if $c_{h^*}(x) >> c_{h^*}(y)$, the user will certainly\nselect $y$ and the other way around, which is where the noise is not\nneeded.\n\nQuery-dependent noise is also supported in the psychology literature,\nwhich means that such an approach is more related to the real world. In\nparticular, psychologists talk about the Luce-Sheppard Choice rule\n[@lus-shep] when talking about comparisons. This rule previously gave\nrise to a logistic model based on the noise [@lus-log] where the\nprobability of observation for a given test is: \n$$\\begin{aligned}\n    p((t, o = x) | h) \\propto exp(-\\gamma * c_h(x))\n\\end{aligned}$$ {#eq-noise_model}\n\n![User response model in the noiseless\nsetting](Figures/Noiseless probs.png){#fig-noiseless_1 width=\"100%\"}\n\n![User response with Luce Sheppard noise\nmodel](Figures/Noise probs.png){#fig-noiseless_2 width=\"100%\"}\n\n[@fig-noiseless_1; @fig-noiseless_2] demonstrate the difference between the\nnoiseless setting and incorporating the Luce-Sheppard Choice rule. GBS\nis the baseline model with only 2 response options, and CLAUS is the\nmodel with the uncertainty option added. The figures show how\nincorporating such noise influences and smoothes the probability\ndistribution of the user's response.\n\n**6.3.3.2 User Uncertainty**\n\nWe will now discuss the functionality of CLAUS, which is an algorithm\ndesigned by [@claus] that allows users to select an uncertain response\nabout the two options that they need to rank. The authors model such\nuncertainty as $\\epsilon$ and it is associated with each $c_h$, so now\nevery hypothesis $h$ is defined over a pair of $(c_h, \\epsilon_h)$. It\nis important to note that the goal is to still learn and maintain our\nobjective on $c$, $\\epsilon$ is only necessary to model the users'\nresponses. The uncertainty relates to the cost function in the following\nway: \n$$\\begin{aligned}\n    |c_h(x) - c_h(y)| < \\epsilon_h\n\\end{aligned}$$  {#eq-eq3.30}\nthis means that the user is uncertain between items $x$\nand $y$ and their cost difference is negligible such that the user is\nnot able to select which item is better. This in turn gives more\ninformation about the real value of the two items, as a binary response\nwould indicate the user's preference towards one item, which will not be\nreal and will skew the cost functions.\n\nThis causes modifications of the problem set-up:\n\n1.  For test $t(x,y)$ the observation will be\n    $o(x,y) \\in \\{x<y, x>y, \\tilde{xy}\\}$, where $\\tilde{xy}$ is the\n    uncertain response.\n\n2.  The probability distribution over the user's response\n    ([@eq-prob_base]) will now be defined as:\n$$\\begin{aligned}\n    p((t, o = x) | h) = \n    \\begin{cases}\n        1 & c_h(x) < c_h(y) - \\epsilon_h\\\\\n        0 & else\n    \\end{cases}\n\\end{aligned}$$ {#eq-eq3.31}\n    \n$$\\begin{aligned}\n    p((t, o = \\tilde{xy}) | h) = \n    \\begin{cases}\n        1 & |c_h(x) - c_h(y)|^2 < \\epsilon_h^2\\\\\n        0 & else\n    \\end{cases}\n\\end{aligned}$$ {#eq-eq3.32}\n\nThis means the user confidently selects $x$ when it is better than $y$ by more than $\\epsilon$, but if the squared\ndifference of the cost functions of two items is negligible by $\\epsilon$ user will choose the indifferent option.\n\n3.  Finally this also updates the noise model ([@eq-noise_model]):\n$$\\begin{aligned}\n    p((t, o = x) | h) \\propto \\exp(-\\gamma * [c_h(x) - c_h(y)])\n\\end{aligned}$$ {#eq-eq3.33}\n\n$$\\begin{aligned}\n    p((t, o = \\tilde{xy}) | h) \\propto exp(-1/\\epsilon_h^2 * [c_h(x) - c_h(y)]^2)\n\\end{aligned}$$ {#eq-eq3.34}\n\n**6.3.3.3 Performance Analysis**\n\n![CLAUS using equivalence classes. Each cost function $c$ corresponds to\nan equivalence class (blue ellipse). Hypotheses (black dots) are\n$\\{c_h,\\epsilon_h\\}$ pairs. Hypotheses sharing a cost $c$ are said to be\ninside the equivalence class of $c$. After performing a test and\nreceiving an observation, the evidence results in downweighting\nconnections among some of the hypotheses.](Figures/equiv.png){#fig-equiv_c width=\"60%\"}\n\nBefore diving deeper into the comparisons of performance, it is\nimportant to indicate that rather than predicting a specific pair\n$(c_h, \\epsilon_h)$, the algorithm focuses on predicting a group of\npairs that are similar to one another, otherwise called equivalence\nclass (@fig-equiv_c), which indicates not essentially different\nhypothesis for the cost function and uncertainty. That information is\nlearned through each new test, as the algorithm updates the information\nabout $c$ and $\\epsilon$ that distinguishes between the distinct $h$,\nfinding the equivalence groups among them. Moreover, the authors tweaked\nthe parameter responsible for the size of the equivalence class (how\nmany hypotheses can be grouped together at a time).\n\n![Performance of GBS and its variants](Figures/GBS:CLAUS.png){#fig-claus_num\nwidth=\"60%\"}\n\n::: {#tbl-claus_tab}\n  **Category**                        **Accuracy**                       **Query Count**\n  --------------------- ------------------------------------ ------------------------------------\n  GBS - About Equal               $94.15 \\pm 0.52$                     $36.02 \\pm 0.03$\n  GBS - Not Sure         $\\textbf{94.66} \\pm \\textbf{0.55}$            $35.95 \\pm 0.04$\n  CLAUS - About Equal             $91.56 \\pm 0.84$            $\\textbf{25.93} \\pm \\textbf{0.41}$\n  CLAUS - Not Sure                $90.86 \\pm 0.74$                     $26.98 \\pm 0.47$\n\n  : Performance of GBS and CLAUS with different labels for the uncertainty\n:::\n\nThe first performance evaluation is done on the number of queries and\nconfirms that it decreases in @fig-claus_num.\nThe GBS model serves as the baseline, as it will do all of the\ncomparison queries using the binary response options. The CLAUS model is\nmeasured over different values of $\\epsilon$ on the x-axis and over\ndifferent sizes of the equivalence sets indicated by different shades of\nblue. Figure shows that all variants of CLAUS use approximately 10 fewer\nqueries on average compared to GBS. Moreover, using bigger-sized\nequivalence classes can further decrease the number of needed queries.\nThe most optimal $\\epsilon \\simeq 0.07$, after which higher $\\epsilon$\ndoes not provide any benefit.\n\nLastly, the authors considered the performance difference, which is\nindicated in @tbl-claus_tab. For that authors used two different labels\nfor the uncertainty button in CLAUS, it was either labeled as \\\"About\nEqual\\\" or \\\"Not Sure\\\" as those can provoke different responses and\nfeelings in users. Moreover, GBS and CLAUS-type responses were mixed in\nthe same set of questions to the user, which splits the metrics for both\nin two as can be seen in @tbl-claus_tab.\nThe performance of CLAUS is lower by $3\\%$ on average, indicating\nsimilar results to @sec-geo_app, showing that a smaller number of queries can\nstill lead to a performance loss. However, the second column of @tbl-claus_tab\nsupports the information in @fig-claus_num,\nas it also shows that 10 fewer queries were conducted on average.\n\n### Active Preference-Based Learning of Reward Functions\n\nActive learning can be essential in learning within dynamic systems and\nenvironments. Say we have an agent in an environment, and we want it to\nconform to a certain behavior as set by a human. How exactly do we go\nabout doing this? In a traditional RL setting, this is solved by a class\nof algorithms under Inverse Reinforcement Learning. Techniques such as\nVICE and GAIL attempt to learn a reward function that can distinguish\nbetween states visited by the agent and states desired to be visited as\ndefined by a human. In effect, a human will demonstrate what it would\nlike the agent to do in the environment, and from there, learning is\ndone. However, what if humans do not precisely know how an agent should\noptimally behave in an environment but still have some opinion on what\ntrajectories would be better than others? This is where a paper like\nActive Preference-Based Learning of Reward Functions comes into the\npicture. The paper aims to use human preferences to aid an agent's\nlearning within a dynamic system.\n\nA dynamic system contains human input, robotic input, and an environment\nstate. The transitions between states is defined by $f_{HR}$, so that we\nhave: \n$$x^{t+1} = f_{HR}(x^t, u_R, u_H)$$  {#eq-eq3.35}\nAt a given time step $t$, we\nhave $x_t$, $u_R^t$, and $u_H^t$. This can be encapsulated into a single\n$d$ dimensional feature vector that the authors denote as $\\phi$. The\npaper then assumes that the underlying reward model we are trying to\nlearn can be represented linearly. If we have our human reward\npreference function defined as $r_H$, this means we can write $r_H$ as:\n$$r_H(x^t, u_R^t, u_H^t) = w^{\\intercal}\\phi(x^t, u_R^t, u_H^t)$$ {#eq-eq3.35}\nBecause the reward function is linear, we can take the weight vector out\nof the summation if we want to calculate the reward over an entire\ntrajectory:\n$$\\begin{aligned}\nR_{H}(x^0, u_R, u_H) &= \\sum_{t=0}^{N} r_{H}(x^t, u^t, u_H^t)\\\\\n\\Phi &= \\sum \\phi(x^t, u_R^t, u_H^t)\\\\ \nR_H(traj) &= w\\cdot\\Phi(traj)\\end{aligned}$$ {#eq-eq3.36}\n\n#### Properties of $W$ {.unnumbered}\n\nFirst, the scale of $w$ does not matter because we only care about the\nrelative rewards produced with $w$ (given two different trajectories, we\nwant to answer the question of which trajectory a human would prefer,\ni.e. which one has a higher preference reward). This means we can\nconstrain $||w|| <= 1$, so the initial prior is uniform over a unit\nball. From here, we can determine a probabilistic expression to assess\nwhether we should prefer trajectory A or B (because it can be noisy with\nhuman input). Let $I_t = +1$ if the human prefers trajectory $A$ and let\n$I_t = -1$ if the human prefers trajectory $B$. We get the following for\n$p(I_t | w)$.\n\n$$\\begin{aligned}\np(I_t = +1|w) &= \\frac{exp(R_H(traj_A))}{exp(R_H(traj_A)) + exp(R_H(traj_B))}\\\\\np(I_t = -1|w) &= \\frac{exp(R_H(traj_B))}{exp(R_H(traj_A)) + exp(R_H(traj_B))}\n\\end{aligned}$$ {#eq-eq3.37}\n\nWe can re-write this expression to make it cleaner, using the following\nsubstitution: $$\\psi = \\Phi(traj_a) - \\Phi(traj_b)$$ {#eq-eq3.38}\n$$f_{\\psi} (w) = p(I_t|w) = \\frac{1}{1 + exp(-I_tw^{\\intercal}\\psi)}$$ {#eq-eq3.39}\n\nThe idea now is that we can update $p(w)$ everytime we get a result from\na human preference query using Bayes:\n\n$$p(w|I_t) <- p(w) \\cdot p(I_t|w)$$ {#eq-eq3.40}\n\nWe do not need to know $p(I_t)$ because we can use an algorithm like the\nMetropolis algorithm to actually sample.\n\n#### Generating Queries {.unnumbered}\n\nThis is where the interesting part of the paper comes into play. How do\nwe actually generate queries for the user to pick between? This paper\nsynthetically generates queries through an optimization process and then\npresents them to a human to pick between. The idea is that we want to\ngenerate a query that maximizes the conditional entropy $H(I|w)$. There\nare a few ways to think about this -- intuitively we want to pick a\nquery that we are most uncertain about given our current weights (thus\nhaving the highest conditional entropy given the weights). The way the\nauthors of the paper frame this originally in the paper is that \\\"we\nwant to find the next query such that it will help us remove as much\nvolume (the integral of the unnormalized pdf over w) as possible from\nthe space of possible rewards.\\\" Mathematically this can be written as:\n\n$$max_{x^0, u_R, u_H^A, u_H^B} min\\{E[1-f_{\\psi}(w)], E[1 - f_{-\\psi}(w)]\\}$$ {#eq-eq3.41}\n\nBut how exactly do we optimize this expression mathematically? After\nall, we need to use this expression to generate synthetic queries. The\nanswer is to sample $w_1, ... w_m$ from $p(w)$. We can assume we are\nsampling points from a point cloud, thus approximating the distribution\n$p(w)$ as\n\n$$p(w) = \\frac{1}{M} \\sum \\delta (w_i).$$  {#eq-eq3.42}\nWe can now approximate the expectation expression like so:\n$$E[1 - f_{\\psi}(w)] = \\frac{1}{M} (\\sum 1 - f_{\\psi}(w_i))$$ {#eq-eq3.43}\n\nand now we can optimize the expression to generate a synthetic query!\nAltogether, the algorithm looks like the following:\n\n```pseudocode\n#| label: alg-design\n\\begin{algorithm}\n\\caption{Preference-Based Learning of Reward Functions}\n\\begin{algorithmic}\n    \\State \\textbf{input:} features $\\phi$, horizon $N$, dynamics $f$, $iter$\n    \\State \\textbf{initialize:} $p(w) \\sim Uniform(B)$, for a unit ball $B$\n    \\While{$t < iter$}\n        \\State $W \\gets M$ samples from $AdaptiveMetropolis(p(w))$\n        \\State $(x^0, u_R, u^A_H, u^B_H) \\gets SynthExps(W,f)$\n        \\State $I_t \\gets QueryHuman(x^0, u_R, u^A_H, u^B_H)$\n        \\State $\\varphi = \\Phi(x^0, u_R, u^A_H) - \\Phi(x^0, u_R, u^B_H)$\n        \\State $f_\\varphi(w) = \\min(1, I_t\\exp(w^\\top \\varphi))$\n        \\State $p(w) \\gets p(w) \\cdot f_\\varphi(w)$\n        \\State $t \\gets t+1$\n    \\EndWhile\n    \\State \\textbf{output:} distribution of $w: p(w)$\n\\end{algorithmic}\n\\end{algorithm}\n```\n\n#### Batching Queries {.unnumbered}\n\nThe algorithm itself works well, however there ends up being a bottle\nneck that each query needs to be synthesized before being sent to the\nhuman -- one at a time. In other words, the human gives their feedback,\nwaits for a query to be synthesized, and then gives another data point\nof feedback. There is no room for parallelization and so the authors\nproposed a second algorithm in a separate paper that allows for the\nbatching of queries. Simply put, we change the mathematical expression\nto the following:\n\n$$max_{\\xi_{ib+1_A}, \\xi_{ib+1_B}, ... , \\xi_{ib+b_A}, \\xi_{ib+b_B} H(I_{ib+1}, I_{ib+2}, .., I_{ib+b} | w)}$$ {#eq-eq3.44}\n\nNaively, we could consider optimizing this in the greedy fashion. This\nwould mean just synthetically generating $b$ independent queries. The\nobvious drawback of this method would be that the queries would likely\nbe very similar to each other. The authors propose a few other\nheuristics that would help guide the algorithm away from generating very\nsimilar queries. As an example, the authors propose Medioid Selection\nwhere we have to cluster $B$ greedy vectors into $b < B$ groups and pick\none vector from each group (the medioid). The authors also propose two\nother methods rooted in providing different queries: boundary medioids\nselection and successive elimination. They are best visually depicted\nas:\n\n![Different selection strategies](Figures/greedy.png){#fig-selection-strategy width=\"75%\"}\n\n#### Results {.unnumbered}\n\nThe authors test both the non-batched and variety of batched learning\nalgorithms on multiple environments:\n\n![Comparison between batched and non-batched algorithms](Figures/activeresults.png){#fig-batch-nonbatch width=\"75%\"}\n\nWhat is interesting to note is that when graphed over $N$ the\nnon-batched active learning approach does in the same ball-park of\nperformance as the batched approaches. However, if you graph it over\ntime, we see that learning is a much slower process when not-batched.\n\n### Application: Foundation Models for Robotics\n\nModern foundation models have been ubiquitous in discussions of\npowerful, general purpose AI systems that can accomplish myriad tasks\nacross many disciplines such as programming, medicine, law, open\nquestion-answering and much more, with rapidly increasing capabilities\n[@bommasani2022opportunities]. However, despite successes from large\nlabs in controlled environments [@brohan2023rt2] foundation models have\nnot seen ubiquitous use in robotics due to shifting robot morphology,\nlack of data, and the sim to real gap in robotics\n[@walke2023bridgedata]. For this subsection we explore two promising\napproaches known as R3M and Voltron which are the first to leverage\npre-training on vast amounts of data towards performance improvement on\ndownstream robotic tasks despite the aforementioned issues\n[@nair2022r3m; @karamcheti2023languagedriven].\n\n#### R3M: Universal Visual Representation for Robotics {.unnumbered}\n\n![R3M pipeline](Figures/r3m.png){#fig-r3m-pipline width=\"95%\"}\n\nR3M represents a significant advancement in the field of robotic\nmanipulation and learning. This model diverges from traditional\napproaches that rely on training from scratch within the same domain on\nthe same robot data as instead it leverags pretraining on large\ndatasets, akin to the practices in computer vision and natural language\nprocessing (NLP) where models are trained on diverse, large-scale\ndatasets to create reusable, general-purpose representations.\n\nThe core principle behind R3M is its training methodology. It is\npre-trained on a wide array of human videos, encompassing various\nactivities and interactions. This diverse dataset enables the model to\ncapture a broad spectrum of physical interactions and dynamics, which\nare crucial for effective robotic manipulation known as EGO4D\n[@grauman2022ego4d]. However, prior papers could not fit this dataset\nwell, and R3M leveraged. The training utilizes a unique objective that\ncombines time contrastive learning, video-language alignment, and a\nsparsity penalty. This objective ensures that R3M not only understands\nthe temporal dynamics of scenes (i.e., how states transition over time)\nbut also focuses on semantically relevant features, such as objects and\ntheir interrelations, while maintaining a compact and efficient\nrepresentation.\n\nWhat sets R3M apart in the realm of robotics is its efficiency and\neffectiveness in learning from a limited amount of data. The model\ndemonstrates remarkable performance in learning tasks in the real world\nwith minimal human supervision -- typically less than 10 minutes. This\nis a stark contrast to traditional models that require extensive and\noften prohibitively large datasets for training. Furthermore, R3M's\npre-trained nature allows for its application across a variety of tasks\nand environments without the need for retraining from scratch, making it\na versatile tool in robotic manipulation. The empirical results from\nusing R3M are compelling, leading to a 10% improvement over training\nfrom a pretrained image-net model, self-supervised approaches such as\nMoCo or even CLIP\n[@deng2009imagenet; @he2020momentum; @radford2021learning]. Note\nhowever, that R3m does **not** use any language data which leaves quite\na bit of supervision to be desired.\n\n#### Voltron: Language Driven Representation Learning for Robotics {.unnumbered}\n\nBuilding off the success of R3M, Voltron proposes a further extension of\nleveraging self-supervision and advancements in foundation models, and\nmulti-modality. Voltron takes on an intuitive and simple dual use\nobjective, where the trained model alternates between predicting the\ntask in an image through natural language and classifying images based\non a natural text label. This forces a nuanced understanding of both\nmodalities [@radford2021learning].\n\nVoltron's approach is distinguished by its versatility and depth of\nlearning. It is adept at handling a wide range of robotic tasks, from\nlow-level spatial feature recognition to high-level semantic\nunderstanding required in language-conditioned imitation and intent\nscoring. This flexibility makes it suitable for various applications in\nrobotic manipulation, from grasping objects based on descriptive\nlanguage to performing complex sequences of actions in response to\nverbal instructions.\n\n![Voltron pipeline](Figures/voltron.png){#fig-voltron-pipeline width=\"95%\"}\n\nThe authors rigorously test Voltron in scenarios such as dense\nsegmentation for grasp affordance prediction, object detection in\ncluttered scenes, and learning multi-task language-conditioned policies\nfor real-world manipulation with up to 15% improvement over baselines.\nIn each of these domains, Voltron has shown a remarkable ability to\noutperform existing models like MVP and R3M, showcasing its superior\nadaptability and learning capabilities [@xiao2022masked].\n\nMoreover, Voltron's framework allows for a balance between encoding\nlow-level and high-level features, which is critical in the context of\nrobotics. This balance enables the model to excel in both control tasks\nand those requiring deeper semantic understanding, offering a\ncomprehensive solution in the realm of robotic vision and manipulation.\n\nVoltron stands as a groundbreaking approach in the field of robotics,\noffering a language-driven, versatile, and efficient approach to\nlearning and manipulation. Its ability to seamlessly integrate visual\nand linguistic data makes it a potent tool in the ever-evolving\nlandscape of robotic technology, with potential applications that extend\nfar beyond current capabilities. Interesting the authors show Voltron\ndoes not beat R3M off the shelf but only when trained on similar amounts\nof data. Nevertheless, Voltron's success in diverse tasks and\nenvironments heralds a new era in robotic manipulation, where language\nand vision coalesce to create more intelligent, adaptable, and capable\nrobotic systems.\n\n### Conclusion\n\nOn the note of applying active learning to RL and environment settings,\nthere have been many recent papers that have attempted to extend this to\nmore modern RL environments. For example, the paper \\\"When to Ask for\nHelp\\\" [@ask_help] examines the intersection of autonomous and active\nlearning. Instead of just expecting an RL agent to autonomously solve a\ntask, making the assumption that an agent could get stuck and need human\ninput to get \\\"unstuck\\\" is a key insight of the paper. In general,\nthere has been an emphasis in recent literature in robotics on not just\nblindly using demonstration data as a form of human input, but rather\nactively querying a human and using this to better synthesize correct\nactions.\n\nActive learning holds promise for enhancing AI models in real-world\nscenarios, yet several challenges persist. This discussion aims to\nprovide an overview of these challenges.\n\n**Task-Specific Considerations:**\n\nFor certain tasks, the input space of a model may have some rare yet\nextremely important pockets which may never be discovered by active\nlearning and may cause severe blindspots in the model. In medical\nimaging for instance, there can be rare yet critical diseases. Designing\nAL strategies for medical image analysis must prioritize rare classes,\nsuch as various forms of cancers. Oftentimes, collecting data around\nthose rare classes is not a recommendation of the active learning\nprocess because these examples constitute heavy distribution drifts from\nthe input distribution a model has seen.\n\n**Complex Task Adaptation:**\n\nAL has predominantly been adopted for simple classification tasks,\nleaving more other types of tasks (generative ones for instance), less\nexplored. In Natural Language Processing, tasks like natural language\ninference, question-answering pose additional complexities that affect\nthe direct application of the active learning process. While machine\ntranslation has seen AL applications, generation tasks in NLP require\nmore thorough exploration. Challenges arise in obtaining unlabeled data,\nparticularly for tasks with intricate inputs.\n\n**Unsupervised and Semi-Supervised Approaches:**\n\nIn the presence of large datasets without sufficient labels,\nunsupervised and semi-supervised approaches become crucial. These\nmethods offer a means to extract information without relying on labeled\ndata for every data point, potentially revolutionizing fields like\nmedical image analysis. There is an ongoing need for methods that\ncombine self/semi-supervised learning with active learning.\n\n**Algorithm Scalability:**\n\nScalability is a critical concern for online AL algorithms, particularly\nwhen dealing with large datasets and high-velocity data streams. The\ncomputational demands of AL can become prohibitive as data volume\nincreases, posing challenges for practical deployment. Issues of\ncatastrophic forgetting and model plasticity further complicate\nscalability, requiring careful consideration in algorithm design.\n\n**Labeling Quality Assurance:**\n\nThe effectiveness of most online AL strategies hinges on the quality of\nlabeled data. Ensuring labeling accuracy in real-world scenarios is\nchallenging, with human annotators prone to errors, biases, and diverse\ninterpretations. Addressing imperfections in labeling through\nconsiderations of oracle imperfections becomes essential in real-life AL\napplications. Solutions for cleaning up data and verifying its quality\nneed to be more aggressively pursued.\n\n**Data Drift Challenges:**\n\nReal-world settings introduce data drift, where distributions shift over\ntime, challenging models to adapt for accurate predictions. These shifts\ncan impact the quality of labeled data acquired in the AL process. For\nexample, the criterion or proxy used for selecting informative instances\nmay be thrown off when the distribution a model is trained on, and the\ndistribution we want it to perform well on, are too far away from one\nanother.\n\n**Evaluation in Real-Life Scenarios**:\n\nWhile AL methods are often evaluated assuming access to ground-truth\nlabels, the real motivation for AL lies in label scarcity. Assessing the\neffectiveness of AL strategies becomes challenging in real-life\nscenarios where ground-truth labels may be limited. In other words, one\nmay verify the goodness of an AL algorithm within the lab, but once the\nalgorithm is deployed for improving all sorts of models on all sorts of\ndata distributions, verifying whether AL is actually improving a model\nis tricky, especially when collecting and labeling data from the target\ndistribution is expensive and defeats the purpose of using AL in the\nfirst place.\n\nBy systematically addressing these challenges, the field of active\nlearning in AI can progress towards more effective and practical\napplications.\n\nIn summary, active learning is a promising modern tool to model training\nthat presents potential benefits. As was mentioned at the start, there\nare numerous approaches that can be employed by active learning,\nstarting from reducing error of model's prediction, reducing variance,\nto more conformal predictions. The flavor of active learning heavily\ndepends on the applications, which include robotics, LLM, autonomous\nvehicles, and more. We discussed in more detail how to perform active\nlearning for variance reduction in the case of predicting kinematics of\nthe robotic arms, which showed decrease in MSE as well as more stable\nreduction in it. Next we talked about using active learning for reducing\nthe number of comparisons required to create a ranking of objects, and\nthe examples discussed were able to achieve that but with some loss in\nthe prediction accuracy. Finally, we discussed how active learning can\nbe used for modeling of reward functions within a dynamical system,\nwhich demonstrated improvements in performance and time required to\nachieve it. For a more hands-on experience with active learning and\ndemonstrated example, we encourage the readers to explore a blogpost by\nMax Halford [@max_halford].\n\n\n## Metric Elicitation {#sec-metric-elicitation}\n\n### Introduction to Performance Metric Elicitation\n\nIn binary classification problems, selecting an appropriate performance metric that aligns with the real-world \ntask is crucial. The problem of *metric elicitation* aims to characterize and discover the performance metric of a \npractitioner, reflecting the rewards or costs associated with correct or incorrect classification. For instance, \nin medical contexts such as diagnosing a disease or determining the appropriateness of a treatment, trade-offs are \nmade for incorrect decisions. Not administering a treatment could lead to the worsening of a disease (a false negative), \nwhereas delivering the wrong treatment could cause adverse side effects worse than not treating the condition (a false positive).\n\nRather than choosing from a limited set of default choices like the F1-score or weighted accuracy, metric elicitation \nconsiders the process of devising a metric that best matches the preferences of practitioners or users. This is \nachieved by querying an \"oracle\" who provides feedback on proposed potential metrics through pairwise comparisons. \nSince queries to humans are often expensive, the goal is to minimize the number of comparisons needed.\n\n**Note:** Contents in this section are derived from \"Performance Metric Elicitation from Pairwise \nClassifier Comparisons\" by [@pmlr-v89-hiranandani19a], which introduced the problem of metric \nelicitation and the framework for binary-class metric elicitation from pairwise comparisons. This section aims to \npresent their work expository while providing additional motivation and intuitive explanations to supplement their work.\n\nThe motivation for the pairwise comparison aspect of metric elicitation stems from a rich history of literature in \npsychology, economics, and computer science [@pref1; @pref2; @pref3; @pref4; @ab], demonstrating that humans are often \nineffective at providing absolute feedback on aspects such as potential prices, user interfaces, or even ML model \noutputs (hence the comparison-based structure of RLHF, for instance). Additionally, confusion matrices accurately \ncapture binary metrics such as accuracy, $F_\\beta$, and Jaccard similarity by recording the number of false positives, \ntrue positives, false negatives, and true negatives obtained by a classifier. The main goal of this chapter is to \nintroduce two binary-search procedures that can approximate the oracle's performance metric for two types of metrics \n(linear and linear-fractional performance metrics) by presenting the oracle with confusion matrices generated by \nvarious classifiers. Essentially, we are learning an optimal threshold for classification given a decision boundary \nfor a binary classification problem.\n\nFirst, we introduce some relevant notation that will later be used to formalize notions of oracle queries, \nclassifiers, and metrics. In this context, $X \\in \\mathcal{X}$ represents an input random variable, while \n$Y \\in \\{0, 1\\}$ denotes the output random variable. We learn from a dataset of size $n$, denoted by \n$\\{(x, y)_i\\}^n_{i=1}$, which is generated independently and identically distributed (i.i.d.) from some \ndistribution $\\mathbb{P}(X, Y)$. The conditional probability of the positive class, given some sample $x$, \nis denoted by $\\eta(\\vec{x}) = \\mathbb{P}(Y=1 | X=x)$. The marginal probability of the positive class \nis represented by $\\zeta = \\mathbb{P}(Y=1)$. \n\nThe set of all potential classifiers is $\\mathcal{H} = \\{h : \\mathcal{X} \\rightarrow \\{0,1\\}\\}$. The confusion \nmatrix for a classifier $h$ is $C(h, \\mathbb{P}) \\in \\mathbb{R}^{2 \\times 2}$, where $C_{ij}(h, \\mathbb{P}) = \\mathbb{P}(Y=i, h=j)$ \nfor $i, j \\in \\{0,1\\}$. These entries represent the false positives, true positives, false negatives, and true negatives, \nensuring that $\\sum_{i,j}C_{ij}=1$. The set of all confusion matrices is denoted by $\\mathcal{C}$. Since \n$FN(h, \\mathbb{P}) = \\zeta - TP(h, \\mathbb{P})$ and $FP(h, \\mathbb{P}) = 1 - \\zeta - TN(h, \\mathbb{P})$, $\\mathcal{C}$ \nis actually a 2-dimensional space, not a 4-dimensional space.\n\nAny hyperplane in the $(tp, tn)$ space is given by $\\ell := a \\cdot tp + b \\cdot tn = c$, where $a, b, c \\in \\mathbb{R}$. \nGiven a classifier $h$, we define a performance metric $\\phi : [0, 1]^{2 \\times 2} \\rightarrow \\mathbb{R}$. The value \n$\\phi(C(h))$, which represents the performance of a classifier with respect to a certain metric, is referred to as the \n*utility* of the classifier $h$. We assume, without loss of generality, that a higher value of $\\phi$ indicates a better \nperformance metric for $h$. Our focus is to recover some metric $\\phi$ using comparisons between confusion matrices $C(h)$, \ndetermined by classifiers $h$, which approximates the oracle's \"ground-truth\" metric $\\phi^*$.\n\nNext, we introduce two classes of performance metricsâ€”*Linear Performance Metrics (LPM)* and *Linear-Fractional \nPerformance Metrics (LFPM)*â€”for which we will present two elicitation algorithms. \n\nAn *LPM*, given constants $\\{a_{11}, a_{01}, a_{10}, a_{00}\\} \\in \\mathbb{R}^{4}$, is defined as:\n\n$$\\begin{aligned}\n\\phi(C) &= a_{11} TP + a_{01} FP + a_{10} FN + a_{00} TN\\\\\n&= m_{11} TP + m_{00} TN + m_{0},\n\\end{aligned}$$ {#eq-eqlpm}\n\nwhere $m_{11} = (a_{11} - a_{10})$, $m_{00} = (a_{00} - a_{01})$, and $m_{0} = a_{10} \\zeta + a_{01} (1 - \\zeta)$. \nThis reparametrization simplifies the metric by reducing dimensionality, making it more tractable for elicitation. \nOne example of an LPM is *weighted accuracy*, defined as $WA = w_1TP + w_2TN$, where adjusting $w_1$ and $w_2$ controls \nthe relative importance of different types of misclassification.\n\nAn *LFPM*, defined by constants $\\{a_{11}, a_{01}, a_{10}, a_{00}, b_{11}, b_{01}, b_{10}, b_{00}\\} \\in \\mathbb{R}^{8}$, is given by:\n\n$$\\begin{aligned}\n\\phi(C) &= \\frac{a_{11} TP + a_{01} FP + a_{10} FN + a_{00} TN}{b_{11} TP + b_{01} FP + b_{10} FN + b_{00} TN}\\\\\n&= \\frac{p_{11} TP + p_{00} TN + p_{0}}{q_{11} TP + q_{00} TN + q_{0}},\n\\end{aligned}$$ {#eq-eqlfpm}\n\nwhere $p_{11} = (a_{11} - a_{10})$, $p_{00} = (a_{00} - a_{01})$, $q_{11} = (b_{11} - b_{10})$, $q_{00} = (b_{00} - b_{01})$, \n$p_{0} = a_{10} \\zeta + a_{01} (1 - \\zeta)$, and $q_{0} = b_{10} \\zeta + b_{01} (1 - \\zeta)$. This parametrization also \nsimplifies the elicitation process by reducing the number of variables. Common LFPMs include the $F_\\beta$ score and Jaccard \nsimilarity, defined as:\n\n$$F_{\\beta} = \\frac{TP}{\\frac{TP}{1+\\beta^{2}} - \\frac{TN}{1+\\beta^{2}} + \\frac{\\beta^{2} \\zeta + 1 - \\zeta}{1+\\beta^{2}}}, \\quad JAC = \\frac{TP}{1 - TN}.$$ {#eq-lfpm_metrics}\n\nSetting $\\beta = 1$ gives the F1 score, which is widely used as a classification metric in machine learning.\n\n### Preliminaries {#sec-me-preliminaries}\n#### Confusion Matrices {#sec-confusion-matrices .unnumbered}\n\nSince we are considering all possible metrics in the LPM and LFPM families, we need to make certain assumptions \nabout $\\mathcal{C}$. Particularly, we will assume that $g(t) = \\mathbb{P}[\\eta(X) \\geq t]$ is continuous and \nstrictly decreasing for $t \\in [0, 1]$; essentially, $\\eta$ has positive density and zero probability.\n\nAdditionally, $\\mathcal{C}$ is convex, closed, and contained within the rectangle $[0, \\zeta] \\times [0, 1-\\zeta]$, \nand is rotationally symmetric around its center, $(\\frac{\\zeta}{2}, \\frac{1-\\zeta}{2})$, where the axes represent the \nproportion of true positives and negatives. The only vertices of $\\mathcal{C}$ are $(0, 1-\\zeta)$ and $(\\zeta, 0)$, \ncorresponding to predicting all $0$'s or all $1$'s on a given dataset. Therefore, $\\mathcal{C}$ is strictly convex, \nand any line tangent to it is tangent at exactly one point, corresponding to one particular confusion matrix; these \nproperties can be visually observed in @fig-c.\n\n![Visual representation of $\\mathcal{C}$](Figures/Screenshot 2023-11-13 at 6.56.44 PM.png){#fig-c width=\"50%\"}\n\nNext, recall that an LPM is represented in terms of three parameters ($\\phi = m_{11}TP + m_{00}TN + m_0$). We have \njust seen that this LPM and its corresponding confusion matrix correspond to a certain point on the boundary of \n$\\mathcal{C}$. We first note that this point is independent of $m_0$. Additionally, we only care about the relative \nweightings of $m_{11}$ and $m_{00}$, not their actual valuesâ€”they are scale invariant. Therefore, we can parametrize \nthe space of LPMs as $\\varphi_{LPM} = \\{\\mathbf{m} = (\\cos \\theta, \\sin \\theta) : \\theta \\in [0, 2\\pi]\\}$, where \n$\\cos \\theta$ corresponds to $m_{00}$ and $\\sin \\theta$ corresponds to $m_{11}$. As we already know, we can recover \nthe Bayes classifier given $\\mathbf{m}$, and it is unique, corresponding to one point on the boundary of $\\mathcal{C}$ \ndue to its convexity. The supporting hyperplane at this point is defined as\n\n$$\\bar{\\ell}_{\\mathbf{m}} := m_{11} \\cdot tp + m_{00} \\cdot tn = m_{11} \\overline{TP}_{\\mathbf{m}} + m_{00} \\overline{TN}_{\\mathbf{m}}$$  {#eq-eq3.47}\n\nWe note that if $m_{00}$ and $m_{11}$ have opposite signs, then $\\bar{h}_m$ is the trivial classifier predicting \nall 1's or all 0's, since either predicting true positives or true negatives results in negative reward. This corresponds \nto a supporting hyperplane with a positive slope, so it can only be tangent at the vertices.\n\nAdditionally, the boundary $\\partial \\mathcal{C}$ can be split into upper and lower boundaries \n($\\partial \\mathcal{C}_{+}, \\partial \\mathcal{C}_{-}$), corresponding to $\\theta \\in (0, \\pi/2)$ and $\\theta \\in (\\pi, 3\\pi/2)$ \nrespectively (and whether $m_{00}, m_{11}$ are positive or negative).\n\n#### Bayes Optimal and Inverse-Optimal Classifiers {.unnumbered}\n\nWe also define the notions of Bayes optimal and inverse-optimal classifiers. Given a performance metric $\\phi$, we define:\n\n-   The *Bayes utility* as $\\bar{\\tau} := \\sup_{h \\in \\mathcal{H}} \\phi(C(h)) = \\sup_{C \\in \\mathcal{C}} \\phi(C)$; \n    this is the highest achievable utility (using the metric $\\phi$) over all classifiers $h \\in $\\mathcal{H}$ for a given problem.\n-   The *Bayes classifier* as $\\bar{h} := \\arg \\max_{h \\in \\mathcal{H}} \\phi(C(h))$; this is the classifier $h$ corresponding to the Bayes utility.\n-   The *Bayes confusion matrix* as $\\bar{C} := \\arg \\max_{C \\in \\mathcal{C}} \\phi(C)$; this is the confusion matrix \n    corresponding to the Bayes utility and classifier.\n\nSimilarly, the inverse Bayes utility, classifier, and confusion matrix can be defined by replacing \"$\\sup$\" with \"$\\inf$\"; \nthey represent the classifier and confusion matrix corresponding to the lower bound on utility for a given problem.\n\nWe also have the following useful proposition:\n\n::: {.callout-note title=\"proposition\"}\n::: {#prp-prp3.1}\nLet $\\phi \\in \\varphi_{LPM}$. Then\n\n::: {.content-visible when-format=\"html\"}\n$$\\bar{h}(x) = \\left\\{\\begin{array}{lr}\n\\unicode{x1D7D9} \\left[\\eta(x) \\geq \\frac{m_{00}}{m_{11} + m_{00}}\\right], & m_{11} + m_{00} \\geq 0 \\\\\n\\unicode{x1D7D9} \\left[\\frac{m_{00}}{m_{11} + m_{00}} \\geq \\eta(x)\\right], & \\text { o.w. }\n\\end{array}\\right\\}$$  {#eq-eq3.45}\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n$$\\bar{h}(x) = \\left\\{\\begin{array}{lr}\n\\mathbbm{1}\\left[\\eta(x) \\geq \\frac{m_{00}}{m_{11} + m_{00}}\\right], & m_{11} + m_{00} \\geq 0 \\\\\n\\mathbbm{1}\\left[\\frac{m_{00}}{m_{11} + m_{00}} \\geq \\eta(x)\\right], & \\text { o.w. }\n\\end{array}\\right\\}$$  {#eq-eq3.46}\n:::\n\nis a Bayes optimal classifier with respect to $\\phi$. The inverse Bayes classifier is given by $\\underline{h} = 1 - \\bar{h}$.\n:::\n:::\n\nThis is a simple derivation based on the fact that we only get rewards from true positives and true negatives. \nEssentially, if we recover an LPM, we can use it to determine the best-performing classifier, obtained by placing \na threshold on the conditional probability of a given sample, that corresponds to a confusion matrix. Therefore, \nthe three notions of Bayes utility, classifier, and confusion matrix are functionally equivalent in our setting.\n\n\n### Problem Setup {#sec-metric-elicitation-setup}\n\nWe will now formalize the problem of metric elicitation. Given two classifiers $h$ and $h'$ (or equivalently, \ntwo confusion matrices $C$ and $C'$), we define an *oracle query* as the function:\n\n::: {.content-visible when-format=\"html\"}\n$$\\Gamma\\left(h, h^{\\prime}\\right)=\\Omega\\left(C, C^{\\prime}\\right)=\\unicode{x1D7D9}\\left[\\phi(C)>\\phi\\left(C^{\\prime}\\right)\\right]=: \\unicode{x1D7D9} \\left[C \\succ C^{\\prime}\\right],$$ {#eq-oracle}\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n$$\\Gamma\\left(h, h^{\\prime}\\right)=\\Omega\\left(C, C^{\\prime}\\right)=\\mathbbm{1}\\left[\\phi(C)>\\phi\\left(C^{\\prime}\\right)\\right]=: \\mathbbm{1} \\left[C \\succ C^{\\prime}\\right],$$ {#eq-oracle}\n:::\n\nwhich represents the classifier preferred by the practitioner. We can then define the metric elicitation problem for populations:\n\n::: {.callout-note title=\"definition\"}\n::: {#def-def3.1}\nSuppose the true (oracle) performance metric is $\\phi$. The goal is to recover a metric $\\hat{\\phi}$ by \nquerying the oracle for as few pairwise comparisons of the form $\\Omega\\left(C, C^{\\prime}\\right)$ so that \n$\\|\\phi - \\hat{\\phi}\\|_{--} < \\kappa$ for a sufficiently small $\\kappa > 0$ and for any suitable norm $\\|\\cdot\\|_{--}$.\n:::\n:::\n\nIn practice, we do not have access to the true probability distribution or the population, which would \nprovide the true values of $C$ and $C'$. However, we can subtly alter this problem description to use \n$\\hat{C}$ and $\\hat{C}^{\\prime}$, which are derived from our dataset of $n$ samples:\n\n::: {.callout-note title=\"definition\"}\n::: {#def-def3.2}\nSuppose the true (oracle) performance metric is $\\phi$. The aim is to recover a metric $\\hat{\\phi}$ \nby querying the oracle for as few pairwise comparisons of the form $\\Omega\\left(\\hat{C}, \\hat{C}^{\\prime}\\right)$ \nso that $\\|\\phi - \\hat{\\phi}\\|_{--} < \\kappa$ for a sufficiently small $\\kappa > 0$ and for any suitable norm $\\|\\cdot\\|_{--}$.\n:::\n:::\n\nAs is common in theoretical ML research, we solve the population problem and then consider ways to extend this to \npractical settings where we only have limited datasets of samples. In our case, this corresponds to calculating the \nconfusion matrices from a portion of the dataset we have access to.\n\n\n### Linear Performance Metric Elicitation {#sec-orgb6dac4e}\n\nFor LPM elicitation, we need one more proposition.\n\n::: {.callout-note title=\"proposition\"}\n::: {#prp-prp3.2}\nFor a metric $\\psi$ (quasiconvex and monotone increasing in TP/TN) or\n$\\phi$ (quasiconcave and monotone increasing), and parametrization\n$\\rho^+$/$\\rho^-$ of upper/lower boundary, composition\n$\\psi \\circ \\rho^-$ is quasiconvex and unimodal on \\[0, 1\\], and\n$\\phi \\circ \\rho^+$ is quasiconcave and unimodal on \\[0, 1\\].\n:::\n:::\n\nQuasiconcavity and quasiconvexity are slightly more general variations\non concavity and convexity. Their main useful property in our setting is\nthat they are unimodal (they have a singular extremum), so we can devise\na binary-search-style algorithm for eliciting the Bayes optimal and\ninverse-optimal confusion matrices for a given setting, as well as the\ncorresponding $\\phi$'s.\n\nWe first note that to maximize a quasiconcave metric, in which $\\phi$ is\nmonotonically increasing in $TP$ and $TN$, we note that the resulting\nmaximizer (and supporting hyperplane) will occur on the upper boundary\nof $\\mathcal{C}$. We thus set our initial search range to be\n$[0, \\pi/2]$ and repeatedly divide it into four regions. Then, we\ncalculate the resulting confusion matrix on the 5 resulting boundaries of these\nregions and query the oracle $4$ times. We repeat this in each iteration\nof the binary search until a maximizer is found.\n\n::: {.callout-note title=\"remark\"}\n::: {#rem-explaination_binary_search}\nIn the case of quasiconcave and quasiconvex search ranges, a slightly more sophisticated variation on typical \nbinary search must be used. To illustrate this, consider the two distributions in @fig-bsearch:\n\n::: {#fig-bsearch layout-ncol=2}\n![First distribution](Figures/normaldistribution.png){#fig-normal-distribution width=\"45%\"}\n![Second distribution](Figures/normaldistribution copy.png){#fig-normal-distribution-copy width=\"45%\"}\n:::\n\nFor both the symmetric and skewed distributions, if we were to divide the search range into two portions \nand compare $A$, $C$, and $E$, we would find that $C > A$ and $C > E$. In both cases, this does not help \nus reduce our search range, since the true maximum could lie on either of the two intervals (as in the second case), \nor at $C$ itself (as in the first case). Therefore, we must make comparisons between all five points $A, B, C, D, and E$. \nThis allows us to correctly restrict our search range to $[B, D]$ in the first case and $[C, E]$ in the second. \nThese extra search requirements are due to the quasiconcavity of the search space we are considering, in which \nthere exists a maximum but we need to make several comparisons at various points throughout the search space \nto be able to reduce its size in each iteration.\n:::\n:::\n\n```pseudocode\n#| label: alg-lpm\n\\begin{algorithm}\n    \\caption{Quasiconcave Metric Maximization}\n    \\begin{algorithmic}\n        \\State \\textbf{input:} $\\epsilon > 0$ and oracle $\\Omega$\n        \\State \\textbf{initialize:} $\\theta_a = 0, \\theta_b = \\frac{\\pi}{2}$\n        \\While{$|\\theta_b - \\theta_a| > \\epsilon$}\n            \\State set $\\theta_c = \\frac{3\\theta_a+\\theta_b}{4}$, $\\theta_d = \\frac{\\theta_a+\\theta_b}{2}$, and $\\theta_e = \\frac{\\theta_a+3\\theta_b}{4}$\n            \n            \\State obtain $h\\theta_a, h\\theta_c, h\\theta_d, h\\theta_e, h\\theta_b$ using Proposition 1\n            \n            \\State Compute $C\\theta_a, C\\theta_c, C\\theta_d, C\\theta_e, C\\theta_b$ using (1)\n            \n            \\State Query $\\Omega(C\\theta_c, C\\theta_a), \\Omega(C\\theta_d, C\\theta_c), \\Omega(C\\theta_e, C\\theta_d)$, and $\\Omega(C\\theta_b, C\\theta_e)$\n\n            \\If{$q_{i,j}$ is ambiguous}\n                \\State request $q_{i,j}$'s label from reference\n            \\Else\n                \\State impute $q_{i,j}$'s label from previously labeled queries\n            \\EndIf\n            \n            \\If{$C\\theta' \\succ C\\theta'' \\succ C\\theta'''$ for consecutive $\\theta < \\theta' < \\theta''$}\n                \\State assume the default order $C\\theta \\prec C\\theta' \\prec C\\theta''$\n            \\EndIf\n\n            \\If{$C\\theta' \\succ C\\theta'' \\succ C\\theta'''$ for consecutive $\\theta < \\theta' < \\theta''$}\n                \\State assume the default order $C\\theta \\prec C\\theta' \\prec C\\theta''$\n            \\EndIf\n            \n            \\If{$C\\theta_a \\succ C\\theta_c$} \n                \\State Set $\\theta_b = \\theta_d$ \n            \\ElsIf{$C\\theta_a \\prec C\\theta_c \\succ C\\theta_d$} \n                \\State Set $\\theta_b = \\theta_d$ \n            \\ElsIf{$C\\theta_c \\prec C\\theta_d \\succ C\\theta_e$} \n                \\State Set $\\theta_a = \\theta_c$ \n                \\State Set $\\theta_b = \\theta_e$ \n            \\ElsIf{$C\\theta_d \\prec C\\theta_e \\succ C\\theta_b$} \n                \\State Set $\\theta_a = \\theta_d$ \n            \\Else \n                \\State Set $\\theta_a = \\theta_d$ \n            \\EndIf\n        \\EndWhile\n        \\State \\textbf{output:} $\\vec{m}, C$, and $\\vec{l}$, where $\\vec{m} = m_l(\\theta_d), C = C\\theta_d$, and $\\vec{l} := (\\vec{m}, (tp, tn)) = (\\vec{m}, C)$\n    \\end{algorithmic}\n\\end{algorithm}\n```\n\nTo elicit LPMs, we run @alg-lpm, querying the oracle in each iteration, and set the\nelicited metric $\\hat{m}$ (which is the maximizer on $\\mathcal{C}$) to\nbe the slope of the resulting hyperplane, since the metric is linear.\n\n::: {.callout-note title=\"remark\"}\n::: {#rem-explaination_lpm}\nTo find the minimum of a quasiconvex metric, we flip all instances of\n$\\prec$ and $\\succ$, and use an initial search range of $[\\pi, 3\\pi/2]$;\nwe use this algorithm, which we refer to as @alg-lfpm, in our\nelicitation of LFPMs.\n:::\n:::\n\nNext, we provide a Python implementation of @alg-lpm.\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef get_m(theta):\n    \"\"\"\n    Inputs: \n    - theta: the value that parametrizes m\n    Outputs:\n    - m_0 and m_1 for the LPM\n    \"\"\"\n\n    return (math.cos(theta), math.sin(theta))\n\ndef lpm_elicitation(epsilon, oracle):\n    \"\"\"\n    Inputs:\n    - epsilon: some epsilon > 0 representing threshold of error\n    - oracle: some function that accepts 2 confusion matrices and\n        returns true if the first is preferred and false otherwise\n    Outputs:\n    - estimate for m, which is used to compute the LPM as described above\n    \"\"\"\n\n    a = 0\n    b = math.pi/2\n    while (b - a > epsilon):\n        c = (3 * a + b) / 4\n        d = (a + b) / 2\n        e = (a + 3 * b) / 4\n\n        m_a, m_b, m_c, m_d, m_e = (get_m(x) for x in [a,b,c,d,e]) # using definition of m\n        c_a, c_b, c_c, c_d, c_e = (get_c(x) for x in [m_a, m_b, m_c, m_d, m_e]) # compute classifier from m's then calculate confusion matrices\n        \n        response_ac = oracle(c_a, c_c)\n        response_cd = oracle(c_c, c_d)\n        response_de = oracle(c_d, c_e)\n        response_eb = oracle(c_e, c_b)\n\n        # update ranges to keep the peak\n        if response_ac:\n            b = d\n        elif response_cd:\n            b = d\n        elif response_de:\n            a = c\n            b = e\n        elif response_eb:\n            a = d\n        else:\n            a = d\n    return get_m(d), get_c(d)\n```\n:::\n\n\n:::\n\n### Linear-Fractional Performance Metric Elicitation {#sec-lfpm-elicitation}\n\nNow, we present the next main result, which is an algorithm to elicit\nlinear-fractional performance metrics. For this task, we will need the\nfollowing assumption:\n\nLet $\\phi \\in \\varphi_{L F P M}$. We assume\n$p_{11}, p_{00} \\geq 0, p_{11} \\geq q_{11}, p_{00} \\geq q_{00},$\n$p_{0}=0, q_{0}=$\n$\\left(p_{11}-q_{11}\\right) \\zeta+\\left(p_{00}-q_{00}\\right)(1-\\zeta)$,\nand $p_{11}+p_{00}=1$.\n\nThese assumptions guarantee that the LFPM $\\phi$ which we are trying to\nelicit is monotonically increasing in $TP$ and $TN$, just as in the LPM\nelicitation case.\n\nWe first provide motivation and an overview of the approach for LFPM\nelicitation and then present pseudocode for the algorithm.\n\nThe general idea of the algorithm is to use @alg-lpm to obtain a\nmaximizer and a minimizer for the given dataset; these result in two\nsystems of equations involving the true LFPM $\\phi^*$ with 1 degree of\nfreedom. Then, we run a grid search that is independent of oracle\nqueries to find the point where solutions to the systems match pointwise\non the resulting confusion matrices; this occurs close to where the true\nmetric lies.\n\nMore formally, suppose that the true metric is\n$$\\phi^{*}(C)=\\frac{p_{11}^{*} T P+p_{00}^{*} T N}{q_{11}^{*} T P+q_{00}^{*} T N+q_{0}^{*}}.$$  {#eq-eq3.48}\nThen, let $\\bar{\\tau}$ and $\\underline{\\tau}$ represent the maximizer\nand minimizer of $\\phi$ over $\\mathcal{C}$, respectively. There exists a\nhyperplane \n$$\\begin{aligned}\n\\bar{\\ell}_{f}^{*}:=\\left(p_{11}^{*}-\\bar{\\tau}^{*} q_{11}^{*}\\right) t p+\\left(p_{00}^{*}-\\bar{\\tau}^{*} q_{00}^{*}\\right) t n=\\bar{\\tau}^{*} q_{0}^{*},\n\\end{aligned}$$   {#eq-eq3.49}\nwhich touches $\\mathcal{C}$ at $\\left(\\overline{T P}^{*}, \\overline{T N}^{*}\\right)$ on\n$\\partial \\mathcal{C}_{+}$.\n\nCorrespondingly, there also exists a hyperplane \n$$\\begin{aligned}\n\\underline{\\ell}_{f}^{*}:=\\left(p_{11}^{*}-\\underline{\\tau}^{*} q_{11}^{*}\\right) t p+\\left(p_{00}^{*}-\\underline{\\tau}^{*} q_{00}^{*}\\right) \\operatorname{tn}=\\underline{\\tau}^{*} q_{0}^{*},\n\\end{aligned}$$   {#eq-eq3.50}\nwhich touches $\\mathcal{C}$ at $\\left(\\underline{TP}^{*}, \\underline{T N}^{*}\\right)$ on\n$\\partial \\mathcal{C}_{-}$. @fig-minmax illustrates this visually on $\\mathcal{C}$.\n\n![Visual representation of the minimizer and maximizer on\n$\\mathcal{C}$](Figures/Screenshot 2023-11-13 at 6.56.52 PM.png){#fig-minmax\nwidth=\"50%\"}\n\nWhile we are unable to obtain @eq-eq3.48 and @eq-eq3.49 directly, we can use @alg-lpm to get a hyperplane\n$$\\bar{\\ell}:=\\bar{m}_{11} t p+\\bar{m}_{00} t n= \\bar{m}_{11} \\overline{T P}^{*}+\\bar{m}_{00} \\overline{T N}^{*} = \\bar{C}_{0},$$  {#eq-eq3.51}\nwhich is equivalent to $\\bar{\\ell}_{f}^{*}$ (@eq-eq3.48) up to a constant\nmultiple. From here, we can obtain the system of equations\n\n$$p_{11}^{*}-\\bar{\\tau}^{*} q_{11}^{*}=\\alpha \\bar{m}_{11}, p_{00}^{*}-\\bar{\\tau}^{*} q_{00}^{*}=\\alpha \\bar{m}_{00}, \\bar{\\tau}^{*} q_{0}^{*}=\\alpha \\bar{C}_{0},$$  {#eq-eq3.52}\nwhere $\\alpha > 0$ (we know it is $\\geq0$ due to our assumptions earlier\nand because $\\bar{m}$ is positive, but if it is equal to $0$ then\n$\\phi^*$ would be constant. So, our resulting system of equations is\n$$\\begin{aligned}\n    p_{11}^{\\prime}-\\bar{\\tau}^{*} q_{11}^{\\prime}=\\bar{m}_{11}, p_{00}^{\\prime}-\\bar{\\tau}^{*} q_{00}^{\\prime}=\\bar{m}_{00}, \\bar{\\tau}^{*} q_{0}^{\\prime}=\\bar{C}_{0}.\n\\end{aligned}$$  {#eq-eq3.53}\n\nNow, similarly, we can approximate @eq-eq3.49 using the algorithm we defined\nfor quasiconvex metrics (@alg-lfpm), where we altered the search range\nand comparisons. After finding the minimizer, we obtain the hyperplane\n$$\\underline{\\ell}:=\\underline{m}_{11} t p+\\underline{m}_{00} t n=\\underline{m}_{11} \\underline{TP}^{*}+\\underline{m}_{00} \\underline{TN}^{*} = \\underline{C}_{0},$$  {#eq-eq3.54}\nwhich is equivalent to $\\underline{\\ell}_{f}^{*}$ (@eq-eq3.49) up to a constant\nmultiple. So then, our system of equations is\n$$p_{11}^{*}-\\underline{\\tau}^{*} q_{11}^{*}=\\gamma \\underline{m}_{11}, p_{00}^{*}-\\underline{\\tau}^{*} q_{00}^{*}=\\gamma \\underline{m}_{00}, \\underline{\\tau}^{*} q_{0}^{*}=\\gamma \\underline{C}_{0},$$  {#eq-eq3.55}\nwhere $\\gamma <0$ (for a reason analogous to why we have $\\alpha >0$),\nmeaning our resulting system of equations is \n$$\\begin{aligned}\n    p_{11}^{\\prime \\prime}-\\underline{\\tau}^{*} q_{11}^{\\prime \\prime}=\\underline{m}_{11}, p_{00}^{\\prime \\prime}-\\underline{\\tau}^{*} q_{00}^{\\prime \\prime}=\\underline{m}_{00}, \\underline{\\tau}^{*} q_{0}^{\\prime \\prime}=\\underline{C}_{0}.\n\\end{aligned}$$  {#eq-eq3.56}\n\n@eq-eq3.55 and @eq-eq3.56 form the two systems of equations mentioned in our overview\nof the algorithm. Next, we demonstrate that they have only one degree of\nfreedom. Note that if we know $p_{11}'$, we could solve both systems of\nequations as follows: \n$$\\begin{aligned}\n    p_{00}^{\\prime}  &=1-p_{11}^{\\prime}, q_{0}^{\\prime}=\\bar{C}_{0} \\frac{P^{\\prime}}{Q^{\\prime}}\\\\\n    q_{11}^{\\prime}  &=\\left(p_{11}^{\\prime}-\\bar{m}_{11}\\right) \\frac{P^{\\prime}}{Q^{\\prime}} \\\\\n    q_{00}^{\\prime}&=\\left(p_{00}^{\\prime}-\\bar{m}_{00}\\right) \\frac{P^{\\prime}}{Q^{\\prime}},\n\\end{aligned}$$   {#eq-eq3.57}\nwhere\n$P^{\\prime}=p_{11}^{\\prime} \\zeta+p_{00}^{\\prime}(1-\\zeta)$ and\n$Q^{\\prime}=P^{\\prime}+\\bar{C}_{0}-$\n$\\bar{m}_{11} \\zeta-\\bar{m}_{00}(1-\\zeta).$\n\nNow, suppose we know $p_{11}'$. We could use this value to solve both systems @eq-eq3.55 and @eq-eq3.56, \nyielding two metrics, $\\phi'$ and $\\phi''$, from the maximizer and minimizer, respectively. Importantly, when\n$$p_{11}^{*} / p_{00}^{*}=p_{11}^{\\prime} / p_{00}^{\\prime}=p_{11}^{\\prime \\prime} / p_{00}^{\\prime \\prime},$$ {#eq-eq3.58} \nthen\n$\\phi^{*}(C)=\\phi^{\\prime}(C) / \\alpha=-\\phi^{\\prime \\prime}(C) / \\gamma$.\nEssentially, when we find a value of $p_{11}'$ that results in $\\phi'$ and $\\phi''$ h\naving constant ratios at all points on the boundary of $\\mathcal{C}$, we can obtain $\\phi^*$, \nas it is derivable from $\\phi'$ and $\\alpha$ (or, alternatively, $\\phi''$ and $\\gamma$).\n\nWe will perform a grid search for $p_{11}'$ on $[0,1]$. For each point in our search, \nwe will compute $\\phi'$ and $\\phi''$. Then, we will generate several confusion matrices \non the boundaries and calculate the ratio $\\phi'' / $\\phi'$ for each. We will select the value \nof $p_{11}'$ for which the ratio $\\phi'' / \\phi'$ is closest to constant and use it to compute \nthe elicited metric $\\hat{\\phi}$. The pseudocode for LFPM elicitation is given in @alg-lfpm.\n\n```pseudocode\n#| label: alg-lfpm\n\\begin{algorithm}\n    \\caption{Grid Search for Best Ratio}\n    \\begin{algorithmic}\n        \\State \\textbf{Input:} $k, \\Delta$.\n        \\State \\textbf{Initialize:} $\\sigma_{\\text{opt}} = \\infty, p'_{11,\\text{opt}} = 0$.\n        \\State Generate $C_1, \\dots, C_k$ on $\\partial C_+$ and $\\partial C_-$ (Section 3).\n        \\State Generate $C_1, \\dots, C_k$ on $\\partial C_+$ and $\\partial C_-$ (Section 3).\n        \\For{$p'_{11} = 0; \\; p'_{11} \\leq 1; \\; p'_{11} = p'_{11} + \\Delta$}\n            \\State Compute $\\phi'$, $\\phi''$ using Proposition 4. \n            \\State Compute array $r = \\left[ \\frac{\\phi'(C_1)}{\\phi''(C_1)}, \\dots, \\frac{\\phi'(C_k)}{\\phi''(C_k)} \\right]$.\n            \\State Set $\\sigma = \\text{std}(r)$.\n            \\If{$\\sigma < \\sigma_{\\text{opt}}$}\n                \\State Set $\\sigma_{\\text{opt}} = \\sigma$ and $p'_{11,\\text{opt}} = p'_{11}$.\n            \\EndIf\n        \\EndFor\n        \\State \\textbf{Output:} $p'_{11,\\text{opt}}$.\n    \\end{algorithmic}\n\\end{algorithm}\n```\n\nWe provide a Python implementation as below.\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef lfpm_elicitation(k, delta):\n    \"\"\"\n    Inputs:\n    - k: the number of confusion matrices to evaluate on\n    - delta: the spacing for the grid search\n    Outputs:\n    - p_11', which will allow us to compute the elicited LFPM\n    \"\"\"\n\n    sigma_opt = np.inf\n    p11_opt = 0\n    C = compute_confusion_matrices(k) # generates k confusion matrices to evaluate on\n\n    for i in range(int(1/delta)):\n        p11 = i * delta\n        phi1 = compute_upper_metric(p11) # solves the first system of equations with p11 \n        phi2 = compute_lower_metric(p11) # solves the second system of equations with p11 \n        utility_1 = [phi1(c) for c in C] #calculate phi for both systems of equations\n        utility_2 = [phi2(c) for c in C]\n\n        r = []\n        for i in range(k):\n            r.append(utility_1[i] / utility_2[i])\n        sigma = np.std(r)\n\n        if(sigma < sigma_opt):\n            sigma_opt = sigma\n            p11_opt = p11\n    return p11_opt\n```\n:::\n\n\n:::\n\nIn summary, to elicit LFPMs, we utilize a special property of the LPM\nminimizer and maximizer on $\\mathcal{C}$--namely, that we can use the\ncorresponding supporting hyperplanes to form a system of equations that\ncan be used to approximate $\\phi^*$ if one parameter ($p_{11}'$) is\nfound, and that this parameter can be found using an oracle-independent\ngrid search.\n\n#### Guarantees {.unnumbered}\n\nImportantly, these algorithms can be shown to satisfy significant theoretical guarantees. We provide \n formal statement and intuitive interpretation of these guarantees here, with their proofs available \n in the appendix of the original paper.\n\nFirst, we define the oracle noise $\\epsilon_{\\Omega}$, which arises from the oracle potentially flipping \nthe comparison output on two confusion matrices that are close enough in utility.\n\n::: {.callout-note title=\"theorem\"}\n::: {#thm-thm1}\nGiven $\\epsilon, \\epsilon_{\\Omega} \\geq 0$ and a metric $\\phi$ satisfying our assumptions, @alg-lpm or \n@alg-lfpm finds an approximate maximizer/minimizer and supporting hyperplane. Additionally, the value of \n$\\phi$ at that point is within $O\\left(\\sqrt{\\epsilon_{\\Omega}} + \\epsilon\\right)$ of the optimum, and the \nnumber of queries is $O\\left(\\log \\frac{1}{\\epsilon}\\right)$.\n:::\n:::\n\n::: {.callout-note title=\"theorem\"}\n::: {#thm-thm2}\nLet $\\mathbf{m}^{*}$ be the true performance metric. Given $\\epsilon > 0$, LPM elicitation outputs a performance \nmetric $\\hat{\\mathbf{m}}$, such that $\\left\\|\\mathbf{m}^{*} - \\hat{\\mathbf{m}}\\right\\|_{\\infty} \\leq \\sqrt{2} \\epsilon + \\frac{2}{k_{0}} \\sqrt{2 k_{1} \\epsilon_{\\Omega}}$.\n:::\n:::\n\nThese two theorems ensure that @alg-lpm and @alg-lfpm find an appropriate maximizer and minimizer in the search \nspace, within a certain range of accuracy that depends on oracle and sample noise, and within a certain number of \nqueries. Both of these statements are guaranteed by the binary search approach.\n\n::: {.callout-note title=\"theorem\"}\n::: {#thm-thm3}\nLet $h_{\\theta}$ and $\\hat{h}_{\\theta}$ be two classifiers estimated using $\\eta$ and $\\hat{\\eta}$, respectively. \nFurther, let $\\bar{\\theta}$ be such that $h_{\\bar{\\theta}} = \\arg \\max _{\\theta} \\phi\\left(h_{\\theta}\\right)$. \nThen $\\|C(\\hat{h}_{\\bar{\\theta}}) - C\\left(h_{\\bar{\\theta}}\\right)\\|_{\\infty} = O\\left(\\left\\|\\hat{\\eta}_{n} - \\eta\\right\\|_{\\infty}\\right)$.\n:::\n:::\n\nThis theorem indicates that the drop in elicited metric quality caused by using a dataset of samples rather than \npopulation confusion matrices is bounded by the drop in performance of the decision boundary $\\eta$. These three \nguarantees together ensure that oracle noise and sample noise do not amplify drops in performance when using metric \nelicitation; rather, these drops in performance are bounded by the drops that would typically occur when using the \nstandard machine learning paradigm of training a decision boundary and using a pre-established metric.\n\nFor further interesting exploration of the types of problems that can be\nsolved using the framework of metric elicitation, we refer the reader to\n[@nips], which performs metric elicitation to determine the oracle's\nideal tradeoff between the classifier's overall performance and the\ndiscrepancy between its performance on certain protected groups.\n\n### Multiclass Performance Metric Elicitation\n\nAlthough the previous section only described metric elicitation for\nbinary classification problems, the general framework can still be\napplied to multiclass classification problems, as described in\n\"Multiclass Performance Metric Elicitation\" by [@NEURIPS2019_1fd09c5f].\n\nConsider the case of classifying subtypes of\nleukemia [@YangNaiman+2014+477+496]. We can train a neural network to\npredict conditional probability of a certain leukemia subtype given\ncertain gene expressions. However, it may not be appropriate to classify\nthe subtype purely based on whichever one has the highest confidence.\nFor instance, a treatment for leukemia subtype C1 may be perfect for\ncases of C1, but it may be ineffective or harmful for certain other\nsubtypes. Therefore, the final response from the classifier may not be\nas simple as as choosing the class with the highest conditional\nprobability, just like how the threshold for binary classification may\nnot always be 50%.\n\nWith multiclass metric elicitation, we can show confusion matrices to an\noracle (like the doctor in the leukemia example) to determine which\nclassifier has the best tradeoffs. In [@NEURIPS2019_1fd09c5f], the\nauthors focus on eliciting linear performance metrics, which is what we\nwill describe in this chapter.\n\n#### Preliminaries {.unnumbered}\n\nMost of the notation from Binary Metric Elicitation still persists, just\nmodified to provide categorical responses:\n\n-   $X \\in \\mathcal{X}$ is the input random variable.\n\n-   $Y \\in [k]$ is the output random variable, where $[k]$ is the index\n    set $\\{1, 2, \\dots, k\\}$.\n\n-   The dataset of size $n$ is denoted by $\\{(\\vec{x}, y)\\}_{i=1}^n$\n    generated independently and identically from $\\mathbb{P}(X, Y)$.\n\n-   $\\eta_i(\\vec{x}) = \\mathbb{P}(Y=i | X=\\vec{x})$ gives the\n    conditional probability of class $i \\in [k]$ given an observation.\n\n-   $\\xi_i = \\mathbb{P}(Y=i)$ is the marginal probability of class\n    $i \\in [k]$.\n\n-   The set of all classifiers is\n    $\\mathcal{H} = \\{h : \\mathcal{X} \\rightarrow \\Delta_k\\}$, where\n    $\\Delta_k$ is (k-1) dimensional simplex. In this case, the outputs\n    of classifiers are 1-hot vectors of size $k$ where the only index\n    with value 1 is the predicted class and all other positions have a\n    value of 0.\n\n-   The confusion matrix for a classifier, $h$, is\n    $C(h, \\mathbb{P}) \\in \\mathbb{R}^{k \\times k}$, where:\n    $$C_{ij}(h, \\mathbb{P}) = \\mathbb{P}(Y=i, h=j) \\text{\\qquad for } i, j \\in [k]$$  {#eq-eq3.59}\n\nNote that the confusion matrices are $k\\times k$ and store the joint\nprobabilities of each type of classification for each possible class.\nThis means that the sum of row $i$ in the confusion matrix equals\n$\\xi_i$, because this is equivalent to adding over all possible\nclassifications. Since we know the sums of each row, all diagonal\nelements can be reconstructed from just the off-diagonal elements, so a\nconfusion matrix $C(h, \\mathbb{P})$ can be expressed as a vector of\noff-diagonal elements,\n$\\vec{c}(h, \\mathbb{P}) = \\textit{off-diag}(C(h, \\mathbb{P}))$, and\n$\\vec{c} \\in \\mathbb{R}^q$ where $q := k^2 - k$. The vector $\\vec{c}$ is\ncalled the vector of *'off-diagonal confusions.'* The space of\noff-diagonal confusions is\n$\\mathcal{C} = \\{\\vec{c}(h, \\mathbb{P}) : h \\in \\mathcal{H}\\}$.\n\nIn cases where the oracle would care about the exact type of\nmisclassification (i.e. misclassifying and object from class 1 as class\n2), this off-diagonal confusion matrix is necessary. However, there are\nmany cases where the performance of a classifier is determined by just\nthe probability of correct prediction for each class, which just\nrequires the diagonal elements. In these cases, we can define the vector\nof *'diagonal confusions'* as\n$\\vec{d}(h, \\mathbb{P}) = \\textit{diag}(C(h, \\mathbb{P})) \\in \\mathbb{R}^k$.\nThe space of diagonal confusions is\n$\\mathcal{D} = \\{\\vec{d}(h, \\mathbb{P}) : h \\in \\mathcal{H}\\}$.\n\nFinally, the setup for metric elicitation is identical to the one\nexamined in the previous chapter. We still assume access to an oracle\nthat can choose between two classifiers or confusion matrices, using\nnotation $\\Gamma$ for comparing two classifiers and $\\Omega$ for\ncomparing confusion matrices, which returns 1 if the first classifier is\nbetter and 0 otherwise. We still assume that the oracle behaves\naccording to some unknown performance metric, and we wish to recover\nthis metric up to some small error tolerance (based on a suitable norm).\n\nThe two different types of confusion vectors result in different\nalgorithms for metric elicitation, which we will explore in later\nsections.\n\n#### Introduction to Diagonal Linear Performance Metric Elicitation {.unnumbered}\n\nA Diagonal Linear Performance Metric (DLPM) is a performance metric that\nonly considers the diagonal elements in the confusion matrix. The metric\nis defined as $\\psi(\\vec{d}) = \\langle \\vec{a}, \\vec{d} \\rangle$, where\n$\\vec{a} \\in \\mathbb{R}^k$ such that $||\\vec{a}||_1 = 1$. It is also\ncalled weighted accuracy [@pmlr-v37-narasimhanb15].\n\nThe family of DLPMs is denoted as $\\varphi_{DLPM}$. Since these only\nconsider the diagonal elements, which we want to maximize, we can focus\non only eliciting monotonically increasing DLPMs, meaning that all\nelements in $\\vec{a}$ are non-negative.\n\n#### Geometry of Space of Diagonal Confusions $\\mathcal{D}$ {.unnumbered}\n\nConsider the trivial classifiers that only predict a single class at all\ntimes. The diagonal confusions when only predicting class $i$ are\n$\\vec{v}_i \\in \\mathbb{R}^k$ with $\\xi_i$ at index $i$ and zero\nelsewhere. Note that this is the maximum possible value in index $i$,\nbecause this represents perfectly classifying all points that have a\ntrue class of $i$.\n\nWe can consider the space of diagonal confusions, visualized in @fig-diag_geom (taken\nfrom [@NEURIPS2019_1fd09c5f]). The space of $\\mathcal{D}$ is strictly\nconvex, closed, and contained in the box\n$[0, \\xi_1] \\times \\dots \\times [0, \\xi_k]$. We also know that the only\nvertices are $\\vec{v}_i$ for each $i \\in [k]^{(k-1)}$.\n\n![(a) Geometry of space of diagonal confusions for $k=3$. This is a\nconvex region with three flat areas representing confusions when\nrestricted to only two classes. (b) Geometry of diagonal confusions when\nrestricted to classes $k_1$ and $k_2$. Notice how this is identical to\nthe space of confusion matrices examined in the previous\nchapter.](Figures/diag_geometry.png){#fig-diag_geom width=\"\\\\textwidth\"}\n\nWe know that this is strictly convex under the assumption that an object\nfrom any class can be misclassified as any other class. Mathematically,\nthe assumption is that\n$g_{ij}(r) = \\mathbb{P} \\left[\\frac{\\eta_i(X)}{\\eta_j(X)} \\geq r \\right]$\n$\\forall i, j \\in [k]$ are continuous and strictly decreasing for\n$r \\in [0, \\infty)$.\n\nWe can also define the space of binary classification confusion matrices\nconfined to classes $k_1$ and $k_2$, which is the 2-D $(k_1, k_2)$\naxis-aligned face of $\\mathcal{D}$, denoted as $\\mathcal{D}_{k_1, k_2}$.\nNote that this is strictly convex, since $\\mathcal{D}$ itself is\nstrictly convex, and it has the same geometry as the space of binary\nconfusion matrices examined in the previous chapter. Therefore, we can\nconstruct an RBO classifier for $\\psi \\in \\varphi_{DLPM}$, parameterized\nby $\\vec{a}$, as follows: \n$$\\begin{aligned}\n\\bar{h}_{k_1, k_2}(\\vec{x})= \\left\\{\n\\begin{array}{ll}\n      k_1, \\text{ if } a_{k_1} \\eta_{k_1}(\\vec{x}) \\geq a_{k_2} \\eta_{k_2}(\\vec{x})\\\\\nk_2, \\text{ o.w.}\n\\end{array}\n\\right\\}.\n\\end{aligned}$$ {#eq-rbo_eq}\n\nWe can parameterize the upper boundary of $\\mathcal{D}_{k_1, k_2}$,\ndenoted as $\\partial \\mathcal{D}^{+}_{k_1, k_2}$, using a single\nparameter $m \\in [0, 1]$. Specifically, we can construct a DLPM by\nsetting $a_{k_1} = m$, $a_{k_2} = 1 - m$, and all others to 0. Using\n[@eq-rbo_eq], we can get the diagonal confusions, so varying $m$ parameterizes\n$\\partial \\mathcal{D}^{+}_{k_1, k_2}$. The parameterization is denoted\nas $\\nu(m; k_1, k_2)$.\n\n#### Diagonal Linear Performance Metric Elicitation {.unnumbered}\n\nSuppose the oracle follows a true metric, $\\psi$, that is linear and\nmonotone increasing across all axes. If we consider the composition\n$\\psi \\circ \\nu(m; k_1, k_2): [0, 1] \\rightarrow \\mathbb{R}$, we know it\nmust be concave and unimodal, because $\\mathcal{D}_{k_1, k_2}$ is a\nconvex set. Therefore, we can find the value of $m$ that maximizes\n$\\psi \\circ \\nu(m; k_1, k_2)$ for any given $k_1$ and $k_2$ using a\nbinary search procedure.\n\nSince the RBO classifier for classes $k_1$ and $k_2$ only rely on the\nrelative weights of the classes in the DLPM (see [@eq-rbo_eq]), finding\nthe value of $m$ that maximizes $\\psi \\circ \\nu(m; k_1, k_2)$ gives us\nthe true relative ratio between $a_{k_1}$ and $a_{k_2}$. Specifically,\nfrom the definition of $\\nu$, we know that\n$\\frac{a_{k_2}}{a_{k_1}} = \\frac{1-m}{m}$. We can therefore simply\ncalculate the ratio between $a_1$ and all other weights to reconstruct\nan estimate for the true metric. A python implementation of this\nalgorithm is provided below.\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\n\ndef rbo_dlpm(m, k1, k2, k):\n    \"\"\"\n    This constructs DLPM weights for the upper boundary of the\n    restricted diagonal confusions, given a parameter m.\n    This is equivalent to \\nu(m; k1, k2)\n    \n    Inputs:\n    - m: parameter (between 0 and 1) for the upper boundary\n    - k1: first axis for this  face\n    - k2: second axis for this face\n    - k: number of classes\n    Outputs:\n    - DLPM weights for this point on the upper boundary\n    \"\"\"\n    new_a = np.zeros(k)\n    new_a[k1] = m\n    new_a[k2] = 1 - m\n    return new_a\n\ndef dlpm_elicitation(epsilon, oracle, get_d, k):\n    \"\"\"\n    Inputs:\n    - epsilon: some epsilon > 0 representing threshold of error\n    - oracle: some function that accepts 2 confusion matrices and\n        returns true if the first is preferred and false otherwise\n    - get_d: some function that accepts dlpm weights and returns \n        diagonal confusions\n    - k: number of classes\n    Outputs:\n    - estimate for true DLPM weights\n    \"\"\"\n    a_hat = np.zeros(k)\n    a_hat[0] = 1\n    for i in range(1, k):\n        # iterate over each axis to find appropriate ratio\n        a = 0  # lower bound of binary search\n        b = 1  # upper bound of binary search\n\n        while (b - a > epsilon):\n            c = (3 * a + b) / 4\n            d = (a + b) / 2\n            e = (a + 3 * b) / 4\n\n            # get diagonal confusions for each point\n            d_a, d_c, d_d, d_e, d_b = (get_d(rbo_dlpm(x, 0, i, k)) \n                for x in [a, c, d, e, b])\n\n            # query oracle for each pair\n            response_ac = oracle(d_a, d_c)\n            response_cd = oracle(d_c, d_d)\n            response_de = oracle(d_d, d_e)\n            response_eb = oracle(d_e, d_b)\n\n            # update ranges to keep the peak\n            if response_ac:\n                b = d\n            elif response_cd:\n                b = d\n            elif response_de:\n                a = c\n                b = e\n            elif response_eb:\n                a = d\n            else:\n                a = d\n\n        midpt = (a + b) / 2\n        a_hat[i] = (1 - midpt) / midpt\n    return a_hat / np.sum(a_hat)\n```\n:::\n\n\n:::\n\nTo use this algorithm for metric elicitation on a real dataset, we need\nto supply the \"oracle\" and \"get_d\" functions. The oracle function is an\ninterface to an expert who judges which of two confusion matrices is\nbetter. The get_d function will need to construct a classifier given the\nDLPM weights, following the principles of the RBO classifier from [@eq-rbo_eq],\nand calculate the confusion matrix from a validation set.\n\n#### Guarantees {.unnumbered}\n\nUsing the same oracle feedback noise model from the binary metric\nelicitation, we can make the following guarantees:\n\n::: {.callout-note title=\"proposition\"}\n::: {#prop-prop_dlpm}\nGiven $\\epsilon, \\epsilon_\\Omega \\geq 0$, and a 1-Lipschitz DLPM\n$\\varphi^*$ parameterized by $\\vec{a}^*$. Then the output $\\hat{a}$ of\nthe DLPM elicitation algorithm after $O((k-1)\\log\\frac{1}{\\epsilon})$\nqueries to the oracle satisfies\n$||\\vec{a}^* - \\hat{a}||_\\infty \\leq O(\\epsilon + \\sqrt{\\epsilon_\\Omega})$,\nwhich is equivalent to\n$||\\vec{a}^* - \\hat{a}||_2 \\leq O(\\sqrt{k}(\\epsilon + \\sqrt{\\epsilon_\\Omega}))$.\n:::\n:::\n\nIn other words, the maximum difference between the estimate and true\nvalue along any component (indicated by the L-infinity norm) is linearly\nbounded by the sum of the epsilon specified by the algorithm and the\nsquare root of the oracle's correctness guarantee ($\\epsilon_\\Omega$).\n\n### Linear Reward Estimation\n\nHow exactly do robots learn human preferences from just the pairwise\ncomparisons, if they need to learn how to act in the environment itself?\nThe comparisons in turn help robots learn the reward function of the\nhuman, which allows them to further take actions in real settings.\n\n#### Geometry of Pairwise Comparisons {.unnumbered}\n\nLet's say there are two trajectories $\\xi_A$ and $\\xi_B$ that might be\ntaken as the next course of action in any context, like choosing the\nnext turn, or choosing the next chatGPT response. The robot is offering\nboth to a human for comparison. To answer which of them is better, the\nhuman would ask themselves if $R(\\xi_A)$ or $R(\\xi_B)$ is bigger, with\n$R(\\xi) = w * \\phi(\\xi)$ being the reward function. In this equation $w$\nand $\\phi(\\xi)$ are vectors of weights and features of the trajectory,\nso alternatively, we can express this as:\n\n$$R(\\xi) = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ ... \\\\ w_N \\end{bmatrix} \\cdot \\begin{bmatrix} \\phi_1(\\xi) \\\\ \\phi_2(\\xi) \\\\ ... \\\\ \\phi_N(\\xi) \\end{bmatrix}$$ {#eq-reward_eq}\n\nIf one says that they preferred $\\xi_2$ less than $\\xi_1$ then it means\n$\\xi_2 < \\xi_1 \\implies R(\\xi_2) < R(\\xi_1) \\implies w * \\phi(\\xi_2) < w * \\phi(\\xi_1) \\implies 0 < w * (\\phi(\\xi_1) - \\phi(\\xi_2)) \\implies 0 < w * \\Phi$.\nAlternatively, if one preferred $\\xi_2$ more than $\\xi_1$, the signs\nwould be flipped, resulting in $0 > w * \\Phi$. The two results can be\nrepresented in the N-dimensional space, where when it is split by the\ndecision boundary, it creates half-spaces indicating preferences for\neach of the sides. For example in @fig-2dcomp we can\nsee how a query between two objects can split the plain into two halves,\nindicating preference towards one of the objects. Such an image can be\nextended into bigger dimensions, where a line would become a separating\nhyperplane like in @fig-2dcomp.\n\n\n::: {#fig-2dcomp layout-ncol=2}\n\n![A single query for a comparison between the two objects splits 2D space\ninto two halves, each of which prefers one of the objects based on\nfeature weights $w_1$ and $w_2$.](Figures/2D-comp.jpg){width=\"40%\"}\n\n![Extension into 3D space](Figures/3D-comp.png){width=\"40%\"}\n\nComparison in 2D and 3D\n:::\n\nIf one is to truly believe the answers of one person, they would remove\neverything from the other side of the hyperplane that does not agree\nwith the received human preference. But since humans are noisy, that\napproach is not optimal, thus most applications up-weight the indicated\nside of the plane to emphasize that points on that side are better, and\ndown-weight the other side as they do not agree with the provided\ncomparison.\n\nHow should someone choose which queries to conduct, otherwise, what is\nthe most informative query sequence? After completing one query, the\nnext query should be orthogonal to the previous one so that the\npotential space consistent with the preferences decreases in half. The\nintuition behind that is the potential space has all of the reward\nfunctions that agree with the provided answers, so to find a specific\nreward function for a human, decreasing the space narrows down the\npossible options. For example, orthogonal query to the query in @fig-2dcomp is\nshown in @fig-2dspace. The original query created the blue space, and\na new one created a red space, resulting in a purple intersection of the\ntwo which is still consistent with both of the queries's results. The\nimage shows that the purple portion is exactly half of the blue portion.\n\n![Creating further comparisons limits the space that agrees with answers\nto all of them. The blue area demonstrates a preference for object 1\nover object 2. The red area demonstrates a preference for object 3 over\nobject 4. Combination (purple area) shows the space that is consistent\nwith both of those preferences.](Figures/2D-space.jpg){#fig-2dspace\nwidth=\"40%\"}\n\nMathematically, from [@pmlr-v87-biyik18a] this can be expressed as set\n$F$ of potential queries $\\phi$, where\n$F = \\{\\phi: \\phi = \\Phi(\\xi_A) - \\Phi(\\xi_B), \\xi_A, \\xi_B \\in \\Xi\\}$\n(defining that a query is the difference between the features of two\ntrajectories). Using that, the authors define a human update function\n$f_{\\phi}(w) = \\min(1, \\exp(I^T\\phi))$ that accounts for how much of the\nspace will still be consistent with the preferences. Finally, for a\nspecific query, they define the minimum volume removed as\n$\\min\\{\\mathbb{E}[1 - f_{\\phi}(w)], \\mathbb{E}[1 - f_{-\\phi}(w)]\\}$\n(expected size of the two sides of the remaining space after it is split\nby a query - purple area in @fig-2dspace), and the final goal is to maximize that amount\nover all possible queries since it is optimal to get rid of as much\nspace as possible to narrow down the options for the reward function:\n$\\max_{\\phi} \\min\\{ \\mathbb{E}[1 - f_{\\phi}(w)], \\mathbb{E}[1 - f_{-\\phi}(w)]\\}$.\nEffectively this is finding such $\\phi$ that maximizes the information\none can get by asking the next comparison query. While this approach\nuses minimum volume removed, there can be other metrics inside the\n$\\max$ function. Some applications like movie recommendations do not\nrequire extra constraints, however in robotics one might want to add\nmore constraints that satisfy certain rules, so that the resulting query\nfollows the dynamics of the physical world.\n\n#### Driving Simulator Example {.unnumbered}\n\nThe first real example of learning reward functions from pairwise\ncomparisons is a 2D driving simulator from [@pmlr-v87-biyik18a]. In @fig-car_direct\nyou can see the setting of a 3-lane road with the orange car being\ncontrolled by the computer.\n\n![The choices presented to a human for feedback are represented by green\nand red trajectories. White trajectory demonstrates the lane change of\nanother vehicle in the space.\n[@pmlr-v87-biyik18a]](Figures/car_dir.png){#fig-car_direct width=\"40%\"}\n\nThe queries conducted for this problem are two different trajectories\npresented to the human, and they are asked to evaluate which one of them\nis better. For the features that contribute to the reward function, it\nis important to consider that robots might not find some of the\ninformation as informative for the learning process as a human would.\nFor this example, the underlying features included the distance between\nlane boundaries, distance to other cars, and the heading and speed of\nthe controlled car. The weights toward the last feature were weighted\nthe highest according to the authors, since it takes a lot of effort for\nthe car to change or correct its direction.\n\nAt the start of the learning process, the car had no direction learned\nand was moving all over the road. In the middle of learning after 30\nqueries, the simulator learned to follow the direction of the road and\ngo straight but still experienced collisions. After 70 queries, the\nsimulator learned to avoid collisions, as well as keep the car within\nthe lane without swerving.\n\n#### Active Learning for Pairwise Comparisons {.unnumbered}\n\nWe have discussed that pairwise comparisons should be selected to\nmaximize the minimum volume of remaining options removed. The question\nthat can come out of the driving example is does it really matter to\nfollow that goal or does random choice of queries performs as well? It\nturns out that indeed most active learning algorithms (purposefully\nselecting queries) over time converge with the performance of the random\nquery selection, so in long term the performance is similar. However,\nwhat is different is that active learning achieves better performance\nearlier, which in time-sensitive tasks can be a critical factor.\n\nOne example of such a setting can be exoskeletons for humans as part of\nthe rehabilitation after surgery [@Li_2021]. Different people have\nsignificantly different walking patterns as well as rehabilitation\nrequirements, so the exoskeleton needs to adapt to the human as soon as\npossible for a more successful rehabilitation. Figure @fig-robotics\ndemonstrates the difference in the time needed between the two\napproaches. In general, in robotics, the time differences that might\nseem small to a human might be detrimental to the final performance.\n\n![Performance of active learning and random query selection algorithms\nin the task of exoskeleton learning with human preferences.\n[@Li_2021]](Figures/robo_graph.png){#fig-robotics width=\"60%\"}\n\n#### Multi-Modal Reward Functions for Pairwise Comparisons {.unnumbered}\n\nWhat if one is working with multiple people and their responses to the\nqueries for comparisons? It will be impossible to recover the different\npersonalities based on the answers, and it might be necessary to conduct\na full ranking before it is clear which responses belonged to which\nperson, but the underlying theory for the number of comparisons is\nnon-trivial. For that, the researchers [@myers2021learning] have used\nmulti-modal models for reward function learning, which allows to account\nfor different types of valid behaviours and trajectories that can come\nfrom different humans.\n\n![The negotiation setting with two people and three shared items. Each\nperson has a desired number of items indicated in their utility box.\nAlice is the controlled agent that has many different response options\nthat are illustrated by the approaches different models might take.\n[@kwon2021targeted]](Figures/negotiation.png){#fig-negotiation\nwidth=\"80%\"}\n\nAn example setting for such type of problem is negotiations\n[@kwon2021targeted]. Let's say there are some shared items and two\npeople with different utilities and desires for items, where each person\nonly knows their utility. In a specific case of\n@fig-negotiation, Bob as a proposing agent and Alice as a\ncontrolled agent who has many different ways of responding to Bob's\nproposals. Different methods can be used to design Alice as an AI agent.\nThe first idea is reinforcement learning, where multiple rounds of\nnegotiations are done, the model simulates game theory and sees how Bob\nreacts. Authors of this setting [@kwon2021targeted] show that over time\nthe model learns to ask for the same thing over and over again, as Alice\nis not trained to be human-like or negotiable, and just tries to\nmaximize Alice's utility. The second approach is supervised learning,\nwhere the model can be trained on some dataset, learning the history of\nnegotiations. This results in Alice being very agreeable, which\ndemonstrates two polar results of the two approaches, and it would be\nideal to find a middle ground and combine both of them. The authors\nproposed the Targeted acquisition approach, which is based on active\nlearning ideas. The model asks diverse questions at different cases and\nstages of negotiations like humans, determining which questions are more\nvaluable to be asked throughout learning. Such an approach ended up in\nmore fair and optimal results than supervised or reinforcement learning\n[@kwon2021targeted].\n\nIn conclusion, pairwise comparisons show to be a great way of learning\nlinear reward functions, but at times present challenges or\nincapabilities that can be further improved with additional\nincorporations of approaches like Active Learning. That improves many\napplications in terms of time spent getting to the result in case of\nexoskeleton adjustments, as well as getting to a middle ground between\npolar behaviors in applications like negotiations.\n\n### Truthful Preference Elicitation with Adversary\n\nIn our study of social choice models in Chapter\n[\\[2model\\]](#2human-decision-making-choice-models){reference-type=\"ref\" reference=\"2model\"}, we study\nhow axiomatic properties are implemented to prevent strategic\nmanipulation of a population. This brings us onto the field of\n**mechanism design**. At its core, mechanism design is the science of\nmaking rules. The intent in this field is to design systems so that the\nstrategic behaviour of individuals leads to desirable outcomes. Just\nthinking about services on the Internet -- file sharing, reputation\nsystems, web search, web advertising, email, Internet auctions,\ncongestion control -- all have to be set up so that an individual's\nselfish behavior leads to better outcomes for the entire community. A\nmore specific example of this is the phenomenon of \"bid-sniping\" that\nwas present on eBay in the early 2000s. When people could bid on E-bay,\nthe rule was that the highest bidder by the end of some specified time\nperiod would get the item. As a result, people would just wait until the\nvery last minute to bid in order to not raise the price of the item too\nearly. On the other hand, when Amazon still allowed bidding, they had a\nrule that any time a bid was placed it would extend the time of the bid\nby ten minutes. This simple difference had drastic effects on bidding\nprices over time. Mechanism design develops the theoretical framework\nfor learning social choices and eliciting truthful preference.\n\nWe will cover frameworks that model several scenarios that mechanism\ndesign is usefully applied to: recommendation systems (where users will\nselfishly try to stick to their preferences while a planner encourages\nexploration); auctions (where bidders will try to maximise their reward\ncompared to others); and peer grading (where truthful reporting is not\nnecessarily an incentive for students).\n\n#### Auction Theory {.unnumbered}\n\n##### Single-Item Auctions {#single-item-auctions .unnumbered}\n\nThe first problem within auction theory we will consider is the\n*single-item auction*. The premise of this problem is that there is a\nsingle item to sell, $n$ bidders (with unknown private valuations of the\nitem $v_1$, \\..., $v_n$). The bidder's individual objective is to\nmaximize utility: the value $v_i$ of the item subtracted by the price\npaid for the item. The auction procedure is standard in the sense that\nbids are solicited and the highest bid will win the auction. While the\nobjective of the individual bidder is clear, there could be a plethora\nof different objectives for the auction as a whole. One option could be\nto maximize social surplus, meaning the goal is to maximize the value of\nthe winner. Another objective could be to maximize seller profit which\nis the payment of the winner. For simplicity, we can focus on the first\nobjective where the goal is to maximize social surplus. If we want to\nmaximize social surplus it turns out that a great way to do this is the\n\"second-price auction\".\n\n###### Maximizing Social Surplus {#maximizing-social-surplus .unnumbered}\n\nIn the second-price auction, we will operate under slightly different\nconditions. In the second-price auction we 1) solicit sealed bids, 2)\nhave the winner be the highest bidder, and 3) charger winner the\nsecond-highest bid price. As an example, if the solicited bids are\n$b = (2, 6, 4, 1)$ the winner will be that who bid $6$, but will pay a\nprice of $4$. From here, we can do some equilibrium analysis to try and\nlearn what the optimal bidding strategy is for each bidder. Let the\namount bidder $i$ bids to be $b_i$, so we have bids\n$b_1, b_2, ..., b_n$. How much should bidder $i$ bid? To analyze this,\nlet us define $t_i = max_{j \\neq i} b_j$ which represents the max of the\nbids that is not from bidder $i$. There are now two cases to consider:\nif $b_i$ \\> $t_i$ and if $b_i$ \\< $t_i$. In the first case the bidder\n$i$ wins, and if the bidder bid $b_i = v_i$, they are guaranteed to have\na positive return on bid. In the other case, they lose the bid and the\nnet loss is 0 because they don't have to pay. From this we can conclude\nthat bidder $i$'s dominant strategy is to just bid $b_i = v_i$.\nRigorously proving this is a little bit trickier, but it was shown from\nVickrey in 1961 \\[cite\\] that truthful bidding is the dominant strategy\nin second-price auctions. A corollary of this is that we are maximizing\nsocial surplus since bids are values and the winner is the bidder with\nhighest valuation.\n\n###### Maximize Seller Profit {#maximize-seller-profit .unnumbered}\n\nIf we want to look at things from the perspective of a seller trying to\nmaximize their profit we need to treat the bidder's bids as uniform\nrandom variables. Consider the example scenario where we have two\nbidders each bidding uniformly between 0 and 1. What is the seller's\nexpected profit? (in this case profit and revenue for the seller are the\nsame because we assume the seller throws away the item if it doesn't\nsell/has no valuation for it).\n\nFrom there the question now becomes, can we get more expected profit\nfrom the seller's perspective? It turns out there is a design where we\ncan add a reserve price of $r$ to the second-price auction. The way this\nworks is we can 1) Insert seller-bid at $r$, 2) solicit bids, 3) pick\nthe highest bidder, and 3) charge the 2nd-highest bid. In effect, this\nis just the second-price auction but with a bid from the seller as well,\nat a price of $r$. A lemma, that we won't prove here, is that the\nsecond-price auction with reserve price $r$ still has a dominant\nstrategy of just being truthful.\n\nLet's now consider what the profit of a second-price auction would be\nwith two bidders that uniformly bid between 0 and 1 -- but this time we\nhave a reserve price of $1/2$. To calculate the expected profit we break\ndown the situation into 3 cases:\n\n-   Case 1:\n    $1/2 > v_1 > v_2 \\rightarrow 1/4 \\text{ probability} \\rightarrow  E[\\text{profit}] = 0$\n\n-   Case 2:\n    $v_1 > v_2 > 1/2 \\rightarrow 1/4 \\text{probability} \\rightarrow E[v2 | case 2] = 2/3$\n\n-   Case 3:\n    $v_1 > 1/2 > v_2 \\rightarrow 1/2 \\text{ probability} \\rightarrow 1/2$\n\nWhy is $E[v2 | case 2] = 2/3$? If $v_1$ and $v_2$ are greater than\n$1/2$, they are evenly spread across the interval, meaning the\nexpectation will be 1/2 + 1/6 = 2/3. Adding up all these cases we get\n$E[profit] = 5/12$. It turns out that second-price auctions with reserve\nactually maximize profit in general (for symmetric bidders)!\n\nIn the previous section we conclude that second-price auctions with\nreserve maximize profit for the seller. In order to prove this, we now\nmove to the more general topic of asking how should a monopolist divide\ngood across separate markets. We can make the assumption that the demand\nmodel is a concave revenue $R(q)$ in quantity $q$. Under this\nassumption, we can just divide supply into $q = q_a + q_b$ such that\n$R'_a(q_a) = R'_b(q_b)$. The idea from here is a theorem from Myerson in\n1981 that states an optimal action maximizes \\\"marginal revenue\\\".\nConsider an example where we have two bidders bidding a uniform value\nbetween 0 and 1. Our revenue curve can now be derived from the offering\nprice $V(q) = 1 - q$ like so: $R(q) = qV(q) = q - q^2$. Taking the\nderivative gives us the marginal revenue $R'(q) = 1-2q$. This means two\nthings: 1) we want to sell to bidder $i$ with the highest $R'(q_i)$ and\n2) we want to sell to bidder $i$ with value at least $1/2$ (if we want a\npositive $R'(q_i)$. But this is just a second-price auction with reserve\n$1/2$! This means that for symmetric bidders, a second price with\nreserve is the optimal auction.\n\n###### What good are auctions? {#what-good-are-auctions .unnumbered}\n\nAn interesting topic to discuss is what benefits auctions bring to the\ntable as opposed to just standard pricing. Online auctions used to be a\nlot more popular in the early 2000s and have been completely replaced by\nstandard online pricing, even on sites like e-bay. While auctions are\nslower and have added inherent complexities, they are actually optimal\non paper. Standard pricing on the other is non-optimal; although it is\nfast and simpler for buyers. There is actually a way to quantify this:\nfor pricing $k$ units, the loss is at most $1 / \\sqrt{2\\pi k}$ of\noptimal profit.\n\nLet's consider applications in duopoly platform design. We know that the\noptimal auction is second-price with reserve, but what happens when we\nintroduce competition between two auction platforms? Some important\ndetails related to the revenue of a second-price auction is that a\nsecond-price auction with no reserve and n bidders leads to larger\nrevenue having an optimal reserve and n - 1 bidders\n [@bulow-klemperer1996]. Additionally, with an entry cost, no reserve is\nthe optimal strategy for maximizing revenue  [@mcafee-87]. Let's\nconsider an example of a competing auction system which is Google ads vs\nBing ads. How should an advertiser divide the budget between Google and\nBing? They should give the same budget to both companies. What happens\nif Bing raises their prices? Then, the advertising company moves more of\nits budget to Google from Bing.\n\n#### Prior-Independent Auctions {#prior-independent-auctions .unnumbered}\n\nThe Bulow-Klemperer theorem demonstrates that increased competition can\nbe more valuable than perfect knowledge of bidders' valuation\ndistributions. This result provides insight into the potential of\nsimple, prior-independent auctions to approach the performance of\noptimal auctions. The theorem states that for a single-item auction with\nbidders' valuations drawn independently from a regular distribution $F$:\n\n::: {.callout-note title=\"theorem\"}\n::: {#thm-bulow-klemperer}\nLet $F$ be a regular distribution and $n$ a positive integer. Then:\n$$E_{v_1,\\ldots,v_{n+1} \\sim F}[\\text{Rev(VA)}(n+1 \\text{ bidders})] \\geq E_{v_1,\\ldots,v_n \\sim F}[\\text{Rev(OPT}_F)(n \\text{ bidders})]$$  {#eq-eq3.64}\nwhere VA denotes the Vickrey auction and $\\text{OPT}_F$ denotes the\noptimal auction for $F$.\n:::\n:::\n\nThis shows that running a simple Vickrey auction with one extra bidder\noutperforms the revenue-optimal auction that requires precise knowledge\nof the distribution. It suggests that in practice, effort spent on\nrecruiting additional bidders may be more fruitful than fine-tuning\nauction parameters.\n\n##### The VCG Mechanism {#the-vcg-mechanism .unnumbered}\n\nThe VCG mechanism is a cornerstone of mechanism design theory, providing\na general solution for welfare maximization in multi-parameter\nenvironments. The key result is:\n\n::: {.callout-note title=\"theorem\"}\n::: {#thm-VCG}\n[]{#thm:VCG label=\"thm:VCG\"} In every general mechanism design\nenvironment, there is a dominant-strategy incentive-compatible (DSIC)\nwelfare-maximizing mechanism.\n:::\n:::\n\nThe VCG mechanism operates as follows:\n\n1.  Given bids $b_1, \\ldots, b_n$, where each $b_i$ is indexed by the\n    outcome set $\\Omega$, the allocation rule is:\n\n    $$x(b) = \\arg \\max_{\\omega \\in \\Omega} \\sum_{i=1}^n b_i(\\omega)$$  {#eq-eq3.65}\n\n2.  The payment rule for each agent $i$ is:\n\n    $$p_i(b) = \\max_{\\omega \\in \\Omega} \\sum_{j \\neq i} b_j(\\omega) - \\sum_{j \\neq i} b_j(\\omega^*)$$  {#eq-eq3.66}\n\n    where $\\omega^* = x(b)$ is the chosen outcome.\n\nThe key insight is to charge each agent its \"externality\" - the welfare\nloss inflicted on other agents by its presence. This payment rule,\ncoupled with the welfare-maximizing allocation rule, yields a DSIC\nmechanism.\n\nThe VCG mechanism can be interpreted as having each agent pay its bid\nminus a \\\"rebate\\\" equal to the increase in welfare attributable to its\npresence:\n\n$$p_i(b) = b_i(\\omega^*) - \\left[ \\sum_{j=1}^n b_j(\\omega^*) - \\max_{\\omega \\in \\Omega} \\sum_{j \\neq i} b_j(\\omega) \\right]$$  {#eq-eq3.67}\n\nWhile the VCG mechanism provides a theoretical solution for DSIC\nwelfare-maximization in general environments, it can be challenging to\nimplement in practice due to computational and communication\ncomplexities.\n\n##### Combinatorial Auctions {#combinatorial-auctions .unnumbered}\n\nCombinatorial auctions are an important class of multi-parameter\nmechanism design problems, with applications ranging from spectrum\nauctions to airport slot allocation. In a combinatorial auction:\n\n-   There are $n$ bidders and a set $M$ of $m$ items.\n\n-   The outcome set $\\Omega$ consists of allocations\n    $(S_1, \\ldots, S_n)$, where $S_i$ is the bundle allocated to bidder\n    $i$.\n\n-   Each bidder $i$ has a private valuation $v_i(S)$ for each bundle\n    $S \\subseteq M$.\n\nWhile the VCG mechanism theoretically solves the welfare-maximization\nproblem, combinatorial auctions face several major challenges in\npractice:\n\n1.  Preference Elicitation: Each bidder has $2^m - 1$ private\n    parameters, making direct revelation infeasible for even moderate\n    numbers of items. This necessitates the use of indirect mechanisms\n    that elicit information on a \\\"need-to-know\\\" basis.\n\n2.  Computational Complexity: Even when preference elicitation is not an\n    issue, welfare-maximization can be an intractable problem. In\n    practice, approximations are often used, hoping to achieve\n    reasonably good welfare.\n\n3.  VCG Limitations: The VCG mechanism can exhibit bad revenue and\n    incentive properties in combinatorial settings. For example, adding\n    bidders can sometimes decrease revenue to zero, and the mechanism\n    can be vulnerable to collusion and false-name bids.\n\n4.  Strategic Behavior in Iterative Auctions: Most practical\n    combinatorial auctions are iterative, comprising multiple rounds.\n    This introduces new opportunities for strategic behavior, such as\n    using bids to signal intentions to other bidders.\n\nThese challenges make combinatorial auctions a rich and complex area of\nstudy, requiring careful design to balance theoretical guarantees with\npractical considerations.\n\n##### Spectrum Auctions {#spectrum-auctions .unnumbered}\n\nSpectrum auctions represent a complex application of combinatorial\nauction theory. With n bidders and m non-identical items, each bidder\nhas a private valuation for every possible bundle of items, making it\nimpractical to directly elicit all preferences. This necessitates the\nuse of indirect, iterative mechanisms that query bidders for valuation\ninformation on a \"need-to-know\" basis, sacrificing some of the desirable\nproperties of direct mechanisms like dominant strategy incentive\ncompatibility (DSIC) and full welfare maximization.\n\nThe fundamental challenge in spectrum auctions lies in the nature of the\nitems being sold. There is a dichotomy between items that are\nsubstitutes (where $v(AB) \\leq v(A) + v(B))$ and those that are\ncomplements (where $v(AB) > v(A) + v(B))$. Substitute items, such as\nlicenses for the same area with equal-sized frequency ranges, are\ngenerally easier to handle. When items are substitutes, welfare\nmaximization is computationally tractable, and the VCG mechanism avoids\nmany undesirable properties. However, complementary items, which arise\nnaturally in spectrum auctions when bidders want adjacent licenses,\npresent significant challenges.\n\nEarly attempts at spectrum auctions revealed the pitfalls of naive\napproaches. Sequential auctions, where items are sold one after another,\nproved problematic as demonstrated by a Swiss auction in 2000. Bidders\nstruggled to bid intelligently without knowing future prices, leading to\nunpredictable outcomes and potential revenue loss. Similarly,\nsimultaneous sealed-bid auctions, as used in New Zealand in 1990,\ncreated difficulties for bidders in coordinating their bids across\nmultiple items, resulting in severely suboptimal outcomes.\n\nThe Simultaneous Ascending Auction (SAA) emerged as a solution to these\nissues and has formed the basis of most spectrum auctions over the past\ntwo decades. In an SAA, multiple items are auctioned simultaneously in\nrounds, with bidders placing bids on any subset of items subject to an\nactivity rule. This format facilitates price discovery, allowing bidders\nto adjust their strategies as they learn about others' valuations. It\nalso allows bidders to determine valuations on a need-to-know basis,\nreducing the cognitive burden compared to direct-revelation auctions.\n\nDespite its advantages, the SAA is not without vulnerabilities. Demand\nreduction, where bidders strategically reduce their demand to lower\nprices, can lead to inefficient outcomes even when items are\nsubstitutes. The exposure problem arises with complementary items, where\nbidders risk winning only a subset of desired items at unfavorable\nprices. These issues highlight the ongoing challenges in designing\neffective spectrum auctions, balancing theoretical guarantees with\npractical considerations.\n\n##### Case study: Classroom Peer Grading {#case-study-classroom-peer-grading .unnumbered}\n\nThis chapter discusses work by Jason Hartline, Yingkai Li, Liren Shan,\nand Yifan Wu at Northwestern University, where researchers examined\nmechanism design for the classroom, specifically in terms of the\noptimization of scoring rules. They explored peer grading in the\nclassroom and how to construct a peer grading system that optimizes the\nobjectives for each stakeholder in the system, including those being\ngraded, the peer graders, the TAs of the class, and the professor.\n\nFirstly, let's think of the classroom like a computer. We can think of\nstudents as local optimizers; their incentive is to minimize the amount\nof work they need to do and maximize the grades that they receive. The\ngraders are imprecise operators, which means that there is some\nuncertainty in their ability to grade the work completed by the\nstudents. The syllabus can be thought of as the rules that map the\nactions of the students to the grade they end up receiving in the class.\nOur overall goals for this classroom based on these definitions is to\nminimize work, maximize learning, and fairly assess the students for the\nwork that they do  [@jasonH2020].\n\nOne basic question that we can examine, is what is the best syllabus\nthat maximizes our objectives for our classroom design. Some components\nof this could include grading randomized exams, grading with partial\ncredit, group projects, and finally, peer grading, which is the\ncomponent that we will be taking a deeper dive into.\n\nThe general situation of the peer grading problem is that proper scoring\nrules make peer grades horrible  [@jasonH2020]. So we want to be able to\noptimize scoring rules and make sure that we are optimizing each\ncomponent of the peer grading pipeline.\n\nThe main algorithms focused on in this peer grading design paper were\nmatching peers and TAs to submissions and the grading of those\nsubmissions from the TAs and the peer reviews  [@jasonH2020]. There are\nquite a number of advantages to peer grading including that peers are\nable to learn from reviewing other people's work, it reduces the work\nfor the teacher, and improves the turnaround time for assignment\nfeedback (which are all part of our overarching goals for our mechanism\ndesign for the classroom). But, it is also important to acknowledge the\npotential disadvantages of the peer grading system: it is possible that\nthe peer graders present inaccurate grades and there is student unrest.\nThis presents us with a challenge: being able to incentivize accurate\npeer reviews.\n\nOne problem that we run into, when we use the proper scoring rule to\nscore peer reviews, if the peer graders use the lazy peer strategy,\nwhich means that they always report 80$\\%$ for their peer reviews, they\nget graded very well using the proper scoring rule algorithm. In fact,\nthe proper scoring rule says that their peer review is 96$\\%$ accurate\n [@jasonH2023]. So how do we incentivize effort in reviews from peer\ngraders? We use a scoring rule that maximizes the difference in score\nbetween effort or no effort reviews as indicated by the peer reviewers\n [@jasonH2023]. So overall, the analysis of datasets leads to decision\noptimizations and, eventually, payoff from those decisions.\n\nTo conclude our mechanism design in the classroom discussion, we have\ntwo key takeaways: scoring rules are essential in being able to\nunderstand and analyze data thoroughly, and optimal scoring rules for\nbinary effort allow us to understand the setting independent of the\ndataset  [@jasonH2023].\n\n#### Mutual Information Paradigm {#mutual-information-paradigm .unnumbered}\n\nIn this section we discuss an influential new framework for designing\npeer prediction mechanisms, the Mutual Information Paradigm (MIP)\nintroduced by Kong and Schoenebeck [@kongschoenebeck2019]. Traditional\npeer prediction approaches typically rely on scoring rules and\ncorrelation between agents' signals. However, these methods often\nstruggle with issues like uninformed equilibria, where agents can\ncoordinate on uninformative strategies that yield higher payoffs than\ntruth-telling. The core idea is to reward agents based on the mutual\ninformation between their report and the reports of other agents.\n\nWe consider a setting with $n$ agents, each possessing a private signal\n$\\Psi_i$ drawn from some set $\\Sigma$. The mechanism asks each agent to\nreport their signal, which we denote as $\\hat{\\Psi}_i$. For each agent\n$i$, the mechanism randomly selects a reference agent $j \\neq i$. Agent\n$i$'s payment is then calculated as: \n$$MI(\\hat{\\Psi}_i; \\hat{\\Psi}_j)$$   {#eq-eq3.68}\nwhere $MI$ is an information-monotone mutual information measure. An\ninformation-monotone $MI$ measure must satisfy the following properties:\n\n-   **Symmetry**: $MI(X; Y) = MI(Y; X)$.\n\n-   **Non-negativity**: $MI(X; Y) \\geq 0$, with equality if and only if\n    $X$ and $Y$ are independent.\n\n-   **Data processing inequality**: For any transition probability $M$,\n    if $Y$ is independent of $M(X)$ conditioned on $X$, then\n    $MI(M(X); Y) \\leq MI(X; Y)$.\n\nTwo important families of mutual information measures that satisfy these\nproperties are $f$-mutual information and Bregman mutual information.\nThe $f$-mutual information is defined as:\n$$MI_f(X; Y) = D_f(U_{X,Y}, V_{X,Y})$$  {#eq-eq3.69}\n where $D_f$ is an $f$-divergence,\n$U_{X,Y}$ is the joint distribution of $X$ and $Y$, and $V_{X,Y}$ is the\nproduct of their marginal distributions. The Bregman mutual information\nis defined as: $$BMI_{PS}(X; Y) = \\mathbb{E}_{X} [D{PS}(U_{Y|X}, U_Y)]$$  {#eq-eq3.70}\nwhere $D_{PS}$ is a Bregman divergence based on a proper scoring rule\n$PS$, $U_{Y|X}$ is the conditional distribution of $Y$ given $X$, and\n$U_Y$ is the marginal distribution of $Y$.\n\nThe MIP framework can be applied in both single-question and\nmulti-question settings. In the multi-question setting, the mechanism\ncan estimate the mutual information empirically from multiple questions.\nIn the single-question setting, additional techniques like asking for\npredictions about other agents' reports are used to estimate the mutual\ninformation.\n\nA key theoretical result of the MIP framework is that when the chosen\nmutual information measure is strictly information-monotone with respect\nto agents' priors, the resulting mechanism is both dominantly truthful\nand strongly truthful. This means that truth-telling is a dominant\nstrategy for each agent and that the truth-telling equilibrium yields\nstrictly higher payoffs than any other non-permutation strategy profile.\n\nAs research continues to address practical implementation challenges of\ndesigning truthful mechanisms, MIP-based approaches have significant\npotential to improve preference elicitation and aggregation in\nreal-world applications lacking verifiable ground truth.\n\n#### Auction Theory 2 {.unnumbered}\n\n##### Single-Item Auctions {#single-item-auctions-1 .unnumbered}\n\nThe first problem within auction we will consider is the *single-item\nauction*. In this problem setup, there is a single item to sell and $n$\nbidders each with unknown private valuations of the item\n$v_1, \\ldots, v_n$,\n\n[^1]: Here, the \"$\\sup$\\\" operation refers to the *supremum*, which is\n    the largest value that a certain set can take on, whereas \"$\\inf$\\\"\n    would refer to the *infimum*, or the smallest value a certain set\n    can take on.\n\n### Application: Guiding Human Demonstrations in Robotics\n\nA strong approach to learning policies for robotic manipulation is\nimitation learning, the technique of learning behaviors from human\ndemonstrations. In particular, interactive imitation learning allows a\ngroup of humans to contribute their own demonstrations for a task,\nallowing for scalable learning. However, not all groups of demonstrators\nare equally helpful for interactive imitation learning.\n\nThe ideal set of demonstrations for imitation learning would follow a\nsingle, optimal method for performing the task, which a robot could\nlearn to mimic. Conversely, *multimodality*, the presence of multiple\noptimal methods in the demonstration set, is challenging for imitation\nlearning since it has to learn from contradicting information for how to\naccomplish a task.\n\nA common reason for multimodality is the fact that different people\noften subconsciously choose different paths for execution, as\nillustrated in @fig-multimodalexecution.\n\n![Examples of two different ways to insert a nut onto a round peg. The\norange demonstration picks up the nut from the hole while the blue\ndemonstration picks up the nut from the side\n[@gandhi2022eliciting]](Figures/multimodal_peg.png){#fig-multimodalexecution\nwidth=\"50%\"}\n\nGandhi et al. [@gandhi2022eliciting] identifies whether demonstrations\nare compatible with one another and offer an active elicitation\ninterface to guide humans to provide better demonstrations in\ninteractive imitation learning. Their key motivation is to allow\nmultiple users to contribute demonstrations over the course of data\ncollection by guiding users towards compatible demonstrations.\n\nTo identify whether a demonstration is \"compatible\" with a base policy\ntrained with prior demonstrations, the researchers measure the\n*likelihood* of demonstrated actions under the base policy, and the\n*novelty* of the visited states. Intuitively, low likelihood and low\nnovelty demonstrations should be excluded since they represent\nconflicting modes of behavior on states that the robot can already\nhandle, and are therefore incompatible. This concept of compatibility is\nused for filtering a new set of demonstrations and actively eliciting\ncompatible demonstrations.\n\nIn the following subsections, we describe the process of estimating\ncompatibility and active elicitation in more detal.\n\n#### Estimating Compatiblity {.unnumbered}\n\nWe want to define a compatibility measure $\\mathcal{M}$, that estimates\nthe performance of policy $\\pi_{base}$ that is retrained on a union of\n$\\mathcal{D}_{base}$, the known base dataset, and $\\mathcal{D}_{new}$,\nthe newly collected dataset. To define this compatibility measure in a\nway that is easy to compute, we can use two interpretable metrics:\nlikelihood and novelty.\n\nThe likelihood of actions $a_{new}$ in $\\mathcal{D}_{new}$ is measured\nas the negative mean squared error between actions predicted by the base\npolicy and this proposed action:\n\n$$\\begin{aligned}\n    likelihood(s_{new}, a_{new}) = -\\mathbb{E}[|| \\pi_{base}(s_{new}) - a_{new} ||^2_2].\n\\end{aligned}$$  {#eq-eq3.61}\n\nThe novelty of the state $s_{new}$ in $\\mathcal{D}_{new}$ is the\nstandard deviation in the predicted actions under base policy:\n\n$$\\begin{aligned}\n    novelty(s_{new}) = \\mathrm{Var}[\\pi_{base}(s_{new})].\n\\end{aligned}$$  {#eq-eq3.62}\n\nWe can plot likelihood and novelty on a 2D plane, as shown in @fig-likelihood_novelty, and identify thresholds on\nlikelihood and novelty, denoted as $\\lambda$ and $\\eta$ respectively.\nIntuitively, demonstrations with low likelihood in low novelty states\nshould be excluded, because this indicates that there is a conflict\nbetween the base behavior and the new demonstration due to\nmultimodality. Note that in high novelty states, the likelihood should\nbe disregarded because the base policy does not have a concrete idea for\nhow to handle these states anyways so more data is needed.\n\n![Examples of plots of likelihood and novelty for compatible and\nincompatible operators\n[@gandhi2022eliciting]](Figures/likelihood_novelty.png){#fig-likelihood_novelty\nwidth=\"80%\"}\n\nThe final compatibility metric, parameterized by the likelihood and\nnovelty thresholds $\\lambda$ and $\\eta$, is\n$\\mathcal{M}(\\mathcal{D}_{base}, (s_{new}, a_{new})) \\in [0, 1]$,\ndefined as:\n\n$$\\begin{aligned}\n    \\mathcal{M} = \\begin{cases} \n        1 - \\min(\\frac{\\mathbb{E}[|| \\pi_{base}(s_{new}) - a_{new} ||^2_2]}{\\lambda}, 1) & \\text{ if } \\text{novelty}(s_{new}) < \\eta \\\\\n        1 & \\text{ otherwise }\n       \\end{cases}.\n\\end{aligned}$$  {#eq-eq3.63}\n\nNote that $\\lambda$ and $\\eta$ need to be specified by hand. This is\naccomplished by assuming the ability to collect *a priori incompatible*\ndemonstrations to identify reasonable thresholds that remove the most\ndatapoints in the incompatible demonstrations while keeping the most\ndatapoints in the compatible demonstrations.\n\n#### Case Studies with Fixed Sets {.unnumbered}\n\nThe researchers evaluate the utility of the compatibility metric on\nthree tasks: placing a square nut on a square peg, placing a round nut\non a round peg, and opening a drawer and placing a hammer inside. For\neach task, they train a base policy using a \"proficient\" operator's\ndemonstration while sampling trajectories from other operators for the\nnew set.\n\nThe naive baseline is to use all datapoints while the\n$\\mathcal{M}$-Filtered demonstrations use the compatibility metric to\nfilter out incompatible demonstrations. The results are presented in\n@tbl-m_filter_table. As you can see, M-filtering results in\nequal or greater performance despite using less data than the naive\nbaseline, demonstrating the effectiveness of compatibility-based\nfiltering.\n\n::: {#tbl-m_filter_table}\n  --------------- ---------------- ------------------------ --------------- ------------------------ ---------------------- ------------------------\n                   **Square Nut**                            **Round Nut**                            **Hammer Placement**  \n  **Operator**         Naive        $\\mathcal{M}$-Filtered       Naive       $\\mathcal{M}$-Filtered          Naive           $\\mathcal{M}$-Filtered\n  Base Operator      38.7 (2.1)               \\-              13.3 (2.3)               \\-                  24.7 (6.1)                  \\-\n  Operator 1         54.3 (1.5)           61.0 (4.4)          26.7 (11.7)         32.0 (12.2)              38.0 (2.0)              39.7 (4.6)\n  Operator 2         40.3 (5.1)           42.0 (2.0)          22.0 (7.2)           26.7 (5.0)              33.3 (3.1)              32.7 (6.4)\n  Operator 3         37.3 (2.1)           42.7 (0.6)          17.3 (4.6)          18.0 (13.9)              8.0 (0.0)               12.0 (0.0)\n  Operator 4         27.3 (3.5)           37.3 (2.1)           7.3 (4.6)           13.3 (1.2)              4.0 (0.0)               4.0 (0.0)\n  --------------- ---------------- ------------------------ --------------- ------------------------ ---------------------- ------------------------\n\n  : Success rates (mean/std across 3 training runs) for policies trained\n  on $\\mathcal{D}_{new}$ by using all the data (Naive) or filtering by\n  compatibility ($\\mathcal{M}$-Filtered) [@gandhi2022eliciting]\n:::\n\n![The phases of the active elicitation interface: (a) initial prompting,\n(b) demonstrations with live feedback, and (c) corrective feedback\n[@gandhi2022eliciting]](Figures/active_elicitation.png){#fig-active_elicitation\nwidth=\"80%\"}\n\n#### Actively Eliciting Compatible Demonstrations {.unnumbered}\n\nIn the previous section, we assume access to a dataset that has already\nbeen collected, and we see how filtering out incompatible demonstrations\nhelps improve performance. However, when collecting a new dataset, it\nwould be better to ensure that operators collect compatible\ndemonstrations from the start, allowing us to retain as much data as\npossible for training.\n\nTo actively elicit compatible demonstrations, the researchers set up a\npipeline for live feedback and examples. At the start, operators are\ngiven a task specification and some episodes to practice using the\nrobot. Then, the active elicitation process begins, as shown in @fig-active_elicitation. Each operator is shown some\nrollouts of the base policy to understand the style of the base\noperator. Next, the operator provides a demonstration similar to the\nones they were shown. As they record their demonstrations, the interface\nprovides online feedback, with green indicating compatible actions and\nred indicating incompatible actions. If the number of incompatible\nstate-action pairs (ones where $\\mathcal{M}$ is zero) exceeds 5% of the\ndemonstration length, the demonstration is rejected. However, to provide\ncorrective feedback, the interface shows the areas of the demonstration\nwith the highest average incompatibility and also provides an expert\ndemo that shows what should actually be done. Demonstrators can use this\nfeedback to provide more compatible demonstrations moving forward.\n\nThis process helps improve the demonstration quality in both simulation\nand real experiments, as show in @tbl-active_elicitation_results. Specifically, on the real\nresults, active elicitation outperformed the base policy by 25% and\nnaive data collection by 55%. Overall, active elicitation is a powerful\ntool to ensure that data collected for imitation learning improves the\nquality of the learned policy.\n\n::: {#tbl-active_elicitation_results}\n  **Task**                                            **Base**     **Naive**    **Naive + Filtered**   **Informed**\n  ------------------------------------------------- ------------ ------------- ---------------------- --------------\n  **Round Nut**                                      13.3 (2.3)    9.6 (4.6)         9.7 (4.2)          15.7 (6.0)\n  **Hammer Placement**                               24.7 (6.1)   20.8 (15.7)       22.0 (15.5)        31.8 (16.3)\n  **$\\left[ \\textup{Real} \\right]$ Food Plating**       60.0      30.0 (17.3)            \\-             85.0 (9.6)\n\n  : Success rates (mean/std across users) for policies trained on\n  $\\mathcal{D}_{new}$ by using all the data (Naive), filtering by\n  compatibility ($\\mathcal{M}$-Filtered), or using informed\n  demonstration collection [@gandhi2022eliciting]\n:::\n\n#### Limitations and Future Work for Active Elicitation {.unnumbered}\n\nA fundamental limitation of eliciting compatible demonstrations is the\nfact that the \"base\" demonstrator is considered the ground truth. When\nthe base demonstrator specifies a preference, all other demonstrators\nmust abide by it, even if they have strong preferences against it. For\ninstance, when pouring milk and cereal into a bowl, different people\nhave different preferences for what is the correct order, but active\nelicitation forces all demonstrators to follow the initial preference of\nthe base operator. The researchers hope that future work can enable\nusers to override the default demonstration set and follow a base\nbehavior that better aligns with their preferences. This could enable\nmultiple modes of behavior to be collected in data while only following\na user's specified preference instead of attempting to collapse all\nmodes into a single policy.\n\nLooking forward, active elicitation provides a foundation for allowing\nrobots to query humans about the type of data needed, enabling more\nefficient data collection through transparency.\n\n### Conclusion\n\nIn summary, this chapter has explored the complexities and innovations\nin interactive learning as applied to large models within robotics. It\nbegins by investigating pairwise comparisons and their role in\nefficiently learning linear reward functions from large datasets,\novercoming limitations in supervised learning. When combined with active\nlearning techniques, these comparisons supply timely, targeted, and\ncontext-appropriate feedback, enhancing performance in time-critical\napplications like exoskeleton adjustments during rehabilitation.\n\nWe then shift to imitation learning or inverse reward learning from\ndemonstrations, emphasizing the difficulties introduced by multimodal\ndemonstration sets. active elicitation approaches to compile compatible\ndemonstrations, streamlining the learning process by guiding users to\nprovide more valuable, steady examples are incredibly promising,\nhowever, to tackling this issue. This method shows promise in refining\nthe interactive imitation learning data collection pipeline, enabling\nmore capable and effective robotic training.\n\nAdditionally, the chapter examines the integration of foundation models\ninto robotics, highlighting the transformative innovations of R3M and\nVoltron. R3M's pre-training on diverse human activities dramatically\nimproves robotic manipulation with minimal supervision. Meanwhile,\nVoltron builds on these capabilities by incorporating language-driven\nrepresentation learning for remarkably adaptable and nuanced robotic\ntask performance. These models represent significant leaps in robotics\nwhile opening new frontiers for future research and applications.\n\n\n\n## Exercises\n### Question 1: Uncertainty Quantification in Preference Learning (40 points) {#sec-question-1-uncertainty-quantification-in-preference-learning-40-points .unnumbered}\n\nIn this question, we will explore Bayesian approaches to logistic\nregression in the context of preference learning using the Bradley-Terry\nmodel. We will compare different models and inference methods, including\nparametric linear models estimated using Metropolis-Hastings, parametric\nneural network models estimated using Hamiltonian Monte Carlo, and\nnon-parametric models with Gaussian Processes. Finally, we will assess\nthe uncertainty quantification in these models using the Expected\nCalibration Error (ECE).\n\nAssume we have a dataset of pairwise preferences\n$\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$, where $x_i \\in \\mathbb{R}^d$\nrepresents the feature difference between two items (i.e.,\n$x_i = e^{(i)}_1 - e^{(i)}_2$ for embeddings $e^{(i)}_1$ and\n$e^{(i)}_2$), and $y_i \\in \\{0, 1\\}$ indicates the preference ($y_i = 1$\nif item 1 is preferred over item 2 in the $i$-th pair).\n\nThe likelihood of observing $y_i$ given $x_i$ and model parameters\n$\\theta$ is given by the logistic function:\n\n$$P(y_i = 1 | x_i, \\theta) = \\sigma(x_i^\\top \\theta) = \\frac{1}{1 + e^{-x_i^\\top \\theta}}.$$\n\nWe will adopt a Bayesian approach by placing priors on the model\nparameters and using Markov Chain Monte Carlo (MCMC) methods to estimate\nthe posterior distributions.\n\n(a) **Uncertainty Quantification and Expected Calibration Error (11\n    points)**\n\n    (i) **(Written, 2 point)**. Spend some time reading\n        <https://tinyurl.com/m77mk9c>. Explain what the Expected\n        Calibration Error (ECE) measures and why it is important for\n        assessing uncertainty quantification in probabilistic models.\n\n    (ii) **(Coding, 6 points)**. In `uncertainty_quantification/ece.py`,\n         implement the ECE using the formula\n         $$\\text{ECE} = \\sum_{k=1}^K \\frac{n_k}{N} \\left| \\text{acc}(B_k) - \\text{conf}(B_k) \\right|,$$\n         where $n_k$ is the number of samples in bin $B_k$, $N$ is the\n         total number of samples, $\\text{acc}(B_k)$ is the accuracy in\n         bin $B_k$, and $\\text{conf}(B_k)$ is the average confidence in\n         bin $B_k$.\n\n    (iii) **(Written, 3 point)**. After doing parts (b), (c), and (d),\n          compare the ECE scores and reliability diagrams of the 3\n          models. Which model(s) provide the best uncertainty\n          quantification? Discuss possible reasons for the observed\n          differences.\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef expected_calibration_error(probs, labels, model_name, n_bins=20, n_ticks=10, plot=True):\n    \"\"\"\n    Computes the Expected Calibration Error (ECE) for a model and plots a refined reliability diagram\n    with confidence histogram and additional calibration statistics.\n    \n    Args:\n    - probs (np.array): Array of predicted probabilities for the positive class (for binary classification).\n    - labels (np.array): Array of true labels (0 or 1).\n    - model_name (str): Name of the model for labeling the plot.\n    - n_bins (int): Number of bins to divide the probability interval [0,1] into.\n    - n_ticks (int): Number of ticks to show along the x-axis.\n    - plot (bool): If True, generates the reliability plot; otherwise, only computes ECE.\n\n    Returns:\n    - float: Computed ECE value.\n    \"\"\"\n    \n    # Ensure probabilities are in the range [0, 1]\n    assert np.all((probs >= 0) & (probs <= 1)), \"Probabilities must be in the range [0, 1]\"\n    \n    # Initialize bin edges, centers, and storage for accuracy, confidence, and counts\n    bin_edges = np.linspace(0, 1, n_bins + 1)\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n    bar_width = 1.0 / n_bins\n\n    accs = np.zeros(n_bins)\n    confs = np.zeros(n_bins)\n    bin_counts = np.zeros(n_bins)\n\n    # Populate bin statistics: accuracy, confidence, and count\n    # YOUR CODE HERE (~7 lines)\n    # Loop over each bin and:\n    # - Find indices of probabilities that fall within the bin.\n    # - Count the number of items in the bin.\n    # - Calculate the accuracy (average of true labels) within the bin.\n    # - Calculate the confidence (average of predicted probabilities) within the bin.\n    pass \n    # END OF YOUR CODE\n    \n    # Compute ECE: weighted average of |accuracy - confidence| across bins\n    # YOUR CODE HERE (1 line)\n    # - Use the bin counts to calculate a weighted average of the differences between accuracy and confidence.\n    ece_value = None\n    # END OF YOUR CODE\n    \n    # Return only ECE if plot is not required\n    if not plot:\n        return ece_value\n\n    # Compute average confidence and accuracy for reference lines\n    avg_confidence = np.mean(probs)\n    avg_accuracy = np.mean(labels)\n    \n    # Create reliability diagram and histogram\n    fig, (ax1, ax2) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [3, 1]}, figsize=(8, 10))\n    \n    # Reliability diagram (top plot)\n    ax1.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n    for i in range(n_bins):\n        # Draw the gap bar starting from the diagonal line (perfect calibration)\n        ax1.bar(bin_centers[i], abs(accs[i] - confs[i]), width=bar_width, bottom=min(accs[i], confs[i]), \n                color='red', alpha=0.3, label='Accuracy-Confidence Gap' if i == 0 else \"\")\n        # Draw the accuracy bar as a small black line on top of the gap bar\n        ax1.plot([bin_centers[i] - bar_width / 2, bin_centers[i] + bar_width / 2], \n                 [accs[i], accs[i]], color='black', linewidth=2)\n\n    # Add a black line as a sample for accuracy in the legend\n    ax1.plot([], [], color='black', linewidth=2, label='Accuracy Marker')\n\n    ax1.set_xlim(0, 1)\n    ax1.set_ylim(0, 1)\n    ax1.set_ylabel('Accuracy')\n    ax1.set_title(f'{model_name}\\nECE={ece_value:.2f}')\n    ax1.legend()\n\n    # Set tick marks based on `n_ticks` evenly spaced along the x-axis\n    tick_positions = np.linspace(0, 1, n_ticks + 1)\n    ax1.set_xticks(tick_positions)\n    ax2.set_xticks(tick_positions)\n    ax1.set_xticklabels([f'{x:.2f}' for x in tick_positions])\n    ax2.set_xticklabels([f'{x:.2f}' for x in tick_positions])\n\n    # Confidence histogram with average markers\n    ax2.bar(bin_centers, bin_counts, width=bar_width, color='blue', alpha=0.6)\n    ax2.axvline(x=avg_confidence, color='gray', linestyle='--', linewidth=2, label='Avg. confidence')\n    ax2.axvline(x=avg_accuracy, color='black', linestyle='-', linewidth=2, label='Avg. accuracy')\n    ax2.set_xlim(0, 1)\n    ax2.set_xlabel('Confidence')\n    ax2.set_ylabel('Count')\n    ax2.legend()\n\n    plt.tight_layout()\n    plt.show()\n    \n    return ece_value\n\nif __name__ == \"__main__\":\n    # Test with random probabilities and labels\n    probs = np.random.rand(10000)  # Random probabilities between 0 and 1\n    labels = np.random.binomial(1, (probs + 1) / 2)\n\n    # Run the function and display the result\n    ece_value = expected_calibration_error(probs, labels, \"Test Model\", plot=True)\n    print(f\"ECE Value: {ece_value}\")\n```\n:::\n\n\n:::\n\n(b) **Parametric Linear Model Estimated Using Metropolis-Hastings (11\n    points)**\n\n    (i) **(Written, 3 points)**. Assume a prior on $\\theta$ such that\n        $\\theta \\sim \\mathcal{N}(0, \\sigma^2 I)$, where $\\sigma^2$ is\n        the variance and $I$ is the identity matrix. Derive the\n        expression for the posterior distribution\n        $P(\\theta | \\mathcal{D})$ up to a normalization constant.\n\n    (ii) **(Coding, 6 points)**. Implement the Metropolis-Hastings\n         algorithm to sample from the posterior distribution of $\\theta$\n         in `uncertainty_quantification/metropolis.py`.\n\n    (iii) **(Written, 2 points)**. Discuss how you chose the proposal\n          variance $\\tau^2$ and the number of iterations $T$ and\n          $T_{\\text{burn-in}}$. How did these choices affect the\n          convergence and mixing of your MCMC chain?\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nimport torch\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport numpy as np\nfrom ece import expected_calibration_error\n\n# Load training and testing data\nx_train = torch.tensor(np.load('../data/differences_train.npy'))\nx_test = torch.tensor(np.load('../data/differences_test.npy'))\ny_train = torch.tensor(np.load('../data/labels_train.npy'))\ny_test = torch.tensor(np.load('../data/labels_test.npy'))\n\n# Likelihood function for logistic regression (per data point)\ndef likelihood(theta, x, y):\n    \"\"\"\n    Computes the likelihood of the data given the logistic regression parameters.\n    \n    Args:\n    - theta (torch.Tensor): Model parameters.\n    - x (torch.Tensor): Input data.\n    - y (torch.Tensor): True labels.\n\n    Returns:\n    - torch.Tensor: Likelihood values for each data point.\n    \"\"\"\n    # YOUR CODE HERE (~3 lines)\n    # Calculate logits as the linear combination of inputs and parameters.\n    # Use the sigmoid function to compute the probability of the positive class.\n    pass\n    # END OF YOUR CODE\n\n# Prior probability (theta ~ N(0, I)) - only depends on theta, not per sample\ndef prior(theta, sigma):\n    \"\"\"\n    Computes the prior probability of theta under a Gaussian distribution with variance sigma^2.\n\n    Args:\n    - theta (torch.Tensor): Model parameters.\n    - sigma (float): Standard deviation of the prior distribution.\n\n    Returns:\n    - torch.Tensor: Prior probability value.\n    \"\"\"\n    # YOUR CODE HERE (~2 lines)\n    # Implement Gaussian prior with zero mean and identity covariance.\n    # Note that the normalization constant is not needed for Metropolis-Hastings.\n    pass\n    # END OF YOUR CODE\n\n# Metropolis-Hastings sampler\ndef metropolis_hastings(x, y, num_samples, burn_in, tau, sigma):\n    \"\"\"\n    Runs the Metropolis-Hastings algorithm to sample from the posterior distribution.\n\n    Args:\n    - x (torch.Tensor): Input data.\n    - y (torch.Tensor): True labels.\n    - num_samples (int): Total number of samples to draw.\n    - burn_in (int): Number of initial samples to discard.\n    - tau (float): Proposal standard deviation.\n    - sigma (float): Prior standard deviation.\n\n    Returns:\n    - torch.Tensor: Collected samples post burn-in.\n    - float: Acceptance ratio.\n    \"\"\"\n    # Initialize theta (starting point of the chain) and containers for samples and acceptance count\n    theta = torch.zeros(x.shape[1])\n    samples = []\n    acceptances = 0\n    \n    # Run the Metropolis-Hastings algorithm\n    for t in tqdm(range(num_samples), desc=\"MCMC Iteration\"):\n        # YOUR CODE HERE (~12-16 lines)\n        # 1. Propose new theta from the proposal distribution (e.g., Gaussian around current theta).\n        # 2. Compute prior and likelihood for current and proposed theta\n        # 3. Calculate the acceptance ratio as the product of likelihood and prior ratios.\n        # 4. Accept or reject the proposal based on the acceptance probability.\n        # 5. Store the sample after the burn-in period\n        pass\n        # END OF YOUR CODE\n    \n    return torch.stack(samples), acceptances / num_samples\n\n# Run Metropolis-Hastings on training data\nnum_samples = 10000\nburn_in = 1000\ntau = 0.01  # Proposal variance (tune this for convergence)\nsigma = 2.0  # Prior variance\n\n# Collect samples and compute acceptance ratio\nsamples, acceptance_ratio = metropolis_hastings(x_train, y_train, num_samples=num_samples, burn_in=burn_in, tau=tau, sigma=sigma)\naveraged_weights = samples.mean(axis=0)\nprint(f'Predicted weights: {averaged_weights}')\nprint(f'Acceptance Ratio: {acceptance_ratio}')\n\n# Evaluate accuracy on training set\ntrain_predictions = (x_train @ averaged_weights > 0).float()\ntrain_acc = (train_predictions == y_train).float().mean()\nprint(f'Train Accuracy: {train_acc}')\n\n# Evaluate accuracy on testing set\ntest_predictions = (x_test @ averaged_weights > 0).float()\nacc = (test_predictions == y_test).float().mean()\nprint(f'Test Accuracy: {acc}')\n\n# Compute expected calibration error on testing set\nexpected_calibration_error(torch.sigmoid(x_test @ averaged_weights).numpy(), y_test.numpy(), model_name=\"Metropolis-Hastings\")\n```\n:::\n\n\n:::\n\n(c) **Parametric Neural Network Model Estimated Using Hamiltonian Monte\n    Carlo (11 points)**\n\n    (i) **(Written, 2 points)**. Explain why Hamiltonian Monte Carlo\n        (HMC) is suitable for sampling from the posterior distribution\n        of neural network parameters compared to Metropolis-Hastings.\n\n    (ii) **(Coding, 7 points)**. Implement HMC to sample from the\n         posterior distribution of the parameters $\\theta$ of a neural\n         network $f(x; \\theta)$ used for preference prediction in\n         `uncertainty_quantification/hmc_nn.py`. This will require a GPU\n         and take around 5 minutes on it!\n\n    (iii) **(Written, 2 points)**. Briefly describe the performance of\n          the HMC and Metropolis-Hastings models and provide the\n          accuracy numbers.\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Use a GPU when running this file! JAX should automatically default to GPU.\nimport jax.numpy as np\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS\nfrom jax import random\nfrom ece import expected_calibration_error\n\n# DO NOT CHANGE! This function can be ignored.\ndef set_numpyro(new_sampler):\n    numpyro.sample = new_sampler\n\n# Define the neural network model with one hidden layer\ndef nn_model(x_data, y_data, hidden_dim=10):\n    \"\"\"\n    Defines a Bayesian neural network with one hidden layer.\n\n    Args:\n    - x_data (np.array): Input data.\n    - y_data (np.array): Target labels.\n    - hidden_dim (int): Number of units in the hidden layer.\n\n    Returns:\n    - hidden_activations: Activations from the hidden layer.\n    - logits: Logits for the output layer.\n    \"\"\"\n    input_dim = x_data.shape[1]\n    \n    # Prior over the weights and biases for the hidden layer\n    w_hidden = numpyro.sample('w_hidden', dist.Normal(np.zeros((input_dim, hidden_dim)), np.ones((input_dim, hidden_dim))))\n    b_hidden = numpyro.sample('b_hidden', dist.Normal(np.zeros(hidden_dim), np.ones(hidden_dim)))\n    \n    # Compute the hidden layer activations using ReLU\n    # YOUR CODE HERE (~1 line)\n    # Implement the hidden layer computation, applying a ReLU activation.\n    pass\n    # END OF YOUR CODE \n    \n    # Prior over the weights and biases for the output layer\n    w_output = numpyro.sample('w_output', dist.Normal(np.zeros(hidden_dim), np.ones(hidden_dim)))\n    b_output = numpyro.sample('b_output', dist.Normal(0, 1))\n    \n    # Compute the logits for the output layer\n    # YOUR CODE HERE (~1 line)\n    # Calculate the logits as the linear combination of hidden activations and output layer weights.\n    pass\n    # END OF YOUR CODE\n\n    # Likelihood (Bernoulli likelihood with logits)\n    numpyro.sample('obs', dist.Bernoulli(logits=logits), obs=y_data)\n    return hidden_activations, logits\n\ndef sigmoid(x):\n    \"\"\"Helper function to compute the sigmoid of x.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\nif __name__ == \"__main__\":\n    # Load training and testing data\n    x_train = np.load('../data/differences_train.npy')\n    x_test = np.load('../data/differences_test.npy')\n    y_train = np.load('../data/labels_train.npy')\n    y_test = np.load('../data/labels_test.npy')\n\n    # HMC Sampler Configuration\n    hmc_kernel = NUTS(nn_model)\n\n    # Running HMC with the MCMC interface in NumPyro\n    num_samples = 200  # Number of samples\n    warmup_steps = 100  # Number of burn-in steps\n    rng_key = random.PRNGKey(0)  # Random seed\n\n    # MCMC object with HMC kernel\n    mcmc = MCMC(hmc_kernel, num_samples=num_samples, num_warmup=warmup_steps)\n    mcmc.run(rng_key, x_train, y_train)\n\n    # Get the sampled weights (theta samples)\n    samples = mcmc.get_samples()\n\n    # Extract the weight samples\n    w_hidden_samples = samples['w_hidden']\n    b_hidden_samples = samples['b_hidden']\n    w_output_samples = samples['w_output']\n    b_output_samples = samples['b_output']\n\n    # Compute the averaged weights and biases\n    w_hidden_mean = np.mean(w_hidden_samples, axis=0)\n    b_hidden_mean = np.mean(b_hidden_samples, axis=0)\n    w_output_mean = np.mean(w_output_samples, axis=0)\n    b_output_mean = np.mean(b_output_samples, axis=0)\n\n    # Forward pass through the network for testing set\n    # YOUR CODE HERE (~2 lines)\n    # Compute hidden layer activations and logits for the test set using the mean weights and biases.\n    pass\n    # END OF YOUR CODE\n    test_predictions = test_logits > 0\n    test_accuracy = np.mean(test_predictions == y_test)\n    print(f'Test Accuracy: {test_accuracy}')\n\n    # Forward pass through the network for training set\n    # YOUR CODE HERE (~2 lines)\n    # Compute hidden layer activations and logits for the training set.\n    pass\n    # END OF YOUR CODE\n    train_predictions = train_logits > 0\n    train_accuracy = np.mean(train_predictions == y_train)\n    print(f'Train Accuracy: {train_accuracy}')\n\n    # Compute expected calibration error on testing set\n    expected_calibration_error(sigmoid(test_logits), y_test, model_name=\"HMC\")\n```\n:::\n\n\n:::\n\n(d) **Non-Parametric Model with Gaussian Process (GP) (7 points)**\n\n    (i) **(Written, 2 point)**. Describe how a Gaussian Process can be\n        used for preference learning in this context (i.e., describe how\n        the latent function is used for classification).\n\n    (ii) **(Coding, 2 points)**. Run the GP classification for\n         preference learning code in\\\n         `uncertainty_quantification/gaussian_process.py` and provide\n         the accuracy numbers. This can only be run on a CPU and may\n         take around 10 minutes to complete.\n\n    (iii) **(Written, 3 point)**. Discuss the computational complexity\n          of the GP model compared to the parametric models. What are\n          the advantages and disadvantages of using a GP in this\n          setting?\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.metrics import accuracy_score\nfrom ece import expected_calibration_error\n\nx_train = np.load('../data/differences_train.npy')\nx_test = np.load('../data/differences_test.npy')\ny_train = np.load('../data/labels_train.npy')\ny_test = np.load('../data/labels_test.npy')\n\nkernel = 1.0 * RBF(length_scale=1.0)\ngp_classifier = GaussianProcessClassifier(kernel=kernel, random_state=42, n_jobs=-1)\ngp_classifier.fit(x_train, y_train)\n\ny_pred_probs = gp_classifier.predict_proba(x_test)[:, 1]\ny_pred_labels = (y_pred_probs > 0.5)\n\ntrain_accuracy = accuracy_score(y_train, gp_classifier.predict(x_train))\nprint(f'Train Accuracy: {train_accuracy:.4f}')\n\ntest_accuracy = accuracy_score(y_test, y_pred_labels)\nprint(f'Test Accuracy: {test_accuracy:.4f}')\n\nexpected_calibration_error(y_pred_probs, y_test, model_name=\"Gaussian Process Classifier\")\n```\n:::\n\n\n:::\n\n### Question 2: Active Learning for Preference Learning (40 points) {#sec-question-2-active-learning-for-preference-learning-40-points .unnumbered}\n\nIn this question, you will explore active learning strategies for\npreference learning using a linear model. We will use expected\ninformation gain as the acquisition function to select the most\ninformative queries, where each query is a pair of items. Assume that we\nmodel the preferences using a simple linear model. Given feature vectors\n$x_1$ and $x_2$ corresponding to two items, the probability that $x_1$\nis preferred over $x_2$ is modeled using a logistic regression model,\ni.e.,\n\n$$P(x_1 \\succ x_2 | \\theta) = \\sigma(\\theta^\\top (x_1 - x_2)),$$\n\nwhere $\\theta \\in \\mathbb{R}^d$ is the model parameter vector, and\n$\\sigma(z)$ is the sigmoid function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$.\nThe goal is to sequentially select pairs of items to maximize the\ninformation gained about $\\theta$ through preference queries.\n\n(a) **Expected Information Gain (15 points)**\n\n    (i) **Derive the Expected Information Gain (Written, 3 points).**\n        Suppose that after observing a preference between two items\n        $x_1$ and $x_2$, the posterior distribution over $\\theta$ is\n        updated. The information gain from this observation is the\n        reduction in uncertainty about $\\theta$ measured using the\n        Kullback-Leibler (KL) divergence between the prior and posterior\n        distributions. Given the current posterior distribution\n        $P(\\theta | \\mathcal{D})$ and a possible observation\n        $y \\in \\{0, 1\\}$ (where $y = 1$ if $x_1$ is preferred over\n        $x_2$, and $y = 0$ otherwise), the expected information gain is: $$\\begin{aligned}\n    \\mathbb{E}[\\text{IG}(x_1, x_2)] = &P(y=1 | x_1, x_2, \\theta) D_{\\text{KL}}\\left( P(\\theta | y = 1, \\mathcal{D}) \\parallel P(\\theta | \\mathcal{D}) \\right) \\\\+ \n    &P(y=0 | x_1, x_2, \\theta) D_{\\text{KL}}\\left( P(\\theta | y = 0, \\mathcal{D}) \\parallel P(\\theta | \\mathcal{D}) \\right)\n\\end{aligned}$$\n\n        Derive this expression for the expected information gain of\n        selecting the pair $(x_1, x_2)$ for a preference query. Start by\n        explaining how the KL divergence measures the information gain,\n        and break down the expectation over the possible outcomes of the\n        query.\n\n    (ii) **Simplifying the KL Divergence (Written, 4 points).** Assuming\n         the prior and posterior distributions over $\\theta$ are\n         Gaussian (i.e., $P(\\theta) \\sim \\mathcal{N}(\\mu, \\Sigma)$ and\n         $P(\\theta | \\mathcal{D}) \\sim \\mathcal{N}(\\mu', \\Sigma')$),\n         show that the KL divergence between the Gaussian posterior and\n         prior simplifies to: $$\\begin{aligned}\n        D_{\\text{KL}}\\left( \\mathcal{N}(\\mu', \\Sigma') \\parallel \\mathcal{N}(\\mu, \\Sigma) \\right) &= \\frac{1}{2} \\left( \\text{tr}(\\Sigma^{-1} \\Sigma') + (\\mu' - \\mu)^\\top \\Sigma^{-1} (\\mu' - \\mu)\\right.\\\\\n        &\\left.- d + \\log\\left( \\frac{\\det(\\Sigma)}{\\det(\\Sigma')} \\right) \\right).\n        \\end{aligned}$$\n\n    (iii) **Approximate Information Gain for a Linear Model (Written, 4\n          points).** In the case of a linear model with Gaussian priors\n          on $\\theta$, assume that the posterior distribution\n          $P(\\theta | \\mathcal{D}) \\sim \\mathcal{N}(\\mu, \\Sigma)$ is\n          updated using Bayes' rule after each observation. The\n          likelihood of observing a preference $y$ is logistic, which\n          does not conjugate with the Gaussian prior. However, for the\n          purposes of this question, assume that after each query, the\n          posterior mean $\\mu'$ and covariance $\\Sigma'$ can be updated\n          using an approximation method such as Laplace's approximation.\n\n          Using these assumptions, compute the expected information gain\n          for a specific query $(x_1, x_2)$ in closed form. You may\n          express the information gain in terms of the updated mean\n          $\\mu'$ and covariance $\\Sigma'$ after observing the preference\n          outcome.\n\n    (iv) **Laplace Approximation for Posterior (Written, 4 points).**\n         The Laplace approximation for the posterior is given by $$\\begin{aligned}\n    \\mu'=\\arg \\min_\\theta -\\log P(\\theta | \\mathcal{D})\\\\\n    \\Sigma'^{-1}=\\nabla_\\theta\\nabla_\\theta -\\log P(\\theta|\\mathcal{D})|_{\\theta=\\mu'}\n\\end{aligned}$$ \n         In our scenario with the Bradley-Terry model\n         for likelihood, simplify $-\\log P(\\theta | \\mathcal{D})$ and\n         its Hessian ignoring the normalization constant.\n\n(b) **Active Learning Algorithm (25 points)** In this section, you will\n    implement an active learning algorithm for selecting the most\n    informative queries using the expected information gain criterion.\n\n    (i) **(Coding, 4 points).** Implement `kl_divergence_gaussians` in\n        `active_learning/main.py`.\n\n    (ii) **(Coding, 4 points).** Following your derived Laplace\n         approximation, implement `negative_log_posterior`.\n\n    (iii) **(Coding, 4 points).** Implement `compute_hessian` that is\n          used to obtain the inverse of the covariance matrix.\n\n    (iv) **(Coding, 3 points).** Implement `expected_information_gain`.\n\n    (v) **(Coding, 4 points).** Finally, implement `active_learning`.\n\n    (vi) **(Coding + Written, 6 points).** Plot the $L^2$ norm of the\n         covariance matrix for each loop of the active learning loop.\n         Additionally, on the same plot, implement a random baseline and\n         plot its $L^2$ covariance matrix norm. The random baseline\n         should randomly select a point in the dataset and not use any\n         acquisition function. Interpret your plot and use it to compare\n         the two methods.\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nimport torch\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\n\nclass LogisticActiveLearning:\n    def __init__(self, test_size=0.2):\n        \"\"\"\n        Initializes LogisticActiveLearning model, sets device, and prepares data.\n        \n        Args:\n        - test_size (float): Proportion of the dataset used for validation.\n        \"\"\"\n        # Make device customizable\n        self.device = torch.device(\"cpu\")\n        X, y = make_classification(n_samples=10000, random_state=42)\n\n        # Convert data and labels to tensors\n        x_data = torch.tensor(X, dtype=torch.float32).to(self.device)\n        y_data = torch.tensor(y, dtype=torch.float32).to(self.device)\n        self.N, self.D = x_data.shape\n\n        # Split into training and validation sets\n        train_indices, val_indices = train_test_split(range(self.N), test_size=test_size, random_state=42)\n        self.x_train = x_data[train_indices]\n        self.y_train = y_data[train_indices]\n        self.x_val = x_data[val_indices]\n        self.y_val = y_data[val_indices]\n\n        # Initialize mean and inverse covariance for the prior\n        self.weights_mean = torch.zeros(self.D, requires_grad=True, device=self.device)\n        self.weights_inv_cov = torch.eye(self.D).to(self.device)  # Start with identity inverse covariance\n\n    def negative_log_posterior(self, w, x, y):\n        \"\"\"\n        Computes the negative log-posterior (negative log-prior + log-likelihood).\n        \n        Args:\n        - w (torch.Tensor): Model weights.\n        - x (torch.Tensor): Input data point.\n        - y (torch.Tensor): True label.\n        \n        Returns:\n        - torch.Tensor: Negative log-posterior value.\n        \"\"\"\n        # YOUR CODE HERE (~4-6 lines)\n        # Compute log-prior term using inverse covariance\n        pass\n        # END OF YOUR CODE\n\n    def optimize_weights(self, w, x, y, num_steps=50, lr=1e-2):\n        \"\"\"\n        Optimizes weights using Adam optimizer.\n        \n        Args:\n        - w (torch.Tensor): Initial weights.\n        - x (torch.Tensor): Input data point.\n        - y (torch.Tensor): True label.\n        - num_steps (int): Number of optimization steps.\n        - lr (float): Learning rate.\n        \n        Returns:\n        - torch.Tensor: Updated weights.\n        - torch.Tensor: Hessian inverse covariance.\n        \"\"\"\n        optimizer = Adam([w], lr=lr)\n        \n        for step in range(num_steps):\n            optimizer.zero_grad()\n            loss = self.negative_log_posterior(w, x, y)\n            loss.backward()\n            optimizer.step()\n\n        # Compute the Hessian of log-posterior, serving as inverse covariance\n        inv_cov = self.compute_hessian(w.detach(), x, y)\n        return w.detach().clone(), inv_cov\n\n    def compute_hessian(self, w, x, y):\n        \"\"\"\n        Computes the Hessian of the negative log-posterior, used as the inverse covariance.\n        \n        Args:\n        - w (torch.Tensor): Model weights.\n        - x (torch.Tensor): Input data point.\n        - y (torch.Tensor): True label.\n        \n        Returns:\n        - torch.Tensor: Hessian of the negative log-posterior.\n        \"\"\"\n        # YOUR CODE HERE (~5-8 lines)\n        # Hessian of the prior term\n        pass\n        # END OF YOUR CODE\n\n    def acquisition_fn(self, x):\n        \"\"\"\n        Computes posterior means and inverse covariances for y=1 and y=0 without modifying original parameters.\n        \n        Args:\n        - x (torch.Tensor): Input data point.\n        \n        Returns:\n        - dict: Posterior properties for y=1 and y=0 cases.\n        \"\"\"\n        weights_y1 = self.weights_mean.clone().detach().requires_grad_(True)\n        weights_y0 = self.weights_mean.clone().detach().requires_grad_(True)\n\n        # Optimize weights and get Hessian for both y=1 and y=0 cases\n        posterior_mean_y1, inv_cov_y1 = self.optimize_weights(weights_y1, x, 1, num_steps=50)\n        posterior_mean_y0, inv_cov_y0 = self.optimize_weights(weights_y0, x, 0, num_steps=50)\n\n        # Calculate probabilities for the acquisition function\n        prob_y1 = torch.sigmoid(torch.dot(self.weights_mean.detach(), x))\n        prob_y0 = 1 - prob_y1\n\n        return {\n            'prob_y1': prob_y1,\n            'prob_y0': prob_y0,\n            'posterior_mean_y1': posterior_mean_y1,\n            'posterior_inv_cov_y1': inv_cov_y1,\n            'posterior_mean_y0': posterior_mean_y0,\n            'posterior_inv_cov_y0': inv_cov_y0\n        }\n\n    def expected_information_gain(self, x):\n        \"\"\"\n        Computes expected information gain for a given point `x`.\n        \n        Args:\n        - x (torch.Tensor): Input data point.\n        \n        Returns:\n        - torch.Tensor: Expected Information Gain (EIG) value.\n        \"\"\"\n        acquisition = self.acquisition_fn(x)\n\n        # Compute KL divergences for y=1 and y=0 using inverse covariances\n        kl_y1 = kl_divergence_gaussians(\n            acquisition['posterior_mean_y1'],\n            acquisition['posterior_inv_cov_y1'],\n            self.weights_mean.detach(),\n            self.weights_inv_cov\n        )\n\n        kl_y0 = kl_divergence_gaussians(\n            acquisition['posterior_mean_y0'],\n            acquisition['posterior_inv_cov_y0'],\n            self.weights_mean.detach(),\n            self.weights_inv_cov\n        )\n\n        # Expected Information Gain (EIG)\n        eig = None # YOUR CODE HERE (1 line)\n        return eig\n\n    def active_learning(self, selected_indices, subset_size=50):\n        \"\"\"\n        Active learning loop that selects the most informative data point based on EIG.\n        \n        Args:\n        - selected_indices (list): Indices of previously selected samples.\n        - subset_size (int): Number of samples to consider in each subset.\n\n        Returns:\n        - best_x, best_x_idx, best_acquisition: Selected data point and acquisition details.\n        \"\"\"\n        best_eig = -float('inf')\n        best_x = None\n        best_x_idx = -1\n        best_acquisition = None\n\n        subset_indices = [i for i in torch.randperm(len(self.x_train)).tolist() if i not in selected_indices][:subset_size]\n\n        # YOUR CODE HERE (~ 10 lines)\n        pass\n        # END OF YOUR CODE\n        return best_x, best_x_idx, best_acquisition\n\n    def validate(self):\n        \"\"\"\n        Computes accuracy on the validation set by predicting labels and comparing to true labels.\n        \n        Returns:\n        - float: Validation accuracy.\n        \"\"\"\n        with torch.no_grad():\n            logits = self.x_val @ self.weights_mean\n            predictions = torch.sigmoid(logits) >= 0.5  # Convert logits to binary predictions\n            accuracy = (predictions == self.y_val).float().mean().item()\n            print(f\"Validation accuracy: {accuracy * 100:.2f}%\")\n        return accuracy\n\n    def train(self, num_iterations=10, subset_size=50):\n        \"\"\"\n        Train the model using active learning with subset sampling.\n        \n        Args:\n        - num_iterations (int): Number of active learning iterations.\n        - subset_size (int): Number of samples to consider in each subset.\n        \"\"\"\n        selected_indices = []\n        for iteration in range(num_iterations):\n            print(f\"Iteration {iteration + 1}/{num_iterations}\")\n\n            # Select the most informative data point from a random subset\n            best_x, best_x_idx, acquisition = self.active_learning(selected_indices, subset_size=subset_size)\n            selected_indices.append(best_x_idx)\n            print(f\"Selected data point with EIG.\")\n\n            # Get the true label for the selected data point\n            y = self.y_train[best_x_idx].item()\n\n            # Update posterior mean and inverse covariance based on true label\n            if y == 1:\n                self.weights_mean = acquisition['posterior_mean_y1']\n                self.weights_inv_cov = acquisition['posterior_inv_cov_y1']\n            else:\n                self.weights_mean = acquisition['posterior_mean_y0']\n                self.weights_inv_cov = acquisition['posterior_inv_cov_y0']\n\n            print(f\"Covariance L2: {torch.inverse(self.weights_inv_cov).norm()}\")\n\n            # Validate model performance on the validation set\n            self.validate()\n\n# KL divergence between two multivariate normal distributions\ndef kl_divergence_gaussians(mu1, sigma1_inv, mu2, sigma2_inv):\n    \"\"\"\n    Computes the KL divergence between two multivariate Gaussian distributions.\n    \n    Args:\n    - mu1, mu2 (torch.Tensor): Mean vectors of the distributions.\n    - sigma1_inv, sigma2_inv (torch.Tensor): Inverse covariance matrices of the distributions. PLEASE NOTE THE INVERSE!\n    \n    Returns:\n    - torch.Tensor: KL divergence value.\n    \"\"\"\n    # YOUR CODE HERE (~ 9-12 lines)\n    pass\n    # END OF YOUR CODE\n\n# Example usage\nmodel = LogisticActiveLearning()\nmodel.train(num_iterations=100, subset_size=50)\n```\n:::\n\n\n:::\n\n### Question 3: Linear Performance Metric Elicitation (30 points) {#sec-question-3-linear-performance-metric-elicitation-30-points .unnumbered}\n\n1.  **(Written, 10 points).** For background on the problem setting,\n    read <https://tinyurl.com/3b92sufm>. Suppose we have a linear\n    performance metric given by $$p(C) = 1-\\alpha (FP)-\\beta (FN)$$\n    where $C$ is a confusion matrix and $FP, FN$ denote false positive\n    and false negative rates. We wish to find the optimal classifier\n    w.r.t. $p$. That is, $$\\phi^* = \\arg \\max_{\\phi\\in\\Phi} p(C(\\phi))$$\n    where $\\Phi$ is the space of all probabilistic binary classifiers\n    from $X\\to [0, 1]$. Note that these classifiers return probabilities\n    corresponding to the label $1$. Show that $\\phi^*$ is in fact\n    deterministic and given by $$\\phi(x)=\\begin{cases}\n        1 & \\text{if } p(y|x) > f(\\alpha,\\beta) \\\\\n        0 & \\text{otherwise}.\n    \\end{cases}$$ for a threshold function $f$ that you must find.\n    (Hint: For a classifier $\\phi$, $FP=P(\\phi=1, y=0)$ and\n    $FN=P(\\phi=0, y=1)$. Marginalize these joint probabilities over $x$\n    and simplify.)\n\n2.  **(Written + Coding, 5 points).** Implement `classifier_metrics` in\n    `lpme/main.py`. After doing so, run `plot_confusion_region` and\n    attach the plot. What do you notice about the region of possible\n    confusion matrices?\n\n3.  **(Coding, 15 points).** Implement `search_theta` in order to elicit\n    the metric used by the oracle (which is parametrized by $\\theta$).\n    Play around with the oracle's theta and run `start_search` to see\n    how close you can approximate it!\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nimport torch\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nclass DataDistribution:\n    def __init__(self, N: int):\n        \"\"\"\n        Initializes the data distribution with a specified number of samples.\n        \n        Args:\n        - N (int): Number of data points.\n        \"\"\"\n        self.weights = torch.tensor([-0.3356, -1.4104, 0.3144, -0.5591, 1.0426, 0.6036, -0.7549, -1.1909, 1.4779, -0.7513])\n        self.D = len(self.weights)\n\n        gen = torch.Generator().manual_seed(42)\n        self.data = torch.randn(N, self.D, generator=gen)\n        self.probs = torch.sigmoid(self.data @ self.weights)\n    \ndef classifier_metrics(data_dist, threshold, upper=True):\n    \"\"\"\n    Computes the True Positive and True Negative rates based on a classifier threshold.\n    \n    Args:\n    - data_dist (DataDistribution): The data distribution instance.\n    - threshold (float): Threshold value for classification.\n    - upper (bool): If True, classifies as positive if above threshold; else, if below.\n    \n    Returns:\n    - tuple (float, float): True Positive Rate (TP) and True Negative Rate (TN) in that order.\n    \"\"\"\n    # YOUR CODE HERE (~3-5 lines)\n    pass\n    # END OF YOUR CODE\n\ndef sweep_classifiers(data_dist: DataDistribution):\n    \"\"\"\n    Sweeps through classifier thresholds and calculates True Positive and True Negative rates.\n    \n    Args:\n    - data_dist (DataDistribution): The data distribution instance.\n    \n    Returns:\n    - tuple: Upper and lower boundary data for True Positive and True Negative rates.\n    \"\"\"\n    thresholds = torch.linspace(0, 1, 100)\n    upper_boundary = []\n    lower_boundary = []\n    \n    for threshold in tqdm(thresholds, desc=\"Thresholds\"):\n        tp_upper, tn_upper = classifier_metrics(data_dist, threshold, upper=True)\n        upper_boundary.append((tp_upper, tn_upper))\n\n        tp_lower, tn_lower = classifier_metrics(data_dist, threshold, upper=False)\n        lower_boundary.append((tp_lower, tn_lower))\n\n    return upper_boundary, lower_boundary\n\nclass Oracle:\n    def __init__(self, theta: float):\n        \"\"\"\n        Initializes the oracle with a given theta for preference evaluation.\n        \n        Args:\n        - theta (float): Oracle angle in radians.\n        \"\"\"\n        self.theta = torch.tensor(theta)\n\n    def evaluate_lpm(self, tp, tn):\n        \"\"\"\n        Computes the linear performance metric (LPM) based on theta.\n        \n        Args:\n        - tp (float): True Positive rate.\n        - tn (float): True Negative rate.\n        \n        Returns:\n        - float: Linear performance metric evaluation.\n        \"\"\"\n        return torch.cos(self.theta) * tp + torch.sin(self.theta) * tn\n    \n    def preferred_classifier(self, tp_1, tn_1, tp_2, tn_2):\n        \"\"\"\n        Determines the preferred classifier based on LPM values.\n        \n        Args:\n        - tp_1, tn_1, tp_2, tn_2 (float): True Positive and True Negative rates for two classifiers.\n        \n        Returns:\n        - bool: True if first classifier is preferred, False otherwise.\n        \"\"\"\n        lpm_1 = self.evaluate_lpm(tp_1, tn_1)\n        lpm_2 = self.evaluate_lpm(tp_2, tn_2)\n        return (lpm_1 > lpm_2).item()\n    \ndef theta_to_threshold(theta):\n    \"\"\"Converts theta angle to classification threshold.\"\"\"\n    return 1 / (1 + torch.tan(theta) ** -1)\n\ndef search_theta(oracle: Oracle, data_dist, lower_bound, upper_bound):\n    \"\"\"\n    Performs a search over theta values to optimize the classification threshold.\n    \n    Args:\n    - oracle (Oracle): The oracle for LPM evaluation.\n    - data_dist (DataDistribution): The data distribution instance.\n    - lower_bound (float): Lower bound for theta.\n    - upper_bound (float): Upper bound for theta.\n    \n    Returns:\n    - tuple: Updated lower and upper bounds for theta.\n    \"\"\"\n    left = 0.75 * lower_bound + 0.25 * upper_bound\n    middle = 0.5 * lower_bound + 0.5 * upper_bound\n    right = 0.25 * lower_bound + 0.75 * upper_bound\n\n    thetas = [lower_bound, left, middle, right, upper_bound]\n    thresholds = theta_to_threshold(torch.tensor(thetas))\n    new_lower, new_upper = None, None\n\n    # YOUR CODE HERE (~18-25 lines)\n    # 1. Collect metrics for each threshold value.\n    # 2. Determine if LPM increases as theta increases.\n    # 3. Check for pattern of increases and decreases in LPM.\n    # 4. Update bounds based on observed LPM patterns.\n    pass\n    # END OF YOUR CODE\n\n    return new_lower, new_upper\n\n# Create instance and get upper & lower boundary data\ndata_dist = DataDistribution(N=10000000)\noracle = Oracle(theta=0.1)\n\ndef plot_confusion_region():\n    \"\"\"\n    Plots the True Positive vs. True Negative rates for the upper and lower classifier boundaries.\n    \"\"\"\n    upper_boundary, lower_boundary = sweep_classifiers(data_dist)\n\n    # Prepare data for plotting for upper and lower boundaries\n    tp_upper, tn_upper = zip(*upper_boundary)\n    tp_lower, tn_lower = zip(*lower_boundary)\n\n    # Plot the results for upper boundary\n    plt.figure(figsize=(8, 6))\n    plt.plot(tp_upper, tn_upper, marker='o', linestyle='-', alpha=0.7, label=\"Upper Boundary\")\n    plt.plot(tp_lower, tn_lower, marker='o', linestyle='--', alpha=0.7, label=\"Lower Boundary\")\n    plt.title(\"True Positive vs. True Negative Rates (Upper & Lower Boundaries)\")\n    plt.xlabel(\"True Positive Rate (TP)\")\n    plt.ylabel(\"True Negative Rate (TN)\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef start_search():\n    \"\"\"\n    Starts the theta search using the LPM-based oracle and prints the search range per iteration.\n    \"\"\"\n    lower_bound = 0\n    upper_bound = torch.pi / 2\n    for _ in tqdm(range(10), desc=\"LPM Search\"):\n        print(f\"Theta Search Space: [{lower_bound}, {upper_bound}]\")\n        lower_bound, upper_bound = search_theta(oracle, data_dist, lower_bound=lower_bound, upper_bound=upper_bound)\n    print(f\"Theta Search Space: [{lower_bound}, {upper_bound}]\")\n```\n:::\n\n\n:::\n\n### Question 4: D-optimal Design with Logistic Model (30 points) {#sec-question-4-d-optimal-design-with-logistic-model-30-points .unnumbered}\n\nIn this question, we explore D-optimal designs in the context of the\nBradley-Terry model. The Bradley-Terry model is a logistic regression\nmodel used for paired comparison data. Given two items $x_1$ and $x_2$,\nthe probability that item $x_1$ is preferred over $x_2$ is modeled as:\n\n$$P(x_1 \\succ x_2 | \\theta) = \\frac{e^{\\theta^\\top x_1}}{e^{\\theta^\\top x_1} + e^{\\theta^\\top x_2}} = \\frac{1}{1 + e^{\\theta^\\top (x_2 - x_1)}}$$\n\nwhere $\\theta \\in \\mathbb{R}^d$ represents the unknown model parameters,\nand $x_1, x_2 \\in \\mathbb{R}^d$ are the feature vectors associated with\nthe two items. D-optimal design aims to maximize the determinant of the\nFisher information matrix, thus minimizing the volume of the confidence\nellipsoid for the estimated parameters. In this exercise, you will\nanalyze D-optimal designs for this model.\n\n(a) **Fisher Information Matrix for the Bradley-Terry Model (12\n    points)**\n\n    (i) **(Written, 6 points).** Derive the Fisher information matrix\n        for the Bradley-Terry model at a design point $(x_1, x_2)$. Show\n        that the Fisher information matrix at a design point is:\n        $$I(x_1, x_2, \\theta) = w(x_1, x_2, \\theta) (x_1 - x_2)(x_1 - x_2)^\\top,$$\n        where $w(x_1, x_2, \\theta)$ is a weight function given by:\n        $$w(x_1, x_2, \\theta) = \\frac{e^{\\theta^\\top x_1} e^{\\theta^\\top x_2}}{\\left(e^{\\theta^\\top x_1} + e^{\\theta^\\top x_2}\\right)^2} =\\sigma'(\\theta^\\top (x_1-x_2)).$$\n        $\\sigma'$ is the derivative of the sigmoid function.\n\n    (ii) **(Coding, 6 points).** Implement `fisher_matrix` in\n         `d_optimal/main.py` based on the derived expression.\n\n(b) **D-optimal Design Criterion (18 points)**\n\n    (i) **(Coding, 11 points).** In the context of the Bradley-Terry\n        model, a D-optimal design maximizes the determinant of the\n        Fisher information matrix. Suppose we have a set of candidate\n        items $\\{x_1, \\dots, x_n\\}$, and we can choose $N$ comparisons\n        to make. Formally, the D-optimal design maximizes:\n        $$\\det\\left( \\sum_{i=1}^N w(x_{i1}, x_{i2}, \\theta) (x_{i1} - x_{i2})(x_{i1} - x_{i2})^\\top \\right),$$\n        where $(x_{i1}, x_{i2})$ denotes a pair of compared items in the\n        design. Implement a greedy algorithm to approximate the\n        D-optimal design. Given a set of $n$ items and their feature\n        vectors $\\{x_1, \\dots, x_n\\}$, your task is to iteratively\n        select the pair of items $(x_{i1}, x_{i2})$ that maximizes the\n        determinant of the Fisher information matrix. Please implement\n        `greedy_fisher`. Note that the setup in the code assumes we have\n        a dataset of all possible differences between pairs of items as\n        opposed to directly selecting the pairs.\n\n    (ii) **(Written + Coding, 7 points).** Notice that\n         `posterior_inv_cov` uses a Laplace approximation for the\n         posterior centered around the ground truth weights after\n         labeling the chosen points. However, it turns out this\n         approximation doesn't actually depend on the labels when taking\n         the Hessian. Please run the file `d_optimal/main.py` and attach\n         a plot of the norm of the covariance matrix of the posterior.\n         What difference do you observe between greedy and random\n         sampling? What is the win rate of greedy?\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\ndef sigmoid(x):\n    \"\"\"Helper function to compute the sigmoid of x.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\nclass LogisticData:\n    def __init__(self, weights, seed=42):\n        \"\"\"\n        Initializes the LogisticData class with specified weights and seed.\n        \n        Args:\n        - weights (np.array): True weights for data generation.\n        - seed (int): Random seed for reproducibility.\n        \"\"\"\n        self.rng = np.random.default_rng(seed)\n        self.weights = weights\n    \n    def generate_data(self, N):\n        \"\"\"\n        Generates synthetic data for logistic regression.\n        \n        Args:\n        - N (int): Number of data points.\n        \n        Returns:\n        - tuple: Generated data and labels.\n        \"\"\"\n        data = self.rng.standard_normal((N, len(self.weights)))\n        probs = sigmoid(data @ self.weights)\n        labels = (self.rng.random(N) < probs).astype(int)\n        return data, labels\n\ndef fisher_matrix(difference_vector, weights):\n    \"\"\"\n    Computes the Fisher information matrix for a single data point.\n    \n    Args:\n    - difference_vector (np.array): Difference vector (input data point).\n    - weights (np.array): Weights for the logistic model.\n    \n    Returns:\n    - np.array: Fisher information matrix for the data point.\n    \"\"\"\n    # YOUR CODE HERE (~2-4 lines)\n    pass\n    # END OF YOUR CODE\n\n# Initialization\ntrue_weights = np.array([-0.3356, -1.4104, 0.3144, -0.5591, 1.0426, 0.6036, -0.7549, -1.1909, 1.4779, -0.7513])\ndata_dim = len(true_weights)\ndataset_generator = LogisticData(weights=true_weights)\n\n# Number of iterations for sampling 500 points\nnum_iterations = 200\n\n# Store covariance matrix norms for comparison\ncov_norms_greedy = []\ncov_norms_random = []\n\ndef greedy_fisher(data, curr_fisher_matrix, selected_indices):\n    \"\"\"\n    Selects the data point that maximizes the Fisher information determinant.\n    \n    Args:\n    - data (np.array): The data matrix.\n    - curr_fisher_matrix (np.array): Fisher matrix of already selected indices.\n    - selected_indices (list): List of already selected indices.\n    \n    Returns:\n    - int: Index of the selected data point.\n    \"\"\"\n    best_det = -np.inf\n    best_index = -1\n    \n    # Iterate over data points to find the one maximizing Fisher determinant.\n    for i, difference_vector in enumerate(data):\n        # YOUR CODE HERE (~5-10 lines)\n        # Make sure to skip already selected data points!\n        pass\n        # END OF YOUR CODE\n    return best_index\n\ndef posterior_inv_cov(X, laplace_center):\n    \"\"\"\n    Computes the posterior inverse covariance matrix using Laplace approximation.\n    \n    Args:\n    - X (np.array): Data matrix.\n    - laplace_center (np.array): Center point (weights).\n    \n    Returns:\n    - np.array: Posterior inverse covariance matrix.\n    \"\"\"\n    # Calculate probabilities for logistic regression model.\n    probs = sigmoid(X @ laplace_center)\n    W = np.diag(probs * (1 - probs))\n    \n    # Compute inverse covariance matrix assuming standard Gaussian prior.\n    inv_cov = X.T @ W @ X + np.eye(len(true_weights))\n    return inv_cov\n\nfor _ in tqdm(range(num_iterations)):\n    # Generate a new sample of 500 data points\n    data, _ = dataset_generator.generate_data(N=500)\n    \n    # Greedy selection of best 30 data points\n    selected_indices = []\n    curr_fisher_matrix = np.zeros((data_dim, data_dim))\n\n    for _ in range(30):\n        # Select the data point maximizing Fisher information determinant.\n        best_index = greedy_fisher(data, curr_fisher_matrix, selected_indices)\n        selected_indices.append(best_index)\n        curr_fisher_matrix += fisher_matrix(data[best_index], true_weights)\n\n    # Prepare greedy and random samples\n    X_greedy = data[selected_indices]\n\n    # Generate 30 random samples for comparison\n    random_indices = np.random.choice(len(data), 30, replace=False)\n    X_random = data[random_indices]\n\n    # Compute posterior inverse covariance matrices for both strategies\n    posterior_inv_cov_greedy = posterior_inv_cov(X_greedy, laplace_center=true_weights) \n    posterior_inv_cov_random = posterior_inv_cov(X_random, laplace_center=true_weights)\n\n    # Calculate covariance matrices (inverse of posterior inverse covariance)\n    cov_matrix_greedy = np.linalg.inv(posterior_inv_cov_greedy)\n    cov_matrix_random = np.linalg.inv(posterior_inv_cov_random)\n\n    # Measure the norm (Frobenius norm) of the covariance matrices\n    cov_norm_greedy = np.linalg.norm(cov_matrix_greedy, 'fro')\n    cov_norm_random = np.linalg.norm(cov_matrix_random, 'fro')\n\n    # Store norms for analysis\n    cov_norms_greedy.append(cov_norm_greedy)\n    cov_norms_random.append(cov_norm_random)\n\n# Display comparison results\nprint(f'Greedy mean: {np.mean(cov_norms_greedy)}')\nprint(f'Random mean: {np.mean(cov_norms_random)}')\nprint(f'Greedy win rate: {(np.array(cov_norms_greedy) < np.array(cov_norms_random)).mean()}')\n\n# Plot the distributions of covariance matrix norms\nplt.hist(cov_norms_greedy, bins=30, alpha=0.7, color='blue', label='Greedy')\nplt.hist(cov_norms_random, bins=30, alpha=0.7, color='red', label='Random')\nplt.xlabel('L2 Norm of Covariance Matrix')\nplt.ylabel('Frequency')\nplt.title('Comparison of Covariance Norms (Greedy vs. Random) Across Iterations')\nplt.legend()\nplt.show()\n```\n:::\n\n\n:::\n\n### Question 5: Nonparametric Metric Elicitation (30 points) {#sec-question-5-nonparametric-metric-elicitation-30-points .unnumbered}\n\nIn this question, we explore the problem of performance metric\nelicitation using a Gaussian Process (GP) to map the elements of the\nconfusion matrix, specifically false positives (FP) and false negatives\n(FN), to an unknown performance metric. The goal is to learn a\nnon-linear function that maps FP and FN to the metric, using relative\npreferences from pairwise classifier comparisons. We will use elliptical\nslice sampling for posterior inference.\n\n(a) **Gaussian Process for Metric Elicitation (10 points)**\n\n    (i) **(Written, 2 points).** Assume that the performance metric\n        $\\phi(C)$ is a non-linear function of the confusion matrix $C$.\n        For simplicity, assume that $\\phi$ depends only on FP and FN,\n        i.e., $$\\phi(\\text{FP}, \\text{FN}) \\sim \\mathcal{GP}(0, k((\\text{FP}, \\text{FN}), (\\text{FP}', \\text{FN}'))),$$ \n        where $k$ is the covariance kernel function of\n        the Gaussian Process. Explain why using a GP allows for flexible\n        modeling of the metric $\\phi$ as a non-linear function of FP and\n        FN. What are the advantages of using a GP over a linear model in\n        this context?\n\n    (ii) **(Written, 2 points).** Suppose we observe pairwise\n         comparisons between classifiers, where a user provides feedback\n         on which classifier they prefer based on the unknown metric\n         $\\phi$. Given two classifiers with confusion matrices\n         $C_1 = (\\text{FP}_1, \\text{FN}_1)$ and\n         $C_2 = (\\text{FP}_2, \\text{FN}_2)$, the user indicates their\n         relative preference. Let the observed preference be modeled by\n         Bradley-Terry as: $$\\Pr(C_1 \\succ C_2) = \\sigma(\\phi(\\text{FP}_1, \\text{FN}_1) - \\phi(\\text{FP}_2, \\text{FN}_2)).$$ \n         where we view $\\phi$ as the reward function.\n         How does this likelihood affect the posterior inference in the\n         GP? Where does it introduce additional complexity?\n\n    (iii) **(Written + Coding, 6 points).** Given a set of observed\n          pairwise comparisons, derive the posterior distribution over\n          the latent function values $\\phi$ given a set of confusion\n          matrices preferences using Bayes' rule. Express the posterior\n          distribution in terms of the GP prior and the pairwise\n          likelihood function. You do not need to include the\n          normalization constant. Implement the likelihood function in\n          `loglik_from_preferences`.\n\n(b) **Elliptical Slice Sampling for Posterior Inference (20 points)**\n\n    (i) **(Written, 3 points).** Read\n        <https://proceedings.mlr.press/v9/murray10a/murray10a.pdf>.\n        Elliptical slice sampling is a sampling method used to generate\n        samples from the posterior distribution of a Gaussian Process.\n        Explain the key idea behind elliptical slice sampling and why it\n        is well-suited for sampling from the GP posterior in this\n        context.\n\n    (ii) **(Coding, 10 points).** Implement elliptical slice sampling in\n         `npme/elliptical_sampler.py` by following Figure 2 in the\n         paper.\n\n    (iii) **(Written, 3 points).** Run the algorithm on a synthetic\n          preference dataset of confusion matrices with pairwise\n          preferences. The synthetic data will be constructed using the\n          metric\n          $$\\phi_{\\text{true}}(\\text{FP}, \\text{FN}) = \\log(1 + \\text{FP}) + \\log(1 + \\text{FN}),$$\n          which captures the idea that the human oracle perceives both\n          false positives and false negatives in a way that flattens out\n          as these values increase (i.e., marginal increases in FP and\n          FN have diminishing effects on the performance metric).\n          Explain the psychological motivation behind this non-linear\n          function. Why might a logarithmic form be appropriate for\n          modeling human perception of classification errors?\n\n          Run the file `npme/main.py` and attach the plot of\n          $\\phi_{\\text{true}}$ vs your elicited metric. What do you\n          notice in the plot?\n\n    (iv) **(Written + Coding, 4 points).** Once the GP has been trained\n         and posterior samples of the function\n         $\\phi(\\text{FP}, \\text{FN})$ have been obtained, how can we\n         evaluate the quality of the elicited metric? Propose a method\n         to evaluate how well the elicited metric $\\phi$ aligns with the\n         user's true preferences and implement it in\n         `evaluate_elicited_metric` taking into the plot you saw in part\n         (iii).\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Callable\nimport numpy as np\nfrom tqdm import tqdm\n\nclass EllipticalSliceSampler:\n    def __init__(self,\n                 prior_cov: np.ndarray,\n                 loglik: Callable):\n        \"\"\"\n        Initializes the Elliptical Slice Sampler.\n        \n        Args:\n        - prior_cov (np.ndarray): Prior covariance matrix.\n        - loglik (Callable): Log-likelihood function.\n        \"\"\"\n        self.prior_cov = prior_cov\n        self.loglik = loglik\n\n        self._n = prior_cov.shape[0]  # Dimensionality of the space\n        self._chol = np.linalg.cholesky(prior_cov)  # Cache Cholesky decomposition\n\n        # Initialize state by sampling from prior\n        self._state_f = self._chol @ np.random.randn(self._n)\n\n    def _indiv_sample(self):\n        \"\"\"\n        Main algorithm for generating an individual sample using Elliptical Slice Sampling.\n        \"\"\"\n        f = self._state_f  # Previous state\n        nu = self._chol @ np.random.randn(self._n)  # Sample from prior for the ellipse\n        log_y = self.loglik(f) + np.log(np.random.uniform())  # Log-likelihood threshold\n\n        theta = np.random.uniform(0., 2 * np.pi)  # Initial proposal angle\n        theta_min, theta_max = theta - 2 * np.pi, theta  # Define bracketing interval\n\n        # Main loop: Accept sample if it meets log-likelihood threshold; otherwise, shrink the bracket.\n        while True:\n            # YOUR CODE HERE (~10 lines)\n            # 1. Generate a new sample point based on the current angle.\n            # 2. Check if the proposed point meets the acceptance criterion.            \n            # 3. If not accepted, adjust the bracket and select a new angle.\n            break\n            # END OF YOUR CODE\n\n    def sample(self,\n               n_samples: int,\n               n_burn: int = 500) -> np.ndarray:\n        \"\"\"\n        Generates samples using Elliptical Slice Sampling.\n\n        Args:\n        - n_samples (int): Total number of samples to return.\n        - n_burn (int): Number of initial samples to discard (burn-in).\n\n        Returns:\n        - np.ndarray: Array of samples after burn-in.\n        \"\"\"\n        samples = []\n        for i in tqdm(range(n_samples), desc=\"Sampling\"):\n            self._indiv_sample()\n            if i > n_burn:\n                samples.append(self._state_f.copy())  # Store sample post burn-in\n\n        return np.stack(samples)\n\ndef sigmoid(x):\n    \"\"\"Sigmoid function to map values between 0 and 1.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\n# Step 1: Define a New Two-Dimensional Non-linear Function\ndef nonlinear_function(x1, x2):\n    \"\"\"\n    Computes a non-linear function of x1 and x2.\n    \n    Args:\n    - x1 (np.array): First input array.\n    - x2 (np.array): Second input array.\n    \n    Returns:\n    - np.array: Computed function values.\n    \"\"\"\n    return np.log(1 + x1) + np.log(1 + x2)\n\n# Generate a 2D grid of points\nx1 = np.linspace(0, 1, 20)\nx2 = np.linspace(0, 1, 20)\nx1_grid, x2_grid = np.meshgrid(x1, x2)\nx_grid_points = np.vstack([x1_grid.ravel(), x2_grid.ravel()]).T\nf_values = nonlinear_function(x_grid_points[:, 0], x_grid_points[:, 1])\n\n# Step 2: Generate Preferences Using Bradley-Terry Model Over the Grid\ndef generate_preferences(f_vals, num_prefs=10000):\n    \"\"\"\n    Generates preferences based on the Bradley-Terry model.\n    \n    Args:\n    - f_vals (np.array): Function values at grid points.\n    - num_prefs (int): Number of preference pairs to generate.\n    \n    Returns:\n    - list of tuple: Generated preference pairs (i, j).\n    \"\"\"\n    preferences = []\n    num_points = len(f_vals)\n    for _ in range(num_prefs):\n        i, j = np.random.choice(num_points, size=2, replace=False)\n        # Probability of preference using Bradley-Terry model\n        p_ij = sigmoid(f_vals[i] - f_vals[j])\n        # Decide preference based on random draw\n        if np.random.rand() < p_ij:\n            preferences.append((i, j))\n        else:\n            preferences.append((j, i))\n    return preferences\n\npreferences = generate_preferences(f_values)\n\n# Step 3: Define the Likelihood Function for Elliptical Slice Sampling\ndef loglik_from_preferences(f):\n    \"\"\"\n    Log-likelihood function using Bradley-Terry model for preferences.\n    \n    Args:\n    - f (np.array): Sampled function values.\n    \n    Returns:\n    - float: Log-likelihood value.\n    \"\"\"\n    log_lik = 0\n    for idx_i, idx_j in preferences:\n        # YOUR CODE HERE (~2 lines)\n        pass\n        # END OF YOUR CODE\n    return log_lik\n\n# Step 4: Define the RBF Kernel to Compute Prior Covariance Matrix\ndef rbf_kernel(X1, X2, length_scale=1.0, sigma_f=1.0):\n    \"\"\"\n    Computes the Radial Basis Function (RBF) kernel between two sets of points.\n    \n    Args:\n    - X1, X2 (np.array): Input data points.\n    - length_scale (float): Kernel length scale parameter.\n    - sigma_f (float): Kernel output scale.\n    \n    Returns:\n    - np.array: RBF kernel matrix.\n    \"\"\"\n    sqdist = np.sum(X1**2, axis=1).reshape(-1, 1) + np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T)\n    return sigma_f**2 * np.exp(-0.5 / length_scale**2 * sqdist)\n\n# Define prior covariance (prior mean is zero vector)\nsigma_prior = rbf_kernel(x_grid_points, x_grid_points, length_scale=1.0, sigma_f=1.0)\n\n# Add small jitter to diagonal for numerical stability\njitter = 1e-6\nsigma_prior += jitter * np.eye(sigma_prior.shape[0])\n\n# Ensure the matrix is symmetric to avoid numerical issues\nsigma_prior = (sigma_prior + sigma_prior.T) / 2\n\n# Step 5: Run Elliptical Slice Sampling\nsampler = EllipticalSliceSampler(sigma_prior, loglik_from_preferences)\nsamples = sampler.sample(1000, n_burn=500)\naverage_samples = np.mean(samples, axis=0)\n\n# Generate true function values on grid points\ntrue_values_on_grid = nonlinear_function(x_grid_points[:, 0], x_grid_points[:, 1])\n\ndef evaluate_elicited_metric(true_metric, elicited_metric):\n    \"\"\"\n    Evaluates and prints the mean and standard deviation of the difference\n    between true and elicited metrics.\n    \n    Args:\n    - true_metric (np.array): True values of the function.\n    - elicited_metric (np.array): Elicited (estimated) function values.\n    \"\"\"\n    # YOUR CODE HERE\n    pass\n    # END OF YOUR CODE\n\nevaluate_elicited_metric(true_values_on_grid, average_samples)\n\n# Step 6: Plot the True Non-linear Function and Elicited Metric in 3D\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot the true function\nx1_fine = np.linspace(0, 1, 50)\nx2_fine = np.linspace(0, 1, 50)\nx1_fine_grid, x2_fine_grid = np.meshgrid(x1_fine, x2_fine)\ntrue_f_values = nonlinear_function(x1_fine_grid, x2_fine_grid)\nax.plot_surface(x1_fine_grid, x2_fine_grid, true_f_values, color='blue', alpha=0.5, label='True Function')\n\n# Plot the averaged samples as a surface\nx1_avg = x_grid_points[:, 0].reshape(20, 20)\nx2_avg = x_grid_points[:, 1].reshape(20, 20)\navg_values = average_samples.reshape(20, 20)\nax.plot_surface(x1_avg, x2_avg, avg_values, color='red', alpha=0.5, label='Estimated Function')\n\n# Customize plot\nax.set_xlabel('x1')\nax.set_ylabel('x2')\nax.set_zlabel('f(x1, x2)')\nax.set_title('True Function vs. Averaged Estimated Function')\nplt.legend()\nplt.show()\n```\n:::\n\n\n:::\n\n",
    "supporting": [
      "003-measure_files/figure-pdf"
    ],
    "filters": []
  }
}