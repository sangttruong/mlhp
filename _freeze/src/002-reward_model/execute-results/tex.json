{
  "hash": "1aa8017d774507f62ddcc9461800870b",
  "result": {
    "engine": "jupyter",
    "markdown": "# Models of Preferences and Decisions {#ch-human-decision-making-choice-models}\n\n::: {.content-visible when-format=\"html\"}\n\n<iframe\n  src=\"https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/\"\n  style=\"width:45%; height:225px;\"\n></iframe>\n<iframe\n  src=\"https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/\"\n  style=\"width:45%; height:225px;\"\n></iframe>\n[Fullscreen Part 1](https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/){.btn .btn-outline-primary .btn role=\"button\"}\n[Fullscreen Part 2](https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/){.btn .btn-outline-primary .btn role=\"button\"}\n\n:::\n\n## Introduction\n\nHuman preference modeling aims to capture humans' decision making processes in a probabilistic framework. Many problems would benefit from a quantitative perspective, enabling an understanding of how humans engage with the world. In this chapter, we will explore how one can model human preferences, including different formulations of such models, how one can optimize these models given data, and considerations one should understand to create such systems. We describe these assumptions in @sec-foundations.\n\n## Foundations of Preference Models {#sec-foundations}\n\n<!--\nAn alternative framework we will explore is ranking, in which we can model an ordering of given choices from most to least desirable. It is possible that there is an infinite set of options; in this case, our model will have to reason about a discretized set of options and may fail to capture the full space of possibilities a human would choose from in the real world.\n-->\n\n### Axiom 1: Construction of Choices Set {#axiom-1-preference-models-model-choice .unnumbered}\n\nHuman preference models model the preferred choices amongst a set of options. For example, this could be modeling which meal from a set of options a person will most likely choose. Preference models must enumerate the set of all possible choices included in a human decision. As such, we must ensure that the choices we enumerate capture the entire domain (collectively exhaustive) but are distinct (mutually exclusive) choices. A discrete set of choices is a constraint we canonically impose to ensure we can tractably model preferences and aptly estimate the parameters of preference models. We assume that if a new option is added to the choice set, the relative probabilities of choosing between the original options remain unchanged. This is known as Independence of Irrelevant Alternatives (IIA) property from Luce's axiom of choices [@Luce1977].\n\n### Axiom 2: Preference Centers around Utility {#axiom-3-preference-centers-around-utility .unnumbered}\n\nHuman preference models are centered around the notion of utility, a scalar quantity representing the benefit or value an individual attains from selecting a given choice. We assume that the underlying utility mechanism of a human preference model captures the final decision output from a human. We use the notation $u_{i,j}$ as the utility of person $i$ choosing item $j$. The utility is a random variable, decomposing into true utility $u_{i,j}^*$ and a random noise $\\epsilon_{i,j}$: $u_{i,j} = u_{i,j}^* + \\epsilon_{i,j}$. Only the relative difference in utility matters to predict the choice among options. As such, the scale of the utilities is irrelevant within a given set of human preference data. The scale of utilities is important when comparing across datasets or across different humans; since utility may be defined differently in various datasets or different human. A common practice to address this consideration is to standardize the utilities in each dataset based on its variance in the observed data.\n\n### Axiom 3: Rationality {#human-rationality .unnumbered}\n\nModeling decision-making must also take into account rationality. Rationality assumption provides a framework for predicting and modeling human behavior by outlining the principles that guide decision-making processes [@keisler2003common]. By incorporating different types of rationality, researchers can create more accurate and realistic models that reflect the complexities of human decision-making [@miljkovic2005rational; @simon1972theories]. Perfect rationality posits that individuals make decisions that maximize their utility, assuming they have complete information and the cognitive ability to process this information to make optimal choices [@miljkovic2005rational]. Numerous studies have shown that this assumption frequently fails to describe actual human behavior, as individuals do not always act in ways that maximize their utility due to various constraints and biases [@miljkovic2005rational]. Bounded rationality acknowledges that individuals operate within the limits of their information and cognitive capabilities. Decisions are made using heuristics rather than through exhaustive analysis, reflecting the practical constraints of real-world decision-making [@simon1972theories]. Bounded rationality acknowledges that decisions are influenced by noise, resulting in probabilistic choice behavior: while individuals aim to maximize their utility, random factors can lead to deviations from perfectly rational choices [@miljkovic2005rational].\n\nBounded rationality can be operationalized through Boltzmann rationalit. It addresses the likelihood of a human selecting an option $o$ from a set $O$. Desirability is represented by a value function $v : O \\rightarrow \\mathbb{R}^+$, with the selection probability calculated as $P(o) = \\frac{v(o)}{\\sum_{o' \\in O} v(o')}$. Assuming there is an underlying reward for each option $R(o) \\in \\mathbb{R}$ such that $v(o) = e^{R(o)}$, we get $P(o) = \\frac{e^{R(o)}}{\\sum_{\\bar{o} \\in \\mathcal{O}} e^{R(\\bar{o})}}$. Essentially, \"A human will act out a trajectory with a probability proportional to the exponentiated return they receive for the trajectory.\" When choices involve trajectories $\\xi \\in \\Xi$ (sequences of actions), the reward $R$ is typically a function of a feature vector $\\phi : \\Xi \\rightarrow \\mathbb{R}^k$, and the probability density is given by $p(\\xi) = \\frac{e^{R(\\phi(\\xi))}}{\\int_{\\Xi} e^{R(\\phi(\\bar{\\xi}))} d\\bar{\\xi}}$.\n\nBoltzmann rationality has the \"duplicates problem,\" where there is no concept of similar actions (e.g., choosing between using a car or a train for transportation, with no particular preference). The probability of making the decision is 50% for either option. However, if we now have 100 cars, under Boltzmann, we would have a 99% probability of choosing a car, which is unrealistic. To address this issue, various extensions have been proposed. One such extension is the attribute rule, which interprets options as bundles of attributes. In this rule, attributes $X$ are associated with options, and they have desirability values $w(x)$. An attribute intensity function $s(x, o)$ indicates the degree to which an attribute is expressed in an option. The probability of choosing option $o$ is\n\n$$P(o) = \\sum_{x \\in \\mathcal{X}_o} \\frac{w(x)}{\\sum_{\\bar{x} \\in \\mathcal{X}_o} w(\\bar{x})} \\cdot \\frac{s(x, o)}{\\sum_{\\tilde{o} \\in \\mathcal{O}} s(x, \\bar{o})}$$\n\nThis equation describes a two-step process where an attribute $x \\in X_O$ is first chosen according to a Boltzmann-like rule and then an option $o \\in O$ with that attribute is selected using another Boltzmann-like rule. This approach handles duplicates gracefully by effectively creating a two-layer hierarchy in choosing an option. Boltzmann rationality finds practical applications in various fields, particularly in reinforcement learning, where it models decision-making in uncertain environments. It also applies to trajectory selection, where the probability of a sequence of actions (trajectory) is proportional to the exponential return. These applications enhance the accuracy of models that interact with or predict human behavior, making Boltzmann Rationality a vital component of the models of interaction.\n\nWe next explore a case study to deepen our understanding of rationality: Limiting Errors due to Similar Selection (LESS) [@2001.04465]. LESS takes inspiration from the attribute rule and extends it to continuous trajectories [@2001.04465]. The key insight is that instead of creating \"attributes\", which group together similar discrete options, it introduces a similarity metric on the space of continuous actions, thereby creating similar groupings on trajectories. The LESS similarity metric could be defined in trajectory space, where the trajectory is some theoretical notion of all states and actions one passes through over time. However, it is instead defined on the measured feature vector $\\phi(\\xi)$ associated with the agent's trajectory $\\xi$. In practice, one can never measure the exact trajectory with perfect fidelity. The feature vector will almost necessarily map in a one-to-many fashion with trajectories. Formally,\nlet $\\phi \\in \\Phi$ be the set of all possible feature vectors $\\xi \\in \\Xi$ the set of all trajectories. The set of feature vectors belonging to a set of trajectories $\\Xi' \\subseteq \\Xi$ is $\\Phi_{\\Xi'}$. We begin with equation (4) and substitute our similarity metric on feature vectors of trajectories.\n\n$$\\begin{aligned}\n    P(\\xi) = \\frac{e^{R(\\phi(\\xi))}}{\\sum_{\\bar{\\phi} \\in \\Phi_{\\Xi}} e^{R(\\hat{\\phi})}} \\cdot \\frac{s(\\phi(\\xi), \\bar{\\xi})}{\\sum_{\\hat{\\xi} \\in \\Xi} s(\\phi(\\xi), \\bar{\\xi})}\n\\end{aligned}$$\n\nThe probability of choosing trajectory $\\xi$ is proportional to the exponentiated reward for the agent's measured trajectory $\\phi(\\xi)$, normalized by the sum of all rewards over all possible measured trajectories. The second half of the product is a normalization factor based on how similar the current trajectory is to other trajectories in feature space. We can define the similarity function as an indicator function, where $s(x, \\xi) = 1$ only if $x = \\phi(\\xi)$. That means that multiple trajectories with the same feature vector will effectively be considered a single option. Thus, we achieve the \"bundling\" of trajectories, in the same way that the attribute rule bundled options under different attributes.\n\nHowever, setting the similarity metric as an indicator function isn't sufficiently flexible. We want a proper metric that acts more as a continuous distance over the feature space. We instead define $s$ to be a soft similarity metric $s : \\Phi \\times \\Xi \\rightarrow \\mathbb{R}^+$ with the following properties:\n\n1.  $s(\\phi(\\xi), \\xi) = \\max_{x \\in \\phi, \\bar{\\xi} \\in \\Xi} s(x, \\hat{\\xi}) \\forall (\\xi \\in \\Xi)$\n\n2.  Symmetric: $s(\\phi(\\xi), \\bar{\\xi}) = s(\\phi(\\bar{\\xi}), \\xi)$\n\n3.  Positive Semidefinite: $s(x, \\xi) \\geq 0$\n\nUsing this redefined similarity metric $s$, we extend (5) to be a probability density on the continuous trajectory space $\\mathcal{E}$, as in (3).\n\n$$p(\\hat{\\xi}) = \\frac{\\frac{e^{R(\\phi(\\xi))}}{\\int_{\\Xi} s(\\phi(\\xi), \\bar{\\xi}) d\\bar{\\xi}}}{\\int_{\\Xi}\\frac{e^{R(\\phi(\\hat{\\xi}))}}{\\int_{\\Xi} s(\\phi(\\hat{\\xi}), \\bar{\\xi}) d\\bar{\\xi}}d\\hat{\\xi}} \\propto \\frac{e^{R(\\phi(\\hat{\\xi}))}}{\\int_{\\Xi} s(\\phi(\\xi), \\bar{\\xi}) d\\bar{\\xi}}$$\n\nUnder this formulation, the likelihood of selecting a trajectory is inversely proportional to its feature-space similarity with other trajectories. This de-weights similar trajectories, which is the desired effect for our LESS model of human decision-making. This means, though, that the \"trajectory bundle\" of similar trajectories still has a reasonable probability of being chosen.\n\n### Axiom 4: Preference captures decision-making {#axiom-2-preference-captures-decision-making .unnumbered}\nHuman preferences are classified into two categories: revealed preferences and stated preferences.\n\nRevealed preferences are those one can observe retroactively from existing data. The implicit decision-making knowledge can be captured via learnable parameters and their usage in models which represent relationships between input decision attributes that may have little human interpretability, but enable powerful models of human preference. For health coaching, we may have information about which foods an individual has chosen previously in different contexts, allowing us to build a model from their decisions. Such data may be easier to acquire and can reflect real-world outcomes (since they are, at least theoretically, inherently based on human preferences). However, if we fail to capture sufficient context in such data, human preference models may not sufficiently capture human preferences.\n\nStated preferences are those individuals explicitly indicate in potentially experimental conditions. The explicit knowledge may be leveraged by including inductive biases during modeling (for example, the context used in a model) which are reasonable assumptions for how a human would consider a set of options.This may include controlled experiments or studies. This may be harder to obtain and somewhat biased, as they can be hypothetical or only accurately reflect a piece of the overall context of a decision. However, they enable greater control of the decision-making process.\n\n## Methods for Collecting Preference Data {#sec-collect}\n\nNext, we explore various mechanisms by which humans can express their preferences, including pairwise sampling, rank-order sampling, rating-scale sampling, best-worst scaling, and multiple-choice samples. In *pairwise sampling*, participants compare two options to determine which is preferred. One of the major advantage of this method is low cognitive demand for rater. Its disavantage is the limited amount of information content elicited by a sample. Next, we will see that we can trading cognitive demand for rater to elicit more nuance preference information. For example, *Rank-order sampling* captures human preferences by having participants rank items from most to least preferred. Used in voting, market research, and psychology, it provides rich preference data but is more complex and cognitively demanding than pairwise comparisons, especially for large item sets. Participants may also rank inconsistently [@ragain2019]. \n\n*Rating-scale sampling*, such as the Likert scale, is a method in which participants rate items on a fixed-point scale (e.g., 1 to 5, \"Strongly Disagree\" to \"Strongly Agree\") to measure levels of preference towards items [@harpe2015]. Participants can also mark a point on a continuous rating scale to indicate their preference or attitude. Commonly used in surveys, product reviews, and psychological assessments, this method provides a more nuanced measure than discrete scales. Rating-scale sampling is simple for participants to understand and use, provides rich data on the intensity of preferences, and is flexible enough for various measurements (e.g., agreement, satisfaction) [@harpe2015]. However, rating-scale sampling methods also have limitations. Ratings can be influenced by personal biases and interpretations of scales, leading to subjectivity. There is a central tendency bias, where participants may avoid extreme ratings, resulting in clustering responses around the middle. Different participants might interpret scale points differently, and fixed-point scales may not capture the full nuance of participants' preferences or attitudes [@harpe2015].\n\n*In Best-worst scaling* (BWS), participants are presented with items and asked to identify the most and least preferred options. The primary objective of BWS is to discern the relative importance or preference of items, making it widely applicable in various fields such as market research, health economics, and social sciences [@campbell2015]. BWS provides rich data on the relative importance of items, helps clarify preferences, reduces biases found in traditional rating scales, and results in utility scores that are easy to interpret. However, BWS also has limitations, including potential scale interpretation differences among participants, and design challenges to avoid biases, such as the order effect or the context in which items are presented.\n\n*Multiple-choice sampling* involve participants selecting one option from a set of alternatives. Multiple-choice sampling is simple for participants to understand and reflect on realistic decision-making scenarios where individuals choose one option from many. It is beneficial in complex choice scenarios, such as modes of transportation, where choices are not independent [@bolt2009]. Multiple-choice sampling often relies on simplistic assumptions such as the independence of irrelevant alternatives (IIA), which may not always hold true. This method may also fail to capture the variation in preferences among different individuals, as it typically records only the most preferred choice without accounting for the relative importance of other options.\n\n## Models of Choices {#sec-models}\n\n#### Binary Choice Model {#binary-choice-model .unnumbered}\n\nBinary choice model is centered around one item. The model predicts, for that option, after observing user choices in the past, whether that option will be chosen or not. We use binary variable $y \\in \\{0, 1\\}$ to represent whether that choice will be picked by the user in the next phase of selection. We denote $P = \\mathbb{P}(y = 1)$. We can formally model $y$ as a function of the utility of the positive choice: $y = \\mathbb{I}[U>0]$. We explore two cases based on the noise distribution. $\\psi$ is the logistic function or the standard normal cummulative distribution function if noise follows logistic distribution and the standard normal distribution, repsectively:\n$$\n\\mathbb{P}(u_{i,j} > 0) = \\mathbb{P}(u_{i,j}^* + \\epsilon > 0) = 1 - \\mathbb{P}( \\epsilon < -u_{i,j}^*) = \\psi(u_{i,j}^*).\n$$\n\n#### Bradley-Terry Model {#bradley-terry-model .unnumbered}\n\nThe Bradley-Terry model compares the utility of choice over all others [@bradley-terry-model] in the set of $J$ choices $i \\in \\{1, 2, \\dots, J\\}$. Each choice can also have its unique random noise variable representing the unobserved factor, although we can also choose to have all choices' unobserved factors follow the same distribution (e.g. independent and identically distributed, IID). The noise is represented as an extreme value distribution, although we can choose alternatives such as a multivariate Gaussian distribution: $\\epsilon \\sim \\mathcal{N}(0, \\Sigma)$. If $\\Sigma$ is not a diagonal matrix, we effectively model correlations in the noise across choices, enabling us to avoid the IID assumption. In the case of the extreme value distribution, we model the probability of a user preferring choice $i$, which we denote as $P_i = Z^{-1}\\exp(u_{i,j}^*)$ where $Z = \\sum_{j = 1}^{J} \\exp(u_{i,j}^*)$.\n\n#### Ordered Preferences Model {#ordered-preferences-model .unnumbered}\n\nPrevious models do not leverage information about ordering of the available options a human can choose from: all choices were treated as independent by the model. The model aims to capture how an individual chooses between them. However, in many cases, we may introduce an inductive bias based on information about the options. For example, in a study for stated preferences, a user may be able to choose from intricately dependent options such as very poor, poor, fair, good, and great. In this case, it can be useful to include this bias in our model to represent a human's decision-making process better. Instead of comparing choices against alternatives, we can focus on a single example and use additional parameters to define classification criteria based on the utility determined by the model. Let us suppose we have a single example with attributes $z_i$, and wish to know which of $J$ predefined options an individual will choose from. We can define $J - 1$ parameters, which act as thresholds on the utility computed by $u_i = u_{i,j}^*$ to classify the predicted choice between these options. For example, if there are 3 predefined options, we can define parameters $a, b \\in \\mathbb{R}$ such that\n\n$$\ny_i =\n\\begin{cases} \n    1 & u < a \\\\\n    2 & a \\le u < b \\\\\n    3 & \\text{else}\n\\end{cases}\n$$\n\nBy assuming the noise distribution to be either logistic or standard normal, we have \n$$\n\\begin{split}\n    \\mathbb{P}(y_i = 1) & = \\mathbb{P}(u < a) = \\mathbb{P}(u_{i,j}^* + \\epsilon < a) = \\psi(a-u_{i,j}^*) \\\\\n    \\mathbb{P}(y_i = 2) & = \\mathbb{P}(a \\le u < b) = \\mathbb{P}(a - u_{i,j}^* \\le \\epsilon < b - u_{i,j}^*) = \\psi(b-u_{i,j}^*)  - \\psi(u_{i,j}^*-a) \\\\\n    \\mathbb{P}(y_i = 3) & = \\mathbb{P}(u > b) = \\mathbb{P}(u_{i,j}^* + \\epsilon > b ) = \\mathbb{P}( \\epsilon > b - u_{i,j}^*) = \\psi(b-u_{i,j}^*)\n\\end{split}\n$$\n\n#### Plackett-Luce Model {#plackett-luce-model .unnumbered}\n\nWe can model an open-ended ranking of the available options with the Plackett-Luce model, in which we jointly model the full sequence of choice ordering [@plackett_luce]. The general form models the joint distribution as the product of conditional probabilities, where each is conditioned on the preceding ranking terms. Given an ordering of $J$ choices $\\{y_1, \\dots, y_J\\}$, we factorize the joint probability into conditionals. Each conditional follows the Bradley-Terry model:\n$$\n\\mathbb{P}(y_1, \\dots, y_J) = \\mathbb{P}(y_1) \\cdot \\mathbb{P}(y_2 | y_1) \\cdot \\dots \\cdot \\mathbb{P}(y_J | y_1, y_2, \\dots y_{J - 1}) = \\prod_{i = 1}^J \\frac{\\exp(u_{i,j}^*)}{\\sum_{j \\ge i} \\exp(u_{i,j}^*)}\n$$\n\n#### Ideal Point Model {#ideal-point-model .unnumbered}\n\nThe ideal point model uses distance functions to compute utility for individual-choice pairs [@huber1976ideal]. Given vector representation $z_i$ of choice $i$ and a vector $v_n$ representing an individual $n$, we can use a distance function to model a stochastic utility function with the unobserved factors following a specified distribution: $u_{n, i} = \\texttt{dist}(z_i, v_n) + \\epsilon_{n, i}$. We assume human preferences follow the choice with maximum utility: $y_{n, i} = \\mathbb{I}[u_{n, i} > u_{n, j} \\ \\forall i \\ne j]$. The intuition is that vectors exist in a shared $n$-dimensional space, and as such we can use geometry to match choices whose representations are closest to that of a given individual. This model can often result in faster learning compared to non-geometric approaches [@ideal_point; @tatli2022distancepreferences] when equipped with a distance metric. Certain distance metrics, such as Euclidian distance or inner product, can easily be biased by the scale of vectors. A distance measure such as cosine similarity, which compensates for scale by normalizing the inner product of two vectors by the product of their magnitudes, can mitigate this bias yet may discard valuable information encoded by the length of the vectors. Beyond the distance metric alone, this model places a strong inductive bias that the individual and choice representations all share a common embedding space. In some contexts, this can be a robust bias to add to the model [@idealpoints], but it is a key factor one must take into account before employing such a model, and is a key design choice for modeling.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\ndef greet(name):\n    return f\"Hello, {name}!\"\n\ngreet(\"Sang\")\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n'Hello, Sang!'\n```\n:::\n:::\n\n\n## Choices Aggregation\n\nGame theory provides a mathematical framework for analyzing strategic\ninteractions among rational agents. These models help in understanding\nand predicting human behavior by considering multiple criteria and the\nassociated trade-offs. They enhance the understanding of preferences\nacross multiple criteria and allow for richer and more accurate feedback\nthrough structured comparisons. Game-theory framings capture the\ncomplexity of preferences and interactions in decision-making processes\n[@bhatia2020preference].\n\nThe most popular form of preference elicitation involves pairwise\ncomparisons. Users are asked to choose between two options, such as\nproduct A or product B. This method is used in various applications like\nsearch engines, recommender systems, and interactive robotics. Key\nconcepts include the Von Neumann Winner and the Blackwell Winner. The\nVon Neumann Winner refers to a distribution over objects that beats or\nties every other object in the collection under the expected utility\nassumption. The Blackwell Winner generalizes the Von Neumann Winner for\nmulti-criteria problems using a target set for acceptable payoff vectors\n[@bhatia2020preference].\n\nGame-theory framings provide a framework for preference learning along\nmultiple criteria. These models use tools from vector-valued payoffs in\ngame theory, with Blackwell's approach being a key concept. This\napproach allows for a more comprehensive understanding of preferences by\nconsidering multiple criteria simultaneously [@bhatia2020preference].\n\nIn game-theory framings, pairwise preferences are modeled as random\nvariables. Comparisons between objects along different criteria are\ncaptured in a preference tensor $P$. This tensor models the probability\nthat one object is preferred over another along a specific criterion,\nallowing for a detailed understanding of preferences across multiple\ndimensions [@bhatia2020preference].\n\nThe preference tensor $P$ captures object comparisons along different\ncriteria. It is defined as:\n$$P(i_1, i_2; j) = P(i_1 \\succ i_2 \\text{ along criterion } j)$$ where\n$P(i_2, i_1; j) = 1 - P(i_1, i_2; j)$. These values are aggregated to\nform an overall preference matrix $P_{ov}$ [@bhatia2020preference].\n\nThe Blackwell Winner is defined using a target set $S$ of acceptable\nscore vectors. The goal is to find a distribution $\\pi^*$ such that\n$P(\\pi^*, \\pi) \\in S$ for all $\\pi$. This method minimizes the maximum\ndistance to the target set, providing a robust solution to\nmulti-criteria preference problems [@bhatia2020preference].\n\nThe optimization problem for finding the Blackwell Winner is defined as:\n$$\\pi(P, S, \\|\\cdot\\|) = \\arg \\min_{\\pi \\in \\Delta_d} \\left[ \\max_{\\pi' \\in \\Delta_d} \\rho(P(\\pi, \\pi'), S) \\right]$$\nwhere $\\rho(u, v) = \\|u - v\\|$. This measures the distance to the target\nset, ensuring that the selected distribution is as close as possible to\nthe ideal preference vector [@bhatia2020preference].\n\n\n## Parameterization and Learning of Utility Functions {#sec-learning}\nThe attributes representing a choice $z_i$ are crucial in defining the human preference model, as they provide the context for capturing human behavior when choice $i$ is made. \n\nWith an understanding of the various techniques we can use to model\nhuman preferences, we can now create robust models which utilize context\nattributes about the options an individual has in front of them and\nmodel their choices. However, these models on their own are powerless;\ntheir parameters are initialized randomly and we must fit the models to\nthe actual human choice data!\n\nEach of the models we have studied contain distinct parameters which aim\nto capture human preferences; for example $\\beta$ is a parameter vector\ncontaining variables which represent a linear function to compute\nutility given a choice's attributes. We can also choose to represent\nstochastic utility functions or embedding functions for Ideal Point\nModels as neural networks. But how can we compute the optimal values of\nthese parameters?\n\nIn this section, we give the reader an overview of the different methods\navailable to tune human preference model parameters using given data. We\nrefer the reader to [@book_estimation_casella; @book_estimation_bock]\nfor first-principle derivations of these methods and a deeper dive into\ntheir theoretical properties (convergence, generalization,\ndata-hungriness, etc.).\n\nA common and powerful approach for computing the parameters of a model\nis maximum likelihood estimation\n[@book_estimation_casella; @book_estimation_bock]. The likelihood of a\nmodel is the probability of the observed data given the model\nparameters; intuitively we wish to maximize this likelihood, as that\nwould mean that our model associates observed human preferences in the\ndata with high probability. We can formally define the likelihood for a\nmodel with parameters $\\beta$ and a given data point $(z_i, y_i)$ as:\n$$\\mathcal{L}(z_i, y_i; \\beta) = \\mathbb{P}(y = y_i | z_i; \\beta)$$\n\nAssuming our data is independent and identically distributed (iid), the\nlikelihood over the entire dataset is the joint probability of all\nobserved data as defined by the model:\n$$\\mathcal{L}(z, Y; \\beta) = \\prod_{i = 1}^J \\mathbb{P}(y = y_i | z_i; \\beta)$$\n\nIn our very first example of binary choice with logistic noise, this was\nsimply the model's probability of the observed preference value:\n$$\\mathcal{L}(z_i, y_i; \\beta) = \\frac{1}{1 + \\exp^{-u_{i,j}^*}}$$\n\nIn the same case with noise following a standard normal distribution,\nthis took the form: $$\\mathcal{L}(z_i, y_i; \\beta) = \\Phi(u_{i,j}^*)$$\n\nFortunately, in these cases, there are straightforward methods for\nparameter estimation: logistic regression and probit regression (binary\nor multinomial, depending on the model), respectively. We can use\nordinal regression to estimate the model's parameters for our ordered\npreference model.\n\nGenerally, the objective function commonly found in parameter learning\ncan be optimized with stochastic gradient descent (SGD)\n[@gradient_descent]. We can define an objective function as the\nlikelihood to maximize this objective. Since SGD minimizes a given\nobjective, we must negate the likelihood, which ensures that a converged\nsolution maximizes the likelihood. SGD operates by computing the\ngradient of the objective with respect to the parameters of the model,\nwhich provides a signal of the direction in which the parameters must\nmove to *maximize* the objective. Then, SGD makes an update step by\nsubtracting this gradient from the parameters (most often with a scale\nfactor called a *learning rate*), to move the parameters in a direction\nwhich *minimizes* the objective. When the objective is the negative\nlikelihood (or sometimes negative log-likelihood for convenience or\ntractability), the result is an increase in the overall likelihood.\n\nIn the case of logistic and Gaussian models, SGD may yield a challenging\noptimization problem as its stochasticity can lead to noisy updates, for\nexample, if certain examples or batches of examples are biased.\nMitigations include batched SGD, in which multiple samples are randomly\nsampled from the dataset at each iteration, learning rates, which reduce\nthe impact of noisy gradient updates, and momentum and higher-order\noptimizers which reduce noise by using movering averages of gradients or\nprovide better estimates of the best direction in which to update the\ngradients. Some models, such as those that use neural networks, may, in\nfact, be intractable to estimate without a method such as SGD (or its\nmomentum-based derivatives). For example, neural networks with many\nlayers, non-linearities, and parameters can only be efficiently computed\nwith gradient-based methods.\n\n### Reward Learning with Large Language Models\n\nTaking a step away from explicitly modeling human bias and preference,\nwe consider applying a deep learning approach to state-of-the-art\nlanguage models. We begin by introducing the concepts of *foundation\nmodels* and *alignment*. A foundation model [@Liang2021] in machine\nlearning typically refers to a large and pre-trained neural network\nmodel that serves as the basis for various downstream tasks. In natural\nlanguage processing, models like GPT-3, Llama, and BERT are considered\nfoundation models. They are pre-trained on a massive corpus of text\ndata, learning to understand language and context, and are capable of\nvarious language-related tasks such as text classification, language\ngeneration, and question answering. Foundation models are important\nbecause they alleviate the need to train massive neural networks from\nscratch, a compute and data expensive endeavor. However, a raw\nfoundation model, trained on a pretraining objective such as a language\nmodeling objective, is not useful on its own. It must be aligned to\nrespond correctly based on human preferences.\n\nIn short, alignment for foundation models is the process by which model\nbehavior is aligned with human values, ethics, and societal norms. Large\nLanguage Models (LLMs) are a foundation model for natural language\nprocessing. They are trained using a next-word prediction objective,\nallowing them to generate coherent language. A simple way to align a\nLarge Language Model is to train it to follow instructions in a\nsupervised way, using instruction-response pairs curated by hand.\nHowever, this limits the upper limit of LLM performance to the\nperformance of the annotators' writing abilities. This type of\nannotation is also expensive.\n\nAn alternative, more promising approach is to train LLMs using\nreinforcement learning, potentially enabling them to surpass human-level\nperformance. The main challenge with this method lies in defining an\nexplicit reward function for generating free-form text. To address this,\na reward model (RM) can be trained based on human preferences, providing\na mechanism to score the quality of the generated text. This approach,\nknown as Reinforcement Learning from Human Feedback (RLHF), leverages\nhuman feedback to guide model training, allowing LLMs to better align\nwith human expectations while continuously improving performance.\n\n![Overall architecture of a reward model based on LLM](Figures/arch.png){#fig-rm-arch}\n\nThe Llama2 reward model [@2307.09288] is initialized from the pretrained\nLlama2 LLM. In the LLM, the last layer is a mapping\n$L: \\mathbb{R}^D \\rightarrow \\mathbb{R}^V$, where $D$ is the embedding dimension\nfrom the transformer decoder stack and $V$ is the vocabulary size. To\nget the RM, we replace that last layer with a randomly initialized\nscalar head that maps $L: \\mathbb{R}^D \\rightarrow \\mathbb{R}^1$. It's important to\ninitialize the RM from the LLM it's meant to evaluate. This is because:\n\n1.  The RM will have the same \"knowledge\" as the LLM. This is\n    particularly useful if evaluating things like \"does the LLM know\n    when it doesn't know?\". However, in cases where the RM is simply\n    evaluating helpfulness or factuality, it may be useful to have the\n    RM know more.\n\n2.  The RM is on distribution for the LLM - it is initialized in a way\n    where it semantically understands the LLM's outputs.\n\nAn RM is trained with paired preferences, following the format:\n$$\\begin{aligned}\n    \\langle prompt\\_history, response\\_accepted, response\\_rejected \\rangle\n\\end{aligned}$$ Prompt_history is a multiturn history of user prompts\nand model generations, response_accepted is the preferred final model\ngeneration by an annotator, and response_rejected is the unpreferred\nresponse. The RM is trained with a binary ranking loss with an optional\nmargin term m(r), shown in equation (7). There is also often a small\nregularization term added to center the score distribution on 0.\n$$\\mathcal{L}_{\\text{ranking}} = -\\log(\\sigma(r_\\theta(x,y_c) - r_\\theta(x,y_r) - m(r)))$$\nThe margin term increases the distance in scores specifically for\npreference pairs annotators rate as easier to separate.\n\n::: {#tbl-margin_nums}\n  -------------- --------------- -------- ---------- -----------------\n                  Significantly   Better   Slightly     Negligibly\n                     Better                 Better    Better / Unsure\n  Margin Small          1          2/3       1/3             0\n  Margin Large          3           2         1              0\n  -------------- --------------- -------- ---------- -----------------\n\n  : Two variants of preference rating based margin with different magnitude.\n:::\n\n![Reward model score distribution shift caused by incorporating\npreference rating based margin in ranking loss. With the margin term,\nwe observe a binary split pattern in reward distribution, especially for\na larger margin.](Figures/margin-2.png){#fig-margin-2\nwidth=\"\\\\linewidth\"}\n\nIt may seem confusing how the margins were chosen. It's primarily\nbecause the sigmoid function, which is used to normalize the raw reward\nmodel score, flattens out beyond the range of $[-4, 4]$. Thus, the\nmaximum possible margin is eight.\n\nWhen training or using a reward model, watching for the following is\nimportant:\n\n1.  **LLM Distribution Shift**: With each finetune of the LLM, the RM\n    should be updated through a collection of fresh human preferences\n    using generations from the new LLM. This ensures that the RM stays\n    aligned with the current distribution of the LLM and avoids drifting\n    off-distribution.\n\n2.  **RM and LLM are coupled**: An RM is generally optimized to\n    distinguish human preferences more efficiently within the specific\n    distribution of the LLM to be optimized. However, this\n    specialization poses a challenge: such an RM will underperform when\n    dealing with generations not aligned with this specific LLM\n    distribution, such as generations from a completely different LLM.\n\n3.  **Training Sensitivities of RMs**: Training RMs can be unstable and\n    prone to overfitting, especially with multiple training epochs. It's\n    generally advisable to limit the number of epochs during RM training\n    to avoid this issue.\n\nThe industry has centered around optimizing for two primary qualities in\nLLMs: helpfulness and harmlessness (safety). There are also other axes\nsuch as factuality, reasoning, tool use, code, multilingual, and more,\nbut these are out of scope for us. In the Llama2 paper, preference data\nwas collected from humans for each quality, with separate guidelines.\nThis presents a challenge for co-optimizing the final LLM towards both\ngoals.\n\nTwo main approaches can be taken for Reinforcement Learning from Human\nFeedback (RLHF) in this context:\n\n1.  Train a unified reward model that integrates both datasets.\n\n2.  Train two separate reward models, one for each quality, and optimize\n    the LLM toward both.\n\nOption 1 is difficult because of the tension between helpfulness and\nharmlessness. They trade off against each other, confusing an RM trained\non both. The chosen solution was option 2, where two RMs are used to\ntrain the LLM in a piecewise fashion. The helpfulness RM is used as the\nprimary optimization term, while the harmlessness RM acts as a penalty\nterm, driving the behavior of the LLM away from unsafe territory only\nwhen the LLM veers beyond a certain threshold. This is formalized as\nfollows, where $R_s$, $R_h$, and $R_c$ are the safety, helpfulness, and\ncombined reward, respectively. $g$ and $p$ are the model generation and\nthe user prompt: $$\\begin{aligned}\n    R_c(g \\mid p) =\n    \\begin{cases}\n        R_s(g \\mid p) & \\text{if } \\text{is\\_safety}(p) \\text{ or } R_s(g \\mid p) < 0.15 \\\\\n        R_h(g \\mid p) & \\text{otherwise}\n    \\end{cases}\n\\end{aligned}$$\n\nThere are several open issues with reward models alluded to in the\npaper. For example, how best to collect human feedback? Training\nannotators and making sure they do the correct thing is hard. What\nshould the guidelines be? Another question is whether RMs can be made\nrobust to adversarial prompts. Last but not least, do RMs have\nwell-calibrated scores? This matters for RLHF - pure preference accuracy\nisn't enough.\n\n### Reward Learning in Robotics\n\nTo help set up our basic reward learning problem, consider a user and a\nrobot. The user's preferences or goals can be represented by an internal\nreward function, R($\\xi$), which the robot needs to learn. Since the\nreward function isn't explicit, there are a variety of ways that the\nrobot can learn this reward function, which we will discuss in the next\nsection. An example method of learning a reward function from human data\nis using pairwise comparison. Consider the robot example from section\none, but now, the robot shows the human two possible trajectories\n$\\xi_A$ and $\\xi_B$ as depicted in the diagram below.\n\n![Two different trajectories taken by a robot to prompt\nuser ranking.](Figures/robots.png){#fig-reward-robot-1 width=\"70%\"}\n\nThe user is show both the trajectories above and asked to rank which one\nis better. Based on iterations of multiple trajectories and ranking, the\nrobot is able to learn the user's internal reward function. There quite\na lot of ways that models can learn a reward function from human data.\nHere's a list [@myers2021learning] of some of them:\n\n1.  Pairwise comparison: This is the method that we saw illustrated in\n    the previous example. The robot is able to learn based on a\n    comparison ranking provided by the user.\n\n2.  Expert demonstrations: Experts perform the task and the robot learns\n    the optimal reward function from these demonstrations.\n\n3.  Sub-optimal demonstrations: The robot is provided with\n    demonstrations that are not quite as good as the expert\n    demonstrations but it is still able to learn a noisy reward function\n    from the demonstrations.\n\n4.  Physical Corrections: While the robot is performing the task, at\n    each point in its trajectory (or at an arbitrary point in its\n    trajectory) its arm is corrected to a more suitable position. Based\n    on these corrections, the robot is able to learn the reward\n    function.\n\n5.  Ranking: This method is similar to pairwise comparison but involves\n    more trajectories than 2. All the trajectories may have subtle\n    differences from each other, but these differences help provide\n    insight to the model.\n\n6.  Trajectory Assessment: Given a single trajectory, the user rates how\n    close it is to optimal, typically using a ranking scale.\n\n    Each of these methods allows the robot to refine its understanding\n    of the user's reward function, but their effectiveness can vary\n    depending on the application. For instance, expert demonstrations\n    tend to produce more reliable results but may not always be feasible\n    in everyday tasks. Pairwise comparison and ranking methods offer\n    more flexibility but might require a higher number of iterations.\n\n### Reward Learning with Meta Learning\n\nLearning a reward function from human preferences is an intricate and\ncomplicated task. At its core, this task is about designing algorithms\nthat can capture what humans value based on their elicited preferences.\nHowever, due to the nuanced and multifaceted nature of human desires,\nlearning reward functions from human can be a difficult task. Therefore,\nmeta-learning rewards may be considered to facilitate the reward\nlearning processes. Meta-learning, often referred to as \"learning to\nlearn,\" aims to design models that can adapt to new tasks with minimal\nadditional efforts. We discuss paper [@hejna2023few] in @sec-few-shot showing how meta-learning can be leveraged for\nfew-shot preference learning, where a system can quickly adapt to a new\ntask after only a few queries to pairwise preferences from human.\n\nMoving beyond the concept of learning from pairwise preferences, in @sec-watch we discuss a different approach where\nmeta-learning intersects with both demonstrations and\nrewards [@zhou2019watch]. This paper considers the use of both\ndemonstrations and rewards elicited from human that guide the learning\nprocess.\n\nIn the regular learning setting, a model is fitted to a dataset with\ncertain learning algorithm. The learning algorithm, for example, can be\nthe minimization of a loss function. To formulate the \"regular\" learning\nprocedure, let's denote the training dataset as $D$, and the test\ndataset as $S$. Given a model parameterized by $\\theta$; training loss\nfunction $L(\\theta, D)$; and test loss function $L(\\theta, S)$, we can\nformulate a process of \"regular\" machine learning process as\n$$\\begin{aligned}\n    \\theta^\\star = \\arg\\min_\\theta\\quad L(\\theta, D).\n\\end{aligned}$$ Note that the minimization of the training loss function\nis essentially *one* possible learning algorithm. For example, instead\nof minimizing the loss function, one may do gradient descent with model\nregularization on the loss function, where the final solution may not be\nthe one that actually minimizes the loss function. As a result, we may\nwant to be more general and more abstract for the moment, and denote the\nlearning algorithm as $\\mathcal{A}$. Thus, we can write\n$$\\begin{aligned}\n    \\theta^\\star = \\mathcal{A}(D),\n\\end{aligned}$$ i.e., the learning algorithm $\\mathcal{A}$ takes in a\ntraining dataset and outputs a model parameter $\\theta^\\star$. Then, the\nperformance of the model is evaluated by the test loss\n$L(\\mathcal{A}(D), S)$. As we can see, in the regime of \"regular\"\nlearning, the learning algorithm $\\mathcal{A}$ is pre-defined and fixed.\n\nMeta-learning, or learning-to-learn, essentially asks the question of\nwhether one can *learn* the learning algorithm $\\mathcal{A}$ from prior\ntasks, such that the modal can adapt to a new task more\nquickly/proficiently. For example, different human languages share\nsimilar ideas, and therefore a human expert who has learned many\nlanguages should be able to learn a new language easier than an average\nperson. In other words, the human expert should have learned how to\nlearn new languages more quickly based on their past experiences on\nlearning languages.\n\nTo mathematically formulate meta-learning, we consider a family of\nlearning algorithms $\\mathcal{A}_\\omega$ parameterized by $\\omega$. The\n\"prior\" tasks are represented by a set of meta-training datasets\n$\\{(D_i, S_i)\\}_{i=1}^N$ consists of $N$ pairs of training dataset $D_i$\nand test dataset $S_i$. As we noted before, a learning algorithm\n$\\mathcal{A}_\\omega$ takes in a training dataset, and outputs a model,\ni.e., $$\\begin{aligned}\n    \\forall i: \\quad \\theta^\\star_i=\\mathcal{A}_\\omega(D_i).\n\\end{aligned}$$\n\nTherefore, the **meta-learning objective** is $$\\begin{aligned}\n    \\min_\\omega \\quad \\sum_{i}\\ L(\\mathcal{A}_\\omega(D_i), S_i).\n\\end{aligned}$$ The above optimization problem gives a solution\n$\\omega^\\star$ which we use as the meta-parameter. Then, when a new task\ncomes with a new training dataset $D_{new}$, we can simply apply\n$\\theta^\\star_{new}=\\mathcal{A}_{\\omega^\\star}(D_{new})$ to obtain the\nadapted model $\\theta^\\star_{new}$. Note that we usually assume the\nmeta-training datasets $D_i, S_i$ and the new dataset $D_{new}$ share\nthe same underlying structure, or they come from the same distribution\nof datasets.\n\nOne of the most popular meta-learning method is Model-Agnosic\nMeta-Learning (MAML) [@finn2017model]. In MAML, the meta-parameter\n$\\omega$ shares the same space as the model parameter $\\theta$. At its\ncore, in MAML the learning algorithm is defined to be $$\\begin{aligned}\n    \\mathcal{A}_\\omega(D_i)=\\omega-\\alpha \\nabla_\\omega L(\\omega, D_i),\n\\end{aligned}$$ where $\\alpha$ is the step size. As we can see, in fact\n$\\omega$ is defined as the initialization of fine-tuning $\\theta$. With\na good $\\omega$ learned, the model can adapt to a new task very quickly.\nIn general, meta-learning can be summarized as follows: Given data from\nprior tasks, learn to solve a new task more quickly/proficiently. Given\nthe general nature of meta-learning, one may be curious about whether\npreference learning can be benefited from meta-learning, which we\ndiscuss in the following section.\n\n#### Few-Shot Preference Learning for Reinforcement Learning {#sec-few-shot}\n\nReinforcement learning (RL) in robotics often stumbles when it comes to\ndevising reward functions aligning with human intentions.\nPreference-based RL algorithms aim to solve this by learning from human\nfeedback, but this often demands a *highly impractical number of\nqueries* or leads to oversimplified reward functions that don't hold up\nin real-world tasks.\n\nTo address the impractical requirement of human queries, as we discussed\nin the previous section, one may apply meta-learning so that the RL\nagent can adapt to new tasks with fewer human queries. [@hejna2023few]\nproposes to pre-training models on previous tasks with the meta-learning\nmethod MAML [@finn2017model], and then the meta-trained model can adapt\nto new tasks with fewer queries.\n\nWe consider Reinforcement Learning (RL) settings where a state is\ndenoted as $s\\in S$, and action is denoted as $a\\in A$, for state space\n$S$ and action space $A$. The reward function $r:S\\times A \\to \\mathbb{R}$ is\nunknown and need to be learned from eliciting human preferences. There\nare multiple tasks, where each task has its own reward function and\ntransition probabilities. The reward model is parameterized by $\\psi$.\nWe denote $\\hat{r}_\\psi(s,a)$ to be a learned estimate of an unknown\nground-truth reward function $r(s,a)$, parameterized by $\\psi$.\nAccordingly, a reward model determines a RL policy $\\phi$ by maximizing\nthe accumulated rewards. The preferences is learned via pairwise\ncomparison of trajectory segments $$\\begin{aligned}\n    \\sigma = (s_t, a_t, s_{t+1}, a_{t+1}, ..., s_{t+k-1}, s_{t+k-1})\n\\end{aligned}$$ of $k$ states and actions.\n\nFor each pre-training task, there is a dataset $D$ consists of labeled\nqueries $(\\sigma_1, \\sigma_2, y)$ where $y\\in \\{0, 1\\}$ is the label\nrepresenting which trajectory is preferred. Therefore, a loss function\n$L(\\psi, D)$ captures how well the reward model characterizes the\npreferences in dataset $D$. In [@hejna2023few] they the preference\npredictor over segments using the Bradley-Terry model of paired\ncomparisons [@bradley1952rank], i.e., $$\\begin{aligned}\n    P[\\sigma_1 \\succ \\sigma_2 ] = \\frac{\\exp \\sum_t \\hat{r}_\\psi(s_t^{1}, a_t^{1})}{\\exp \\sum_t \\hat{r}_\\psi(s_t^{1}, a_t^{1}) + \\exp \\sum_t \\hat{r}_\\psi(s_t^{2}, a_t^{2})}.\n\\end{aligned}$$ Then, the loss function is essentially a binary\ncross-entropy which the reward model $\\psi$ aims to minimize, i.e.,\n$$\\begin{aligned}\n    {L}(\\psi,  {D}) = - \\mathbb{E}_{(\\sigma^1, \\sigma^2, y) \\sim {D}} \\left[ y(1) \\log (P[\\sigma_1 \\succ \\sigma_2 ]) + y(2)\\log(1 - P[\\sigma_1 \\succ \\sigma_2 ]) \\right].\n\\end{aligned}$$\n\n##### Method Component 1: Pre-Training with Meta Learning {#method-component-1-pre-training-with-meta-learning .unnumbered}\n\nTo efficiently approximate the reward function $r_\\text{new}$ for a new\ntask with minimal queries, as described in [@hejna2023few], we aim to\nutilize a pre-trained reward function $\\hat{r}_\\psi$ that can be quickly\nfine-tuned using just a few preference comparisons. By pre-training on\ndata from prior tasks, we can leverage the common structure across tasks\nto speed up the adaptation process. Although any meta-learning method is\ncompatible, [@hejna2023few] opt for Model Agnostic Meta-Learning (MAML)\ndue to its simplicity. Therefore, the pre-training update for the reward\nmodel $\\psi$ is $$\\begin{aligned}\n    \\psi \\xleftarrow{} \\psi - \\beta \\nabla_\\psi \\sum_{i = 1}^N {L} (\\psi - \\alpha \\nabla_\\psi {L}(\\psi, {D}_i), {D}_i),\n\\end{aligned}$$ where $\\alpha, \\beta$ are the inner and outer learning\nrate, respectively. We note that data $\\{D_i\\}_i$ of labeled preferences\nqueries for prior tasks can come from offline datasets, simulated\npolicies, or actual humans.\n\n##### Method Component 2: Few-Shot Adaptation {#method-component-2-few-shot-adaptation .unnumbered}\n\nWith the aforementioned pre-training with meta learning, the\nmeta-learned reward model can then be used for few-shot preference based\nRL during an online adaptation phase. The core procedure of the few-shot\nadaption is descibed as below\n\n1.  Given a pre-trained reward model $\\psi$\n\n2.  For time step $t=1, 2, \\dots$\n\n    1.  Find pairs of trajectories $(\\sigma_1, \\sigma_2)$ with\n        preference uncertainty based on $\\psi$.\n\n    2.  Query human preference $y$ and forms a new dataset $D_{new}$\n\n    3.  Update the reward model by\n        $\\psi'\\leftarrow \\psi - \\alpha \\nabla_\\psi L(\\psi, D_{new})$\n\n    4.  Update the policy with the new reward model $\\psi'$\n\nAs mentioned in [@hejna2023few], uncertain queries are selected using\nthe disagreement of an ensemble of reward functions over the preference\npredictors. Specifically, comparisons that maximize\n$\\texttt{std}(P[\\sigma_1 \\succ \\sigma_2])$ are selected each time\nfeedback is collected.\n\nThe whole pipeline of the method is outlined in @fig-few-1.\n\n![An overview of the proposed method in [@hejna2023few]. **Pre-training\n(left):** In the pre-training phase, trajectory segment comparisons are\ngenerated using data from previously learned tasks. Then, they are used\nto train a reward model. **Online-Adaptation (Right)**: After\npre-training the reward model, it is adapted to new data from human\nfeedback. The adapted reward model is then used to train a policy for a\nnew task in a closed loop manner.](Figures/overview-few.png){#fig-few-1\nwidth=\"\\\\linewidth\"}\n\nWe present one set of experiment from the paper, as it illustrates the\neffectiveness of the proposed method in a straightforward way. The\nexperiment test the propoesed method on the Meta-World\nbenchmark [@yu2020meta]. Three baselines are compared with the proposed\nmethod:\n\n1.  SAC: The Soft-Actor Critic RL algorithm trained from ground truth\n    rewards. This represents the standard best possible method given the\n    ground-truth reward.\n\n2.  PEBBLE: The PEBBLE algorithm [@lee2021pebble]. It does not use\n    information from pripor tasks.\n\n3.  Init: This method initialize the reward model with the pretained\n    weights from meta learning. However, instead of adapting the reward\n    model to the new task, it performs standard updates as in PEBBLE.\n\nThe results are shown in @fig-few-exp, where we can see that the proposed methord\noutperforms all of the baselines.\n\n![Results on MetaWorld tasks. The title of each subplot indicates the\ntask and number of artificial feedback queries used in training. Results\nfor each method are shown across five seeds.\n](Figures/few-exp.png){#fig-few-exp width=\"\\\\linewidth\"}\n\nThis paper [@hejna2023few] shows that meta reward learning indeed reduce\nthe number of queries of human preferences. However, as mentioned in the\npaper, there are still some drawbacks, as shown in the following.\n\nMany of the queries the model pick for human preference elicitation are\nactually almost identical to human. After all, the model would pick the\nmost uncertain pair of trajectories for human preference queries, and\nsimilar trajectories are for sure having high uncertainty in their\npreference. This suggest the need of new ways for designing the query\nselection strategy.\n\nMoreover, despite the improved query complexity, it still needs an\nimpractical amount of queries. As shown in @fig-few-exp, the \"sweep into\" task still needs 2500 human\nqueries for it to work properly, which is still not ideal for what we\nwant them to be.\n\nIn addition, it is mentioned in the paper that the proposed method may\nbe even worse than training from scratch, if the new task is too\nout-of-distribution. Certainly, since meta-learning assumes\nin-distribution tasks, we cannot expect the proposed method to be good\nfor out-of-distribution task. It is thus an interesting future direction\nto investigate whether one can design a method that automatically\nbalance between using the prior information or training from scratch.\n\n#### Watch Try Learn {#sec-watch}\n\nWatch, Try, Learn: Meta-Learning from Demonstrations and Rewards\n[@zhou2019watch] asks the question \"How can we efficiently learn both\nfrom expert demonstrations and from trials where we only get **binary**\nfeedback from a human\\\". Why do we care about this question? In the\ncontext of robotics, a very compelling answer is the *cost of\ndata-collection*. In a hypothetical world in which we have a vast number\nof **expert demonstrations** of robots accomplishing a large number of\ndiverse tasks, we don't necessarily need to worry about learning from\ntrials or from humans. We could simply learn a very capable imitation\nagent to perform any task. Natural Language Processing could be seen as\nliving in this world, because internet-scale data is available.\n**Robots, however, are expensive**, so people generally don't have\naccess to them, and therefore cannot use them to produce information to\nimitate. Similarly, **human time is expensive**, so even for large\norganizations that do have access to a lot of robots, it's still hard to\ncollect a lot of expert demonstrations.\n\nThe largest available collection of robotics datasets today is Open\nX-Embodiment ([@padalkar2023open]), which consists of around 1M episodes\nfrom more than 300 different scenes. Even such large datastes are not\nenough to learn generally-capable robotic policies from imitation\nlearning alone.\n\n![Visualization of the Open X-Embodiment dataset collection. Even this\nlarge-scale dataset for robot learning is not yet enough to learn\ngenerally-capable robotic\npolicies.](Figures/open_x_embodiment.png){#fig-open-x-embodiment}\n\n**Main insight:** binary feedback is much cheaper to obtain than expert\ndemonstrations! Instead of hiring people to act as robot operators to\ntell the robot exactly what to do, if there was a way of having many\nrobots trying things in parallel, we can have humans watch videos of\nwhat the robots did and then give a success classification of whether\nthe robot accomplished the goal. This is a much cheaper form of human\nsupervision because the human labels don't necessarily need to be given\nin real time, so one human labeler can label many trajectories in\nparallel, and the human doesn't need to be a skilled robot operator.\n\nConcretely, this paper seeks to learn new tasks with the following\ngeneral problem setting:\n\n1.  We only get 1 expert demonstration of the target task\n\n2.  After seeing the expert demonstration, we have robots try to solve\n    the task 1 or more times.\n\n3.  The user (or some pre-defined reward function) annotates each trial\n    as success/failure.\n\n4.  The agent learns from both the demos and the annotated trials to\n    perform well on the target task.\n\nNote that this work falls under the **meta-learning** umbrella, because\nwe are learning an algorithm for quickly learning new tasks given new\nobservations (demos, trials, and success labels.)\n\nThe **main contribution** of this paper is a meta-learning algorithm for\nincorporating demonstrations and binary feedback from trials to solve\nnew tasks.\n\nMeta-Learning deals with efficient learning of new tasks. In the context\nof robotics or reinforcement learning in general, **how do we define\ntasks**? We will use the Markov decision process (**MDP**) formalism. A\ntask $T_i$ is described with the tuple $\\{S, A, r_i, P_i\\}$.\n\n1.  $S$ represents the *state-space* of the task, or all possible states\n    the agent could find itself in. This work uses image-observations,\n    so $S$ is the space of all possible RGB images.\n\n2.  $A$ is the action space, meaning the set of all possible actions the\n    agent could take. In robotics there are many ways of representing\n    action spaces, and this work considers end-effector positions,\n    rotations, and opening.\n\n3.  $r_i$ is the reward function for the task, with function signature\n    $r_i : S \\times A \\to \\mathbb{R}$. This work assumes all reward functions\n    are binary.\n\n4.  $P_i$ is the transition dynamics function. It's a function that maps\n    state-action pairs to probability distributions over next states.\n\nNotice that $S$ and $A$ are shared across tasks. Transition dynamics\nfunctions are normally also shared between tasks because they represent\nthe laws of physics. However, this work considers environments with\ndifferent objects, so they don't share the dynamics function. Given this\ndefinition for tasks, they assume that the tasks from the data that they\nget come from some unknown task-generating distribution $p(T)$.\n\nLet's give a more precise definition of the problem statement considered\nby **Watch, Try, Learn**. As the paper name suggests, there are 3 phases\nfor the problem statement.\n\n**Watch:** During the *watch* phase, we give the agent $K$\ndemonstrations of the target tasks. This paper considers the case where\n$K$ always equals 1, and all demonstrations are successful. That is,\neach demonstration consists of a trajectory\n$\\{(s_0, a_0), \\ldots, (s_H, a_H)\\}$ where $H$ is the task horizon, and\nthe final state is always successful, that is\n$r_i(s_H, a_H) = 1, r_i(s_j, a_j) = 0$ for every $j \\neq H$.\n\nImportantly, these demonstrations alone might not be sufficient for\n**full task specification**. As an example, consider a demonstration in\nwhich an apple is moved to the right, next to a pan. Seeing this\ndemonstration alone, the task could be always moving the apple to the\nright, or it could be always moving the apple next to the pan,\nirrespective of where the pan is. The expected output after the Watch\nphase is a policy capable of gathering information about a task, given\ndemonstrations.\n\n**Try:** In the Try phase, we use the agent learned during the Watch\nphase to attempt the task for $L$ trials. As specified earlier, this\npaper considers the casae where $L$ always equals 1. After the agent\ncompletes the trials, humans (or pre-programmed reward functions)\nprovide one binary reward for each trial, indicating whether the trial\nwas successful. The expected output of this phase is $L$ trajectories\nand corresponding feedback that hopefully *disambiguate* the task.\n\n**Learn:** After completing the trials, the agent must learn from both\nthe original expert demonstrations and the trials, and become capable of\nsolving the target task.\n\n**Given Data:** To train agents that can Watch, Try, and Learn, we are\ngiven a dataset of expert demonstrations containing multiple demos for\neach task, and the dataset contains hundreds of tasks. Importantly, **no\nonline interaction** is needed for training, and this method trains only\nwith **supervised learning** and no reinforcement learning.\n\nThis section describes exactly how this paper trains an agent from the\ngiven expert demonstrations, and how to incorporate the trials and human\nfeedback into the loop.\n\n**Training to Watch:** We now describe the algorithm to obtain an agent\nconditioned on the given expert demonstration. In particular, what we\nwant to obtain out of the Watch phase is a policy conditioned on a set\nof expert demonstrations. Formally, we want to obtain\n$\\pi_\\theta^{\\text{watch}}(a | s, \\{d_{i,k}\\})$.\n\nThe way we can obtain this policy is through **meta-imitation\nlearning**. Given the demonstrations $\\{\\textbf{d}_{i,k}\\}$ for task\n$i$, we sample another *different* demonstration coming from the same\ntask $\\textbf{d}_i^{\\text{test}}$. The key insight here is that\n$\\textbf{d}_i^{\\text{test}}$ is an example of **optimal behavior** given\nthe demonstrations. Therefore, to obtain\n$\\pi_\\theta^{\\text{watch}}(a | s, \\{d_{i,k}\\})$, we simply regress the\npolicy to imitate actions taken on $\\textbf{d}_i^{\\text{test}}$.\nConcretely, we train policy parameters $\\theta$ to minimize the\nfollowing loss:\n\n$\\mathcal{L}^\\text{watch}(\\theta, \\mathcal{D}_i^*) = \\mathbb{E}_{\\{d_{i,k}\\} \\sim \\mathcal{D}_i^*} \\mathbb{E}_{\\{d_{i,k}^{\\text{test}}\\} \\sim \\mathcal{D}_i^*  \\{d_{i,k}\\}} \\mathbb{E}_{(s_t, a_t) \\sim d_i^{\\text{test}}} \\big[ \n- \\log \\pi_\\theta^{\\text{watch}} (a_t | s_t, \\{d_{i,k}\\}) \\big]$\n\nThis corresponds to doing imitation learning by minimizing the negative\nlog-likelihood of the test trajectory actions, conditioning the policy\non the entire demo set. However, how is the conditioning on the demo set\nachieved?\n\n![Vision-based policy architecture that conditions on a set of\ndemonstrations.](Figures/watch-try-learn-architecture.png){#fig-watch-try-learn-arch}\n\n@fig-watch-try-learn-arch visualizes how Watch Try Learn\ndeals with conditioning on demonstrations. In addition to using features\nobtained from the images of the current state, the architecture uses\nfeatures from frames sampled (in order) from the demonstration episodes,\nwhich are concatenated together.\n\n**Trying:** On the **Try** phase, when the agent is given a set of\ndemonstrations $\\{\\textbf{d}_{i,k}\\}$, we deploy the policy\n$\\pi_\\theta^{\\text{watch}}(a | s, \\{\\textbf{d}_{i,k}\\})$ to collect $L$\ntrials. There is no training involved in the Try phase, we simply\ncondition the policy on the given demonstrations\n\n**Training to Learn:** During the Watch phase the objective was to train\na policy conditioned on demonstrations\n$\\pi_\\theta^{\\text{watch}}(a | s, \\{\\textbf{d}_{i,k}\\})$. The authors of\nWatch, Try, Learn use a similar strategy as the Watch phase for the\nLearn phase. We now want to train a policy that is conditioned on the\ndemonstrations, as well as the trials and binary feedback. That is, we\nwant to learn\n$\\pi_\\phi^{\\text{watch}}(a | s, \\{\\textbf{d}_{i,k}\\}, \\{\\mathbf{\\tau}_{i, l}\\})$.\nTo train the policy, we again use meta-imitation learning where we\nadditionally sample yet another trajectory from the same task.\nConcretely, we train policy parameters $\\phi$ to minimize the following\nloss:\n\n$\\mathcal{L}^{\\text{learn}}(\\phi, \\mathcal{D}_i, \\mathcal{D}_i^*) = \\mathbb{E}_{(\\{d_{i,k}\\}, \\{\\mathbf{\\tau}_{i,l}\\}) \\sim \\mathcal{D}_i} \\mathbb{E}_{\\{d_{i,k}^{\\text{test}}\\} \\sim \\mathcal{D}_i^* \\{d_{i,k}\\}} \\mathbb{E}_{(s_t, a_t) \\sim d_i^{\\text{test}}} \\big[ \n- \\log \\pi_\\theta^{\\text{learn}} (a_t | s_t, \\{d_{i,k}\\}, \\{\\tau_{i,l}\\}) \\big]$\n\nThe conditioning on both the demo episodes and the trial episodes is\nachieved in the exact same way as in the Watch phase, and is visualized\nin @fig-watch-try-learn-arch. The architecture is simply\nadjusted to be able to take in more images fro mthe trial episodes.\n\nIn this section, we describe the evaluation suite for the paper,\nincluding the simulation benchmark used, the baselines considered, and\nthe results.\n\n**Gripper environment setup:**\n\n![Visualization of different tasks from the simulated benchmark for\nWatch Try Learn.](Figures/watch-try-learn-envs.png){#fig-envs}\n\n@fig-envs illustrates the different task families considered in the simulated\nGripper environment. Button Pressing, Grasping, Pushing, and Pick and\nPlace. For each task family, the environment supports hundreds of\ndifferent tasks by changing the objects in the scene and the objectives\n(e.g. which object to pick and where to place). For each task in each\ntask family, a handful of expert demonstrations are given in a\ndemonstrations dataset. As mentioned previously, the environment gives\nthe agent image observations, and take in actions as end-effector\n(gripper) positions, angles, and opening.\n\n**Baselines:** The following three baselines are considered:\n\n1.  **Behavior Cloning**: simple imitation learning based on maximum\n    log-likelihood training using data from all tasks.\n\n2.  **Meta-imitation learning**: This baseline corresponds to simply\n    running the policy from the Watch step, without using any trial\n    data. That is, we only condition on the set of expert\n    demonstrations, but no online trials.\n\n3.  **Behavior Cloning + SAC**: Pre-train a policy with Behavior Cloning\n    on all data, and follow that with Reinforcement Learning fine-tuning\n    for the specific target task, using the maximum-entropy algorithm\n    SAC ([@haarnoja2018soft]).\n\n![Results for Watch Try Learn on the gripper control environment, and\ncomparisons with\nbaselines.](Figures/watch-try-learn-results.png){#fig-watch-try-learn-results\nwidth=\"50%\"}\n\n::: {#tbl-watch-try-learn-table}\n  **METHOD**                     **SUCCESS RATE**\n  ----------------------------- ------------------\n  BC                              .09 $\\pm$ .01\n  MIL                             .30 $\\pm$ .02\n  WTL, 1 TRIAL (OURS)             .42 $\\pm$ .02\n  **RL FINE-TUNING WITH SAC**   \n  BC + SAC, 1500 TRIALS           .11 $\\pm$ .07\n  BC + SAC, 2000 TRIALS           .29 $\\pm$ .10\n  BC + SAC, 2500 TRIALS           .39 $\\pm$ .11\n\n  : Average success rates over all tasks.\n:::\n\n@fig-watch-try-learn-results shows average success rates for\nWatch Try Learn compared to baselines. Watch Try Learn significantly\noutperforms baselines on every task family. In particular, it is far\nsuperior to Behavior Cloning, which is a very weak baseline, and it\nsignificantly surpasses Meta-Imitation Learning on 3 out of 4 task\nfamilies. @tbl-watch-try-learn-table includes comparison with BC\nfine-tuned with Reinforcement Learning. Even after 2500 online trials,\nSAC is not able to obtain the success rate that Watch Try Learn achieves\nafter only 1 trial. Overall, Watch Try Learn exhibits very significant\nperformance gains over prior methods.\n\n### Direct Preference Optimization\n\nA modern method for estimating the parameters of a human preference\nmodel is direct preference optimization [@rafailov2023direct], which is\nused in the context of aligning language models to human preferences. A\nrecent approach [@christiano2023deep] first trains a reward model that\ncaptures human preferences and then uses proximal policy optimization to\ntrain a language model-based policy to reflect those learned\npreferences. Direct Preference Optimization (DPO), on the other hand,\nremoves the need for a reward model by directly using the model\nlikelihood of two outcomes (a preferred or highly-ranked sequence and an\nunpreferred or low-ranked sequence) to capture the preference\nrepresented in the data. DPO provides a simpler framework than its\nreinforcement learning approach and results in comparable performance\nwith improved stability. Furthermore, it obviates the need to train a\nreward model, instead using a language model policy and human preference\ndataset to align the policy directly to human preferences.\n\n### Model Design Consideration\n\nWhen designing models and learning their parameters, one must account\nfor important tradeoffs when designing and optimizing a model to learn\nhuman preferences.\n\n**Bias vs. Variance Trade-off.** In modeling human preferences, we aim\nto ensure that predicted utilities accurately reflect overall human\npreferences. One key challenge is managing the bias and variance\ntrade-off.\n\nBias refers to assumptions made during model design and training that\ncan skew predictions. For example, in Ideal Point Models, we make the\nassumption that the representations we use for individuals and choices\nare aligned in the embedding space, and that this representation is\nsufficient to capture human preferences using distance metrics. However,\nthere are myriad cases in which this may break down, for example if the\ntwo sets of vectors follow different distributions each with their own\nunique biases. If the representations do not come from the same domain,\none may have little visibility into how a distance metric computes the\nfinal utility value for a choice for a given individual. Some ways to\nmitigate bias in human preference models include increasing the number\nof parameters in a model (allowing for better learning of patterns in\nthe data) or removing inductive biases based on our assumptions of the\nunderlying data.\n\nOn the other hand, variance refers to the model's sensitivity to small\nchanges in the input, which leads to significant changes in the outp ut.\nThis phenomenon is often termed 'overfitting' or 'overparameterization.'\nThis behavior can occur in models that have many parameters, and learn\ncorrelations in the data that do not contribute to learning human\npreferences, but are artifacts of noise in the dataset that one should\nultimately ignore. One can address variance in models by reducing the\nnumber of parameters or incorporating biases in the model based on\nfactors we can assume about the data.\n\n**Model Scope.** One important consideration unique to human preference\nmodels is that we wish to model individual preferences, and we may\nchoose to do so at arbitrary granularity. For example, we can fit models\nto a specific individual or even multiple models for an individual, each\nfor different purposes or contexts. On the other end of the spectrum, we\nmay create a model to capture human preferences across large populations\nor the world.\n\nIndividual models may certainly prove to be more powerful, as they do\nnot need to generalize across multiple individuals and can dedicate all\nof their parameters to learning the preferences of a single user. In the\ncontext of human behavior, this can be a significant advantage as any\ntwo individuals can be arbitrarily different or even opposite in their\npreferences. On the other hand, models fit only one person can\ntremendously overfit to the training distribution and capture noise in\nthe data, which is not truly representative of human preferences.\n\nOn the end of the spectrum, models fit to the entire world may be\ninadequate to model human preferences for arbitrary individuals,\nespecially those whose data it has not been fit to. As such, models may\nunderfit the given training distribution. These models aim to generalize\nto many people but may fail to capture the nuances of individual\npreferences, especially for those whose data is not represented in the\ntraining set. As a result, they may not perform well for arbitrary\nindividuals within the target population\n\nChoosing the appropriate scope for a model is crucial. ne must balance\nthe trade-off between overfitting to noise in highly granular models and\nunderfitting in broader models that may not capture individual nuances.\n\n## Multimodal Preferences\n\nOne of the core assumptions about learning a reward function is that it\nis unimodal, meaning that it consists of data from one person with a\ncertain set of preferences or a group of people with similar\npreferences. However, the model of unimodality often oversimplifies\nhuman preferences and their often conflicting nature. To accurately\ncapture all the nuances of human preference, we examine a multi-modal\ndistribution with some baseline assumptions. Consider a scenario where\nwe, as regular drivers, make a left-hand turn at an intersection\n[@myers2021learning]. What would we do if we saw a car speeding down the\nroad approaching us? The figure below describes some options. Following\na timid driving pattern, some vehicles would stop to let the other car\ngo, preventing a collision. Other vehicles would be more aggressive and\ntry to make the turn before colliding with the oncoming vehicle. Given\nthe data of one of these driving patterns, our model (our autonomous\nvehicle) can make an appropriate decision. However, what if our model\nwas given data from both aggressive and timid drivers, and we don't know\nwhich data corresponds to which type of driver? If we applied standard\nlearning based on comparison techniques, we see, as illustrated by the\nfigure below, that the car would have an accident trying to find a\npolicy close enough to both driving patterns.\n\n![[@myers2021learning] shows the possibilities of 2 different driving\npatterns when a car is taking a left-hand turn at an intersection and\nsees another car approaching\nhead-on.](Figures/driving-patt.png){#fig-driving-patt}\n\n![The figure [@myers2021learning] depicts the resultant collision when we\ntry to find a policy close enough to both the driving\npatterns.](Figures/driving-coll.png){#fig-driving-coll}\n\nAs illustrated by the driving example, we see that multi-modality for\nour reward function is extremely important and, in some cases, if it is\nnot considered, can lead to fatal decisions [@myers2021learning]. But why\ncan't we label the groups, which would be the timid and aggressive\ndrivers in the driving case, and then learn separate reward functions\nfor each driver? The first problem with this approach is that it is\ninefficient and time-consuming to separate the data into groups because\nwe would have to cluster and label the data. Secondly, it would not be\naccurate just to split the data because a more timid driver can be\naggressive when they are in a hurry.\n\nTo formulate this problem of learning reward functions and mixing\ncoefficients from ranking queries in a fully observable deterministic\ndynamical system, we begin by describing the system as a trajectory\n$\\xi = (s_0, a_0, ..., s_T, a_T)$, where the sequence of states and\nactions represents the system's evolution over time. Assume there are\n$M$ different reward functions, each representing an expert's\npreferences. Using the linearity assumption in reward learning, we model\neach expert's reward function as a linear combination of features in a\nknown, fixed feature space $\\phi(\\xi)$. The reward for the $m$-th expert\nis given by: $$R_m(\\xi) = \\omega^T_m \\phi(\\xi),$$ where $\\omega_m$ is a\nvector of parameters corresponding to the $m$-th expert's preferences.\nThere exists an unknown distribution over the reward parameters and we\ncan represent this distribution with mixing coefficients $\\alpha_m$ such\nthat $\\sum_M^{m = 1} \\alpha_m = 1$. Our goal is to learn reward\nfunctions and mixing coefficients using ranking queries.\n\nTo define our problem, let's consider a robot who performs the following\ntrajectories and asks a user to rank all the trajectories.\n\n![The figure [@myers2022learning] depicts a few different trajectories\nfor an example multi-modal ranking\nscenario.](Figures/robot-traj.png){#fig-robot-traj}\n\nThe robot will be given back a set of trajectory rankings, coming from M\nhumans and the objective is to learn the underlying reward function. We\ncan represent the response of the ranking query as\n$x = (\\xi_{a_1},\\ ...\\ ,\\xi_{a_K})$ where $a_1$ is the index of the\nexpert's top choice, $a_2$ is the index of the expert's second choice,\n\\... and so on. With the response $x$, we generate a probability\ndistribution with the softmax rule [@myers2022learning]:\n$Pr(x_1 = \\xi_{a_1} | R = R_m) = \\frac{e^R_m(\\xi_{a_1})}{\\sum_{j=1}^Ke^R_m(\\xi_{a_j})}$.\nwhere $R_m(\\xi_{a_i})$ denotes the reward assigned by the $m$-th expert\nto trajectory $\\xi_{a_i}$. Then, we randomly sample our probability\ndistribution to pick our top choice. From the remaining trajectories, we\nnoisily choose from our distribution to rank our second-best option. We\nrepeat this process until we have ranked all our trajectories. This\nfollows what is known as the Plackett-Luce Ranking Model.\n\nGiven knowledge of the true reward function weights $\\omega_m$ and\nmixing coefficients $\\alpha_m$, we have the following joint mass over\nobservations x from a query Q:\n$Pr(x\\ |\\ Q) = \\sum_{m = 1}^M \\alpha_m\\prod_{i = 1}^K\\frac{e^{\\omega_m^T \\Phi(\\xi_{a_i})}}{\\sum_{j = i}^K e^{\\omega_m^T \\Phi(\\xi_{a_j})}}$.\n\nWith the above formulation of the joint mass distribution over\nobservation and queries, we can now formulate an objective.\nSpecifically, it is to present users with the best set of queries that\nlearn reward weights, $\\omega$, and mixing coefficient, $\\alpha$, based\nupon user rankings of preferred query responses. By learning these\nparameters, we can have an accurate estimation of the joint mass\ndistribution of the observations.\n\nTo learn these parameters, we use a Bayesian learning framework. The\ngoal will be to learn the reward weights, $\\omega_m$, and all mixing\ncoefficients $\\alpha_m$. Thus, define the parameters to be\n$\\theta = \\{\\omega, \\alpha\\}$. We start by simplifying the posterior\nover the parameters.\n\n$$\\begin{aligned}\n\\Pr(\\Theta | Q^{(1)}, x^{(1)}, Q^{(2)}, x^{(2)}, \\ldots) & \\propto \\Pr(\\Theta) \\Pr(Q^{(1)} | x^{(1)}, Q^{(2)}, x^{(2)}, \\ldots | \\Theta) \\\\\n& = \\Pr(\\Theta) \\prod_t \\Pr(x^{(t)} | Q^{(t)}, \\Theta, Q^{(1)}, x^{(1)}, \\ldots, Q^{(t-1)}, x^{(t-1)}) \\\\\n& \\propto \\Pr(\\Theta) \\prod_t \\Pr(x^{(t)} | \\Theta, Q^{(t)})\n\\end{aligned}$$\n\nNote that the first proportionality term is directly from Bayes rule\n(removing normalization constant). The first equation comes directly\nfrom the assumption that the queries at timestamp $t$ are conditionally\nindependent of the parameters given previous queries & rankings. This\nassumption is reasonable because the previous queries & rankings ideally\ngive all the information to inform the choice of the next set of. The\nlast proportionality term comes from the assumption that the ranked\nqueries are conditionally independent given the parameters\n\nThe prior distribution is dependent on use case. For example, in the\nuser studies conducted by the authors to verify this method, they use a\nstandard Gaussian for the reward weights and the mixing coefficients to\nbe uniform on a $M - 1$ simplex to ensure that they add up to 1. Then we\ncan use maximum likelihood estimation to compute the parameters with the\nsimplified posterior.\n\n<!-- {{< include psets/pset1.qmd >}} -->\n\nThrough our exploration of human preference models, we will ground ourselves in\nbuilding a health coaching system that can provide meal recommendations aligned with a user's dietary needs and preferences. Examples of scenarios which can benefit from a model of how humans make choices include:\n\n1.  **Health coaching:** Humans express their preferences every time\n    they pick lunch for consumption. Humans may have several goals\n    related to nutrition, such as weight loss and improving\n    concentration. We can learn how a given individual or set of\n    individuals prefer to eat to provide personalized recommendations to\n    help them attain their goals. This chapter will use this use case to\n    ground human preference modeling in a real-life application.\n\n2.  **Social media:** Platforms have a far greater amount of content\n    than one can consume in a lifetime, yet such products must aim to\n    maximize user engagement. To accomplish this, we can learn what\n    specific things people like to see in their feeds to optimize the\n    value they gain out of their time on social media. For example, the\n    video feed social media platform [TikTok](https://www.tiktok.com/)\n    has had viral adoption due to its notorious ability to personalize a\n    feed for its users based on their preferences.\n\n3.  **Shopping:** Retail corporations largely aim to maximize revenue by\n    making it easy for people to make purchases. Recommendation systems\n    on online shopping platforms provide a mechanism for curating\n    specific items based on an individual's previous purchases (or even\n    browsing history) to make shoppers aware of items they may like and,\n    therefore, purchase.\n\n<!--\nTwo key models used in pairwise sampling are the Thurstonian and Bradley-Terry models [@cattelan2012]. The Thurstonian model assumes each item $i$ has a true score $u_i$ following a normal distribution. The difference $d_{ij} = u_i - u_j$ is also normally distributed. The probability that item $i$ is preferred over item $j$ is given by $P(i \\succ j) = \\Phi \\left( \\frac{u_i - u_j}{\\sqrt{2\\sigma^2}} \\right)$, where $\\Phi$ is the cumulative normal distribution function. The denominator $\\sqrt{2\\sigma^2}$ is the standard deviation of the difference $d_{ij} = u_i - u_j$ when $u_i$ and $u_j$ are normally distributed with variance $\\sigma^2$[@cattelan2012]. The Bradley-Terry model defines the probability of preference based on latent scores $\\beta_i$ and $\\beta_j$. The probability that item $i$ is preferred over item $j$ is $P(i \\succ j) = \\frac{e^{\\beta_i}}{e^{\\beta_i} + e^{\\beta_j}}$. This model is used to estimate relative strengths or preferences based on latent scores. [@cattelan2012].\n\n::: {#tbl-philosophy}\n  -----------------------------------------------------------------------\n  Application                         Human Preference\n  ----------------------------------- -----------------------------------\n  Computer vision: train a neural     This is how humans process images\n  network to predict bounding boxes   by identifying the position and\n  delineating all instances of dogs   geometry of the things we see in\n  in an image                         them\n\n  Natural language processing: train  Coherent text is itself a\n  a model to generate coherent text   human-created and defined concept,\n                                      and we prefer that any\n                                      synthetically generated text\n                                      matches that of humans\n\n  Computer vision: train a diffusion  Humans prefer that images\n  model to generate realistic images  accurately capture the world as\n  of nature                           observed by humans, and this\n                                      generative model should reflect the\n                                      details that comprise that\n                                      preference\n  -----------------------------------------------------------------------\n\n  : Examples of machine learning tasks and their interpretation as\n  modeling human preferences.\n:::\n-->\n\n",
    "supporting": [
      "002-reward_model_files/figure-pdf"
    ],
    "filters": []
  }
}