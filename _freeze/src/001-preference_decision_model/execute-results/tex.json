{
  "hash": "777c5e0470236518f34cdbf6a7f42a62",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Models of Preferences and Decisions\nformat: html\nfilters:\n  - pyodide\nexecute:\n  engine: pyodide\n  pyodide:\n    auto: true\n\n---\n\n\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n<iframe\n  src=\"https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/\"\n  style=\"width:45%; height:225px;\"\n></iframe>\n<iframe\n  src=\"https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/\"\n  style=\"width:45%; height:225px;\"\n></iframe>\n[Fullscreen Part 1](https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/){.btn .btn-outline-primary .btn role=\"button\"}\n[Fullscreen Part 2](https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/){.btn .btn-outline-primary .btn role=\"button\"}\n\n:::\n\nHuman preference modeling aims to capture humans' decision making processes in a probabilistic framework. Many problems would benefit from a quantitative perspective, enabling an understanding of how humans engage with the world. In this chapter, we will explore how one can model human preferences, including different formulations of such models, how one can optimize these models given data, and considerations one should understand to create such systems.\n\n## The Construction of Preference {#sec-foundations}\n\n### Axiom 1: Construction of Choices Set {#axiom-1-preference-models-model-choice}\nPreference models model the preferred choices amongst a set of options. Preference models must enumerate the set of all possible choices included in a human decision. As such, we must ensure that the choices we enumerate capture the entire domain (collectively exhaustive) but are distinct (mutually exclusive) choices. A discrete set of choices is a constraint we canonically impose to ensure we can tractably model preferences and aptly estimate the parameters of preference models. We assume that if a new option is added to the choice set, the relative probabilities of choosing between the original options remain unchanged. This is known as Independence of Irrelevant Alternatives (IIA) property from Luce's axiom of choices [@Luce1977].\n\n### Axiom 2: Preference Centers around Utility {#axiom-2-preference-centers-around-utility}\nPreference models are centered around the notion of utility, a scalar quantity representing the benefit or value an individual attains from selecting a given choice. We assume that the underlying utility mechanism of a human preference model captures the final decision output from a human. We use the notation $u_{i,j}$ as the utility of person $i$ choosing item $j$. The utility is a random variable, decomposing into true utility $u_{i,j}^*$ and a random noise $\\epsilon_{i,j}$: $u_{i,j} = u_{i,j}^* + \\epsilon_{i,j}$. According to [@mcfadden_conditional_1974], utility can further be decomposed into user-specific utility $\\theta_i$ and item-specific utility $z_j$: $u_{i,j}^* = \\theta_i + z_j$. This decomposition indicate that for a single users, only the relative difference in utility matters to predict the choice among options and the scale of utilities is important when comparing across user. \n\n### Axiom 3: Preference captures decision-making {#axiom-3-preference-captures-decision-making}\nHuman preferences are classified into two categories: revealed preferences and stated preferences. Revealed preferences are those one can observe retroactively from existing data. The implicit decision-making knowledge can be captured via learnable parameters and their usage in models which represent relationships between input decision attributes that may have little interpretability, but enable powerful models of human preference. Such data may be easier to acquire and can reflect real-world outcomes (since they are, at least theoretically, inherently based on human preferences). However, if we fail to capture sufficient context in such data, human preference models may not sufficiently capture human preferences. Stated preferences are those individuals explicitly indicate in potentially experimental conditions. The explicit knowledge may be leveraged by including inductive biases during modeling (for example, the context used in a model) which are reasonable assumptions for how a human would consider a set of options.This may include controlled experiments or studies. This may be harder to obtain and somewhat biased, as they can be hypothetical or only accurately reflect a piece of the overall context of a decision. However, they enable greater control of the decision-making process.\n\n### Axiom 4: Rationality {#human-rationality}\nPreference model assumes that human is rational. Perfect rationality posits that individuals make decisions that maximize their utility, assuming they have complete information and the cognitive ability to process this information to make optimal choices. Numerous studies have shown that this assumption frequently fails to describe actual human behavior. Bounded rationality acknowledges that individuals operate within the limits of their information and cognitive capabilities [@simon1972theories]. Here, decisions being influenced by noise, resulting in probabilistic choice behavior: while individuals aim to maximize their utility, noise can lead to deviations from perfectly rational choices [@miljkovic2005rational]. Instead of deterministic utility maximization, the decision maker will choose an item with probability proportional to the utility they receive for that item. This probabilistic model can be operationalized with Boltzmann distribtion. Utility of an item $j$ is computed by a function $f: e_i \\rightarrow \\mathbb{R}$, where $e_j \\in \\mathbb{R}^d$ is a vector representation of item $j$. The probability of item $j$ being preferred over all other alternatives in the choice set $\\mathcal{C}$ is\n\n$$\np(j \\succ j': j' \\neq j \\forall j' \\in \\mathcal{C}) = Z^{-1} \\exp(f(e_j)) \\text{ where } Z = \\sum_{j' \\in \\mathcal{C}} \\exp(f(e_{j'}))\n$$\n\nOne can extend the above model in various way. For example, the above model has no concept of similar actions. Consider the following example when choosing mode of transportation: car ad train with no particular preference for either choice: the preferred probability is 50% for either option. However, if we have 99 cars and 1 train in the choice set, we would have a 99% probability of choosing a car. To address this issue, various extensions have been proposed. One such extension is the attribute rule, which interprets options as bundles of attributes. In this rule, attributes $X$ are associated with options, and they have desirability values $w(x)$ [@2001.04465]. An attribute intensity function $s(x, o)$ indicates the degree to which an attribute is expressed in an option. The probability of choosing option $o$ is\n\n$$P(o) = \\sum_{x \\in \\mathcal{X}_o} \\frac{w(x)}{\\sum_{\\bar{x} \\in \\mathcal{X}_o} w(\\bar{x})} \\cdot \\frac{s(x, o)}{\\sum_{\\tilde{o} \\in \\mathcal{O}} s(x, \\bar{o})}$$\n\nAn attribute $x \\in X_O$ is first chosen according to a Boltzmann distribution and then an option $o \\in O$ with that attribute is selected using another Boltzmann distribution. Instead of creating attributes, we can introduce a similarity metric on the space of continuous actions, thereby creating similar groupings on trajectories. Let $\\phi \\in \\Phi$ be the set of all possible feature vectors $\\xi \\in \\Xi$ the set of all trajectories. The set of feature vectors belonging to a set of trajectories $\\Xi' \\subseteq \\Xi$ is $\\Phi_{\\Xi'}$. We begin with equation (4) and substitute our similarity metric on feature vectors of trajectories.\n\n$$\\begin{aligned}\n    P(\\xi) = \\frac{e^{R(\\phi(\\xi))}}{\\sum_{\\bar{\\phi} \\in \\Phi_{\\Xi}} e^{R(\\hat{\\phi})}} \\cdot \\frac{s(\\phi(\\xi), \\bar{\\xi})}{\\sum_{\\hat{\\xi} \\in \\Xi} s(\\phi(\\xi), \\bar{\\xi})}\n\\end{aligned}$$\n\nThe probability of choosing trajectory $\\xi$ is proportional to the exponentiated reward for the agent's measured trajectory $\\phi(\\xi)$, normalized by the sum of all rewards over all possible measured trajectories. The second half of the product is a normalization factor based on how similar the current trajectory is to other trajectories in feature space. We can define the similarity function as an indicator function, where $s(x, \\xi) = 1$ only if $x = \\phi(\\xi)$. That means that multiple trajectories with the same feature vector will effectively be considered a single option. Thus, we achieve the \"bundling\" of trajectories, in the same way that the attribute rule bundled options under different attributes.\n\nHowever, setting the similarity metric as an indicator function isn't sufficiently flexible. We want a proper metric that acts more as a continuous distance over the feature space. We instead define $s$ to be a soft similarity metric $s : \\Phi \\times \\Xi \\rightarrow \\mathbb{R}^+$ with the following properties: Identity (an item is most similar to itself), symmetric (the similarity of item $j$ to $j'$ is the same as that of $j'$ to $j$), and positive semidefinite (similarity metric is non-negative). This metric enable extending (5) to be a probability density on the continuous trajectory space $\\mathcal{E}$, as in (3).\n\n$$p(\\hat{\\xi}) = \\frac{\\frac{e^{R(\\phi(\\xi))}}{\\int_{\\Xi} s(\\phi(\\xi), \\bar{\\xi}) d\\bar{\\xi}}}{\\int_{\\Xi}\\frac{e^{R(\\phi(\\hat{\\xi}))}}{\\int_{\\Xi} s(\\phi(\\hat{\\xi}), \\bar{\\xi}) d\\bar{\\xi}}d\\hat{\\xi}} \\propto \\frac{e^{R(\\phi(\\hat{\\xi}))}}{\\int_{\\Xi} s(\\phi(\\xi), \\bar{\\xi}) d\\bar{\\xi}}$$\n\nHere, the probability of selecting an item is inversely proportional to its feature-space similarity with alternatives. This de-weights similar items, which is the desired effect for human decision-making.\n\n## Models of Preferences and Decisions {#preference-model}\n\nNext, we explore various mechanisms by which humans can express their preferences, including accept-reject sampling, pairwise sampling, rank-order sampling, rating-scale sampling, best-worst scaling, and multiple-choice samples. We will understand the process of collecting data through simulation, and when appropriate, discuss the real world application of these models. Each item $i$ is represented by a $d=2$ dimentional vector $x^i$. There is only one user in the simulation, and they have a latent utility function $f$ that they use to compute the latent utility of item from the features. Here, the latent utility function is the Ackley function \\cite{ackley1987}.\n\n::: {.callout-note title=\"code\"}\n```{pyodide-python}\nimport numpy as np\nnp.random.seed(0)\n\ndef ackley(X, a=20, b=0.2, c=2*np.pi):\n    \"\"\"\n    Compute the Ackley function.\n    Parameters:\n      X: A NumPy array of shape (n, d) where each row is a d-dimensional point.\n      a, b, c: Parameters of the Ackley function.\n    Returns:\n      A NumPy array of function values.\n    \"\"\"\n    X = np.atleast_2d(X)\n    d = X.shape[1]\n    sum_sq = np.sum(X ** 2, axis=1)\n    term1 = -a * np.exp(-b * np.sqrt(sum_sq / d))\n    term2 = -np.exp(np.sum(np.cos(c * X), axis=1) / d)\n    return term1 + term2 + a + np.e\n```\n:::\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nnp.random.seed(0)\n\ndef ackley(X, a=20, b=0.2, c=2*np.pi):\n    \"\"\"\n    Compute the Ackley function.\n    Parameters:\n      X: A NumPy array of shape (n, d) where each row is a d-dimensional point.\n      a, b, c: Parameters of the Ackley function.\n    Returns:\n      A NumPy array of function values.\n    \"\"\"\n    X = np.atleast_2d(X)\n    d = X.shape[1]\n    sum_sq = np.sum(X ** 2, axis=1)\n    term1 = -a * np.exp(-b * np.sqrt(sum_sq / d))\n    term2 = -np.exp(np.sum(np.cos(c * X), axis=1) / d)\n    return term1 + term2 + a + np.e\n```\n:::\n\n\nWe next define a function to visualize the surface:\n\n::: {.callout-note title=\"code\"}\n```{pyodide-python}\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nccmap = LinearSegmentedColormap.from_list(\"ackley\", [\"#f76a05\", \"#FFF2C9\"])\nplt.rcParams.update({\n    \"font.size\": 14,\n    \"axes.labelsize\": 16,\n    \"xtick.labelsize\": 14,\n    \"ytick.labelsize\": 14,\n    \"legend.fontsize\": 14,\n    \"axes.titlesize\": 16,\n})\n\ndef draw_surface():\n    inps = np.linspace(-2, 2, 100)\n    X, Y = np.meshgrid(inps, inps)\n    grid = np.column_stack([X.ravel(), Y.ravel()])\n    Z = ackley(grid).reshape(X.shape)\n    \n    plt.figure(figsize=(6, 5))\n    contour = plt.contourf(X, Y, Z, 50, cmap=ccmap)\n    plt.contour(X, Y, Z, levels=15, colors='black', linewidths=0.5, alpha=0.6)\n    plt.colorbar(contour, label=r'$f(x)$', ticks=[0, 3, 6])\n    plt.xlim(-2, 2)\n    plt.ylim(-2, 2)\n    plt.xticks([-2, 0, 2])\n    plt.yticks([-2, 0, 2])\n    plt.xlabel(r'$x_1$')\n    plt.ylabel(r'$x_2$')\n```\n:::\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nccmap = LinearSegmentedColormap.from_list(\"ackley\", [\"#f76a05\", \"#FFF2C9\"])\nplt.rcParams.update({\n    \"font.size\": 14,\n    \"axes.labelsize\": 16,\n    \"xtick.labelsize\": 14,\n    \"ytick.labelsize\": 14,\n    \"legend.fontsize\": 14,\n    \"axes.titlesize\": 16,\n})\nplt.rcParams['text.usetex'] = True\n\ndef draw_surface():\n    inps = np.linspace(-2, 2, 100)\n    X, Y = np.meshgrid(inps, inps)\n    grid = np.column_stack([X.ravel(), Y.ravel()])\n    Z = ackley(grid).reshape(X.shape)\n    \n    plt.figure(figsize=(6, 5))\n    contour = plt.contourf(X, Y, Z, 50, cmap=ccmap)\n    plt.contour(X, Y, Z, levels=15, colors='black', linewidths=0.5, alpha=0.6)\n    plt.colorbar(contour, label=r'$f(x)$', ticks=[0, 3, 6])\n    plt.xlim(-2, 2)\n    plt.ylim(-2, 2)\n    plt.xticks([-2, 0, 2])\n    plt.yticks([-2, 0, 2])\n    plt.xlabel(r'$x_1$')\n    plt.ylabel(r'$x_2$')\n```\n:::\n\n\n### Item-wise Model {#item-wise-model}\nOne method for data collection is accept-reject sampling, where the user consider one item at a time and decide if they like it or not. Below is an example survey using accept-reject sampling:\n\n::: {.content-visible when-format=\"html\"}\n<iframe\n  src=\"https://app.opinionx.co/5f30e903-c7b2-42e3-821a-e65271144bd9\"\n  style=\"width:100%; height:450px;\"\n></iframe>\n:::\n\nWe will familiarize ourselves with accept-reject sampling through a simulation. In the surface below, blue and red points corresponds to accept or reject points.\n\n::: {.callout-note title=\"code\"}\n```{pyodide-python}\nd = 2\nn_items = 800\nitems = np.random.randn(n_items, d)*0.5 + np.ones((n_items, d))*0.5\nutilities = ackley(items)\ny = (utilities > utilities.mean())\ndraw_surface()\nplt.scatter(items[:, 0], items[:, 1], c=y, cmap='coolwarm', alpha=0.5)\nplt.show()\n```\n:::\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nd = 2\nn_items = 800\nitems = np.random.randn(n_items, d)*0.5 + np.ones((n_items, d))*0.5\nutilities = ackley(items)\ny = (utilities > utilities.mean())\ndraw_surface()\nplt.scatter(items[:, 0], items[:, 1], c=y, cmap='coolwarm', alpha=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](001-preference_decision_model_files/figure-pdf/cell-4-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nBinary choice model is centered around one item. The model predicts, for that option, after observing user choices in the past, whether that option will be chosen or not. We use binary variable $y \\in \\{0, 1\\}$ to represent whether that choice will be picked by the user in the next phase of selection. We denote $P = \\mathbb{P}(y = 1)$. We can formally model $y$ as a function of the utility of the positive choice: $y = \\mathbb{I}[U>0]$. We explore two cases based on the noise distribution. $\\psi$ is the logistic function or the standard normal cummulative distribution function if noise follows logistic distribution and the standard normal distribution, repsectively:\n$$\n\\mathbb{P}(u_{i,j} > 0) = \\mathbb{P}(u_{i,j}^* + \\epsilon > 0) = 1 - \\mathbb{P}( \\epsilon < -u_{i,j}^*) = \\psi(u_{i,j}^*).\n$$\n\nThe utility of the item can take parametric form, such as $z_j = f_{\\theta}(x_j)$. It can also take the nonparametric form, such as commonly used in ideal point model, where the utility of an item $j$ is calculated by the distance from the item to human in some embedding space[@huber1976ideal]. Given vector representation $z_i$ of choice $i$ and a vector $v_n$ representing an individual $n$, we can use a distance function to model a stochastic utility function with the unobserved factors following a specified distribution: $u_{n, i} = \\texttt{dist}(z_i, v_n) + \\epsilon_{n, i}$. The intuition is that vectors exist in a shared $n$-dimensional space, and as such we can use geometry to match choices whose representations are closest to that of a given individual [@ideal_point; @tatli2022distancepreferences] when equipped with a distance metric. Certain distance metrics, such as Euclidian distance or inner product, can easily be biased by the scale of vectors. A distance measure such as cosine similarity, which compensates for scale by normalizing the inner product of two vectors by the product of their magnitudes, can mitigate this bias yet may discard valuable information encoded by the length of the vectors. Beyond the distance metric alone, this model places a strong inductive bias that the individual and choice representations all share a common embedding space. In some contexts, this can be a robust bias to add to the model [@idealpoints], but it is a key factor one must take into account before employing such a model, and is a key design choice for modeling.\n\nA generalization of accept-reject sampling is rating-scale sampling. Rating-scale sampling, such as the Likert scale, is a method in which participants rate items on a fixed-point scale (e.g., 1 to 5, \"Strongly Disagree\" to \"Strongly Agree\") to measure levels of preference towards items [@harpe2015]. Participants can also mark a point on a continuous rating scale to indicate their preference or attitude. Commonly used in surveys, product reviews, and psychological assessments, this method provides a more nuanced measure than discrete scales. Rating-scale sampling is simple for participants to understand and use, provides rich data on the intensity of preferences, and is flexible enough for various measurements (e.g., agreement, satisfaction). However, rating-scale sampling methods also have limitations. Ratings can be influenced by personal biases and interpretations of scales, leading to subjectivity. There is a central tendency bias, where participants may avoid extreme ratings, resulting in clustering responses around the middle. Different participants might interpret scale points differently, and fixed-point scales may not capture the full nuance of participants' preferences or attitudes.\n\n::: {.callout-note title=\"code\"}\n```{pyodide-python}\nfrom matplotlib.colors import LinearSegmentedColormap\nlikert_cmap = LinearSegmentedColormap.from_list(\"likert_scale\", [\"red\", \"blue\"], N=5)\nnormalized = (utilities - utilities.min()) / (utilities.max() - utilities.min())\nratings = np.round(normalized * 4).squeeze()\n\ndraw_surface()\nscatter = plt.scatter(items[:, 0], items[:, 1], c=ratings, cmap=likert_cmap, alpha=0.5)\nplt.show()\n```\n:::\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom matplotlib.colors import LinearSegmentedColormap\nlikert_cmap = LinearSegmentedColormap.from_list(\"likert_scale\", [\"red\", \"blue\"], N=5)\nnormalized = (utilities - utilities.min()) / (utilities.max() - utilities.min())\nratings = np.round(normalized * 4).squeeze()\n\ndraw_surface()\nscatter = plt.scatter(items[:, 0], items[:, 1], c=ratings, cmap=likert_cmap, alpha=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](001-preference_decision_model_files/figure-pdf/cell-5-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nLet us suppose we have a single example with attributes $z_i$, and wish to know which of $J$ rating scales an individual will choose from. We can define $J - 1$ parameters, which act as thresholds on the utility computed by $u_i = u_{i,j}^*$ to classify the predicted choice between these options. For example, if there are 3 predefined options, we can define parameters $a, b \\in \\mathbb{R}$ such that\n\n$$\ny_i =\n\\begin{cases} \n    1 & u < a \\\\\n    2 & a \\le u < b \\\\\n    3 & \\text{else}\n\\end{cases}\n$$\n\nBy assuming the noise distribution to be either logistic or standard normal, we have \n$$\n\\begin{split}\n    \\mathbb{P}(y_i = 1) & = \\mathbb{P}(u < a) = \\mathbb{P}(u_{i,j}^* + \\epsilon < a) = \\psi(a-u_{i,j}^*) \\\\\n    \\mathbb{P}(y_i = 2) & = \\mathbb{P}(a \\le u < b) = \\mathbb{P}(a - u_{i,j}^* \\le \\epsilon < b - u_{i,j}^*) = \\psi(b-u_{i,j}^*)  - \\psi(u_{i,j}^*-a) \\\\\n    \\mathbb{P}(y_i = 3) & = \\mathbb{P}(u > b) = \\mathbb{P}(u_{i,j}^* + \\epsilon > b ) = \\mathbb{P}( \\epsilon > b - u_{i,j}^*) = \\psi(b-u_{i,j}^*)\n\\end{split}\n$$\n\nHaving the model, we next explore the estimation of model parameters. A common approach for parameters estimation is maximum likelihood [@book_estimation_casella; @book_estimation_bock]. The likelihood of a model is the probability of the observed data given the model parameters; intuitively we wish to maximize this likelihood, as that would mean that our model associates observed human preferences in the data with high probability. We can formally define the likelihood for a model with parameters $\\beta$ and a given data point $(z_i, y_i)$ as: $$\\mathcal{L}(z_i, y_i; \\beta) = \\mathbb{P}(y = y_i | z_i; \\beta)$$ Assuming our data is independent and identically distributed (iid), the likelihood over the entire dataset is the joint probability of all observed data as defined by the model:\n\n$$\\mathcal{L}(z, Y; \\beta) = \\prod_{i = 1}^J \\mathbb{P}(y = y_i | z_i; \\beta)$$\n\nIn binary choice with logistic noise, this was simply the model's probability of the observed preference value: $$\\mathcal{L}(z_i, y_i; \\beta) = \\frac{1}{1 + \\exp^{-u_{i,j}^*}}$$ Fortunately, in these cases, there are straightforward methods for parameter estimation: logistic regression and probit regression (binary or multinomial, depending on the model), respectively. We can use ordinal regression to estimate the model's parameters for our ordered preference model.\n\nThe objective function commonly found in parameter learning can be optimized with stochastic gradient descent (SGD) [@gradient_descent]. We can define an objective function as the likelihood to maximize this objective. Since SGD minimizes a given objective, we must negate the likelihood, which ensures that a converged solution maximizes the likelihood. SGD operates by computing the gradient of the objective with respect to the parameters of the model, which provides a signal of the direction in which the parameters must move to *maximize* the objective. Then, SGD makes an update step by subtracting this gradient from the parameters (most often with a scale factor called a *learning rate*), to move the parameters in a direction which *minimizes* the objective. When the objective is the negative likelihood (or sometimes negative log-likelihood for convenience or tractability), the result is an increase in the overall likelihood.\n\nIn the case of logistic and Gaussian models, SGD may yield a challenging optimization problem as its stochasticity can lead to noisy updates, for example, if certain examples or batches of examples are biased. Mitigations include batched SGD, in which multiple samples are randomly sampled from the dataset at each iteration, learning rates, which reduce the impact of noisy gradient updates, and momentum and higher-order optimizers which reduce noise by using movering averages of gradients or provide better estimates of the best direction in which to update the gradients. Some models, such as those that use neural networks, may, in fact, be intractable to estimate without a method such as SGD (or its momentum-based derivatives). For example, neural networks with many layers, non-linearities, and parameters can only be efficiently computed with gradient-based methods.\n\n::: {.callout-note title=\"code\"}\n```{pyodide-python}\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\n\n# Set random seed for reproducibility (optional)\nnp.random.seed(42)\n\n# Number of users and items\nnum_users = 50\nnum_items = 100\n\n# Generate user-specific and item-specific utilities\ntheta_true = np.random.randn(num_users)\nz_true = np.random.randn(num_items)\n\n# Define the logistic (sigmoid) function\ndef sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-x))\n\n# Generate observed choices using the logistic function\n# Compute probability matrix: shape (num_users, num_items)\nprobs = sigmoid(theta_true[:, None] - z_true[None, :])\n# Sample binary responses (0 or 1) from a Bernoulli distribution\ndata = np.random.binomial(1, probs)\n\n# Mask out a fraction of the response matrix (80% observed, 20% missing)\nmask = np.random.rand(num_users, num_items) > 0.2  # boolean mask\n# Create a version of the data with missing values (not needed for optimization, but for reference)\ndata_masked = data.copy().astype(float)\ndata_masked[~mask] = np.nan\n\n# Count of observed entries (used for averaging)\nobserved_count = np.sum(mask)\n\n# We will optimize over parameters theta and z.\n# Initialize estimates (random starting points)\ntheta_init = np.random.randn(num_users)\nz_init = np.random.randn(num_items)\n\n# Pack parameters into a single vector for the optimizer.\n# First num_users elements are theta_est, next num_items are z_est.\nparams_init = np.concatenate([theta_init, z_init])\n\ndef objective(params):\n    \"\"\"\n    Computes the loss and gradient for the current parameters.\n    Loss is defined as the negative log likelihood (averaged over observed entries).\n    \"\"\"\n    # Unpack parameters\n    theta = params[:num_users]\n    z = params[num_users:]\n    \n    # Compute difference and estimated probabilities\n    diff = theta[:, None] - z[None, :]  # shape: (num_users, num_items)\n    sigma = sigmoid(diff)\n    \n    # To avoid log(0), clip probabilities a little bit\n    eps = 1e-8\n    sigma = np.clip(sigma, eps, 1 - eps)\n    \n    # Compute negative log likelihood only on observed entries\n    # For each observed entry: if data == 1 then -log(sigma) else -log(1-sigma)\n    log_likelihood = data * np.log(sigma) + (1 - data) * np.log(1 - sigma)\n    loss = -np.sum(mask * log_likelihood) / observed_count\n    \n    # Compute gradient with respect to the difference x = theta_i - z_j\n    # d(loss)/d(x) = sigma - data  (for observed entries, zero otherwise)\n    diff_grad = (sigma - data) * mask  # shape: (num_users, num_items)\n    \n    # Gradients for theta: sum over items (axis 1)\n    grad_theta = np.sum(diff_grad, axis=1) / observed_count\n    # Gradients for z: negative sum over users (axis 0)\n    grad_z = -np.sum(diff_grad, axis=0) / observed_count\n    \n    # Pack gradients back into a single vector\n    grad = np.concatenate([grad_theta, grad_z])\n    return loss, grad\n\n# Callback to track progress (optional)\niteration_progress = tqdm()\n\ndef callback(xk):\n    iteration_progress.update(1)\n\n# Optimize using L-BFGS-B\nresult = minimize(\n    fun=lambda params: objective(params),\n    x0=params_init,\n    method=\"L-BFGS-B\",\n    jac=True,\n    callback=callback,\n    options={\"maxiter\": 100, \"disp\": True}\n)\niteration_progress.close()\n\n# Extract the estimated parameters\ntheta_est = result.x[:num_users]\nz_est = result.x[num_users:]\n\n# Compute final estimated probabilities\nprobs_final = sigmoid(theta_est[:, None] - z_est[None, :])\n\n# Compute AUC ROC on observed (training) and missing (test) entries\ntrain_probs = probs_final[mask]\ntest_probs = probs_final[~mask]\ntrain_labels = data[mask]\ntest_labels = data[~mask]\n\nauc_train = roc_auc_score(train_labels, train_probs)\nauc_test = roc_auc_score(test_labels, test_probs)\n\nprint(f\"Train AUC: {auc_train:.4f}\")\nprint(f\"Test AUC: {auc_test:.4f}\")\n\n```\n:::\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributions import Bernoulli\nfrom tqdm import tqdm\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Number of users and items\nnum_users = 50\nnum_items = 100\n\n# Generate user-specific and item-specific utilities\ntheta = torch.randn(num_users, device=device, requires_grad=True)\nz = torch.randn(num_items, device=device, requires_grad=True)\n\n# Generate observed choices using logistic function\nprobs = torch.sigmoid(theta[:, None] - z[None, :])\ndata = Bernoulli(probs=probs).sample()\n\n# Mask out a fraction of the response matrix\nmask = torch.rand_like(data) > 0.2  # 80% observed, 20% missing\ndata_masked = data.clone()\ndata_masked[~mask] = float('nan')\n\n# Initialize parameters for EM algorithm\ntheta_est = torch.randn(num_users, device=device, requires_grad=True)\nz_est = torch.randn(num_items, device=device, requires_grad=True)\n\n# Optimizer\noptimizer = optim.LBFGS([theta_est, z_est], lr=0.1, max_iter=20, history_size=10, line_search_fn=\"strong_wolfe\")\n\ndef closure():\n    optimizer.zero_grad()\n    probs_est = torch.sigmoid(theta_est[:, None] - z_est[None, :])\n    loss = -(Bernoulli(probs=probs_est).log_prob(data) * mask).mean()\n    loss.backward()\n    return loss\n\n# EM Algorithm\npbar = tqdm(range(100))\nfor iteration in pbar:\n    if iteration > 0:\n        previous_theta = theta_est.clone()\n        previous_z = z_est.clone()\n        previous_loss = loss.clone()\n    \n    loss = optimizer.step(closure)\n    \n    if iteration > 0:\n        d_loss = (previous_loss - loss).item()\n        d_theta = torch.norm(previous_theta - theta_est, p=2).item()\n        d_z = torch.norm(previous_z - z_est, p=2).item()\n        grad_norm = torch.norm(optimizer.param_groups[0][\"params\"][0].grad, p=2).item()\n        grad_norm += torch.norm(optimizer.param_groups[0][\"params\"][1].grad, p=2).item()\n        pbar.set_postfix({\"grad_norm\": grad_norm, \"d_theta\": d_theta, \"d_z\": d_z, \"d_loss\": d_loss})\n        if d_loss < 1e-5 and d_theta < 1e-5 and d_z < 1e-5 and grad_norm < 1e-5:\n            break\n\n# Compute AUC ROC on observed and inferred data\nfrom torchmetrics import AUROC\nauroc = AUROC(task=\"binary\")\nprobs_final = torch.sigmoid(theta_est[:, None] - z_est[None, :])\ntrain_probs = probs_final[mask]\ntest_probs = probs_final[~mask]\ntrain_labels = data[mask]\ntest_labels = data[~mask]\nauc_train = auroc(train_probs, train_labels)\nauc_test = auroc(test_probs, test_labels)\nprint(f\"train auc: {auc_train}\")\nprint(f\"test auc: {auc_test}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrain auc: 0.835555374622345\ntest auc: 0.8040801882743835\n```\n:::\n:::\n\n\n### Pairwise Model {#pairwise-model}\n\nIn *pairwise sampling*, participants compare two options to determine which is preferred. One of the major advantage of this method is low cognitive demand for rater. Its disavantage is the limited amount of information content elicited by a sample. Below is a survey based on pairwise sampling:\n\n::: {.content-visible when-format=\"html\"}\n<iframe\n  src=\"https://app.opinionx.co/6bef4ca1-82f5-4c1d-8c5a-2274509f22e2\"\n  style=\"width:100%; height:450px;\"\n></iframe>\n:::\n\n::: {.callout-note title=\"code\"}\n```{pyodide-python}\nn_pairs = 10000\npair_indices = np.random.randint(0, n_items, size=(n_pairs, 2))\n# Exclude pairs where both indices are the same\nmask = pair_indices[:, 0] != pair_indices[:, 1]\npair_indices = pair_indices[mask]\n\nscores = np.zeros(n_items, dtype=int)\nwins = utilities[pair_indices[:, 0]] > utilities[pair_indices[:, 1]]\n\n# For pairs where the first item wins:\n#   - Increase score for the first item by 1\n#   - Decrease score for the second item by 1\nnp.add.at(scores, pair_indices[wins, 0], 1)\nnp.add.at(scores, pair_indices[wins, 1], -1)\n\n# For pairs where the second item wins or it's a tie:\n#   - Decrease score for the first item by 1\n#   - Increase score for the second item by 1\nnp.add.at(scores, pair_indices[~wins, 0], -1)\nnp.add.at(scores, pair_indices[~wins, 1], 1)\n\n# Determine preferred and non-preferred items based on scores\npreferred = scores > 0\nnon_preferred = scores < 0\n\ndraw_surface()\nplt.scatter(items[preferred, 0], items[preferred, 1], c='blue', label='Preferred', alpha=0.5)\nplt.scatter(items[non_preferred, 0], items[non_preferred, 1], c='purple', label='Non-preferred', alpha=0.5)\nplt.legend()\nplt.show()\n```\n:::\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nn_pairs = 10000\npair_indices = np.random.randint(0, n_items, size=(n_pairs, 2))\n# Exclude pairs where both indices are the same\nmask = pair_indices[:, 0] != pair_indices[:, 1]\npair_indices = pair_indices[mask]\n\nscores = np.zeros(n_items, dtype=int)\nwins = utilities[pair_indices[:, 0]] > utilities[pair_indices[:, 1]]\n\n# For pairs where the first item wins:\n#   - Increase score for the first item by 1\n#   - Decrease score for the second item by 1\nnp.add.at(scores, pair_indices[wins, 0], 1)\nnp.add.at(scores, pair_indices[wins, 1], -1)\n\n# For pairs where the second item wins or it's a tie:\n#   - Decrease score for the first item by 1\n#   - Increase score for the second item by 1\nnp.add.at(scores, pair_indices[~wins, 0], -1)\nnp.add.at(scores, pair_indices[~wins, 1], 1)\n\n# Determine preferred and non-preferred items based on scores\npreferred = scores > 0\nnon_preferred = scores < 0\n\ndraw_surface()\nplt.scatter(items[preferred, 0], items[preferred, 1], c='blue', label='Preferred', alpha=0.5)\nplt.scatter(items[non_preferred, 0], items[non_preferred, 1], c='purple', label='Non-preferred', alpha=0.5)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](001-preference_decision_model_files/figure-pdf/cell-7-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe Bradley-Terry model compares the utility of choice over all others [@bradley-terry-model] in the set of $J$ choices $i \\in \\{1, 2, \\dots, J\\}$. Each choice can also have its unique random noise variable representing the unobserved factor, although we can also choose to have all choices' unobserved factors follow the same distribution (e.g. independent and identically distributed, IID). The noise is represented as an extreme value distribution, although we can choose alternatives such as a multivariate Gaussian distribution: $\\epsilon \\sim \\mathcal{N}(0, \\Sigma)$. If $\\Sigma$ is not a diagonal matrix, we effectively model correlations in the noise across choices, enabling us to avoid the IID assumption. In the case of the extreme value distribution, we model the probability of a user preferring choice $i$, which we denote as $P_i = Z^{-1}\\exp(u_{i,j}^*)$ where $Z = \\sum_{j = 1}^{J} \\exp(u_{i,j}^*)$.\n\nWe can model an open-ended ranking of the available options with the Plackett-Luce model, in which we jointly model the full sequence of choice ordering [@plackett_luce]. The general form models the joint distribution as the product of conditional probabilities, where each is conditioned on the preceding ranking terms. Given an ordering of $J$ choices $\\{y_1, \\dots, y_J\\}$, we factorize the joint probability into conditionals. Each conditional follows the Bradley-Terry model:\n$$\n\\mathbb{P}(y_1, \\dots, y_J) = \\mathbb{P}(y_1) \\cdot \\mathbb{P}(y_2 | y_1) \\cdot \\dots \\cdot \\mathbb{P}(y_J | y_1, y_2, \\dots y_{J - 1}) = \\prod_{i = 1}^J \\frac{\\exp(u_{i,j}^*)}{\\sum_{j \\ge i} \\exp(u_{i,j}^*)}\n$$\n\nPairwise sampling has proven to be very useful in aligning large language model (LLM) with human preference. A LLM, such as GPT-4, Llama 3.2, and BERT, typically refers to a large and pre-trained neural network that serves as the basis for various downstream tasks. They are pre-trained on a massive corpus of text data, learning to understand language and context, and are capable of various language-related tasks such as text classification, language generation, and question answering. A LLM should be aligned to respond correctly based on human preferences. A promising approach is to train LLMs using reinforcement learning with the reward model (RM) learned from human preference data, providing a mechanism to score the quality of the generated text. This approach, known as reinforcement learning from human feedback (RLHF), leverages human feedback to guide model training, allowing LLMs to better align with human expectations while continuously improving performance.\n\nWe discuss the reward model in used in Llama2 model. The Llama2 RM [@2307.09288] is initialized from the pretrained Llama2 LLM. In the LLM, the last layer is a mapping $L: \\mathbb{R}^D \\rightarrow \\mathbb{R}^V$, where $D$ is the embedding dimension from the transformer decoder stack and $V$ is the vocabulary size. To get the RM, we replace that last layer with a randomly initialized scalar head that maps $L: \\mathbb{R}^D \\rightarrow \\mathbb{R}^1$. It's important to initialize the RM from the LLM it's meant to evaluate. This is because the RM will have the same \"knowledge\" as the LLM. This is particularly useful for evaluation object such as \"does the LLM know when it doesn't know?\". However, in cases where the RM is simply evaluating helpfulness or factuality, it may be useful to have the RM know more. In addition, the RM is on distribution for the LLM - it is initialized in a way where it semantically understands the LLM's outputs. An RM is trained with paired preferences: (prompt history, accepted response , rejected response). Prompt history is a multiturn history of user prompts and model generations, response accepted is the preferred final model generation by an annotator, and rejected response is the unpreferred response. The RM is trained with maximum likelihood under Bradley-Terry model with an optional margin term m(r):\n\n$$p(y_c \\succ y_r | x) = \\sigma(r_\\theta(x,y_c) - r_\\theta(x,y_r) - m(r))$$ \n\nThe margin term increases the distance in scores specifically for preference pairs annotators rate as easier to separate. Margins were designed primarily based on the sigmoid function, which is used to normalize the raw reward model score, flattens out beyond the range of $[-4, 4]$. Thus, the maximum possible margin is eight. There is also often a small regularization term added to center the score distribution on 0. \n\n::: {#tbl-margin_nums}\n  -------------- ---------------------- -------- ------------------ -----------------------------\n                  Significantly Better   Better   Slightly Better    Negligibly Better / Unsure                     \n  Margin Small          1                 2/3       1/3              0\n  Margin Large          3                 2         1                0\n  -------------- ---------------------- -------- ------------------ -----------------------------\n\n  : Two variants of preference rating based margin with different magnitude.\n:::\n\n### List-wise Model {#list-wise-model}\n*Multiple-choice sampling* involve participants selecting one option from a set of alternatives. Multiple-choice sampling is simple for participants to understand and reflect on realistic decision-making scenarios where individuals choose one option from many. It is beneficial in complex choice scenarios, such as modes of transportation, where choices are not independent [@bolt2009]. Multiple-choice sampling often relies on simplistic assumptions such as the independence of irrelevant alternatives (IIA), which may not always hold true. This method may also fail to capture the variation in preferences among different individuals, as it typically records only the most preferred choice without accounting for the relative importance of other options.\n\nIn *rank-order sampling*, participants rank items from most to least preferred. Used in voting, market research, and psychology, it provides rich preference data but is more complex and cognitively demanding than pairwise comparisons, especially for large item sets. Participants may also rank inconsistently [@ragain2019]. \n\n*In Best-worst scaling* (BWS), participants are presented with items and asked to identify the most and least preferred options. The primary objective of BWS is to discern the relative importance or preference of items, making it widely applicable in various fields such as market research, health economics, and social sciences [@campbell2015]. BWS provides rich data on the relative importance of items, helps clarify preferences, reduces biases found in traditional rating scales, and results in utility scores that are easy to interpret. However, BWS also has limitations, including potential scale interpretation differences among participants, and design challenges to avoid biases, such as the order effect or the context in which items are presented.\n\n## Choices Aggregation {#sec-choices-aggregation}\n\nIn many applications, human preferences must be aggregated across multiple individuals to determine a collective decision or ranking. This process is central to social choice theory, which provides a mathematical foundation for preference aggregation. Unlike individual preference modeling, which focuses on understanding how a single person makes decisions, social choice theory addresses the challenge of combining multiple preference profiles into a single, coherent outcome. One of the most widely used approaches to aggregating preferences is voting. A **voting rule** is a function that maps a set of individual preference rankings to a collective decision. The outcome of a vote is determined by a **social choice function (SCF)**, which selects a winner based on the aggregated preferences. Several voting rules exist, each with different properties:\n\n- **Plurality Rule:** Each voter assigns one point to their top choice, and the alternative with the most points wins.\n- **Borda Count:** Voters rank all alternatives, and points are assigned based on the position in each ranking.\n- **Single Transferable Vote (STV):** Voters rank choices, and rounds of elimination occur until a candidate has a majority.\n- **Condorcet Methods:** The **Condorcet winner** is the option that would win in all pairwise comparisons against other alternatives (if one exists).\n\nHowever, preference aggregation is not always straightforward. The **Condorcet Paradox** illustrates that no single alternative may be a clear winner due to cycles in majority preferences, violating transitivity. Additionally, different voting rules can yield different winners, highlighting the importance of selecting an appropriate aggregation method. A fundamental result in social choice theory is **Arrow’s Impossibility Theorem**, which states that when there are three or more alternatives, no voting system can simultaneously satisfy the following fairness criteria:\n\n1. **Unanimity (Pareto efficiency):** If all individuals prefer one option over another, the group ranking should reflect this.\n2. **Independence of Irrelevant Alternatives (IIA):** The relative ranking of two options should not be influenced by a third, unrelated option.\n3. **Non-dictatorship:** No single individual's preference should always determine the group's outcome.\n\nArrow’s theorem suggests that every fair aggregation method must compromise on at least one of these desirable properties. Additionally, the **Gibbard-Satterthwaite Theorem** proves that any deterministic voting rule that selects a single winner is either **dictatorial** (one person always determines the result) or **manipulable** (voters can strategically misrepresent their preferences to achieve a better outcome). While manipulation is theoretically possible, certain voting rules, such as STV, introduce computational complexity that makes strategic voting impractical in real-world scenarios.\n\nPreference aggregation is also critical in reinforcement learning from human feedback (RLHF), where human judgments guide model training. Aggregating human preferences in RLHF faces challenges similar to traditional voting, such as inconsistencies in preferences and strategic bias. Several approaches address these challenges:\n\n- **Majority Voting:** Simple aggregation by selecting the most preferred response.\n- **Weighted Voting:** Adjusting vote weights based on expertise or trustworthiness.\n- **Jury Learning:** A method that integrates dissenting opinions, ensuring that minority perspectives are not entirely disregarded.\n- **Social Choice in AI Alignment:** Incorporating diverse human feedback to align AI behavior with a broad spectrum of human values.\n\nThese approaches highlight the interplay between human preference modeling and machine learning, where designing aggregation mechanisms that reflect collective human values is an ongoing research challenge.\n\nWhile traditional social choice methods focus on aggregation, recent work in pluralistic alignment suggests alternative frameworks that preserve the diversity of human preferences rather than collapsing them into a single decision. Pluralistic AI systems aim to:\n\n1. **Present a spectrum of reasonable responses** instead of forcing a single choice.\n2. **Allow steering towards specific perspectives** while maintaining fairness.\n3. **Ensure distributional pluralism**, calibrating AI systems to diverse human viewpoints.\n\nThis perspective is particularly relevant in generative AI, where models trained on aggregated preferences may fail to capture the nuances of diverse human values.\n\nAggregating human preferences is a complex task, influenced by both mathematical constraints and strategic considerations. Voting-based methods provide well-studied mechanisms for aggregation, but they face fundamental limitations as outlined by Arrow’s and Gibbard-Satterthwaite’s theorems. Beyond traditional aggregation, emerging approaches in reinforcement learning and AI alignment seek to balance fairness, robustness, and pluralism. As machine learning systems increasingly interact with human preferences, designing aggregation frameworks that capture the richness of human decision-making remains an active and critical area of research.\n\n## Exercises\n### Question 1: Choice Modeling (15 points) {#question-1-choice-modeling-15-points .unnumbered}\n\nIn Chapter 2, we discussed discrete choice modeling in the context of\nutility being a linear function. Suppose we are deciding between $N$\nchoices and that the utility of each choice is given by\n$U_i=\\beta_i\\mathbf{x}+\\epsilon_i$ for $i=1, 2, \\cdots, N$. We view\n$\\mathbf{x}$ as the data point that is being conditioned on for deciding\nwhich choice to select, and $\\beta_i$ as the weights driving the linear\nutility model. The noise $\\epsilon_i$ is i.i.d. sampled from a type of\nextreme value distribution called the *Gumbel* distribution. The\nstandard Gumbel distribution is given by the density function\n$f(x)=e^{-(x+e^{-x})}$ and cumulative distribution function\n$F(x)=e^{-e^{-x}}.$ Fix $i$. Our objective is to calculate\n$\\Pr(U_i\\,\\, \\text{has max utility})$.\n\n(a) **(Written, 2 points)**. To start, set $U_i=t$ and compute\n    $\\Pr(U_j<t)$ for $j\\neq i$ in terms of $F$. Use this probability to\n    derive an integral for $\\Pr(U_i\\,\\,  \\text{has max utility})$ over\n    $t$ in terms of $f$ and $F$.\n\n    Example of solution environment.\n\n(b) **(Written, 4 points)**. Compute the integral derived in part (a)\n    with the appropriate $u$-substitution. Show your work. You should\n    arrive at multi-class logistic regression in the end!\n\nNext, you will implement logistic regression to predict preferred prompt\ncompletions. We will use the preference dataset from [RewardBench](<https://huggingface.co/datasets/allenai/reward-bench>). Notice the\nprovided `data/chosen_embeddings.pt` and `data/rejected_embeddings.pt`\nfiles. These files were constructed by feeding the prompt alongside the\nchosen/rejected responses through Llama3-8B-Instruct and selecting the\nlast token's final hidden embedding. Let $e_1$ and $e_2$ be two hidden\nembeddings with $e_1\\succ e_2$. We assume weights $w$ exist for which\nthe Bradley-Terry reward of an embedding $e$ can be modeled as\n$r=w\\cdot e$. In this setting, the probability of $e_1\\succ e_2$ is\n$$\\frac{e^{w\\cdot e_1}}{e^{w\\cdot e_1}+e^{w\\cdot e_2}}=\\frac{1}{1+e^{w\\cdot(e_2-e_1)}}=\\sigma(w\\cdot(e_1-e_2)).$$\nHence, we can view maximum likelihood across the preference dataset with\nthis model as logistic regression on $e_1-e_2$ without a bias term and\nall labels being $1$.\n\nIn biasless logistic regression, we are given a dataset $X$ with $N$\nrows of datapoints and $D$ features per datapoint. The weights of the\nmodel are parametrized by $\\theta$, a $D$-dimensional column vector.\nGiven binary labels $y$ of shape $N$ by $1$, the binary cross-entropy\nloss is\n$$J(\\theta)=-\\frac{1}{N}(y^T\\log(\\sigma(X\\theta)) + (1-y)^T\\log(1-\\sigma(X\\theta)))$$\nwhere $\\sigma$ is the sigmoid function and is applied element-wise along\nwith $\\log$. The gradient of loss is\n$$\\nabla_\\theta J(\\theta)=\\frac{1}{N}X^T(\\sigma(X\\theta)-y).$$\n\n1.  **(Coding, 3 points)**. Open the file\n    `logistic_regression/logistic_regression.py`. Implement the function\n    `train` in the biasless case.\n\n2.  **(Coding, 2 points)**. Implement the function `predict_probs`.\n\n3.  **(Written, 4 points)**. Open the notebook\n    `rewardbench_preferences.ipynb` and run all the cells. Make sure to\n    tune the `learning_rate` and `num_iterations`. Report your final\n    expected accuracy on the training and validation sets. How close are\n    the two expected accuracies? You should be able to achieve\n    $\\approx 90\\%$ expected accuracy on validation. You may add loss\n    reporting to the `train` function to verify your model is improving\n    over time.\n\n::: {.callout-note title=\"code\"}\n```{pyodide-python}\nfrom sklearn.model_selection import train_test_split\nimport torch\n\nclass LogisticRegression:\n    def __init__(self):\n        self.weights = None  # Initialized during training\n\n    def train(self, X, y, learning_rate, num_iterations):\n        \"\"\"\n        Train the logistic regression model using gradient descent (no bias).\n        Each gradient update should be with respect to the entire dataset X.\n\n        Parameters:\n        - X (torch.Tensor): Training data of shape (n_samples, n_features).\n        - y (torch.Tensor): Target labels of shape (n_samples,).\n        \"\"\"\n        n_samples, n_features = X.shape\n\n        # Initialize weights without the bias term\n        self.weights = torch.zeros(n_features)\n\n        for i in range(num_iterations):\n            # YOUR CODE HERE (~4-5 lines)\n                pass\n            # END OF YOUR CODE\n\n    def predict_probs(self, X):\n        \"\"\"\n        Predict probabilities for samples in X (no bias).\n\n        Parameters:\n        - X (torch.Tensor): Input data of shape (n_samples, n_features).\n\n        Returns:\n        - y_probs (torch.Tensor): Predicted probabilities.\n        \"\"\"\n        y_probs = None\n\n        # YOUR CODE HERE (~2-3 lines)\n        pass\n        # END OF YOUR CODE\n\n        return y_probs\n\n\nif __name__ == \"__main__\":\n    # %%\n    # Load in Llama3 embeddings of prompt + completions on RewardBench\n    chosen_embeddings = torch.load('data/chosen_embeddings.pt')\n    rejected_embeddings = torch.load('data/rejected_embeddings.pt')\n\n    # Subtract the embeddings according to the Bradley-Terry reward model setup presented in the problem \n    X = (chosen_embeddings - rejected_embeddings).to(torch.float)\n    y = torch.ones(X.shape[0])\n\n    # Split dataset 80/20 into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)  \n\n    print(\"Training set size:\", X_train.shape)\n    print(\"Validation set size:\", X_val.shape)\n\n    model = LogisticRegression()\n\n    # Tune the learning_rate and num_iterations until you achieve expected validation accuracy of at least 90%\n    learning_rate = None\n    num_iterations = None\n\n    model.train(X_train, y_train, learning_rate=learning_rate, num_iterations=num_iterations)\n\n    y_train_probs = model.predict_probs(X_train)\n    print(f\"Expected Train Accuracy: {y_train_probs.mean()}\")\n\n    y_val_probs = model.predict_probs(X_val)\n    print(f\"Expected Validation Accuracy: {y_val_probs.mean()}\") # Should reach at least 90%\n\n```\n:::\n\n### Question 2: Revealed and Stated Preferences (20 points) {#question-2-revealed-and-stated-preferences-20-points .unnumbered}\n\nAlice and Bob are running for president. For $R$ voters, we have access\nto their revealed candidate preferences through some means (e.g., social\nmedia, blogs, event history). Assume there is an underlying probability\n$z$ of voting for Alice among the population that is unknown. The aim of\nthis question is to estimate $z$ through *maximum likelihood estimation*\nby also incorporating stated preferences. In this scenario, we collect\nstated preferences through surveys. When surveyed, voters tend to be\nmore likely to vote for Alice with probability $\\frac{z+1}{2}$ for\nreasons of \"political correctness.\"\n\n(a) **(Written, 5 points)**. Suppose there are $R_A$ revealed\n    preferences for Alice, $R_B$ revealed preferences for Bob, $S_A$\n    stated preferences for Alice, and $S_B$ stated preferences for Bob.\n    Note $R=R_A+R_B$. Compute the log-likelihood of observing such\n    preferences in terms of $z, R_A, R_B, S_A, S_B$.\n\n(b) **(Coding, 1 point)**. Implement the short function `stated_prob` in\n    the file `voting/simulation.py`.\n\n(c) **(Coding, 5 points)**. Implement the class `VotingSimulation`.\n\n(d) **(Coding, 7 points)**. Implement your derived expression from\n    part (a) in the `log_likelihoods` function.\n\n(e) **(Written, 2 points)**. Finally, implement the `average_mae_mle`\n    method that will allow us to visualize the mean absolute error (MAE)\n    of our maximum likelihood estimate $\\hat{z}$ (i.e., $|\\hat{z}-z|$)\n    as the number of voters surveyed increases. Open\n    `voting/visualize_sim.ipynb` and run the cells to get a plot of MAE\n    vs. voters surveyed averaged across $100$ simulations. Attach the\n    plot to this question and briefly explain what you notice.\n\n::: {.callout-note title=\"code\"}\n```{pyodide-python}\nimport torch\nimport random\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nrandom.seed(42)\ntorch.manual_seed(42)\n\ndef stated_prob(z_values):\n    \"\"\"\n    Computes the probability of stated preferences based on z values.\n    \n    Args:\n        z_values (torch.Tensor): The z value(s), where z represents the true probability of voting for Alice.\n\n    Returns:\n        torch.Tensor: Probability for stated preferences, derived from z values.\n    \"\"\"\n    # YOUR CODE HERE (~1 line)\n    # END OF YOUR CODE\n\nclass VotingSimulation:\n    \"\"\"\n    A class to simulate the voting process where revealed and stated preferences are generated.\n    \n    Attributes:\n        R (int): Number of revealed preferences.\n        z (float): The true probability of voting for Alice.\n        revealed_preferences (torch.Tensor): Simulated revealed preferences of R voters using Bernoulli distribution.\n                                             Takes on 1 for Alice, and 0 for Bob.\n        stated_preferences (torch.Tensor): Simulated stated preferences, initialized as an empty tensor.\n                                           Takes on 1 for Alice, and 0 for Bob.\n    \"\"\"\n    def __init__(self, R, z):\n        self.R = R\n        self.z = z\n        self.revealed_preferences = None # YOUR CODE HERE (~1 line)\n        self.stated_preferences = torch.tensor([])\n\n    def add_survey(self):\n        \"\"\"\n        Simulates an additional stated preference based on stated_prob and adds it to the list.\n        This updates the self.stated_preferences tensor by concatenating on a new simulated survey result.\n        \"\"\"\n        # YOUR CODE HERE (~3 lines)\n        # END OF YOUR CODE\n\ndef log_likelihoods(revealed_preferences, stated_preferences, z_values):\n    \"\"\"\n    Computes the log likelihoods across both revealed and stated preferences.\n    Use your answer in part (a) to help.\n    \n    Args:\n        revealed_preferences (torch.Tensor): Tensor containing revealed preferences (0 or 1).\n        stated_preferences (torch.Tensor): Tensor containing stated preferences (0 or 1).\n        z_values (torch.Tensor): Tensor of underlying z values to calculate likelihood for.\n\n    Returns:\n        torch.Tensor: Log likelihood for each z value.\n    \"\"\"\n    # YOUR CODE HERE (~10-16 lines)\n    pass\n    # END OF YOUR CODE \n\ndef average_mae_mle(R, z, survey_count, num_sims, z_sweep):\n    \"\"\"\n    Runs multiple simulations to compute the average mean absolute error (MAE) of Maximum Likelihood Estimation (MLE) \n    for z after increasing number of surveys.\n    \n    Args:\n        R (int): Number of revealed preferences.\n        z (float): The true probability of voting for Alice.\n        survey_count (int): Number of additional surveys to perform.\n        num_sims (int): Number of simulation runs to average over.\n        z_sweep (torch.Tensor): Range of z values to consider for maximum likelihood estimation.\n\n    Returns:\n        torch.Tensor: Tensor of mean absolute errors averaged over simulations.\n                      Should have shape (survey_count, )\n    \"\"\"\n    all_errors = []\n    for _ in tqdm(range(num_sims)):\n        errors = []\n        vote_simulator = VotingSimulation(R=R, z=z)\n\n        for _ in range(survey_count):\n            revealed_preferences = vote_simulator.revealed_preferences\n            stated_preferences = vote_simulator.stated_preferences\n\n            # YOUR CODE HERE (~6-8 lines)\n            pass # Compute log_likelihoods across z_sweep. Argmax to find MLE for z. \n                 # Append the absolute error to errors and add a survey to the simulator.\n            # END OF YOUR CODE\n\n        errors_tensor = torch.stack(errors) \n        all_errors.append(errors_tensor)\n\n    # Calculate the average error across simulations \n    mean_errors = torch.stack(all_errors).mean(dim=0)\n    return mean_errors\n\nif __name__ == \"__main__\":\n    # DO NOT CHANGE!\n    max_surveys = 2000\n    z = 0.5\n    R = 10\n    num_sims = 100\n    z_sweep = torch.linspace(0.01, 0.99, 981)\n\n    # Compute and plot the errors. Attach this plot to part (d).\n    mean_errors = average_mae_mle(R, z, max_surveys, num_sims, z_sweep)\n    plt.plot(mean_errors)\n\n    plt.xlabel('Surveys Conducted')\n    plt.ylabel('Average Error')\n    plt.title(f'MLE MAE Error (z={z}, {num_sims} simulations)')\n    plt.show()\n```\n:::\n\n### Question 3: Probabilistic Multi-modal Preferences (25 points) {#question-3-probabilistic-multi-modal-preferences-25-points .unnumbered}\n\nSuppose you are part of the ML team on the movie streaming site\nCardinalStreams. After taking CS329H, you collect a movie preferences\ndataset with $30000$ examples of the form $(m_1, m_2, \\text{user id})$\nwhere $m_1$ and $m_2$ are movies with $m_1\\succ m_2$. The preferences\ncome from $600$ distinct users with $50$ examples per user. Each movie\nhas a $10$-dimensional feature vector $m$, and each user has a\n$10$-dimensional weight vector $u$. Given movie features $m_1, m_2$ and\nuser weights $u$, the user's preference between the movies is given by a\nBradley-Terry reward model, i.e.,\n$$P(m_1\\succ m_2)=\\frac{e^{u\\cdot m_1}}{e^{u\\cdot m_1} + e^{u\\cdot m_2}}=\\frac{1}{1+e^{u\\cdot (m_2-m_1)}}=\\sigma(u\\cdot (m_1-m_2)).$$\n\nYou realize that trying to estimate the weights for each user with only\n$50$ examples will not work due to the lack of data. Instead, you choose\nto drop the user IDs column and shuffle the dataset in order to take a\n*multi-modal preferences* approach. For simplicity, you assume a model\nwhere a proportion $p$ of the users have weights $w_1$ and the other\n$1-p$ have weights $w_2$. In this setting, each user belongs to one of\ntwo groups: users with weights $w_1$ are part of Group 1, and users with\nweights $w_2$ are part of Group 2.\n\n(a) **(Written, 3 points)**. For a datapoint $(m_1, m_2)$ with label\n    $m_1\\succ m_2$, compute the data likelihood\n    $P(m_1\\succ m_2 | p, w_1, w_2)$ assuming $p, w_1, w_2$ are given.\n\n(b) **(Written, 3 points)**. As a follow up, use the likelihood to\n    simplify the posterior distribution of $p, w_1, w_2$ after updating\n    on $(m_1, m_2)$ leaving terms for the priors unchanged.\n\n(c) **(Written, 4 points)**. Assume priors $p\\sim B(1, 1)$,\n    $w_1\\sim\\mathcal{N}(0, \\mathbf{I})$, and\n    $w_2\\sim\\mathcal{N}(0, \\mathbf{I})$ where $B$ represents the Beta\n    distribution and $\\mathcal{N}$ represents the normal distribution.\n    You will notice that the posterior from part (b) has no simple\n    closed-form. As a result, we must resort to *Markov Chain Monte\n    Carlo (MCMC)* approaches to sample from the posterior. These\n    approaches allow sampling from highly complex distributions by\n    constructing a Markov chain $\\{x_t\\}_{t=1}^\\infty$ so that\n    $\\lim_{t\\to\\infty}x_t$ act as desired samples from the target\n    distribution. You can think of a Markov chain as a sequence with the\n    special property that $x_{t+1}$ only depends on $x_t$ for all\n    $t\\ge 1$.\n\n    The most basic version of MCMC is known as Metropolis-Hastings.\n    Assume $\\pi$ is the target distribution we wish to sample from where\n    $\\pi(z)$ represents the probability density at point $z$.\n    Metropolis-Hastings constructs the approximating Markov chain $x_t$\n    as follows: a proposal $P$ for $x_{t+1}$ is made via sampling from a\n    chosen distribution $Q(\\,\\cdot\\,| x_t)$ (e.g., adding Gaussian\n    noise). The acceptance probability of the proposal is given by\n    $$A= \\min \\left( 1, \\frac{\\pi(P)Q(x_t | P)}{\\pi(x_t)Q(P | x_t)} \\right).$$\n    That is, $$x_{t+1}=\\begin{cases} \n    P & \\text{with probability } A, \\\\\n    x_t & \\text{with probability } 1 - A.\n    \\end{cases}$$ To extract our samples from $\\pi$, we run the Markov\n    chain for $N$ timesteps and disregard the first $T<N$ timesteps in\n    what is called the *burn-in or mixing time* (i.e., our final samples\n    are $x_{T+1}, x_{T+2},\\cdots, x_{N}$). The mixing time is needed to\n    ensure that the Markov chain elements are representative of the\n    distribution $\\pi$ -- initial elements of the chain will not be a\n    good approximation of $\\pi$ and depend more on the choice of\n    initialization $x_1$.\n\n    To build some intuition, suppose we have a biased coin that turns\n    heads with probability $p_{\\text{heads}}$. We observe $12$ coin\n    flips to have $9$ heads and $3$ tails. If our prior for\n    $p_{\\text{heads}}$ was $B(1, 1)$, then our posterior will be\n    $B(1+9, 1+3)=B(10, 4)$. The Bayesian update is given by\n\n    $$\\begin{aligned}\n        P(p_{\\text{heads}}|9\\text{ heads}, 3\\text{ tails})&=\\frac{P(9\\text{ heads}, 3\\text{ tails} | p_{\\text{heads}})B(1, 1)(p_{\\text{heads}})}{\\int_0^1 P(9\\text{ heads}, 3\\text{ tails} | p_{\\text{heads}})B(1, 1)(p_{\\text{heads}}) dp_{\\text{heads}}}\\\\\n        &=\\frac{P(9\\text{ heads}, 3\\text{ tails} | p_{\\text{heads}})}{\\int_0^1 P(9\\text{ heads}, 3\\text{ tails} | p_{\\text{heads}})  dp_{\\text{heads}}}.\n    \\end{aligned}$$\n\n    **Find the acceptance probablity** $A$ in the\n    setting of the biased coin assuming the proposal distribution\n    $Q(\\cdot|x_t)=x_t+N(0,\\sigma)$ for given $\\sigma$. Notice that this\n    choice of $Q$ is symmetric, i.e., $Q(x_t|P)=Q(P|x_t)$. In addition,\n    you will realize that is unnecessary to compute the normalizing\n    constant of the Bayesian update (i.e., the integral in the\n    denominator) which is why MCMC is commonly used to sample from\n    posteriors!\n\n(d) **(Written + Coding, 6 points)**. Implement Metropolis-Hastings to\n    sample from the posterior distribution of the biased coin in\n    `multimodal_preferences/biased_coin.py`. Attach a histogram of your\n    MCMC samples overlayed on top of the true posterior $B(10, 4)$ by\n    running `python biased_coin.py`.\n\n::: {.callout-note title=\"code\"}\n```{pyodide-python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\ndef likelihood(p: float) -> float:\n    \"\"\"\n    Computes the likelihood of 9 heads and 3 tails assuming p_heads is p.\n\n    Args:\n    p (float): A value between 0 and 1 representing the probability of heads.\n\n    Returns:\n    float: The likelihood value at p_heads=p. Return 0 if p is outside the range [0, 1].\n    \"\"\"\n    # YOUR CODE HERE (~1-3 lines)\n    pass\n    # END OF YOUR CODE\n\n\ndef propose(x_current: float, sigma: float) -> float:\n    \"\"\"\n    Proposes a new sample from the proposal distribution Q.\n    Here, Q is a normal distribution centered at x_current with standard deviation sigma.\n\n    Args:\n    x_current (float): The current value in the Markov chain.\n    sigma (float): Standard deviation of the normal proposal distribution.\n\n    Returns:\n    float: The proposed new sample.\n    \"\"\"\n    # YOUR CODE HERE (~1-3 lines)\n    pass\n    # END OF YOUR CODE\n\n\ndef acceptance_probability(x_current: float, x_proposed: float) -> float:\n    \"\"\"\n    Computes the acceptance probability A for the proposed sample.\n    Since the proposal distribution is symmetric, Q cancels out.\n\n    Args:\n    x_current (float): The current value in the Markov chain.\n    x_proposed (float): The proposed new value.\n\n    Returns:\n    float: The acceptance probability\n    \"\"\"\n    # YOUR CODE HERE (~4-6 lines)\n    pass\n    # END OF YOUR CODE\n\n\ndef metropolis_hastings(N: int, T: int, x_init: float, sigma: float) -> np.ndarray:\n    \"\"\"\n    Runs the Metropolis-Hastings algorithm to sample from a posterior distribution.\n\n    Args:\n    N (int): Total number of iterations.\n    T (int): Burn-in period (number of initial samples to discard).\n    x_init (float): Initial value of the chain.\n    sigma (float): Standard deviation of the proposal distribution.\n\n    Returns:\n    list: Samples collected after the burn-in period.\n    \"\"\"\n    samples = []\n    x_current = x_init\n\n    for t in range(N):\n        # YOUR CODE HERE (~7-10 lines)\n        # Use the propose and acceptance_probability functions to get x_{t+1} and store it in samples after the burn-in period T\n        pass\n        # END OF YOUR CODE\n\n    return samples\n\n\ndef plot_results(samples: np.ndarray) -> None:\n    \"\"\"\n    Plots the histogram of MCMC samples along with the true Beta(10, 4) PDF.\n\n    Args:\n    samples (np.ndarray): Array of samples collected from the Metropolis-Hastings algorithm.\n\n    Returns:\n    None\n    \"\"\"\n    # Histogram of the samples from the Metropolis-Hastings algorithm\n    plt.hist(samples, bins=50, density=True, alpha=0.5, label=\"MCMC Samples\")\n\n    # True Beta(10, 4) distribution for comparison\n    p = np.linspace(0, 1, 1000)\n    beta_pdf = beta.pdf(p, 10, 4)\n    plt.plot(p, beta_pdf, \"r-\", label=\"Beta(10, 4) PDF\")\n\n    plt.xlabel(\"p_heads\")\n    plt.ylabel(\"Density\")\n    plt.title(\"Metropolis-Hastings Sampling of Biased Coin Posterior\")\n    plt.legend()\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    # MCMC Parameters (DO NOT CHANGE!)\n    N = 50000  # Total number of iterations\n    T = 10000  # Burn-in period to discard\n    x_init = 0.5  # Initial guess for p_heads\n    sigma = 0.1  # Standard deviation of the proposal distribution\n\n    # Run Metropolis-Hastings and plot the results\n    samples = metropolis_hastings(N, T, x_init, sigma)\n    plot_results(samples)\n```\n:::\n\n(e) **(Coding, 9 points)**. Implement Metropolis-Hastings in the movie\n    setting inside\\\n    `multimodal_preferences/movie_metropolis.py`. The movie dataset we\n    use for grading will not be provided. However, randomly constructed\n    datasets can be used to test your implementation by running\n    `python movie_metropolis.py`. You should be able to achieve a $90\\%$\n    success rate with most `fraction_accepted` values above $0.1$.\n    Success is measured by thresholded closeness of predicted parameters\n    to true parameters. You may notice occasional failures that occur\n    due to lack of convergence which we will account for in grading.\n\n::: {.callout-note title=\"code\"}\n```{pyodide-python}\nimport torch\nimport torch.distributions as dist\nimport math\nfrom tqdm import tqdm\nfrom typing import Tuple\n\ndef make_data(\n    true_p: torch.Tensor, true_weights_1: torch.Tensor, true_weights_2: torch.Tensor, num_movies: int, feature_dim: int\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Generates a synthetic movie dataset according to the CardinalStreams model.\n\n    Args:\n        true_p (torch.Tensor): Probability of coming from Group 1.\n        true_weights_1 (torch.Tensor): Weights for Group 1.\n        true_weights_2 (torch.Tensor): Weights for Group 2.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: A tuple containing the dataset and labels.\n    \"\"\"\n    # Create movie features\n    first_movie_features = torch.randn((num_movies, feature_dim))\n    second_movie_features = torch.randn((num_movies, feature_dim))\n\n    # Only care about difference of features for Bradley-Terry\n    dataset = first_movie_features - second_movie_features\n\n    # Get probabilities that first movie is preferred assuming Group 1 or Group 2\n    weight_1_probs = torch.sigmoid(dataset @ true_weights_1)\n    weight_2_probs = torch.sigmoid(dataset @ true_weights_2)\n\n    # Probability that first movie is preferred overall can be viewed as sum of conditioning on Group 1 and Group 2\n    first_movie_preferred_probs = (\n        true_p * weight_1_probs + (1 - true_p) * weight_2_probs\n    )\n    labels = dist.Bernoulli(first_movie_preferred_probs).sample()\n    return dataset, labels\n\n\ndef compute_likelihoods(\n    dataset: torch.Tensor,\n    labels: torch.Tensor,\n    p: torch.Tensor,\n    w_1: torch.Tensor,\n    w_2: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"\n    Computes the likelihood of each datapoint. Use your calculation from part (a) to help.\n\n    Args:\n        dataset (torch.Tensor): The dataset of differences between movie features.\n        labels (torch.Tensor): The labels where 1 indicates the first movie is preferred, and 0 indicates preference of the second movie.\n        p (torch.Tensor): The probability of coming from Group 1.\n        w_1 (torch.Tensor): Weights for Group 1.\n        w_2 (torch.Tensor): Weights for Group 2.\n\n    Returns:\n        torch.Tensor: The likelihoods for each datapoint. Should have shape (dataset.shape[0], )\n    \"\"\"\n    # YOUR CODE HERE (~6-8 lines)\n    pass\n    # END OF YOUR CODE\n\ndef compute_prior_density(\n    p: torch.Tensor, w_1: torch.Tensor, w_2: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"\n    Computes the prior density of the parameters.\n\n    Args:\n        p (torch.Tensor): The probability of preferring model 1.\n        w_1 (torch.Tensor): Weights for model 1.\n        w_2 (torch.Tensor): Weights for model 2.\n\n    Returns:\n        torch.Tensor: The prior densities of p, w_1, and w_2.\n    \"\"\"\n    # Adjusts p to stay in the range [0.3, 0.7] to prevent multiple equilibria issues at p=0 and p=1\n    p_prob = torch.tensor([2.5]) if 0.3 <= p <= 0.7 else torch.tensor([0.0])\n\n    def normal_pdf(x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Computes the PDF of the standard normal distribution at x.\"\"\"\n        return (1.0 / torch.sqrt(torch.tensor(2 * math.pi))) * torch.exp(-0.5 * x**2)\n\n    weights_1_prob = normal_pdf(w_1)\n    weights_2_prob = normal_pdf(w_2)\n\n    # Concatenate the densities\n    concatenated_prob = torch.cat([p_prob, weights_1_prob, weights_2_prob])\n    return concatenated_prob\n\n\ndef metropolis_hastings(\n    dataset: torch.Tensor,\n    labels: torch.Tensor,\n    sigma: float = 0.01,\n    num_iters: int = 30000,\n    burn_in: int = 20000,\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]:\n    \"\"\"\n    Performs the Metropolis-Hastings algorithm to sample from the posterior distribution.\n    DO NOT CHANGE THE DEFAULT VALUES!\n\n    Args:\n        dataset (torch.Tensor): The dataset of differences between movie features.\n        labels (torch.Tensor): The labels indicating which movie is preferred.\n        sigma (float, optional): Standard deviation for proposal distribution.\n            Defaults to 0.01.\n        num_iters (int, optional): Total number of iterations. Defaults to 30000.\n        burn_in (int, optional): Number of iterations to discard as burn-in.\n            Defaults to 20000.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]: Samples of p,\n        w_1, w_2, and the fraction of accepted proposals.\n    \"\"\"\n    feature_dim = dataset.shape[1]\n\n    # Initialize random starting parameters by sampling priors\n    curr_p = 0.3 + 0.4 * torch.rand(1)\n    curr_w_1 = torch.randn(feature_dim)\n    curr_w_2 = torch.randn(feature_dim)\n\n    # Keep track of samples and total number of accepted proposals\n    p_samples = []\n    w_1_samples = []\n    w_2_samples = []\n    accept_count = 0 \n\n    for T in tqdm(range(num_iters)):\n        # YOUR CODE HERE (~3 lines)\n        pass # Sample proposals for p, w_1, w_2\n        # END OF YOUR CODE\n\n        # YOUR CODE HERE (~4-6 lines)\n        pass # Compute likehoods and prior densities on both the proposed and current samples\n        # END OF YOUR CODE\n\n        # YOUR CODE HERE (~2-4 lines)\n        pass # Obtain the ratios of the likelihoods and prior densities between the proposed and current samples \n        # END OF YOUR CODE \n\n        # YOUR CODE HERE (~1-2 lines)\n        pass # Multiply all ratios (both likelihoods and prior densities) and use this to calculate the acceptance probability of the proposal\n        # END OF YOUR CODE\n\n        # YOUR CODE HERE (~4-6 lines)\n        pass # Sample randomness to determine whether the proposal should be accepted to update curr_p, curr_w_1, curr_w_2, and accept_count\n        # END OF YOUR CODE \n\n        # YOUR CODE HERE (~4-6 lines)\n        pass # Update p_samples, w_1_samples, w_2_samples if we have passed the burn in period T\n        # END OF YOUR CODE \n\n    fraction_accepted = accept_count / num_iters\n    print(f\"Fraction of accepted proposals: {fraction_accepted}\")\n    return (\n        torch.stack(p_samples),\n        torch.stack(w_1_samples),\n        torch.stack(w_2_samples),\n        fraction_accepted,\n    )\n\n\ndef evaluate_metropolis(num_sims: int, num_movies: int, feature_dim: int) -> None:\n    \"\"\"\n    Runs the Metropolis-Hastings algorithm N times and compare estimated parameters\n    with true parameters to obtain success rate. You should attain a success rate of around 90%. \n\n    Note that there are two successful equilibria to converge to. They are true_weights_1 and true_weights_2 with probabilities\n    p and 1-p in addition to true_weights_2 and true_weights_1 with probabilities 1-p and p. This is why even though it may appear your\n    predicted parameters don't match the true parameters, they are in fact equivalent. \n\n    Args:\n        num_sims (int): Number of simulations to run.\n\n    Returns:\n        None\n    \"\"\"\n    \n    success_count = 0\n    for _ in range(num_sims):\n        # Sample random ground truth parameters\n        true_p = 0.3 + 0.4 * torch.rand(1)\n        true_weights_1 = torch.randn(feature_dim)\n        true_weights_2 = torch.randn(feature_dim)\n\n        print(\"\\n---- MCMC Simulation ----\")\n        print(\"True parameters:\", true_p, true_weights_1, true_weights_2)\n\n        dataset, labels = make_data(true_p, true_weights_1, true_weights_2, num_movies, feature_dim)\n        p_samples, w_1_samples, w_2_samples, _ = metropolis_hastings(dataset, labels)\n\n        p_pred = p_samples.mean(dim=0)\n        w_1_pred = w_1_samples.mean(dim=0)\n        w_2_pred = w_2_samples.mean(dim=0)\n\n        print(\"Predicted parameters:\", p_pred, w_1_pred, w_2_pred)\n\n        # Do casework on two equilibria cases to check for success\n        p_diff_case_1 = torch.abs(p_pred - true_p)\n        p_diff_case_2 = torch.abs(p_pred - (1 - true_p))\n\n        w_1_diff_case_1 = torch.max(torch.abs(w_1_pred - true_weights_1))\n        w_1_diff_case_2 = torch.max(torch.abs(w_1_pred - true_weights_2))\n\n        w_2_diff_case_1 = torch.max(torch.abs(w_2_pred - true_weights_2))\n        w_2_diff_case_2 = torch.max(torch.abs(w_2_pred - true_weights_1))\n\n        pass_case_1 = (\n            p_diff_case_1 < 0.1 and w_1_diff_case_1 < 0.5 and w_2_diff_case_1 < 0.5\n        )\n        pass_case_2 = (\n            p_diff_case_2 < 0.1 and w_1_diff_case_2 < 0.5 and w_2_diff_case_2 < 0.5\n        )\n        passes = pass_case_1 or pass_case_2\n\n        print(f'Result: {\"Success\" if passes else \"FAILED\"}')\n        if passes:\n            success_count += 1\n    print(f'Success rate: {success_count / num_sims}')\n\n\nif __name__ == \"__main__\":\n    evaluate_metropolis(num_sims=10, num_movies=30000, feature_dim=10)\n```\n:::\n\n",
    "supporting": [
      "001-preference_decision_model_files/figure-pdf"
    ],
    "filters": []
  }
}