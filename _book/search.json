[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning from Human Preferences",
    "section": "",
    "text": "Introduction\nFullscreen Slide\nMachine learning is increasingly shaping various aspects of our lives, from education and healthcare to scientific discovery. A key challenge in developing trustworthy intelligent systems is ensuring they align with human preferences. Learning from human feedback offers a promising solution to this challenge. This book introduces the foundations and practical applications of machine learning from human preferences. Instead of manually predefining the learning goal, the book presents preference-based learning that incorporates human feedback to guide the learning process, drawing insights from related fields such as economics, psychology, and human-computer interaction. By the end of this book, readers will be equipped with the key concepts and tools needed to design systems that effectively align with human preferences.\nThe book is intended for researchers, practitioners, and students who are interested in intergrating machine learning with human-centered application. We assume some basic knowledge of probability and statistics, but provides sufficient background and references for the readers to follow the main ideas. The book also provides illustrative program examples and datasets. The field of machine learning from human preference is a vibrant area of research and practice with many open challenges and opportunities, and we hope that this book will inspire readers to further explore and advance this exciting field.\nWe hope with the present book to both allow more use of human preferences in machine learning, and new data modalities as Artificial Intelligence systems become increasingly important.\nStanford, May 2025, THK",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#structure-of-this-book",
    "href": "index.html#structure-of-this-book",
    "title": "Machine Learning from Human Preferences",
    "section": "Structure of this book",
    "text": "Structure of this book\nThe book has three parts which introduce fundamental models, present learning paradigms, and discuss assumptions.\n\nBackground\nWe provide background on axioms underlying comparisons in Chapter 1. We discover key modeling assumptions It covers random preference models the Independence of Irrelevant Alternatives (IIA), and types of comparison data (binary rankings, accept-reject, lists). The chapter also discusses the main limitations of IIA based on heterogeneity.\n\n\nLearning\nThe second part introduces several approaches to learning from comparisons.\n\nChapter 2 considers a setting where comparison data is given and studies both maximum likelihood and posterior-based learning of comparison models. It uses case studies from language modeling and robotics. We discuss the challenges in learning multimodal/heterogenous rewards that fail to satisfy IIA.\nChapter 3 considers active data collection of comparisons with the goal of optimal inference on comparison models using Various strategies are explored, including reducing the learner’s variance, exploiting ambiguity and domain knowledge in ranking, with a case study from robotics.\nChapter 4 studies processes where comparisons are used to guide decisions. We first set up the bandit approach to recommending maximal objects with respect to comparisons, and discuss dueling bandits. We then consider as well as reinforcement learning from human feedback (RLHF) to align language models that decide on which text to generate. We highlight the role of uncertainty quantification and exploration for decision-making.\nChapter 5 considers decision-making in the presence of heterogeneity. We first focus on dealing with heterogeneity to maximize average utility using personalization. We then discuss aggregation mechanisms that are voting-based and decisions that are independent of some some features of the outcome.\n\n\n\nReflection\nThe final part of the book discusses limitations of comparison data, and opportunities resulting from stated preference data.\n\nChapter 6 critiques machine learning from comparisons. It takes different disciplinary lenses, from social psychology, philosophy, and critical studies, to highlight where comparisons are limited in the expression of human preferences, and what are alternatives.\nChapter 7 considers models that are broader than comparisons in our model, many of which we can think of as stated preferences. These are models in which value judgments are given in terms of Likert scales or textual descriptions. We propose ways in how such feedback can be merged with comparison data to better express preferences.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-to-engage-with-this-book",
    "href": "index.html#how-to-engage-with-this-book",
    "title": "Machine Learning from Human Preferences",
    "section": "How to engage with this book",
    "text": "How to engage with this book\nThrere are three models of reading, and teaching with, this book. Chapter 1 is underlying all of the book, so is part of all of these pathways.\n\nFor practitioners and those teaching applied AI content, we recommend a reading of Chapters 1, 2, 4, and 7, which can be used as a sequence in an early graduate course on Machine Learning. It allows to highlight human data sources in an introductory machine learning course.\nFor people with background in discrete choice, we propose to skim Chapter 1, and study Chapters 2 and 4. These studies allow readers to integrate machine learning in their studies of discrete choice, demand models, and Industrial Organization.\nFor those with deep background in machine learning, we propose to study chapters 2-4 and 7. These chapters maximize the amount of machine learning covered, and is suitable for a deep learning-based course of machine learning.\nFor those interested in the methodological and theoretical foundations of machine leraning from comparisons, we recommend a reading of chapters 1, 5, 6, and 7. Chapter 1 and 5 study the underpinnings of revealed preferences and aggregation, chapter 6 critiques these assumptions, and chapter 7 looks at broader ways of eliciting preferences. It is suitable for critical study in a course on Computation and Society.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#prior-knowledge",
    "href": "index.html#prior-knowledge",
    "title": "Machine Learning from Human Preferences",
    "section": "Prior knowledge",
    "text": "Prior knowledge\nThe book assumes knowledge of the fundamentals of statistics, linear algebra and machine learning. Many example code excerpts are written in python, and make experience in the python programming language valuable for readers.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#additional-materials",
    "href": "index.html#additional-materials",
    "title": "Machine Learning from Human Preferences",
    "section": "Additional Materials",
    "text": "Additional Materials\nEvery chapter has problems for readers and slides for teaching of the material available. They are available on the book’s website.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "src/chap2.html",
    "href": "src/chap2.html",
    "title": "1  Background",
    "section": "",
    "text": "1.1 Random Preferences as a Model of Comparisons\nFullscreen Part 1 Fullscreen Part 2\nThis is a book about machine learning from human preferences. This first chapter is about generative models for human actions, in particular for comparisons. In classical supervised learning, comparisons implicitly arise for a trained model: If the logit of a particular output from a supervised learning model is higher for the label \\(y\\) than \\(y'\\) we would say that the model is more likely to produce \\(y\\) than \\(y'\\). While this introduces some amount of comparison on model outputs, it does not help us if the data is given by \\(y\\) being preferred to \\(y'\\), written \\(y \\succ y'\\).\nFirst, hence, we introduce stochastic preferences as a model of preferences. We then discuss the most important assumption made in stochastic choice, the Independence of Irrelevant Alternatives (IIA), and discuss its advantages and pitfalls. Chapters 1-5 will restrict to comparisons, including binary comparisons, accept-reject decisions, and ranking lists. Other related data types, such as Likert scales, will be considered in (chapter-beyond?).\nWe start with a set of objects \\(y \\in Y\\)—be they products, robot trajectories, or language model responses. We will consider models to generate comparisons that are orders. For realism, but also for mathematical simplicity, we will assume in this book that the set \\(Y\\) of objects is discrete and has \\(n\\) objects.\nComparisons may be random and are generated by random draws of (total) orders. (Total) Orders have two properties.\nIn the following, we consider randomness as generated from a decision-maker who has an order, or preference relation, \\(\\prec\\) on a set of objects \\(Y\\). We refer to the random object \\(\\prec\\) as the oracle preference. Each preference \\(\\prec\\) has an associated probability mass \\(\\mathbb{P}[\\mathord{\\prec}]\\), leading to an \\((n!-1)\\)-dimensional vector encoding the full random preference set. (This might look like, and is already for small values of \\(n\\) a large number. Reducing this representational complexity is a goal of this chapter.)\nOne might wonder why we need to have a random preference. Deterministic preferences are conceptually helpful constructs and are used broadly in the fields of Consumer theory (e.g., Mas-Colell et al. (1995)). However, they suffer when bringing them to data, as data is inherently noisy.\nEven when allowing for randomness, assumptions we will impose in this section, be it transitivity or the Independence of Irrelevant Alternatives, are stark yet practical. In many situations they will fail, for good reasons. Whether these are human’s inability to express rankings, contextual challenges of domains, or community norms, we will discuss them in, humans cannot clearly rank alternatives, their choices reflect individualistic norms, or they might have self-control pictures. Many of these wrinkles on the approach to preferences presented here is contained in ?sec-beyond. Until then, we will make the fullest use of learning stochastic preferences.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "src/chap2.html#sec-foundations",
    "href": "src/chap2.html#sec-foundations",
    "title": "1  Background",
    "section": "",
    "text": "First, for two objects \\(y, y'\\) either \\(y \\prec y'\\) and/or \\(y \\succ y'\\) must hold, an assumption called totality: Either \\(y\\) is weakly preferred to \\(y'\\) or \\(y'\\) is weakly preferred to \\(y\\).\nThe second assumption is transitivity: if \\(y \\succ y'\\) and \\(y' \\succ y''\\), then also \\(y \\succ y''\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "src/chap2.html#types-of-comparison-data",
    "href": "src/chap2.html#types-of-comparison-data",
    "title": "1  Background",
    "section": "1.2 Types of Comparison Data",
    "text": "1.2 Types of Comparison Data\nThere are different types of comparison data we may observe. We can relate them back to the population preferences \\(\\prec\\).\n\n1.2.1 Full Preference Lists\nThe conceptually simplest and practically most verbose preference sampling is to get the full preference ranking, i.e. \\(L = (y_1, y_2, \\dots, y_n)\\), where \\(y_1 \\succ y_2 \\succ \\cdots \\succ y_n\\). In this case, we know not only that \\(y_1\\) is preferred to \\(y_2\\), but also, by transitivity, that it is preferred to all other options. Similarly, we know that \\(y_2\\) is preferred to all options but \\(y_1\\), etc. In many cases, we do not observe full preferences as the cognitive load for humans is too high.\n\n\n1.2.2 The Most-Preferred Element from a Subset: (Binary) Choices\nAnother type of sample is \\((y, Y')\\) where \\(y\\) is the most preferred alternative from \\(Y'\\) for a sampled preference. Formally, \\(y \\prec y'\\) for all \\(y' \\in Y' \\setminus \\{y\\}\\)—\\(y\\) is preferred to all elements of \\(Y\\) but \\(y'\\).\nFormally, the probability that we observe \\((y, Y')\\) is \\[\n\\mathbb{P}[(y, Y')] = \\sum_{\\prec: y \\prec y' \\forall y' \\in Y' \\setminus \\{y\\}} \\mathbb{P} [\\mathord{\\prec}].\n\\] That is, the probability of observing \\((y, Y')\\) is given by the sum of all preferred samples \\(\\prec\\) such that \\(y\\) is preferred to all \\(y'\\) in \\(Y'\\) other than \\(y\\).\nIf the choice is binary, \\(Y' = \\{y, y'\\}\\), we also write \\((y \\succ y')\\) for a sample \\((x, \\{x,y\\})\\). We highlight that these objects are random, and depend on the sample of \\(\\prec\\). Binary data is convenient and quick to elicit and has been prominently applied in language model finetuning and evaluation.\nSometimes, particularly when a decision-maker is offered an object or “nothing”, we will implicitly assume that there is an “outside option” \\(y_0\\) in \\(Y\\), allowing us to interpret \\((y, \\{y, y_0\\})\\) as “accepting” \\(y\\), and \\((y_0, \\{y, y_0\\})\\) as rejecting it. Outside options can be thought of as fundamental limits to what a system designer can obtain. Consider a recommendation system. A user of that system might engage with content or not. In principle, instead of engaging, they will do something else. We do not model this in out set of objects \\(Y\\) as a fundamental abstraction. All models are wrong, but some are useful.\n\n\n1.2.3 Mind the Context\nChoices are often conditional, and data is given by \\((x, L)\\) (for list-based data), \\((x, y, Y')\\) (for general choice-based data), or \\((x, y, y')\\) for binary data. \\(x \\in X\\) is some context: the environment of a purchase, the goal of a robot, or a user prompt for a large language model. It can also be a prompt to the decision-maker, e.g., to human raters on whether they should pick preferences based on helplessness or harmlessness Ganguli et al. (2022). The inclusion of context in learning allows for the generalization of preferences, as we will see in subsequent chapters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "src/chap2.html#random-utility-models",
    "href": "src/chap2.html#random-utility-models",
    "title": "1  Background",
    "section": "1.3 Random Utility Models",
    "text": "1.3 Random Utility Models\nAn equivalent way to represent random preferences is to identify a sample \\(\\prec\\) with a vector \\(u_{\\prec} = (u_{\\mathord{\\prec}} (y))_{y \\in Y} \\in \\mathbb R^Y\\) where \\(y \\succ y'\\) if and only if \\(u(y) &gt; u(y')\\). (For the concerned reader: We assume that \\(u(y) = u(y')\\) happens with zero probability; and for discrete \\(Y\\) such a vector always exists.)\nTo get a sense for different random utility models, we consider a particular model that has the complexity of many models in modern machine learning: The Ackley function. In this model, each alternative is represented by a \\(d\\)-dimensional vector \\((x_1, \\ldots, x_d) \\in \\mathbb{R}^d\\), the Ackley function is given by \\[\n\\text{Ackley}(x_1, x_2, \\dots, x_d) = -a e^{-b \\sqrt{\\frac{1}{d} \\sum_{j=1}^d x_j^2}} - e^{\\frac{1}{d} \\sum_{j=1}^d \\cos(c x_j)} + a + e.\n\\] for some constants \\(a, b, c \\in \\mathbb{R}\\). By stacking a number \\(k\\) of human preferences, we can compute for \\(k\\) samples from a random model the function in a vectorized way.\n\n\n\n\n\n\nCode\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nimport numpy as np\nnp.random.seed(0)\n\ndef ackley(X, a=20, b=0.2, c=2*np.pi):\n    \"\"\"\n    Compute the Ackley function.\n    Parameters:\n      X: A NumPy array of shape (n, d) where each row is a n-dimensional point.\n      a, b, c: Parameters of the Ackley function.\n    Returns:\n      A NumPy array of shape (n,) of function values\n    \"\"\"\n    X = np.atleast_2d(X)\n    d = X.shape[1]\n    sum_sq = np.sum(X ** 2, axis=1)\n    term1 = -a * np.exp(-b * np.sqrt(sum_sq / d))\n    term2 = -np.exp(np.sum(np.cos(c * X), axis=1) / d)\n    return term1 + term2 + a + np.e\n\nWe can think of the rows of \\(X\\) as features of different alternatives. We can visualize the full landscape of the utility function for two alternatives (\\(n=2\\)) and features of a single dimension (\\(d=1\\)).\n\n\n\n\n\n\nCode\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nccmap = LinearSegmentedColormap.from_list(\"ackley\", [\"#f76a05\", \"#FFF2C9\"])\nplt.rcParams.update({\n    \"font.size\": 14,\n    \"axes.labelsize\": 16,\n    \"xtick.labelsize\": 14,\n    \"ytick.labelsize\": 14,\n    \"legend.fontsize\": 14,\n    \"axes.titlesize\": 16,\n})\nplt.rcParams['text.usetex'] = True\n\ndef draw_surface():\n    inps = np.linspace(-2, 2, 100)\n    X, Y = np.meshgrid(inps, inps)\n    grid = np.column_stack([X.ravel(), Y.ravel()])\n    Z = ackley(grid).reshape(X.shape)\n    \n    plt.figure(figsize=(6, 5))\n    contour = plt.contourf(X, Y, Z, 50, cmap=ccmap)\n    plt.contour(X, Y, Z, levels=15, colors='black', linewidths=0.5, alpha=0.6)\n    plt.colorbar(contour, label=r'$f(x)$', ticks=[0, 3, 6])\n    plt.xlim(-2, 2)\n    plt.ylim(-2, 2)\n    plt.xticks([-2, 0, 2])\n    plt.yticks([-2, 0, 2])\n    plt.xlabel(r'$x_1$')\n    plt.ylabel(r'$x_2$')\n\nIn this model, we can sample choice data when assuming a random generating model of, e.g., np.random.randn(n, d)*0.5 + np.ones((n, d))*0.5.\n\n1.3.1 Item-wise Model\nOne method for data collection is accept-reject sampling, where the decision-maker considers one item at a time and decides if they like it compared to an outside option. This is common in applications like recommendation systems, where accepting refers to a consumption signal.\nWe will use a simulation to familiarize ourselves with accept-reject sampling. On the surface below, blue and red points correspond to accept or reject points.\n\n\n\n\n\n\nCode\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nd = 2\nn = 800\nitems = np.random.randn(n, d)*0.5 + np.ones((n, d))*0.5\nutilities = ackley(items)\ny = (utilities &gt; utilities.mean())\ndraw_surface()\nplt.scatter(items[:, 0], items[:, 1], c=y, cmap='coolwarm', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1.3.2 Pairwise Model\nPairwise comparisons, now used to fine-tune large language models can similarly be generated in this model.\n\n\n\n\n\n\n\n\nCode\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nn_pairs = 10000\npair_indices = np.random.randint(0, n, size=(n_pairs, 2))\n# Exclude pairs where both indices are the same\nmask = pair_indices[:, 0] != pair_indices[:, 1]\npair_indices = pair_indices[mask]\n\nscores = np.zeros(n, dtype=int)\nwins = utilities[pair_indices[:, 0]] &gt; utilities[pair_indices[:, 1]]\n\n# For pairs where the first item wins:\n#   - Increase score for the first item by 1\n#   - Decrease score for the second item by 1\nnp.add.at(scores, pair_indices[wins, 0], 1)\nnp.add.at(scores, pair_indices[wins, 1], -1)\n\n# For pairs where the second item wins or it's a tie:\n#   - Decrease score for the first item by 1\n#   - Increase score for the second item by 1\nnp.add.at(scores, pair_indices[~wins, 0], -1)\nnp.add.at(scores, pair_indices[~wins, 1], 1)\n\n# Determine preferred and non-preferred items based on scores\npreferred = scores &gt; 0\nnon_preferred = scores &lt; 0\n\ndraw_surface()\nplt.scatter(items[preferred, 0], items[preferred, 1], c='blue', label='Preferred', alpha=0.5)\nplt.scatter(items[non_preferred, 0], items[non_preferred, 1], c='purple', label='Non-preferred', alpha=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nSimilarly, we can sample data for \\((y, Y')\\) for \\(y \\in Y' \\subseteq Y\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "src/chap2.html#mean-utilities",
    "href": "src/chap2.html#mean-utilities",
    "title": "1  Background",
    "section": "1.4 Mean Utilities",
    "text": "1.4 Mean Utilities\nWe can view a random utility model \\(u_{\\mathord{\\prec}}\\) as a deterministic part and a random part: \\[\nu_{\\mathord{\\prec}} (y) = u(y) + \\varepsilon_{\\mathord{\\prec}y}.\n\\] The vector \\(u(y)\\) is deterministic, and a vector \\((\\varepsilon_{\\mathord{\\prec}y})_{y \\in Y}\\) of noise is independent for different \\(\\prec\\). We say that \\(u(y)\\) is the mean utility and \\(\\varepsilon_{\\mathord{\\prec}y}\\) is the noise. There are different forms of noise possible. We will focus on a particular one (called Type-1 Extreme value), but others are also popular, for example Gaussian noise.\nThere are (at least) three different ways to view this noise:\n\nEither it is capturing the heterogeneity of different decision-makers—a view that is taken in the Economics field of Industrial Organization. Under this view, observing \\((y, Y')\\) more frequently than \\((y', Y')\\) is a sign of there being a higher number of decision-makers preferring \\(y\\) over \\(y'\\) than the other way around.\nor as errors of a decision-maker’s optimization of utilities \\(u(y)\\). This view is endorsed in the literature on Bounded Rationality. Under this view, it cannot be directly concluded from frequent observation of \\((y, Y')\\) compared to \\((y', Y')\\) that \\(y\\) is preferred to \\(y\\). It might have been chosen in error.\nWe can also view it as a belief of the designer about the preferences \\((u(y))_{y \\in Y}\\). In this view, the posterior after observing data can be used to make claims about relative preferences.\n\nThe interpretation will guide our decision-making predictions in Chapters 4 and 5.\nWe next introduce a main way to simplify learning utility functions: The axiom of Independence of Irrelevant Alternatives.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "src/chap2.html#independence-of-irrelevant-alternatives",
    "href": "src/chap2.html#independence-of-irrelevant-alternatives",
    "title": "1  Background",
    "section": "1.5 Independence of Irrelevant Alternatives",
    "text": "1.5 Independence of Irrelevant Alternatives\nIn later chapters, we will consider cases where we sample from the most preferred elements from all objects \\((y,Y)\\), which we call generation task. A simple assumption will allow us to recover the probabilities of \\((y, Y)\\), and in fact, the full distribution of \\(\\prec\\) from binary comparisons: The so-called Independence of Irrelevant Alternatives, introduced in Luce et al. (1959). This assumption not only allows us to easily identify a preference model, it will also massively reduce what is needed to be estimated from data: Instead of the full \\(n!-1\\)-dimensional object, it will be sufficient to learn \\(n\\) values.\nIIA assumes that the relative likelihood of choosing \\(y\\) compared to z does not change whether a third alternative \\(w\\) is in the choice set or not. Formally, for every \\(Y' \\subseteq Y\\), \\(y,z \\in Y'\\), and \\(w \\in Y \\setminus Y'\\), \\[\n\\frac{\\mathbb{P}[(y, Y')]}{\\mathbb{P}[(z, Y')]} = \\frac{\\mathbb{P}[(y, Y' \\cup \\{w\\})]}{\\mathbb{P}[(z, Y' \\cup \\{w\\})]}.\n\\] (In particular, it must be that \\(\\mathbb{P}[(z, Y')] \\neq 0\\) and \\(\\mathbb{P}[(z, Y' \\cup \\{w\\})] \\neq 0\\).) That is, the relative probability of choosing \\(y\\) over \\(y''\\) and \\(y'\\) over \\(y''\\) should be independent of whether \\(z\\) is present in the choice set \\(Y' \\subseteq Y\\). We will show that this single assumption is sufficient to make the choice model \\(n\\)-dimensional, making learning feasible.\nFirst, to our primary example: All random utility models with independent and identically distributed noise terms satisfy IIA. (We ask the reader to convince themselves that the Ackerman function does not satisfy IIA.)\n\n\n\n\n\n\ntheorem\n\n\n\nA random utility model \\(u_{\\mathord{\\prec}}(y)\\) satisfies IIA if and only if we can write it as \\(u_{\\mathord{\\prec}}(y) = u(y) + \\varepsilon_{\\mathord{\\prec}y}\\), where \\(u(y)\\) is deterministic and \\(\\varepsilon_{\\mathord{\\prec}y}\\) is sampled independently and identically from the Gumbel distribution. The Gumbel distribution has cumulative distribution function \\(F(x) = e^{-e^{-x}}\\).\n\n\nThis is quite strong, and an equivalence. If we are willing to assume IIA, it is sufficient to learn \\(n\\) parameters to characterize the full distribution—an exponential decrease in parameters to learn. The Gumbel model may be unusual in particular for those with a stronger background in machine learning. A more familiar formulation arises for the probabilities of choice.\n\n\n\n\n\n\ntheorem\n\n\n\nAssume a random preference model satisfies IIA, hence \\(u_{\\mathord{\\prec}} (y)= u(y) + \\varepsilon_{\\mathord{\\prec}y}\\). Then, the probabilities of lists are: \\[\n\\mathbb{P}[(y_1 \\succ y_2 \\succ \\cdots \\succ y_n)] = \\frac{e^{u(y_1)}}{\\sum_{i=1}^n e^{u(y_1)}} \\cdot \\frac{e^{u(y_2)}}{\\sum_{i=2}^n e^{u(y_i)}}\\cdot \\frac{e^{u(y_3)}}{\\sum_{i=3}^n e^{u(y_1)}} \\cdots \\frac{e^{u(y_{n-1})}}{ e^{u(y_{n-1})} +  e^{u(y_{n})}}.\n\\] For choices from sets, \\[\n\\mathbb{P} [(y, Y')] = \\frac{e^{u(y)}}{\\sum_{y' \\in Y'} e^{u(y')}} = \\operatorname{softmax}_y ((u(y'))_{y' \\in Y'}).\n\\] In particular, for binary comparisons \\[\n\\mathbb{P} [(y_1 \\succ y_2)] = \\frac{e^{u(y_1)}}{e^{u(y_1)} + e^{u(y_1)}} = \\frac{1}{1 + e^{u(y_1) - u(y_2)}} = \\sigma (u(y_1) - u(y_2)).\n\\] where \\(\\sigma = 1/(1 + e^x)\\) is the sigmoid function.\n\n\nIn particular, the choice probabilities \\(\\mathbb{P} [(y, Y)]\\) are equivalent to the multi-class logistic regression model (also called multinomial logit model), and generation from this model, that is, sampling \\(y\\) with probability \\(\\mathbb{P}[(y, Y)]\\) is given by softmax-sampling \\(\\operatorname{softmax}((u(y))_{y \\in Y})\\).\nThis model has many names, depending on the feedback type we consider. For binary comparison data, it is the Bradley-Terry model Bradley and Terry (1952). If data is in forms of list, it is called the Plackett-Luce model Plackett (1975). For accept-reject sampling it is also called logistic regression. For choices from subsets \\(Y\\), is is called the (discrete choice) logit model. We will usually call the model the logit model and specify the feedback type.\nThe IIA assumption has many desirable properties, such as stochastic transitivity and relativity. The reader is asked to prove them in the exercises to this chapter. The learning of, and optimization based on, the mean utilities \\((u(y))_{y \\in Y}\\) is one of the central goal of this book. Supervised learning based on it will be covered in the next chapter.\nOne note on identification, that is, whether different utility functions \\(u\\) generate the same random preference model \\(\\prec\\). The models that are implied by \\(u(y)\\) and by \\(u(y) + c\\) for any constant \\(c \\in \\mathbb{R}\\) are the same, which means that any learning of the function \\(u\\), which we will engage in in the next chapters, will need to fix one of the values \\(u(y)\\). If there is an outside option \\(y_0\\), then, it is typically chosen \\(u(y_0) = 0\\) and all mean utilities are in comparison to the outside option.\nIIA has limitations, which might require to allow for more flexible specifications of noise and heterogeneity.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "src/chap2.html#iias-limitations",
    "href": "src/chap2.html#iias-limitations",
    "title": "1  Background",
    "section": "1.6 IIA’s Limitations",
    "text": "1.6 IIA’s Limitations\nIIA is surprisingly strong, but does not allow for choice probabilities that are, fundamentally, results of multiple decision-makers making choices together. A first restriction is given by\n\n1.6.1 IIA and Heterogeneity\nA crucial shortcoming of IIA is that if sub-populations satisfy IIA this does not mean that the full population satisfies IIA. Assume that our population consists of sub-populations \\(i = 1, \\dots, m\\) which have mass \\(\\alpha_1, \\alpha_2, \\dots, \\alpha_m\\), respectively, in the population, and that each of the groups has preferences satisfying IIA. Because of IIA, we can represent each sub-group’s stochastic preferences with an average utility \\(u_i \\colon Y \\to \\mathbb R\\), \\(i = 1, 2, \\dots, n\\). The distribution of the full population is then given by a mixture of the sub-population preferences. For example, for binary comparisons\n\\[\n\\mathbb{P} [y_1 \\succ y_2 ] = \\sum_{i=1}^n \\alpha_i \\mathbb{P} [y_1 \\succ y_2  | \\text{ group } i]   = \\sum_{i = 1}^m \\alpha_i \\sigma (u_i (y_1) - u_i(y_2)).\n\\]\nSadly, such mixtures are far from IIA, as we see in the following coding example.\n\n\n\n\n\n\nCode\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nimport numpy as np\n\n# Define utilities for each group\ngroup1_utilities = {'A': 1.0, 'B': 2.0, 'C': 3.0}\ngroup2_utilities = {'A': 3.0, 'B': 2.0, 'C': 1.0}\n\n# Group weights\nalpha1 = 0.5\nalpha2 = 0.5\n\ndef softmax(utilities):\n    exp_vals = np.exp(list(utilities.values()))\n    total = np.sum(exp_vals)\n    return {k: np.exp(v) / total for k, v in utilities.items()}\n\n# Compute mixed logit probabilities over all 3 options\np1_full = softmax(group1_utilities)\np2_full = softmax(group2_utilities)\np_mix_full = {k: alpha1 * p1_full[k] + alpha2 * p2_full[k] for k in group1_utilities}\n\n# Compute ratio A/B in full model\nratio_full = p_mix_full['A'] / p_mix_full['B']\n\n# Now remove option C\nreduced_utils1 = {'A': group1_utilities['A'], 'B': group1_utilities['B']}\nreduced_utils2 = {'A': group2_utilities['A'], 'B': group2_utilities['B']}\np1_reduced = softmax(reduced_utils1)\np2_reduced = softmax(reduced_utils2)\np_mix_reduced = {k: alpha1 * p1_reduced[k] + alpha2 * p2_reduced[k] for k in reduced_utils1}\n\n# Compute ratio A/B in reduced model\nratio_reduced = p_mix_reduced['A'] / p_mix_reduced['B']\n\n# Show violation of IIA\nprint(f\"Ratio A/B with all options: {ratio_full:.4f}\")\nprint(f\"Ratio A/B after removing C: {ratio_reduced:.4f}\")\n\nRatio A/B with all options: 1.5431\nRatio A/B after removing C: 1.0000\n\n\nAn intuition for IIA’s failure comes from viewing it as random utility functions: A mixture of vectors that have independent entries is not Gaussian.\n\n\n\n\n\n\nCode\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Define two Gaussian distributions\nmu1, sigma1 = 0, 1\nmu2, sigma2 = 4, 1\nx = np.linspace(-4, 8, 1000)\n\n# Evaluate the PDFs\npdf1 = norm.pdf(x, mu1, sigma1)\npdf2 = norm.pdf(x, mu2, sigma2)\n\n# Mixture: equal weight\nmixture_pdf = 0.5 * pdf1 + 0.5 * pdf2\n\n# Plot\nplt.plot(x, pdf1, label='N(0, 1)', linestyle='--')\nplt.plot(x, pdf2, label='N(4, 1)', linestyle='--')\nplt.plot(x, mixture_pdf, label='Mixture 0.5*N(0,1) + 0.5*N(4,1)', linewidth=2)\nplt.title(\"Mixture of Two Gaussians is Not Gaussian\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nClassic ways to solve this concern is to consider a model with explicit representation of heterogeneity, where \\(y \\prec y'\\) holds if \\(\\alpha \\sim F\\) for some distribution \\(F\\), and \\((u(y))_{y \\in Y} \\alpha\\) is a random utility model with independent error terms. For example, consider a logit random utility model \\[\nu(y) = \\beta^\\top x + \\varepsilon_y\n\\] and assume \\(\\beta \\sim N(\\mu, \\Sigma)\\) is a normally distributed vector, a model called the random coefficients logit model. Equivalently, we can view this as a model with correlated utility shocks.\n\n\n1.6.2 Similar Options\nA second limitation is only relevant if we move beyond binary choices, or observe preference lists. Let \\(Y = \\{y, y', z\\}\\), where \\(y\\) and \\(y'\\) are (almost) identical and different from \\(z\\). (In the classical example, \\(y, y'\\) are red and blue buses, respectively, and \\(z\\) is a train). Assume an IIA model given by average utility \\(u \\colon Y \\to \\mathbb{R}\\). As \\(y\\) and \\(y'\\) are almost identical, assume \\(u(y) = u(y')\\). We have \\(\\mathbb{P}[z, \\{y, z\\}] = \\mathbb{P}[z, \\{y', z\\}]\\). How do these values compare to \\(\\mathbb{P}[z, \\{y, y', z\\}]\\)? It would be intuitive to think that \\(z\\) is chosen with the same frequency, as there should not be more “demand” for object \\(z\\) only because \\(y\\) is cloned. This is not the case.\n\n\n\n\n\n\nCode\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nimport numpy as np\n\n# Deterministic utilities\nv_car = 1.0\nv_bus = 2.0  # Initially a single bus alternative\n\n# Logit choice probabilities (before splitting bus)\ndef softmax(utilities: np.ndarray) -&gt; np.ndarray:\n    exp_util = np.exp(utilities)\n    return exp_util / np.sum(exp_util)\n\n# Before splitting: two alternatives\nutilities_before = np.array([v_car, v_bus])\nprobs_before = softmax(utilities_before)\nprint(\"Before splitting (Car, Bus):\", probs_before)\n\n# After splitting: three alternatives\nv_red_bus = v_bus\nv_blue_bus = v_bus\nutilities_after = np.array([v_car, v_red_bus, v_blue_bus])\nprobs_after = softmax(utilities_after)\nprint(\"After splitting (Car, Red Bus, Blue Bus):\", probs_after)\nprint(\"After splitting, total bus share:\", probs_after[1] + probs_after[2])\n\nBefore splitting (Car, Bus): [0.26894142 0.73105858]\nAfter splitting (Car, Red Bus, Blue Bus): [0.1553624 0.4223188 0.4223188]\nAfter splitting, total bus share: 0.8446375965030364\n\n\nThe choice probability of car is reduced. Why is our intuition making us think that \\(y\\) and \\(y'\\) should split their choice probability? One option is because we assume some correlation: If you like \\(y\\) over \\(z\\) then you should also like \\(y'\\) over \\(z\\), and vice versa. Hence, we would like the correlation between choice probabilities in random utility models. For example, if we allow in a logit random utility model the error terms in \\(y, y'\\) to be perfectly correlated (and we break ties uniformly at random), then \\[\n(\\mathbb{P}[y, \\{y, y', z\\}], \\mathbb{P}[y', \\{y, y', z\\}], \\mathbb{P}[z, \\{y, y', z\\}]) = \\left(\\frac{\\mathbb{P}[y, \\{y, z\\}]}{2}, \\frac{\\mathbb{P}[y', \\{y', z\\}]}{2}, \\frac{\\mathbb{P}[z, \\{y, z\\}]}{2}\\right),\n\\] confirming our intuition.\nThis is the end of our discussion of the Independence of Irrelevant Alternatives. Additional features can be found in (Train 2009; Ben-Akiva and Lerman 1985; McFadden 1981) and the original paper for logit analysis McFadden (1972).\nThe next chapter is the first to study learning of average utility functions from preference data, and assumes that a dataset is given of (average) utility functions \\(u \\colon Y \\to \\mathbb R\\) for different types of sampling and for different notions of “inference”.\n\n\n\nTable 1.1: Table 1 — Notation used in Chapter “Background”.\n\n\n\n\n\n\n\n\n\n\nNotation\nMeaning\nDomain / Type\n\n\n\n\n\\(Y\\)\nFinite set of objects/alternatives\n\\(\\{y_1,\\dots ,y_n\\}\\)\n\n\n\\(n\\)\nNumber of objects\n\\(\\lvert Y\\rvert\\in\\mathbb N\\)\n\n\n\\(y,y',y''\\)\nGeneric objects in \\(Y\\)\nElements of \\(Y\\)\n\n\n\\(y_0\\)\nOutside (“no-choice”) option\nElement of \\(Y\\) (reference)\n\n\n\\(\\prec\\) / \\(\\succ\\)\nWeak preference relation / its strict part\nBinary relation on \\(Y\\)\n\n\n\\(\\mathord{\\prec}\\)\nRandom preference (draw of \\(\\prec\\))\nRV over total orders on \\(Y\\)\n\n\n\\(L=(y_1,\\dots ,y_n)\\)\nFull ranking (preference list)\nPermutation of \\(Y\\)\n\n\n\\((y,Y')\\)\nObservation that \\(y\\) is chosen from subset \\(Y'\\)\n\\(y\\in Y'\\subseteq Y\\)\n\n\n\\(x\\in X\\)\nExogenous context / features\n\\(X\\) (arbitrary feature space)\n\n\n\\(u(y)\\)\nMean (deterministic) utility of \\(y\\)\n\\(\\mathbb R\\)\n\n\n\\(u_{\\mathord{\\prec}}(y)\\)\nRandom utility in draw \\(\\mathord{\\prec}\\)\n\\(\\mathbb R\\)\n\n\n\\(\\varepsilon_{\\mathord{\\prec}y}\\)\nStochastic utility shock\n\\(\\mathbb R\\) (i.i.d.)\n\n\n\\(\\mathbb P[\\cdot]\\)\nProbability measure over preferences/choices\n\\([0,1]\\)\n\n\n\\(\\operatorname{softmax}_y\\bigl((u(y'))_{y'\\in Y'}\\bigr)\\)\nLogit/Plackett-Luce choice probability of \\(y\\) from \\(Y'\\)\n\\([0,1]\\)\n\n\n\\(\\sigma(z)\\)\nSigmoid \\(1/(1+e^{-z})\\)\n\\([0,1]\\)\n\n\n\\(d\\)\nDimensionality of feature vectors\n\\(\\mathbb N\\)\n\n\n\\(\\boldsymbol{x}\\in\\mathbb R^{d}\\)\nFeature vector of an object\n\\(\\mathbb R^{d}\\)\n\n\n\\(\\text{Ackley}(\\boldsymbol{x})\\)\nAckley test-function value\n\\(\\mathbb R\\)\n\n\n\\(a,b,c\\)\nAckley parameters\nScalars\n\n\n\\(k\\)\nNumber of preference samples\n\\(\\mathbb N\\)\n\n\n\\(\\alpha_i\\)\nPopulation weight of subgroup \\(i\\)\n\\((0,1)\\) with \\(\\sum_i\\alpha_i=1\\)\n\n\n\\(\\beta\\)\nRandom-coefficients vector in linear RUM\n\\(\\mathbb R^{d}\\)\n\n\n\\(\\Sigma\\)\nCovariance matrix of \\(\\beta\\)\n\\(\\mathbb R^{d\\times d}\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "src/chap2.html#discussion-questions",
    "href": "src/chap2.html#discussion-questions",
    "title": "1  Background",
    "section": "1.7 Discussion Questions",
    "text": "1.7 Discussion Questions\n\nHow does modeling preferences as random (rather than deterministic) help us capture real-world choice behavior?\n\nWhat is the Independence of Irrelevant Alternatives (IIA) axiom, and why does it simplify the estimation of choice models?\n\nWhy do i.i.d. Gumbel shocks in a random utility model lead to the Plackett–Luce (list) and softmax/logit (choice) formulas?\n\nIn what ways can binary comparisons, choice-from-a-set, and full rankings each be seen as observations of the same underlying stochastic preference distribution?\n\nWhat are the practical advantages and drawbacks of eliciting full preference lists versus pairwise comparisons from human subjects?\n\nHow does introducing an “outside option” \\(y_{0}\\) allow us to interpret accept–reject data within the same logit framework?\n\nWhy does mixing multiple IIA-satisfying sub-populations generally violate IIA at the aggregate level?\n\nExplain the “red bus–blue bus” problem: why does splitting a single alternative into two identical ones distort logit choice probabilities?\n\nHow does context \\(x\\) enter the random utility framework, and what role does it play in generalizing preferences to new situations?\n\nWhat identification issues arise from the fact that adding a constant to all utilities \\(u(y)\\) does not change observable choice probabilities?\n\nIn what scenarios would you consider relaxing IIA and what additional model complexity does that introduce?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "src/chap2.html#exercises",
    "href": "src/chap2.html#exercises",
    "title": "1  Background",
    "section": "1.8 Exercises",
    "text": "1.8 Exercises\nWe place ⭐, ⭐⭐, and ⭐⭐⭐ for exercises we deem relatively easy, medium, and hard, respectively.\n\n1.8.1 Properties of IIA Models ⭐\nProve that if a preference model satisfies IIA, it will also satisfy \\(\\mathbb{P}[(y, Y')] \\le \\mathbb{P}[(y, Y'')]\\) for any \\(y \\in Y\\) and \\(Y' \\subseteq Y'' \\subseteq Y\\) (called regularity) and for all \\((x,y,z)\\), if \\(\\mathbb{P}[(x, \\{x,y\\})] \\ge 0.5\\) and \\(\\mathbb{P}[(y, \\{y,z\\})] \\ge 0.5\\), then necessarily \\(\\mathbb{P}[(x, \\{x,z\\})] \\ge 0.5\\).\n\n\n1.8.2 Discrete Choice Models ⭐⭐\nConsider a linear random utility model \\(u(y)=\\beta_i^\\top x+\\epsilon_i\\) for \\(i=1, 2, \\cdots, N\\), where \\(\\varepsilon_y\\) is i.i.d. sampled from a Gumbel distribution. We would like to compute \\(\\mathbb{P}[(y, Y)]\\) and connect it to multi-class logistic regression.\n\nFirst \\(\\mathbb{P}[u(y)&lt;t]\\) for any $ for \\(j\\neq i\\) in terms of \\(F\\). Use this probability to provide a formula for \\(\\mathbb{P}[(y, Y)]\\) over \\(t\\) in terms of \\(f\\) and \\(F\\).\nCompute the integral derived in part (a) with the appropriate \\(u\\)-substitution. You should arrive at the multi-class logistic regression model.\n\n\n\n1.8.3 Mixtures and correlations ⭐\nProve that the class of random preferences induced by the following two are identical: (a) mixtures of IIA random utility models (that is, those with i.i.d. noise) (b) random utility models with correlated noise.\n\n\n1.8.4 Sufficient Statistics ⭐⭐⭐\n\nShow that choice data completely specifies the preference model. That is, express \\(\\mathbb{P}[\\mathord{\\prec}]\\) for any \\(\\prec\\) in terms of \\(\\mathbb{P}[(y, Y')]\\), \\(y \\in Y' \\subseteq Y\\).\nShows that this is not the case for binary comparisons. That is, give an example of two different preference models that induce the same probabilities \\(\\mathbb{P}[y, \\{x, y\\}]\\).\n\n\n\n1.8.5 Non-Random Utility Models ⭐⭐⭐\nNot all probability assignments for binary comparisons \\(p_{y_1y_2} = \\mathbb{P}[y_1 \\prec y_1]\\) can be realized with a random preference model. Give an example of binary comparisons \\((p_{y_1y_2}, p_{y_2y_3}, p_{y_3y_1})\\) that cannot be a result of a random preference model.\n\n\n1.8.6 Posterior Inference for Mixture Preferences ⭐⭐\n(This exercise previews some of the aspects for learning utility functions from the next chapter but is self-contained.) You are part of the ML team on the movie streaming site “Preferential”. You receive full preference orderings in the form \\(y_1 \\succ y_2 \\succ \\cdots \\succ y_n\\), where \\(y_1\\) is the most, and \\(y_n\\) the least preferred option. The preferences come from \\(600\\) distinct users with \\(50\\) examples per user. Each movie has a \\(10\\)-dimensional feature vector \\(m_y\\), and each user has a \\(10\\)-dimensional weight vector \\(v_i\\). The preferences for user \\(i\\) follow the random utility model \\(u(y) = v_i^\\top m_y + \\varepsilon_y\\), where \\(\\varepsilon_y\\) is i.i.d. Gumbel distributed.\nSadly, you lost all user identifiers. Unashamedly, you assume a model where a proportion \\(p\\) of the users have weights \\(w_1\\), and a proportion \\(1-p\\) has weights \\(w_2\\). Each user belongs to one of two groups: users with weights \\(w_1\\) are part of Group 1, and users with weights \\(w_2\\) are part of Group 2.\n\nFor a datapoint \\((y_1 \\succ y_2)\\) with label and conditional on \\(p\\), \\(w_1\\) and \\(w_2\\), compute the likelihood \\(P(y_1\\succ y_2 | p, w_1, w_2)\\).\nUse the likelihood to simplify the posterior distribution of \\(p, w_1, w_2\\) after updating on \\((m_1, m_2)\\) leaving terms for the priors unchanged.\nAssume priors \\(p\\sim B(1, 1)\\), \\(w_1\\sim N (0, \\mathbf{I})\\), and \\(w_2\\sim N(0, \\mathbf{I})\\) where \\(B\\) represents the Beta distribution and \\(\\mathcal{N}\\) represents the normal distribution (all three sampled independently). You will notice that the posterior from part (b) has no simple closed form, requiring numerical methods. One such method, allowing to approximate sample from the posterior \\(\\pi\\), is called Metropolis-Hastings. (The reason why one might want to sample from the posterior will be discussed in ?sec-beyond.) Broadly, the idea of Metropolis-Hastings and similar, so-called Markov Chain Monte Carlo methods is the following: Construct a Markov chain \\(\\{x_t\\}_{t=1}^\\infty\\) which has as “ergodic” distribution given by your desired distribution. By properties of Markov chains, for \\(t \\gg 0\\), \\(x_t\\) will be almost as good as sampled from the “ergodic” distribution. In Metropolis-Hastings, the distribution is a proposal \\(P\\) for \\(x_{t+1}\\) is made via sampling from a chosen probability kernel \\(Q(\\bullet | x_t)\\) (e.g., adding Gaussian noise). The acceptance probability of the proposal is given by\n\n\\[\nx_{t+1}=\\begin{cases} \\tilde Q(\\bullet | x_t) & \\text{with probability } A, \\\\ x_t & \\text{with probability } 1 - A. \\end{cases}\n\\] where \\[\nA= \\min \\left( 1, \\frac{\\pi(\\bullet )Q(x_t | \\bullet )}{\\pi(x_t)Q( \\bullet | x_t)} \\right).\n\\] We will extract samples from the Markov chain after a “burn-in period”, \\((x_{T+1}, x_{T+2},\\cdots, x_{N})\\).\nTo build some intuition, suppose we have a biased coin that turns heads with probability \\(p_{\\text{heads}}\\). We observe \\(12\\) coin flips to have \\(9\\) heads (H) and \\(3\\) tails (T). If our prior for \\(p_{\\text{H}}\\) was \\(B(1, 1)\\), then, by properties of the Beta distribution, our posterior will be \\(B(1 + 9, 1 + 3)=B(10, 4)\\). The Bayesian update is given by\n\\[\np(p_{\\text{H}}|9\\text{H}, 3\\text{T}) = \\frac{p(9\\text{H}, 3\\text{T} | p_{\\text{H}})B(1, 1)(p_{\\text{H}})}{\\int_0^1 P(9\\text{H}, 3\\text{T} | p_{\\text{H}})B(1, 1)(p_{\\text{H}}) \\, \\mathrm dp_{\\text{H}}} =\\frac{p(9\\text{H}, 3\\text{T} | p_{\\text{H}})}{\\int_0^1 p(9\\text{H}, 3\\text{T} | p_{\\text{H}}) \\, \\mathrm dp_{\\text{H}}}.\n\\]\nFind the acceptance probability \\(A\\) in the setting of the biased coin assuming the proposal distribution \\(Q(\\cdot|x_t)=x_t+N(0,\\sigma)\\) for given \\(\\sigma\\). Notice that this choice of \\(Q\\) is symmetric, i.e., \\(Q(x_t|p)=Q(p|x_t)\\) for all \\(p \\in \\mathbb R\\). Note that it is unnecessary to compute the normalizing constant of the Bayesian update (i.e., the integral in the denominator). This simplification is one of the main practical advantages of Metropolis-Hastings.\n\nImplement Metropolis-Hastings to sample from the posterior distribution of the biased coin in multimodal_preferences/biased_coin.py. Attach a histogram of your MCMC samples overlayed on top of the true posterior \\(B(10, 4)\\) by running python biased_coin.py.\n\n\n\n\n\n\n\nCode\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nImplement Metropolis-Hastings in the movie setting inside multimodal_preferences/movie_metropolis.py. You should be able to achieve a \\(90\\%\\) success rate with most fraction_accepted values above \\(0.1\\). Success is measured by thresholded closeness of predicted parameters to true parameters. You may notice occasional failures that occur due to lack of convergence which we will account for in grading.\n\n\n\n\n\n\n\nCode\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "src/chap3.html",
    "href": "src/chap3.html",
    "title": "2  Learning",
    "section": "",
    "text": "2.1 Data Generating Process: A Latent Variable Perspective\nDesigning a good reward signal by hand for a complex AI system is difficult and error-prone. Instead of manually specifying desirable behavior, we can learn a utility signal from preference data. In this chapter, we explore how to infer an underlying utility from various forms of feedback. Throughout, we include mathematical formulations and code examples to illustrate the learning process.\nBefore introducing models for preference data, it is useful to begin with some stylized empirical observations. For example, surveys indicate that only a small fraction of the US population report liking durian, while a much larger fraction endorse liking banana or strawberry. Some people appear to have a general fondness for fruit, while others are relatively indifferent or even averse. Interestingly, even within fruit-likers, there is heterogeneity: some prefer durian over banana, while others show the opposite ordering. These facts suggest several latent forces at work (appetites, baseline appeal, trait heterogeneity) that any plausible data-generating process (DGP) for preference must accommodate. Below we propose one such DGP as a conceptual scaffold for understanding the observed patterns.\nWe denote \\(Y_{i,j}\\) to be the response of user \\(i\\) to item \\(j\\), which is drawn from a collection of \\(M\\) items. The response is \\(1\\) if the user likes the item and zero otherwise. The Rasch model posits that response is a Bernoulli random variable governed by user’s appetite \\(U_i \\in \\mathbb{R}\\) and item’s appeal \\(V_j \\in \\mathbb{R}\\):\n\\[\np(Y_{i,j} = 1 | U_i, V_j) = \\sigma(H_{i,j}), \\quad H_{ij} = U_i + V_j\n\\] where \\(\\sigma\\) is the logistic function. The user appetite is a latent variable represents the appetite construct, which is the tendency of the user to like items in the collection in general, and the item appeal is how easy it is for the item to be like by the population of users.\nThe user can express their affinity via several mechanisms. For example: thumbs-up/down on an e-commerce site, swiping on a dating site, or skipping a track on a music app. User- and item-specific parameters do not need to be unidimensional. When appetide and appeal are \\(K\\) dimensional, the data generating process is \\[\np(Y_{i,j} = 1 | U_i, V_j, Z_j) = \\sigma(H_{i,j}), \\quad H_{ij} = d(U_i, V_j + Z_j),\n\\] where \\(d\\) is some distance function in the \\(K\\) dimensional space, \\(U_i\\) is the user-specific appetide for each dimension, \\(V_j\\) is the item loading, and \\(Z_j\\) is the offset. When \\(d\\) is the negative Euclidean distance, the model is know as the Ideal Point Model, which is popular in political sciences and psychometrics: \\[\nH_{ij} = -||U_i -  V_j ||_2 + Z_j\n\\] In this case, \\(U_i\\) is the user embedding and \\(V_j\\) is the item embedding with some scalar offset \\(Z_j\\). When the distance function is the dot product, the above generating process is known as the logistic factor model, a popular class of model in psychometrics: \\[\nH_{ij} = U_i^\\top V_j + Z_j\n\\]\nEach of the \\(K\\) dimension represents different property of the items. For example, when \\(K=2\\), a fruit might be sweet and have a strong aroma, and some users might look for sweetness, while others who might avoid sugar and look for strong aroma.\nIn many case, the user might be asked to express their preference in term of comparison between two item \\(j\\) and \\(j'\\), in which the response \\(Y_{i,jj'}\\) is one if the item \\(i\\) is preferred and zero otherwise. \\[\np(Y_{i,jj'} = 1 | i, j, j') = \\sigma(H_{i,j} - H_{i,j'}) = \\frac{\\exp(H_{ij})}{\\exp(H_{ij}) + \\exp(H_{ij'})},\n\\] where the last equality uses the logistic identity. This is also known in the literature as the Bradley-Terry model. For a single user, if the generating process for item-wise preference is Rasch, we can further simplify \\(H_{i,j} - H_{i,j'} = U_i + V_j - U_i - V_{j'} = V_j - V_{j'}\\). Hence, for a single user, the logit does not depend on the user-specific appetide, but only on the item parameters. This is generally not true for other classes of model.\nIn the item-wise preference, the responses for \\(N\\) user and \\(M\\) items is a 2D matrix with shape \\(N \\times M\\), while in the pairwise preference, the responses for the same number of users and items is a 3D tensor with shape \\(N \\times M \\times M\\).\nGiven a data generating process, we can synthesize some data. The code below is for generate a response matrix for Rasch response:\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rcParams.update({\n    \"font.size\": 14,\n    \"axes.labelsize\": 16,\n    \"xtick.labelsize\": 14,\n    \"ytick.labelsize\": 14,\n    \"legend.fontsize\": 14,\n    \"axes.titlesize\": 16,\n    \"figure.figsize\": (6, 6),\n    \"figure.autolayout\": True,\n})\nrng = np.random.default_rng(2601)\n\nN = 50 # users\nM = 30 # items\nU = rng.normal(loc=0.0, scale=1, size=N) # users' appetites\nV = rng.normal(loc=0.0, scale=1, size=M) # items' appeals\nP = 1.0 / (1.0 + np.exp(-(U[:, None] + V[None, :])))\nY = (rng.random(size=(N, M)) &lt; P)\nY = Y[np.argsort(Y.mean(axis=1))][:, np.argsort(Y.mean(axis=0))]\n\nplt.imshow(Y, cmap='coolwarm')\nplt.title(\"Rasch Responses\")\nplt.xlabel(\"Items\")\nplt.ylabel(\"Users\")\nplt.show()\nTaken a user and the item-wise generating process is Rasch, we generate their Bradley-Terry response below.\nP = 1.0 / (1.0 + np.exp(-(V[:, None] - V[None, :])))\nY_BT = np.full((M, M), np.nan)\ntriu_idx = np.triu_indices(M, k=1)\nrandu = rng.random(size=triu_idx[0].shape[0])\nwins_j_over_k = (randu &lt; P[triu_idx])  # 1 if j beats k\nY_BT[triu_idx] = wins_j_over_k\nY_BT[(triu_idx[1], triu_idx[0])] = 1.0 - wins_j_over_k\norder = np.argsort(np.nanmean(Y_BT, axis=1))\nY_BT = Y_BT[order][:, order]\n\nim0 = plt.imshow(Y_BT, cmap='coolwarm')\nplt.title(\"Bradley-Terry Response\")\nplt.xlabel(\"Items\")\nplt.ylabel(\"Items\")\nplt.show()\nRemember, the Bradley-Terry response above is just for a single user. To work with \\(N\\) user, we need to obtain the \\(N \\times M \\times M\\) tensor. To see heterogeneity, we need to simulate data under the K-dimensional factor model. The Rasch model does not have this effect since the response of each user does not depend on the user appetide.\nfrom matplotlib.colors import ListedColormap\nfrom mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n\nK = 3 # latent dimensions\nrng = np.random.default_rng(13)\nU = rng.normal(0, 1.0, size=(N, K))             # user embeddings\nV = rng.normal(0, 1.0, size=(M, K))             # item embeddings\nZ = rng.normal(0, 1, size=(M,))               # item offsets\nH = U @ V.T + Z[None, :]\nH_expanded_j = H[:, :, None]       # (N, M, 1)\nH_expanded_k = H[:, None, :]       # (N, 1, M)\ndiff = H_expanded_j - H_expanded_k # (N, M, M)\nP_pair = 1.0 / (1.0 + np.exp(-diff))\n\n# Mask diagonals (j == k)\nfor i in range(N):\n    np.fill_diagonal(P_pair[i], np.nan)\nrandu = rng.random(size=P_pair.shape)\nY_pair = (randu &lt; np.nan_to_num(P_pair, nan=0.0)).astype(int)\n# Force antisymmetry: Y[i,j,k] = 1 - Y[i,k,j], ignore diagonal\nfor i in range(N):\n    for j in range(M):\n        Y_pair[i, j, j] = 0\n        for k in range(j+1, M):\n            Y_pair[i, k, j] = 1 - Y_pair[i, j, k]\n# Mask lower triangle (keep j&gt;k) to avoid duplicates\ntri_mask = np.triu(np.ones((M, M), dtype=bool), k=1)\nvolume = np.zeros_like(Y_pair, dtype=bool)\nvolume[:, :, :] = tri_mask[None, :, :]\n\n# We'll assign colors based on Y_pair, but only keep voxels where tri_mask is True\nvalues = np.where(volume, Y_pair, -1)  # -1 means no voxel\nred = np.array([1.0, 0.0, 0.0, 0.95])\nblue = np.array([0.0, 0.0, 1.0, 0.95])\ntransparent = np.array([0, 0, 0, 0.0])\n\nfacecolors = np.empty(values.shape + (4,))\nfacecolors[:] = transparent\nfacecolors[values == 1] = red\nfacecolors[values == 0] = blue\n\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111, projection='3d')\nax.voxels(values != -1, facecolors=facecolors, edgecolor='k', linewidth=0.2)\n\nax.set_xlabel(\"Users\")\nax.set_ylabel(\"Item\")\nax.set_zlabel(\"Item\")\nax.set_xticks(np.arange(0, N, max(1, N // 5)))\nax.set_yticks(np.arange(0, M, max(1, M // 5)))\nax.set_zticks(np.arange(0, M, max(1, M // 5)))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learning</span>"
    ]
  },
  {
    "objectID": "src/chap3.html#data-generating-process-a-latent-variable-perspective",
    "href": "src/chap3.html#data-generating-process-a-latent-variable-perspective",
    "title": "2  Learning",
    "section": "",
    "text": "Code\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learning</span>"
    ]
  },
  {
    "objectID": "src/chap3.html#parameter-learning-via-full-information-maximum-likelihood-estimation",
    "href": "src/chap3.html#parameter-learning-via-full-information-maximum-likelihood-estimation",
    "title": "2  Learning",
    "section": "2.2 Parameter Learning via Full Information Maximum Likelihood Estimation",
    "text": "2.2 Parameter Learning via Full Information Maximum Likelihood Estimation\nGiven a response from a particular generating process, there are various standard statistical inference procedure, such as maximum likelihood, maximum marginal likelihood, or Bayesian inference. These procedure is designed to infer parameters that are generalizable to new datasets. Hence, we discuss the training and testing dataset next.\nThe simplest train/test splitting is random, where we select random element of the response matrix to be in the training set, and the rest in the test set. We can demonstrate, for example, with the Bradley-Terry response with a 80% training and 20% testing data:\n\nV = rng.normal(loc=0.0, scale=1, size=M)\ndiff = V[:, None] - V[None, :]\nP = 1.0 / (1.0 + np.exp(-diff))\n\nY_BT = np.full((M, M), np.nan)\ntriu_idx = np.triu_indices(M, k=1)\nrandu = rng.random(size=triu_idx[0].shape[0])\nwins_j_over_k = (randu &lt; P[triu_idx])  # 1 if j beats k\nY_BT[triu_idx] = wins_j_over_k\nY_BT[(triu_idx[1], triu_idx[0])] = 1.0 - wins_j_over_k\n\n# Indices for upper triangle (excluding diagonal)\ntriu_r, triu_c = np.triu_indices(M, k=1)\n\n# Keep only pairs that are observed (not NaN) in the source matrix\nvalid_mask = ~np.isnan(Y_BT[triu_r, triu_c])\ntriu_r = triu_r[valid_mask]\ntriu_c = triu_c[valid_mask]\n\nn_pairs = triu_r.shape[0]\nn_train = int(0.8 * n_pairs)\nidx = rng.choice(n_pairs, size=n_train, replace=False)\n\ntrain_pairs = np.zeros(n_pairs, dtype=bool)\ntrain_pairs[idx] = True\n\nY_train = np.full_like(Y_BT, np.nan, dtype=float)\nY_test  = np.full_like(Y_BT, np.nan, dtype=float)\n\nr_tr, c_tr = triu_r[train_pairs], triu_c[train_pairs]\nY_train[r_tr, c_tr] = Y_BT[r_tr, c_tr]\nY_train[c_tr, r_tr] = 1.0 - Y_BT[r_tr, c_tr]\n\n# Fill test set (remaining pairs), both symmetric entries\nr_te, c_te = triu_r[~train_pairs], triu_c[~train_pairs]\nY_test[r_te, c_te] = Y_BT[r_te, c_te]\nY_test[c_te, r_te] = 1.0 - Y_BT[r_te, c_te]\n\n# Diagonals stay NaN\nnp.fill_diagonal(Y_train, np.nan)\nnp.fill_diagonal(Y_test, np.nan)\n\ncmap = plt.cm.get_cmap('coolwarm').copy()\ncmap.set_bad(color='white')\nplt.subplot(1, 2, 1)\nim1 = plt.imshow(np.ma.masked_invalid(Y_train), vmin=0, vmax=1, cmap=cmap)\nplt.title(\"Train\")\nplt.xlabel(\"Items\")\nplt.ylabel(\"Items\")\nplt.subplot(1, 2, 2)\nim2 = plt.imshow(np.ma.masked_invalid(Y_test), vmin=0, vmax=1, cmap=cmap)\nplt.title(\"Test\")\nplt.xlabel(\"Items\")\nplt.colorbar(im2, fraction=0.046, pad=0.04)\nplt.show()\n\n\n\n\n\n\n\n\nHaving the train and test datasets from the Bradley-Terry response, which is denoted as \\(\\mathcal{D}_{\\text{train}}\\) and \\(\\mathcal{D}_{\\text{test}}\\), we first demonstrate the parameter learning with full information maximum likelihood estimation:\n\\[\n\\hat{V} = \\arg\\max_{V} \\sum_{j,j' \\in \\mathcal{D}_{\\text{train}}} \\log p(Y_{0,jj'} | P_{0,jj'}), \\quad P_{0, jj'} = V 1_M^\\top - 1_M V^\\top,\n\\] where \\(1_M\\) is a column vector of 1 with length \\(M\\). The optimization can be carried out with standard optimizers, such as gradient descent.\nLet \\(\\mathcal{N}^+_{m={(m,k) \\in \\mathcal D*{\\text{train}}}}\\) be pairs recorded in the order \\((m,k)\\). Let \\(\\mathcal{N}^-_{m={(k,m)\\in\\mathcal D*{\\text{train}}}}\\) be pairs recorded in the order \\((k,m)\\). Define residuals \\(r_{mk} = y_{mk} - \\sigma(V_m - V_k)\\) and \\(r_{km} = y_{km} - \\sigma(V_k-V_m)\\). Then\n\\[\n\\frac{\\partial \\ell}{\\partial V_m}\n= \\sum_{(m,k)\\in \\mathcal{N}^+_m} r_{mk}\n- \\sum_{(k,m)\\in \\mathcal{N}^-_m} r_{km}.\n\\]\nThis shows the contribution of data about \\(m\\) only. Each comparison with opponent \\(k\\) adds a term equal to the observed-minus-predicted win indicator, with a plus sign if \\(m\\) is listed first and a minus sign if \\(m\\) is listed second. Intuition. If \\(m\\) beats \\(k\\) more often than the model predicts, the residual is positive and the gradient pushes \\(V_m\\) up. If \\(m\\) loses to \\(k\\) more than predicted, the residual is negative and the gradient pushes \\(V_m\\) down.\n\ndef auc_from_scores(scores, labels):\n    pos = scores[labels == 1]\n    neg = scores[labels == 0]\n    if pos.size == 0 or neg.size == 0:\n        return np.nan\n    # Brute-force definition to avoid dependencies and handle ties\n    cmp = (pos[:, None] &gt; neg[None, :]).mean()\n    ties = (pos[:, None] == neg[None, :]).mean()\n    return cmp + 0.5 * ties\n\n# Collect observed training pairs and labels from the upper triangle only\nr_obs, c_obs = np.where(~np.isnan(Y_train))\nupper_mask = r_obs &lt; c_obs\nr_tr_fit = r_obs[upper_mask]\nc_tr_fit = c_obs[upper_mask]\ny_tr_fit = Y_train[r_tr_fit, c_tr_fit].astype(float)\n\nepochs = 100\nlr = 0.01\ntrain_auc_hist = []\nn_pairs = r_tr_fit.size\nV_hat = np.zeros(M, dtype=float)\n\nfor t in range(epochs):\n    # Scores and probabilities for observed training pairs\n    H = V_hat[r_tr_fit] - V_hat[c_tr_fit]\n    p = 1.0 / (1.0 + np.exp(-H))\n    err = (y_tr_fit - p)\n    grad = np.zeros_like(V_hat)\n    np.add.at(grad, r_tr_fit, err)\n    np.add.at(grad, c_tr_fit, -err)\n    V_hat += lr * grad\n    train_auc_hist.append(auc_from_scores(H, y_tr_fit))\n\n# Collect observed test pairs and labels from the upper triangle\nr_te_obs, c_te_obs = np.where(~np.isnan(Y_test))\nupper_mask_te = r_te_obs &lt; c_te_obs\nr_te_fit = r_te_obs[upper_mask_te]\nc_te_fit = c_te_obs[upper_mask_te]\ny_te_fit = Y_test[r_te_fit, c_te_fit].astype(float)\n\n# Scores for test pairs\ns_te = V_hat[r_te_fit] - V_hat[c_te_fit]\ntest_auc = auc_from_scores(s_te, y_te_fit)\n\n# Plot\nplt.plot(np.arange(1, epochs + 1), train_auc_hist, label=\"Train\")\nplt.hlines(test_auc, xmin=0, xmax=epochs, linestyle=\"--\", label=\"Test\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Train AUC\")\nplt.ylim(0.75, 0.85)\nplt.show()\n\n\n\n\n\n\n\n\nIn this synthetic setting, one way to interpret the result is to compare with the Bayes optimal AUC. Here we quantify an upper bound on achievable test AUC under the assumed data-generating process. For each test pair \\((j,k)\\) we know the ground-truth win probability \\(P_{jk}=\\sigma(V_j-V_k)\\) from the simulator. The Bayes-optimal score for that pair is exactly this probability, \\(s^*_{jk}=P_{jk}\\). Any classifier that ranks pairs by \\(s^*\\) maximizes AUC in expectation because it orders pairs by their true success probabilities. To estimate the corresponding Bayes AUC on our finite test set, we keep the same index set of pairs and repeatedly resample binary labels \\(Y_{jk} \\sim \\mathrm{Bern}(P_{jk})\\), then compute \\(\\mathrm{AUC}(s^*, Y)\\) on each resample using a tie-aware definition. The mean of these Monte Carlo AUCs is the Bayes-optimal test AUC, and the empirical quantiles give a sampling range induced purely by label noise. For comparison, we compute the model’s AUC by replacing \\(s^*_{jk}\\) with the learned scores \\(s^{*\\text{model}}_{jk}=\\sigma(\\hat V_j-\\hat V_k)\\). The gap between the two summarizes how far the fitted model is from the oracle ranking implied by the true \\(P^*_{jk}\\) on the exact same test pairs.\n\ndef collect_pairs(Y):\n    r, c = np.where(~np.isnan(Y))\n    upper = r &lt; c\n    return r[upper], c[upper], Y[r[upper], c[upper]].astype(float)\n\n# Unordered test set\nr_te_u, c_te_u, y_te_u = collect_pairs(Y_test)\n\n# Bayes scores and model scores for those exact pairs\ns_bayes = P[r_te_u, c_te_u]                 # ground-truth probs\ns_model = 1.0/(1.0+np.exp(-(V_hat[r_te_u]-V_hat[c_te_u])))\n\nauc_bayes = auc_from_scores(s_bayes, y_te_u)\nauc_model = auc_from_scores(s_model, y_te_u)\nprint(f\"Test AUC. Bayes: {auc_bayes:.4f}. Model: {auc_model:.4f}.\")\n\ndef resample_labels_from_P(r_idx, c_idx, P_mat, rng):\n    p = P_mat[r_idx, c_idx]\n    return (rng.random(size=p.size) &lt; p).astype(float)\n\nR = 2000  # repeats\nbayes_list = []\nmodel_list = []\nfor _ in range(R):\n    y_resamp = resample_labels_from_P(r_te_u, c_te_u, P, rng)\n    bayes_list.append(auc_from_scores(s_bayes, y_resamp))\n    model_list.append(auc_from_scores(s_model, y_resamp))\n\nprint(f\"Mean AUC over resamples. Bayes: {np.mean(bayes_list):.4f}. Model: {np.mean(model_list):.4f}.\")\nprint(f\"5–95% range Bayes: {np.percentile(bayes_list,[5,95])}. Model: {np.percentile(model_list,[5,95])}.\")\n\nTest AUC. Bayes: 0.8195. Model: 0.8443.\nMean AUC over resamples. Bayes: 0.8259. Model: 0.8071.\n5–95% range Bayes: [0.75343203 0.89377286]. Model: [0.73076311 0.8762815 ].\n\n\nThe result shows that the inference has found a good solution compare to the Bayes optimal estimator! A additional, natural test is to see if the estimated parameters match the true ones. Since the likelihood function only pays attention of the difference of item appeal for any pair, the appeal is only identifiable up to a shift and scale transform. The standard practice is to center and whiten the solution.\n\n# Center and whiten V_hat (zero mean, unit variance)\nV_hat_norm = (V_hat - V_hat.mean()) / (V_hat.std() + 1e-12)\n\n# Fit best affine map a*V_hat_norm + b to V (least squares)\nA = np.vstack([V_hat_norm, np.ones_like(V_hat_norm)]).T\na, b = np.linalg.lstsq(A, V, rcond=None)[0]\nV_hat_aligned = a * V_hat_norm + b\n\n# After alignment, points should cluster near the diagonal\nplt.scatter(V, V_hat, s=12, c=\"red\")\nplt.scatter(V, V_hat_aligned, s=12, c=\"blue\")\nlims = [-3, 3]\nplt.plot(lims, lims, linestyle=\"--\")\nplt.xlabel(\"V\")\nplt.ylabel(\"Estimated V\")\n\n# Report alignment quality\ncorr = np.corrcoef(V, V_hat)[0, 1]\nmse = np.mean((V - V_hat) ** 2)\nprint(f\"Before center and whiten: Corr {corr:.4f}, MSE: {mse:.6f}\")\n\ncorr = np.corrcoef(V, V_hat_aligned)[0, 1]\nmse = np.mean((V - V_hat_aligned) ** 2)\nprint(f\"After center and whiten: Corr {corr:.4f}, MSE: {mse:.6f}\")\n\nBefore center and whiten: Corr 0.9346, MSE: 0.165199\nAfter center and whiten: Corr 0.9346, MSE: 0.146000",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learning</span>"
    ]
  },
  {
    "objectID": "src/chap3.html#parameter-learning-via-bayesian-inference",
    "href": "src/chap3.html#parameter-learning-via-bayesian-inference",
    "title": "2  Learning",
    "section": "2.3 Parameter Learning via Bayesian Inference",
    "text": "2.3 Parameter Learning via Bayesian Inference\nA Bayesian approach provides a natural alternative to maximum likelihood for parameter estimation in the Bradley–Terry model. Instead of finding a single point estimate for the item parameters (V), we place a prior distribution on them (e.g., independent standard normals) and update this prior using the Bernoulli likelihood from the observed comparisons. This yields a posterior distribution over (V), which captures both central estimates and the uncertainty around them. In practice, this posterior cannot be computed in closed form, so we turn to approximate inference. One common method is Markov chain Monte Carlo (MCMC), which constructs a sequence of samples that, in the limit, follow the posterior distribution.\nIn particular, the Metropolis–Hastings (MH) algorithm is straightforward to apply: at each step, we propose a new value for one or more coordinates of (V) (e.g., from a Gaussian centered at the current state), compute the acceptance ratio as the ratio of posterior densities between the proposed and current states, and accept the proposal with that probability. Repeating this process produces a chain of samples that can be used to approximate posterior means, variances, or other functionals of interest. This approach not only yields point predictions but also quantifies uncertainty about the relative strengths of items under the Bradley–Terry model.\nLet the prior be i.i.d. standard normal for each item parameter: \\(p(V) = \\prod_{j=1}^M \\mathcal N(V_j \\mid 0, 1)\\). Then the posterior distribution is \\(p(V \\mid \\mathcal D) = p(\\mathcal D \\mid V)p(V)/p(\\mathcal D),\\) where the likelihood is \\[\np(\\mathcal D \\mid V) = \\prod_{(j,j')\\in\\mathcal I} \\sigma(V_j - V_{j'})^{Y_{jj'}}\n\\bigl(1-\\sigma(V_j - V_{j'})\\bigr)^{1-Y_{jj'}}.\n\\]\nThe denominator is the marginal likelihood (evidence), which is intractable in closed form, so we resort to MCMC (e.g., MH) to sample from the posterior. Suppose we are at current (parameter) state \\(V^{(t)}\\). We propose a new state \\(V'\\) from a proposal distribution, such as Gaussian: \\(q(V' \\mid V^{(t)}) = \\mathcal N (V'; V^{(t)}, \\tau^2 I),\\) where \\(\\tau^2\\) is a step-size variance. This proposal is symmetric, meaning \\(q(V' \\mid V^{(t)}) = q(V^{(t)} \\mid V').\\) For any proposal distribution \\(q(V' \\mid V^{(t)})\\), the Metropolis–Hastings acceptance probability is\n\\[\n\\alpha = \\min \\{ {1; \\frac{p(V' \\mid \\mathcal D), q(V^{(t)} \\mid V')}{p(V^{(t)} \\mid \\mathcal D), q(V' \\mid V^{(t)})}} \\}.\n\\]\nThis says: accept the proposal with probability proportional to how much more plausible it is under the posterior, adjusted by how easy it is to propose back. If we choose a Gaussian proposal, the proposal terms cancel out in the ratio due to symmetry. So the acceptance rule simplifies to \\[\n\\alpha = \\min \\{1, \\frac{p(V' \\mid \\mathcal D)}{p(V^{(t)} \\mid \\mathcal D)} \\} = \\min \\{1, \\frac{p(\\mathcal D \\mid V'), p(V')}{p(\\mathcal D \\mid V^{(t)}), p(V^{(t)})} \\}.\n\\]\n\ndef logpost(v, r_idx, c_idx, y_obs, prior_var=1.0):\n    s = v[r_idx] - v[c_idx]\n    # Stable log-sigmoid pieces\n    # log p = -softplus(-s), log(1-p) = -softplus(s)\n    ll = (y_obs * -np.log1p(np.exp(-s)) + (1 - y_obs) * -np.log1p(np.exp(s))).sum()\n    lp = -0.5 * np.dot(v, v) / prior_var\n    return ll + lp\n\nsteps = 10000\nprop_scale = 0.05\n\n# Test labels (upper)\ny_te = Y_test[r_te, c_te].astype(float)\n\nrng = np.random.default_rng(3)\nv_cur = rng.normal(scale=0.1, size=M)\nv_cur -= v_cur.mean()\nlp_cur = logpost(v_cur, r_tr_fit, c_tr_fit, y_tr_fit)\n\ntrace_every = 1\ntrace = []\nacc = 0\n\nfor t in range(steps):\n    v_prop = v_cur.copy()\n    # Single-coordinate random walk\n    j = rng.integers(M)\n    v_prop[j] += rng.normal(scale=prop_scale)\n    # Fix shift invariance\n    v_prop -= v_prop.mean()\n    lp_prop = logpost(v_prop, r_tr_fit, c_tr_fit, y_tr_fit)\n    if np.log(rng.random()) &lt; (lp_prop - lp_cur):\n        v_cur, lp_cur = v_prop, lp_prop\n        acc += 1\n    if (t + 1) % trace_every == 0:\n        trace.append(v_cur.copy())\n\ntrace = np.array(trace)  # shape (steps/trace_every, M)\nacc_rate = acc / steps\nprint(f\"[MH] Acceptance rate: {acc_rate:.3f} with prop_scale={prop_scale}\")\n\n# Show trace for a few items\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\naxes[0].plot(trace[:, 0], lw=0.8)\naxes[0].set_title(\"Trace of v[0]\")\naxes[0].set_xlabel(\"Iteration / 5\")\naxes[0].set_ylabel(\"Value\")\n\naxes[1].hist(trace[:, 0], bins=100, density=True)\naxes[1].set_title(\"Posterior of v[0]\")\nplt.show()\n\n[MH] Acceptance rate: 0.952 with prop_scale=0.05",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learning</span>"
    ]
  },
  {
    "objectID": "src/chap3.html#online-parameter-learning-with-elo-rule",
    "href": "src/chap3.html#online-parameter-learning-with-elo-rule",
    "title": "2  Learning",
    "section": "2.4 Online Parameter Learning with Elo Rule",
    "text": "2.4 Online Parameter Learning with Elo Rule\nIn many applications, comparisons between items arrive sequentially over time rather than being observed all at once. For example, players in online games are continuously matched, or recommendation systems log one user preference at a time. In such settings, it is often computationally infeasible to refit the full Bradley–Terry model by maximum likelihood or MCMC after each new observation. Instead, we want an online update rule that adjusts item strengths incrementally as new outcomes arrive.\nThis is precisely the motivation behind the Elo rating system, originally introduced for ranking chess players and later widely adopted in competitive games, online platforms, and even information retrieval. The key idea is to maintain a current estimate of each item’s (or player’s) latent strength, and update only the two items involved in a match when a new result comes in.\nThe Elo rule can be derived as a stochastic gradient ascent method on the Bradley–Terry log-likelihood. Suppose item (j) plays against item (j’). Given the current parameters, the log-likelihood gradient with respect to \\(V_j\\) and \\(V_{j'}\\) is \\[\n\\frac{\\partial \\ell}{\\partial V_j} = (y - p), \\qquad\n\\frac{\\partial \\ell}{\\partial V_{j'}} = -(y - p).\n\\]\nA stochastic gradient ascent step with learning rate \\(\\eta\\) gives the update: \\[\n  V_j \\leftarrow V_j + \\eta (y - p), \\qquad V_{j'} \\leftarrow V_{j'} - \\eta (y - p).\n\\]\nThis is exactly the Elo update rule. The learning rate \\(\\eta\\) is often called the K-factor in Elo literature. If \\(y=1\\) (item j wins) but the model predicted a low (p), then (y - p) is positive and (V_j) increases, (V_{j’}) decreases — the system learns that (j) is stronger than previously believed If \\(y=0\\) (item j’ wins), the opposite adjustment happens. The magnitude of the update is larger when the outcome is surprising (large prediction error), and smaller when the outcome is expected (small prediction error). Thus, Elo is an online learning algorithm for the Bradley–Terry model, interpretable as stochastic gradient ascent with a fixed step size.\n\n# Build a stream of observed pairs for the single user from Y_BT upper triangle\npairs_stream = list(zip(triu_r, triu_c))\nlabels_stream = Y_BT[triu_r, triu_c].astype(float)\n\n# Initialize ratings r (Elo analog of v)\nr = np.zeros(M)\nKfactor = 1.0  # step size\n\ndef logistic_prob(a, b):\n    return 1.0 / (1.0 + np.exp(-(a - b)))\n\nfor (j, k), y in zip(pairs_stream, labels_stream):\n    p = logistic_prob(r[j], r[k])\n    # Update: winner gets +K*(1-p), loser gets -K*(1-p)\n    # If y=1 means j wins. If y=0 means k wins.\n    if y == 1.0:\n        r[j] += Kfactor * (1 - p)\n        r[k] -= Kfactor * (1 - p)\n    else:\n        r[k] += Kfactor * p\n        r[j] -= Kfactor * p\n    # optional centering\n    r -= r.mean()\n\n# Evaluate Elo ratings on held-out test pairs (same y_te from above)\np_te_elo = 1.0 / (1.0 + np.exp(-(r[r_te] - r[c_te])))\nauc_elo = auc_from_scores(p_te_elo, y_te)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learning</span>"
    ]
  },
  {
    "objectID": "src/chap4.html",
    "href": "src/chap4.html",
    "title": "3  Elicitation",
    "section": "",
    "text": "3.1 The Active Learning Problem\nFullscreen - AL Fullscreen - ME\nAcquiring labeled data is expensive. Active learning (AL) is a learning paradigm that aims to reduce the amount of labeled data required to train a model to achieve high accuracy. AL algorithms iteratively select an input datapoint for an oracle (e.g., a human annotator) to label such that when the label is observed, the model improves the most. Two primary setups in AL is pool-based and stream-based. In pool-based AL, the model selects samples from a large unlabeled pool of data. For example, a model for text classification selects the most uncertain texts from a large pool to ask a human annotator to label. In stream-based AL, the model receives samples sequentially (one sample at a time) and decides whether to label them. The data is gone if the decision maker decides not to label it. In AL, a model is trained on the current dataset, and a set of candidate points is evaluated for potential inclusion. AL selects one of these points to add to the dataset based on an “acquisition function” defined with respect to the current model to estimate the value of each candidate point for improving model performance. The dataset is updated with the newly queried point, and the cycle repeats until the budget is exhausted or a predefined reliability criterion is met.\nAL has successfully enhance various real-world systems. For example, AL can improve the computer vision models used in autonomous vehicles (Jarl et al. 2021). Probing a model to understand what type of data it would benefit from is more practical. In robotics, autonomous agents may query humans when unsure how to act when facing new situations (Taylor, Berrueta, and Murphey 2021). Here, collecting data often incurs significant financial and time costs because physical robot arm worns out over time. In meteorology, AL can help decide where to place additional sensors for weather predictions (Singh, Nowak, and Ramanathan 2006). Sensor placement involves deploying teams to remote locations and expensive construction for an extra data point. Choosing these locations and allocating resources wisely is of interest to governments and businesses. AL could also be employed to select data for fine-tuning large language models (LLMs) for specific downstream tasks (Margatina et al. 2023). Here, it might be difficult to fully describe a targeted NLP task. Often, instead of defining a task via a dataset of examples, it may be easier for a human to interact with the LLM for a specific use case, identify gaps in the model, and address those using AL.\nTypically, in robotic, robots learn by observing human demonstrations. However, expert demonstrations are often limited, and training a supervised learning model would require vast amounts of demonstration data, which is difficult to obtain at scale. Demonstrations tend to be variable, reflecting the actions of individual humans, making the data collection process inconsistent. To address these limitations, alternative approaches have been proposed, such as using pairwise comparisons, where humans evaluate two action trajectories to determine the superior one, or employing physical corrections, in which reward functions are learned through human-robot interactions, with humans guiding the robot’s actions during the task. AL algorithms can be employed in preference learning tasks, where the objective is to develop a model that aligns with human preferences while minimizing the need for extensive labeled data or reducing the high cost of annotations.\nMotivating by the pairwise preference setting, we consider a binary classification problem. The model is trained on a small labeled dataset \\(\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N\\), where \\(x_i\\) represents the input data and \\(y_i\\) is the corresponding label. The model is uncertain about the class labels of some data points and can query an oracle to obtain the true labels of these data points. The goal is to minimize the number of queries to the oracle while maximizing the model’s performance. Here, the value of a datapoint is in how much it helps identify the underlying model, and this notion of informativeness is often quantify with uncertainty. Two primary types of uncertainty are often considered: epistemic and aleatoric uncertainty. Epistemic uncertainty, or model uncertainty, arises from a lack of knowledge and can be reduced by acquiring more data. This type of uncertainty is especially significant when the model lacks confidence due to insufficient or incomplete information in its training set. On the other hand, aleatoric uncertainty, or data uncertainty, stems from the inherent randomness within the data itself. Unlike epistemic uncertainty, aleatoric uncertainty cannot be reduced, even with additional data, as it reflects noise or unpredictability in the real data-generating process. AL often focuses on selecting data that reduce the epistemic uncertainty.\nThere are several method for quantify model uncertainty. Bayesian methods, such as Bayesian neural networks and Gaussian processes, offer a principled way of estimating uncertainty of parameter posterior distribution by iteratively updating a prior distribution over model. Exact posterior computation can become computationally prohibitive, especially for complex likelihood function, and approximated Bayesian computation is proposed to address this. For example, ensemble methods involve training multiple models and combining their predictions to provide an estimate of uncertainty. Ensemble methods are relatively easy to implement, but they are noisy and still somewhat expensive. Conformal prediction methods also provide a framework for estimating uncertainty by offering a measure of confidence in predictions based on the conformity of a given instance with the training data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Elicitation</span>"
    ]
  },
  {
    "objectID": "src/chap4.html#estimating-the-value-of-additional-data-with-acquisition-function",
    "href": "src/chap4.html#estimating-the-value-of-additional-data-with-acquisition-function",
    "title": "3  Elicitation",
    "section": "3.2 Estimating the Value of Additional Data with Acquisition Function",
    "text": "3.2 Estimating the Value of Additional Data with Acquisition Function\nUncertainty quantification plays a vital role in acquisition functions, which are central to AL strategies. These functions determine which samples are most valuable to label by evaluating their utility based on the model’s current uncertainty estimates. Common acquisition functions include uncertainty sampling (Zhu et al. 2010), which selects samples the model is least confident about, query-by-committee (Beluch et al. 2018), which utilizes a set of models to choose the most uncertain samples, and Bayesian AL by Disagreement (BALD) (Houlsby et al. 2011), which selects samples that maximize information gain by reducing model uncertainty. Through careful uncertainty quantification, acquisition functions guide the AL process, improving the model’s efficiency in learning from limited data. Other acquisition functions that can be employed include:\n\nExpected model change (Cai, Zhang, and Zhou 2013): This approach focuses on labeling points that would have the most impact on changing the current model parameters.\nExpected error reduction (Mussmann et al. 2022): Points that would most effectively reduce the model’s generalization error are labeled using this strategy.\nVariance reduction (Cohn, Ghahramani, and Jordan 1996): This approach labels points that would minimize output variance, which is one component of error. By selecting points that reduce variability in the model’s predictions, it aims to improve overall performance.\n\nUncertainty sampling (Zhu et al. 2010) selects data points for which the model exhibits the greatest uncertainty, focusing labeling efforts on ambiguous samples where additional information is likely to yield the greatest benefit. Several acquisition strategies fall under uncertainty sampling, including entropy sampling, margin sampling, and least confidence sampling. Entropy sampling measures value of addition data by the entropy of the predicted probability distribution: \\(\\alpha(x) = - \\sum_{y} p(y|x) \\log p(y|x)\\). Margin sampling focuses on the difference between the two highest predicted probabilities for a sample: \\(\\alpha(x) = p(y_1|x) - p(y_2|x)\\), where \\(y_1\\) and \\(y_2\\) are two most likely classes. Least confidence sampling measures value of additional data by the lowest predicted probability for its most likely class: \\(\\alpha(x) = 1 - p(y_{\\text{max}}|x)\\), where \\(y_{\\text{max}}\\) is the class with the highest probability. Consider a binary classification problem with three candidate \\(x_1, x_2, x_3\\). The code below demonstrate that uncertainty sampling methods yield the same conclusion of selecting \\(x_1\\).\n\n\n\n\n\n\ncode\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nQuery-by-Committee (Beluch et al. 2018) is selects samples for labeling based on the level of disagreement among members of a committee. Several acquisition functions can be employed under this framework to quantify the disagreement. The vote entropy measures the uncertainty based on how often the committee members vote for each class. The acquisition function is defined as \\(\\alpha(x) = \\mathbb{H}\\left[V(y)/C\\right]\\), where \\(V(y)\\) is the number of votes for class \\(y\\) and \\(C\\) is the number of committee members. Consensus Entropy measures the entropy of the average probability distribution across committee members. It is given by \\(\\alpha(x) = \\mathbb{H}[p_C(y|x)]\\), where \\(p_C(y|x)\\) is the average probability distribution for sample \\(x\\) across all committee members. The KL divergence quantifies the disagreement by comparing the probability distribution of each committee member to the average distribution. The acquisition function is given by \\(\\alpha(x) = \\frac{1}{C} \\sum_{c=1}^{C} D_{KL}[p_C(y|x) || p_C(y|x)]\\), where \\(p_C(y|x)\\) is the probability distribution of committee member \\(c\\) and \\(p_C(y|x)\\) is the average distribution across the committee. As an example, consider a binary classification problem with three candidate \\(x_1\\), \\(x_2\\), and \\(x_3\\) and three committee members. Numerical result below show that all acquisition functions selects \\(x_1\\).\n\n\n\n\n\n\ncode\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nBayesian AL by Disagreement (BALD) (Houlsby et al. 2011) selects the samples for which the model expects to gain the most Shannon information when corresponding labels are observed:\n\\[\n\\begin{aligned}\n&\\mathbb{I}(\\theta; y|x, \\mathcal{D}) = \\mathbb{H}[p(y|x, \\mathcal{D})] - \\mathbb{E}_{p(\\theta | \\mathcal{D})} [\\mathbb{H}[p(y|x, \\theta, \\mathcal{D})]] \\\\\n&\\mathbb{H}[p(y|x, \\mathcal{D})] = \\mathbb{H}\\left[\\int_{\\theta} p(y|x, \\theta, \\mathcal{D}) p(\\theta | \\mathcal{D}) d\\theta\\right] \\approx \\mathbb{H}\\left[\\frac{1}{N}\\sum_{i=1}^{N} p(y|x, \\theta_i, \\mathcal{D})\\right] = \\mathbb{H}\\left[\\overline{p}(y|x, \\mathcal{D})\\right] \\\\\n&\\mathbb{E}_{p(\\theta|\\mathcal{D})} [\\mathbb{H}[p(y|x, \\theta, \\mathcal{D})]] = \\mathbb{E}_{p(\\theta|\\mathcal{D})} \\left[ - \\sum_{y} p(y|x, \\theta, \\mathcal{D}) \\log p(y|x, \\theta, \\mathcal{D}) \\right] \\approx - \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\sum_{y} p(y|x, \\theta_i, \\mathcal{D}) \\log p(y|x, \\theta_i, \\mathcal{D}) \\right)\n\\end{aligned}\n\\]\nWhen there is significant disagreement among models, the predictive entropy (the first term) will be large, while the expected entropy (the second term) will be smaller. This difference represents the degree to which the models disagree. BALD selects points where this disagreement is maximized. As an example, consider a binary classification problem with two classes, \\(y_1\\) and \\(y_2\\). We have two samples, \\(x_1\\) and \\(x_2\\). BALD selects \\(x_1\\) for labeling.\n\n\n\n\n\n\ncode\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nAL by Variance Reduction (Cohn, Ghahramani, and Jordan 1996) is an algorithm designed to select the next data point for labeling based on the anticipated reduction in the model’s variance. The objective is to identify the point \\(x \\sim p(x)\\) that, when labeled \\(y_x\\), will most effectively decrease the model’s variance. The expected error at a given input \\(x\\) is \\(\\mathbb{E}_{\\hat{y} \\sim p(\\hat{y} | \\mathcal{D}; x), y \\sim p(y|x)} (\\hat{y} - y)^2\\). \\(\\hat{y}\\) represents the model’s prediction, and \\(y\\) denotes the true label at \\(x\\). Using bias-variance decomposition (Geman, Bienenstock, and Doursat 1992), the expected error is decomposed as \\[\\begin{aligned}\n\\mathbb{E} (\\hat{y} - y)^2 = \\mathbb{E}[(\\hat{y} - \\mathbb{E}[y|x]) + (\\mathbb{E}[y|x] - y)]^2 = \\mathbb{E} [(y - \\mathbb{E}[y|x])^2] + 2\\mathbb{E} [(\\hat{y} - \\mathbb{E}[y|x])(\\mathbb{E}[y|x] - y)] + \\mathbb{E}(\\hat{y} - \\mathbb{E}[y|x])^2\n\\end{aligned}\\] where the expectation is taken over \\(\\hat{y} \\sim p(\\hat{y} | \\mathcal{D}; x), y \\sim p(y|x)\\). The first term represents the variance of the true label \\(y\\), the second term evaluates to zero since \\(\\mathbb{E}_{\\hat{y}, y}[\\mathbb{E}[y|x] - y] = 0\\), and the third term accounts for the variance of the model’s prediction \\(\\hat{y}\\): \\[\\mathbb{E}(\\hat{y} - \\mathbb{E}[y|x])^2 = \\mathbb{E}[(\\hat{y} - \\mathbb{E}[\\hat{y}] + \\mathbb{E}[\\hat{y}] - \\mathbb{E}[y|x])^2] = \\mathbb{E}[(\\hat{y} - \\mathbb{E}[\\hat{y}])^2] + (\\mathbb{E}[\\hat{y}] - \\mathbb{E}[y|x])^2\\]\nHence, \\[\\mathbb{E} (\\hat{y} - y)^2 = \\mathbb{E}_{y} [(y - \\mathbb{E}[y|x])^2] + (\\mathbb{E}_{\\hat{y}} [\\hat{y} - \\mathbb{E}[y|x]] )^2 + \\mathbb{E}_{\\hat{y}} [(\\hat{y} - \\mathbb{E}_{\\hat{y}}[\\hat{y}])^2]\\]\nHere, the first term signifies the variance of the true label, which remains constant for a given \\(x\\). The second term captures how much the average model prediction deviates from the expected true label. The third term quantifies the model’s uncertainty at \\(x\\). Cohn, Ghahramani, and Jordan (1996) denotes the uncertainty term as \\(\\sigma^2_{\\hat{y}} (x | \\mathcal{D}) = \\mathbb{E}_{\\hat{y}} [(\\hat{y} - \\mathbb{E}_{\\hat{y}}[\\hat{y}])^2]\\). The acquisition function is \\(\\mathbb{E}_{p(x)} [\\sigma^2_{\\hat{y}} (x | \\tilde{\\mathcal{D}})]\\). One could rely on empirical measure like a loss on test labelled data to gauge model improvement, which can help decide the termination of data acquisition. The size of the data set and its relationship to the loss is tied to the model complexity. To evaluate the performance of variance reduction strategy, Cohn, Ghahramani, and Jordan (1996) studies the Arm2D problem. Arm2D is a kinematics problem where learner has to predict the tip position of a robotic arm given a set of joint angles \\(\\mathbf{\\theta_1}, \\mathbf{\\theta_2}\\). In this analysis, the two models are the Gaussian mixture model and locally-weighted regression (LOESS). The results shown that the variance of the learner decreases because the authors selected points to minimize expected variance. Additionally, we observe a related decrease in the mean square error (MSE) of both models as the dataset size increases. This is a notable outcome because the expected learner variance for these models can be computed accurately and efficiently relative to a new point. When integrated into the general AL loop, this significantly enhances model performance. In the case of the locally-weighted regression model (?fig-empirical:regress), it is surprising that if points were chosen randomly, the MSE would be highly unstable, with sharp fluctuations. However, when AL by variance reduction is applied, using expected learner variance as a proxy, the MSE decreases almost smoothly, aside from some initial instabilities.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Elicitation</span>"
    ]
  },
  {
    "objectID": "src/chap4.html#active-preference-learning-with-ideal-point-model",
    "href": "src/chap4.html#active-preference-learning-with-ideal-point-model",
    "title": "3  Elicitation",
    "section": "3.3 Active Preference Learning with Ideal Point Model",
    "text": "3.3 Active Preference Learning with Ideal Point Model\nFor any \\(n\\) elements to be ranked, there are \\(n!\\) possible orderings that can result in the correct complete ranking. Given that a lower bound on sorting is \\(n\\log n\\), obtaining a guaranteed true rating over \\(n\\) items requires \\(n\\log n\\) pairwise comparisons if those comparisons are chosen at random. This number can be quite high and costly in many applications, especially since most ranking information comes from humans. The more comparisons they have to make, the more money and time is spent. This process can also be inefficient, as some comparisons provide more value to the learning process than others, making some comparisons a waste. This inefficiency can be detrimental in fields like psychology and market research, where comparisons are heavily utilized, and a faster process could offer significant benefits. The reason the lower bound on the number of comparisons is \\(n\\log n\\) is that it assumes no prior information about the underlying space and field, so comparisons are chosen at random. However, leveraging the structures within the comparison space can provide more information about which comparisons are most valuable. For example, (G. and Nowak 2011) discusses how eye doctors have a wide range of options when assigning prescriptions for glasses, yet patients do not see them making many comparisons before deciding on the best option. This is because eye doctors incorporate domain knowledge into the process and only ask clients for comparisons when necessary. Applying similar knowledge in the ranking field leads to an AL approach that selects data based on the relevance of a comparison query toward finding the final \\(\\sigma(\\Theta)\\).\nG. and Nowak (2011) explores AL within data that can be embedded in a \\(d\\)-dimensional embedding space, where comparisons between two different items divide the space into halves, with one object being superior in each half. By leveraging such geometry, the paper develops a geometric AL approach. Let \\(\\theta\\) be the item representation in the embedding space. For each ranking \\(\\sigma\\), there is a reference point \\(r_{\\sigma} \\in \\mathbb{R}^d\\), such that if \\(\\theta_{i} \\succ \\theta_{j}\\), \\(||\\theta_i - r_{\\sigma}|| &lt; ||\\theta_j - r_{\\sigma}||\\). In other words, object \\(i\\) is closer to the reference point \\(r_{\\sigma}\\) than object \\(j\\). \\(\\Sigma_{n,d}\\) is the set of all possible rankings of the \\(n\\) items that satisfy the above embedding distances condition. Not all rankings will satisfy the embedding conditions, but multiple rankings might satisfy all those conditions. For every ranking \\(\\sigma\\), there is \\(M_n(\\sigma)\\), the number of pairwise comparisons needed to identify the ranking. When comparisons are done at random, \\(\\mathbb{E}[M_n(\\sigma)] = n\\log n\\), and it can be reduced by incorporating geometry. \\(q_{i,j}\\) is the query of comparison between items \\(i\\) and \\(j\\).\nAs an example, G. and Nowak (2011) studies a 2D space with three items: \\(\\theta_1\\), \\(\\theta_2\\), and \\(\\theta_3\\). There are pairwise queries \\(q_{1,3}\\), \\(q_{2,3}\\), and \\(q_{1,2}\\) between them, denoted by solid lines equidistant from the two items they compare. These lines split the \\(R^2\\) space into halves, with each half closer to one of the two items. The paper colors the side of the worse object for each query in dark grey and takes the intersection of these halves, resulting in the dark grey region in the image. This region indicates \\(\\Sigma_{n,2}\\) since all points follow the embedding conditions. Specifically, for every point \\(r\\) in the dark grey area, \\(||\\theta_3 - r|| &lt; ||\\theta_2 - r|| &lt; ||\\theta_1 - r||\\), meaning \\(\\theta_3 &lt; \\theta_2 &lt; \\theta_1\\). Thus, every point \\(r\\) is one of the \\(r_\\sigma\\) representing their respective rankings \\(\\sigma \\in \\Sigma_{n,2}\\). In other words, the paper aims to have the reference points and dark grey region closest to the worst object and furthest from the best object.\nThe authors also denote the label for each query \\(q_{i,j}\\), such as label \\(y_{i,j} = 1\\{q_{i,j}\\}\\) (for example, \\(y_{1,2} = 0, y_{3,2} = 1\\)). This allows for deciding how to label new queries represented by dashed and dotted lines, depending on which items each query compares. Focusing on the dotted line, called \\(q_{i,4}\\), where \\(i={1,2,3}\\), and considering potential locations of \\(\\theta_4\\), the line must be equidistant from one of the three items in the picture and \\(\\theta_4\\), meaning \\(\\theta_4\\) can be placed in three different locations. If the query performed is \\(q_{2,4}\\), then \\(\\theta_4\\) will be closer to the dark grey area than \\(\\theta_2\\), thus \\(y_{2,4} = 0\\). However, if \\(q_{1,4}\\) or \\(q_{3,4}\\) are performed, \\(\\theta_4\\) will be further from the dark grey area than \\(\\theta_1\\) or \\(\\theta_3\\), meaning \\(y_{1,4} = y_{3,4} = 1\\). In this case, the labels are contradictory and depend on which object they are compared with, making such a query \\(q_{i,4}\\) ambiguous.\nIn contrast, the authors analyze the dashed line, called \\(q_{i,5}\\), where \\(i={1,2,3}\\), and consider potential locations of \\(\\theta_5\\). Since the line must be equidistant from one of the three items in the picture and \\(\\theta_5\\), it can be placed in three different locations. If one of the three potential queries is performed, \\(\\theta_5\\) will be closer to the dark grey area than \\(\\theta_1\\), \\(\\theta_2\\), and \\(\\theta_3\\), meaning \\(y_{1,5} = y_{2,5} = y_{3,5} = 0\\). In this case, all labels are the same regardless of which object is used, meaning such a query will not be contradictory, as all agree on the label. The goal is to perform as many ambiguous queries as possible and skip non-ambiguous queries to decrease the total \\(M_n(\\sigma)\\). Intuitively, if there is contradictory information about a query, it needs to be erformed so that a human can clarify its direction. Conversely, if all sources of information from the domain space agree on the query’s label, that information can be used without asking a human, incorporating the knowledge of the embedding distances. Lastly, to consider the general case of the \\(R^d\\) space, rather than discussing halves of the image, it is essential to discuss half-spaces. Similarly, consider the half-space that assigns a label of \\(1\\) to the query and the half-space assigning a label of \\(0\\). If both half-spaces exist, they have conflicting information on the query, making the query ambiguous. However, if one of the half-spaces does not exist, it means the other is the full space, representing consistency in the label assignment and a non-ambiguous query.\nIt is important to demonstrate that the number of comparisons decreases. Specifically, (G. and Nowak 2011) shows that this algorithm has \\(E[M_n(\\sigma)] = O(d\\log n)\\), where \\(d\\) is the dimension of the space and \\(d &lt; n\\), which improves on the \\(O(n\\log n)\\) baseline. The proof can be studied in detail in the paper itself, but at a high level, it starts by reasoning about the probability of a query being ambiguous and a comparison being requested from a human, thus representing \\(M_n = \\Sigma_{k=1}^{n-1}\\Sigma_{i=1}^k 1\\{Requestq_{i,k+1}\\}\\). For that, the authors define \\(Q(i,j)\\), which represents the number of different rankings that exist for \\(i\\) elements in \\(j\\)-dimensional space (e.g., \\(Q(1,d) = 1, Q(n,0) = 1, Q(n,1) = n!\\)). In that case, \\(|\\Sigma_{n,d}| = Q(n,d)\\). Further, using recurrence relations for \\(Q(i,j)\\), the authors derive that \\(|\\Sigma_{n,d}| = Q(n,d) = O(n^{2d})\\), which is omitted here. Analogously, the authors define \\(P(i,j)\\), which represents the number of rankings in \\(\\Sigma_{n,d}\\) that will still be possible with the addition of a new element \\(i+1\\) to the ranking items. \\(P(i,j)\\) estimates how much of the dark grey area will still exist after making a query for \\(i+1\\). As indicated there, the dotted line ambiguous query did not change the dark grey a rea at all (\\(P(n,d) = Q(n,d)\\)), whereas the dashed non-ambiguous query would cut a piece from it (\\(P(n,d) &lt; Q(n,d)\\)). Thus, \\(Request q_{i,k+1} = P(k,d) / Q(k,d)\\), so a higher value indicates more possible rankings and an ambiguous query that needs to be requested to obtain more useful information. With this in mind, the authors derive that \\(E[M_n(\\sigma)] = O(d\\log n)\\), showing that fewer queries are needed for effective ranking.\nThe issue with this algorithm is that only one human provides the answers to the requested queries, which means it does not account for their biases. An alternative approach is a Robust Query Selection Algorithm (RQSA) (G. and Nowak 2011), which uses majority voting for every query to indicate the ground truth of the query’s label. However, the authors consider that a group of people can still give incorrect or divided responses. If the votes for each answer are almost equal in number, the authors push that query to the end of the algorithm to see if it can become a non-ambiguous query with more information learned. If it does not, an odd number of voters is used to determine the final ranking.\n\n\n\nTable 3.1: Statistics for the Robust Query Selection Algorithm (RQSA) (G. and Nowak 2011) and the baseline of conducting all comparisons. \\(y\\) serves as a noisy ground truth, \\(\\tilde{y}\\) is the result of all comparisons, and \\(\\hat{y}\\) is the output of the RQSA.\n\n\n\n\n\nDimension\n\n2\n3\n\n\n\n\n% of queries\nmean\n14.5\n18.5\n\n\n\nstd\n5.3\n6\n\n\nAverage error\n\\(d(\\bar{y}, y)\\)\n0.23\n0.21\n\n\n\n\\(d(\\bar{y}, y)\\)\n0.31\n0.29\n\n\n\n\n\n\nWith regard to the accuracy and performance of the method, the authors did a ranking experiment on 100 different audio signals, results of which can be seen in Table 3.1. The ground truth labels came from humans, indicated by \\(y\\) in the table. That resulted in the existence of noise and potential errors in the ground truth, which could influence the performance of both the baseline algorithm that does all comparisons (\\(\\tilde{y}\\)) and the Robust Query Selection Algorithm (RQSA) (\\(\\hat{y}\\)). As can be seen in both 2 and 3-dimensional spaces RQSA performed worse by \\(8\\%\\) compared to the baseline, which indicates that AL that uses the domain information can still be erroneous due to the inference of certain comparisons that sometimes may not be entirely correct. However, as can be seen by the upper part of Table 3.1, significantly less queries were requested compared to the baseline, which means that the approach can have a significant benefit at a cost of slight loss in accuracy.\n\nUser Information as Domain Knowledge for Active Learning\nAn alternative source of domain knowledge could be users themselves, who can indicate their uncertainty when it comes to comparing two items. Prior studies have shown (Amershi et al. 2014) that when presented with only two options when selecting which object is better, but not being able to properly decide, users would get frustrated and tend to respond more faultyly, creating noise and incorrect responses in the data. Through feedback and other studies (Guillory and Bilmes 2011) it was determined that presenting users with an option of indifference between the two items can remove those problems. Moreover, in connection to AL, the authors show that such an option helps to select more informative queries since it provides more domain knowledge that can be used, resulting in a decrease in the number of queries required. For this problem, the following terms are defined:\n\n\\(c\\) - a cost function that represents user preferences, and the result the model has to determine at the end of training. The preferred items will have lower costs, and less preferred ones will have higher costs. The goal is to determine this function with the fewest possible number of queries using AL.\n\\(H\\) - a set of hypotheses over the possible cost functions, where for each \\(h \\in H\\) there is a cost function \\(c_h\\) associated with it.\n\\(h^*\\) - a true hypothesis that the model needs to determine, which has cost \\(c_{h^*}\\) associated with it\n\\(t(x,y)\\) - a test performed to compare items \\(x\\) and \\(y\\) (the user is being asked to provide a response to which item is better). Those tests result in changes and adjustments to \\(H\\) as more information is learned.\n\\(o(x,y)\\) - observation or result of \\(t(x,y)\\), where \\(o(x,y) \\in \\{x&lt;y, x&gt;y\\}\\)\n\\(S = \\{(t_1, o_1), (t_2, o_2),...,(t_m, o_m)\\}\\) - a sequence of \\(m\\) pairs of tests and observations\n\\(w(H|S)\\) - probability mass of all hypotheses that are still consistent with the observations (similar to the dark grey area and \\(Q(i,j)\\)). This means that if \\(h \\in H\\) is inconsistent with user responses received, it is removed from \\(H\\).\n\nWith the key terms defined, let’s consider the noiseless base setting where users only have two options for response. Those components will also later be translated to the setting with the third option so the true cost function can be determined there. \\(w(H|S)\\) is the sum of the weights of all hypotheses that are still consistent with the evidence: \\(w(H|S) = \\sum_{h \\in H} w(h | S)\\). Each \\(w(h|S)\\) is a probability of the evidence’s existence given such hypothesis: \\(w(h|S) = p(S|h)\\). Such probability comes from the test-observation pairs since they compose the set \\(S\\). Moreover, each test is independent of other tests, which gives \\(p(S|h) = \\prod_{(t,o) \\in S} p((t,o) | h)\\). In the noiseless setting, users will select an option that minimizes their cost function (selecting more preferred items), mathematically defined as: \\[\\begin{aligned}\n    p((t, o = x) | h) =\n    \\begin{cases}\n        1 & c_h(x) &lt; c_h(y)\\\\\n        0 & else\n    \\end{cases}\n\\end{aligned}\\]\nUsers are not perfect evaluators. Prior work (Amershi et al. 2014) has shown that treating users as perfect can lead to poor performance. That gave rise to accounting for noise in users’ responses, but a majority of such work applies the same noise to all queries and all responses. While those led to great performance results (Guillory and Bilmes 2011), they don’t accurately reflect the real world, which gave rise to the idea of creating query-based noise. Effectively, for some of the queries it is important to incorporate the fact that the user is unsure and noisy, but for others, if the user is confident, noise in the response is not needed at all. For comparison-based learning, this means that the noise is related to the costs of the two items compared. Specifically for items \\(x\\) and \\(y\\), if \\(c_{h^*}(x) \\simeq c_{h^*}(y)\\) then the items are hard to distinguish for the user, so here it is preferred to incorporate user uncertainty and noise. But if \\(c_{h^*}(x) &gt;&gt; c_{h^*}(y)\\), the user will certainly select \\(y\\) and the other way around, which is where the noise is not needed. Query-dependent noise is also supported in the psychology literature, which means that such an approach is more related to the real world. In particular, psychologists talk about the Luce-Sheppard Choice rule (Shepard 1957) when talking about comparisons. This rule previously gave rise to a logistic model based on the noise (Viappiani and Boutilier 2010) where the probability of observation for a given test is \\(p((t, o = x) | h) \\propto exp(-\\gamma * c_h(x))\\)\n\n\n\n\n\n\nFigure 3.1: User response model in the noiseless setting\n\n\n\n\n\n\n\n\n\nFigure 3.2: User response with Luce Sheppard noise model\n\n\n\nFigure 3.1, Figure 3.2 demonstrate the difference between the noiseless setting and incorporating the Luce-Sheppard Choice rule. GBS is the baseline model with only 2 response options, and CLAUS is the model with the uncertainty option added. The figures show how incorporating such noise influences and smoothes the probability distribution of the user’s response.\nWe will now discuss the functionality of CLAUS, which is an algorithm designed by (Holladay et al. 2016) that allows users to select an uncertain response about the two options that they need to rank. The authors model such uncertainty as \\(\\epsilon\\) and it is associated with each \\(c_h\\), so now every hypothesis \\(h\\) is defined over a pair of \\((c_h, \\epsilon_h)\\). It is important to note that the goal is to still learn and maintain our objective on \\(c\\), \\(\\epsilon\\) is only necessary to model the users’ responses. The uncertainty relates to the cost function as \\(|c_h(x) - c_h(y)| &lt; \\epsilon_h\\). This means that the user is uncertain between items \\(x\\) and \\(y\\) and their cost difference is negligible such that the user is not able to select which item is better. This in turn gives more information about the real value of the two items, as a binary response would indicate the user’s preference towards one item, which will not be real and will skew the cost functions. This causes modifications of the problem set-up:\n\nFor test \\(t(x,y)\\) the observation will be \\(o(x,y) \\in \\{x&lt;y, x&gt;y, \\tilde{xy}\\}\\), where \\(\\tilde{xy}\\) is the uncertain response.\nThe probability distribution over the user’s response (?eq-prob_base) will now be defined as:\n\n\\[\\begin{aligned}\n    p((t, o = x) | h) =\n    \\begin{cases}\n        1 & c_h(x) &lt; c_h(y) - \\epsilon_h\\\\\n        0 & else\n    \\end{cases}, \\quad\n    p((t, o = \\tilde{xy}) | h) =\n    \\begin{cases}\n        1 & |c_h(x) - c_h(y)|^2 &lt; \\epsilon_h^2\\\\\n        0 & else\n    \\end{cases}\n\\end{aligned}\\]\nThis means the user confidently selects \\(x\\) when it is better than \\(y\\) by more than \\(\\epsilon\\), but if the squared difference of the cost functions of two items is negligible by \\(\\epsilon\\) user will choose the indifferent option.\n\nFinally this also updates the noise model: \\[\\begin{aligned}\n&p((t, o = x) | h) \\propto \\exp(-\\gamma * [c_h(x) - c_h(y)]) \\\\\n&p((t, o = \\tilde{xy}) | h) \\propto exp(-1/\\epsilon_h^2 * [c_h(x) - c_h(y)]^2)\n\\end{aligned}\\]\n\nRather than predicting a specific pair \\((c_h, \\epsilon_h)\\), the algorithm focuses on predicting a group of pairs that are similar to one another, otherwise called equivalence class (?fig-equiv_c), which indicates not essentially different hypothesis for the cost function and uncertainty. That information is learned through each new test, as the algorithm updates the information about \\(c\\) and \\(\\epsilon\\) that distinguishes between the distinct \\(h\\), finding the equivalence groups among them. Moreover, the authors tweaked the parameter responsible for the size of the equivalence class (how many hypotheses can be grouped together at a time).\n\n\n\nTable 3.2: Performance of GBS and CLAUS with different labels for the uncertainty\n\n\n\n\n\nCategory\nAccuracy\nQuery Count\n\n\n\n\nGBS - About Equal\n\\(94.15 \\pm 0.52\\)\n\\(36.02 \\pm 0.03\\)\n\n\nGBS - Not Sure\n\\(\\textbf{94.66} \\pm \\textbf{0.55}\\)\n\\(35.95 \\pm 0.04\\)\n\n\nCLAUS - About Equal\n\\(91.56 \\pm 0.84\\)\n\\(\\textbf{25.93} \\pm \\textbf{0.41}\\)\n\n\nCLAUS - Not Sure\n\\(90.86 \\pm 0.74\\)\n\\(26.98 \\pm 0.47\\)\n\n\n\n\n\n\nThe first performance evaluation is done on the number of queries and confirms that it decreases. The GBS model serves as the baseline, as it will do all of the comparison queries using the binary response options. The CLAUS model is measured over different values of \\(\\epsilon\\) on the x-axis and over different sizes of the equivalence sets indicated by different shades of blue. Figure shows that all variants of CLAUS use approximately 10 fewer queries on average compared to GBS. Moreover, using bigger-sized equivalence classes can further decrease the number of needed queries. The most optimal \\(\\epsilon \\simeq 0.07\\), after which higher \\(\\epsilon\\) does not provide any benefit.\nLastly, the authors considered the performance difference, which is indicated in Table 3.2. For that authors used two different labels for the uncertainty button in CLAUS, it was either labeled as “About Equal” or “Not Sure” as those can provoke different responses and feelings in users. Moreover, GBS and CLAUS-type responses were mixed in the same set of questions to the user, which splits the metrics for both in two as can be seen in Table 3.2. The performance of CLAUS is lower by \\(3\\%\\) on average, showing that a smaller number of queries can still lead to a performance loss. However, the second column of Table 3.2 supports the information, as it also shows that 10 fewer queries were conducted on average.\nAL can be essential in learning within dynamic systems and environments. Say we have an agent in an environment, and we want it to conform to a certain behavior as set by a human. How exactly do we go about doing this? In a traditional RL setting, this is solved by a class of algorithms under Inverse Reinforcement Learning. Techniques such as VICE and GAIL attempt to learn a reward function that can distinguish between states visited by the agent and states desired to be visited as defined by a human. In effect, a human will demonstrate what it would like the agent to do in the environment, and from there, learning is done. However, what if humans do not precisely know how an agent should optimally behave in an environment but still have some opinion on what trajectories would be better than others? This is where a paper like Active Preference-Based Learning of Reward Functions comes into the picture. The paper aims to use human preferences to aid an agent’s learning within a dynamic system.\nA dynamic system contains human input, robotic input, and an environment state. The transitions between states is defined by \\(f_{HR}\\), so that we have \\(x^{t+1} = f_{HR}(x^t, u_R, u_H)\\). At a given time step \\(t\\), we have \\(x_t\\), \\(u_R^t\\), and \\(u_H^t\\). This can be encapsulated into a single \\(d\\) dimensional feature vector that the authors denote as \\(\\phi\\). The paper then assumes that the underlying reward model we are trying to learn can be represented linearly. If we have our human reward preference function defined as \\(r_H\\), this means we can write \\(r_H\\) as \\(r_H(x^t, u_R^t, u_H^t) = w^{\\intercal}\\phi(x^t, u_R^t, u_H^t)\\). Because the reward function is linear, we can take the weight vector out of the summation if we want to calculate the reward over an entire trajectory:\n\\[R_{H}(x^0, u_R, u_H) = \\sum_{t=0}^{N} r_{H}(x^t, u^t, u_H^t) \\quad \\Phi = \\sum \\phi(x^t, u_R^t, u_H^t) \\quad R_H(traj) = w\\cdot\\Phi(traj)\\]\nFirst, the scale of \\(w\\) does not matter because we only care about the relative rewards produced with \\(w\\) (given two different trajectories, we want to answer the question of which trajectory a human would prefer, i.e. which one has a higher preference reward). This means we can constrain \\(||w|| &lt;= 1\\), so the initial prior is uniform over a unit ball. From here, we can determine a probabilistic expression to assess whether we should prefer trajectory A or B (because it can be noisy with human input). Let \\(I_t = +1\\) if the human prefers trajectory \\(A\\). According to Bradley-Terry model, \\(p(A \\succ B|w) = \\sigma(R_H(traj_A) - R_H(traj_B))\\). Let \\(\\psi = \\Phi(traj_a) - \\Phi(traj_b). Then f_{\\psi} (w) = p(I_t|w) = \\sigma(I_t w^{\\intercal}\\psi)\\). We can update \\(p(w)\\) everytime we get a result from a human preference query using Bayes’ rule: \\(p(w|I_t) &lt;- p(w) \\cdot p(I_t|w)\\) via Markov chain Monte Carlo method. This paper synthetically generates queries through an optimization process and then presents them to a human to pick between. The idea is that we want to generate a query that maximizes the conditional entropy \\(H(I|w)\\). We want to pick a query that we are most uncertain about given our current weights (thus having the highest conditional entropy given the weights): \\[\\max_{x^0, u_R, u_H^A, u_H^B} \\min\\{\\mathbb{E}[1-f_{\\psi}(w)], \\mathbb{E}[1 - f_{-\\psi}(w)]\\}\\]\nTo do so, we sample \\(w_1, ... w_m\\) from \\(p(w)\\), approximating the distribution \\(p(w)\\) as \\(p(w) = \\frac{1}{M} \\sum \\delta (w_i).\\) We can now approximate the expectation expression as \\(E[1 - f_{\\psi}(w)] = \\frac{1}{M} (\\sum 1 - f_{\\psi}(w_i))\\), and now we can optimize the expression to generate a synthetic query. The algorithm itself works well, however there ends up being a bottle neck that each query needs to be synthesized before being sent to the human – one at a time. There is no room for parallelization and so the authors proposed a second algorithm in a separate paper that allows for the batching of queries:\n\\[\\max_{\\xi_{ib+1_A}, \\xi_{ib+1_B}, ... , \\xi_{ib+b_A}, \\xi_{ib+b_B}} \\mathbb{H}(I_{ib+1}, I_{ib+2}, .., I_{ib+b} | w)\\]\nWe could consider optimizing this in the greedy fashion. This would mean just synthetically generating \\(b\\) independent queries. The drawback of this method would be that the queries would likely be very similar to each other. The authors propose a few other heuristics that would help guide the algorithm away from generating very similar queries, such as Medioid Selection where we have to cluster \\(B\\) greedy vectors into \\(b &lt; B\\) groups and pick one vector from each group (the medioid). The authors also propose two other methods rooted in providing different queries: boundary medioids selection and successive elimination. The authors test both the non-batched and variety of batched learning algorithms on multiple environments. When graphed over \\(N\\) the non-batched AL approach does in the same ball-park of performance as the batched approaches. However, over time, we see that learning is a much slower process when not-batched.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Elicitation</span>"
    ]
  },
  {
    "objectID": "src/chap4.html#sec-metric-elicitation",
    "href": "src/chap4.html#sec-metric-elicitation",
    "title": "3  Elicitation",
    "section": "3.4 Case Study 2: Performance Metric Elicitation",
    "text": "3.4 Case Study 2: Performance Metric Elicitation\nIn binary classification problems, selecting an appropriate performance metric that aligns with the real-world task is crucial. The problem of metric elicitation aims to characterize and discover the performance metric of a practitioner, reflecting the rewards or costs associated with correct or incorrect classification. For instance, in medical contexts such as diagnosing a disease or determining the appropriateness of a treatment, trade-offs are made for incorrect decisions. Not administering a treatment could lead to the worsening of a disease (a false negative), whereas delivering the wrong treatment could cause adverse side effects worse than not treating the condition (a false positive). Rather than choosing from a limited set of default choices like the F1-score or weighted accuracy, metric elicitation considers the process of devising a metric that best matches the preferences of practitioners or users. This is achieved by querying an “oracle” who provides feedback on proposed potential metrics through pairwise comparisons. Since queries to humans are often expensive, the goal is to minimize the number of comparisons needed.\nThe motivation for the pairwise comparison aspect of metric elicitation (Hiranandani et al. 2019a) stems from a rich history of literature in psychology, economics, and computer science (Samuelson 1938; Mas-Colell 1977; Varian 2006; Braziunas and Boutilier 2012; Tamburrelli and Margara 2014), demonstrating that humans are often ineffective at providing absolute feedback on aspects such as potential prices, user interfaces, or even ML model outputs (hence the comparison-based structure of RLHF, for instance). Additionally, confusion matrices accurately capture binary metrics such as accuracy, \\(F_\\beta\\), and Jaccard similarity by recording the number of false positives, true positives, false negatives, and true negatives obtained by a classifier. The main goal of this chapter is to introduce two binary-search procedures that can approximate the oracle’s performance metric for two types of metrics (linear and linear-fractional performance metrics) by presenting the oracle with confusion matrices generated by various classifiers. Essentially, we are learning an optimal threshold for classification given a decision boundary for a binary classification problem.\nFirst, we introduce some relevant notation that will later be used to formalize notions of oracle queries, classifiers, and metrics. In this context, \\(X \\in \\mathcal{X}\\) represents an input random variable, while \\(Y \\in \\{0, 1\\}\\) denotes the output random variable. We learn from a dataset of size \\(n\\), denoted by \\(\\{(x, y)_i\\}^n_{i=1}\\), which is generated independently and identically distributed (i.i.d.) from some distribution \\(\\mathbb{P}(X, Y)\\). The conditional probability of the positive class, given some sample \\(x\\), is denoted by \\(\\eta(\\vec{x}) = \\mathbb{P}(Y=1 | X=x)\\). The marginal probability of the positive class is represented by \\(\\zeta = \\mathbb{P}(Y=1)\\). The set of all potential classifiers is \\(\\mathcal{H} = \\{h : \\mathcal{X} \\rightarrow \\{0,1\\}\\}\\). The confusion matrix for a classifier \\(h\\) is \\(C(h, \\mathbb{P}) \\in \\mathbb{R}^{2 \\times 2}\\), where \\(C_{ij}(h, \\mathbb{P}) = \\mathbb{P}(Y=i, h=j)\\) for \\(i, j \\in \\{0,1\\}\\). These entries represent the false positives, true positives, false negatives, and true negatives, ensuring that \\(\\sum_{i,j}C_{ij}=1\\). The set of all confusion matrices is denoted by \\(\\mathcal{C}\\). Since \\(FN(h, \\mathbb{P}) = \\zeta - TP(h, \\mathbb{P})\\) and \\(FP(h, \\mathbb{P}) = 1 - \\zeta - TN(h, \\mathbb{P})\\), \\(\\mathcal{C}\\) is actually a 2-dimensional space, not a 4-dimensional space.\nAny hyperplane in the \\((tp, tn)\\) space is given by \\(\\ell := a \\cdot tp + b \\cdot tn = c\\), where \\(a, b, c \\in \\mathbb{R}\\). Given a classifier \\(h\\), we define a performance metric \\(\\phi : [0, 1]^{2 \\times 2} \\rightarrow \\mathbb{R}\\). The value \\(\\phi(C(h))\\), which represents the performance of a classifier with respect to a certain metric, is referred to as the utility of the classifier \\(h\\). We assume, without loss of generality, that a higher value of \\(\\phi\\) indicates a better performance metric for \\(h\\). Our focus is to recover some metric \\(\\phi\\) using comparisons between confusion matrices \\(C(h)\\), determined by classifiers \\(h\\), which approximates the oracle’s “ground-truth” metric \\(\\phi^*\\). Next, we introduce two classes of performance metrics—Linear Performance Metrics (LPM) and Linear-Fractional Performance Metrics (LFPM)—for which we will present two elicitation algorithms.\nAn LPM, given constants \\(\\{a_{11}, a_{01}, a_{10}, a_{00}\\} \\in \\mathbb{R}^{4}\\), is defined as \\(\\phi(C) = a_{11} TP + a_{01} FP + a_{10} FN + a_{00} TN = m_{11} TP + m_{00} TN + m_{0}\\), where \\(m_{11} = (a_{11} - a_{10})\\), \\(m_{00} = (a_{00} - a_{01})\\), and \\(m_{0} = a_{10} \\zeta + a_{01} (1 - \\zeta)\\). This reparametrization simplifies the metric by reducing dimensionality, making it more tractable for elicitation. One example of an LPM is weighted accuracy, defined as \\(WA = w_1TP + w_2TN\\), where adjusting \\(w_1\\) and \\(w_2\\) controls the relative importance of different types of misclassification. An LFPM, defined by constants \\(\\{a_{11}, a_{01}, a_{10}, a_{00}, b_{11}, b_{01}, b_{10}, b_{00}\\} \\in \\mathbb{R}^{8}\\), is given by: \\[\\phi(C) = \\frac{a_{11} TP + a_{01} FP + a_{10} FN + a_{00} TN}{b_{11} TP + b_{01} FP + b_{10} FN + b_{00} TN} = \\frac{p_{11} TP + p_{00} TN + p_{0}}{q_{11} TP + q_{00} TN + q_{0}},\\] where \\(p_{11} = (a_{11} - a_{10})\\), \\(p_{00} = (a_{00} - a_{01})\\), \\(q_{11} = (b_{11} - b_{10})\\), \\(q_{00} = (b_{00} - b_{01})\\), \\(p_{0} = a_{10} \\zeta + a_{01} (1 - \\zeta)\\), and \\(q_{0} = b_{10} \\zeta + b_{01} (1 - \\zeta)\\). This parametrization also simplifies the elicitation process by reducing the number of variables. Common LFPMs include the \\(F_\\beta\\) score and Jaccard similarity, defined as:\n\\[F_{\\beta} = \\frac{TP}{\\frac{TP}{1+\\beta^{2}} - \\frac{TN}{1+\\beta^{2}} + \\frac{\\beta^{2} \\zeta + 1 - \\zeta}{1+\\beta^{2}}}, \\quad JAC = \\frac{TP}{1 - TN}. \\tag{3.1}\\]\nSetting \\(\\beta = 1\\) gives the F1 score, which is widely used as a classification metric. Since we are considering all possible metrics in the LPM and LFPM families, we need to make certain assumptions about \\(\\mathcal{C}\\). Particularly, we will assume that \\(g(t) = \\mathbb{P}[\\eta(X) \\geq t]\\) is continuous and strictly decreasing for \\(t \\in [0, 1]\\); essentially, \\(\\eta\\) has positive density and zero probability.\nAdditionally, \\(\\mathcal{C}\\) is convex, closed, and contained within the rectangle \\([0, \\zeta] \\times [0, 1-\\zeta]\\), and is rotationally symmetric around its center, \\((\\frac{\\zeta}{2}, \\frac{1-\\zeta}{2})\\), where the axes represent the proportion of true positives and negatives. The only vertices of \\(\\mathcal{C}\\) are \\((0, 1-\\zeta)\\) and \\((\\zeta, 0)\\), corresponding to predicting all \\(0\\)’s or all \\(1\\)’s on a given dataset. Therefore, \\(\\mathcal{C}\\) is strictly convex, and any line tangent to it is tangent at exactly one point, corresponding to one particular confusion matrix. Next, recall that an LPM is represented in terms of three parameters (\\(\\phi = m_{11}TP + m_{00}TN + m_0\\)). We have just seen that this LPM and its corresponding confusion matrix correspond to a certain point on the boundary of \\(\\mathcal{C}\\). We first note that this point is independent of \\(m_0\\). Additionally, we only care about the relative weightings of \\(m_{11}\\) and \\(m_{00}\\), not their actual values—they are scale invariant. Therefore, we can parametrize the space of LPMs as \\(\\varphi_{LPM} = \\{\\mathbf{m} = (\\cos \\theta, \\sin \\theta) : \\theta \\in [0, 2\\pi]\\}\\), where \\(\\cos \\theta\\) corresponds to \\(m_{00}\\) and \\(\\sin \\theta\\) corresponds to \\(m_{11}\\). As we already know, we can recover the Bayes classifier given \\(\\mathbf{m}\\), and it is unique, corresponding to one point on the boundary of \\(\\mathcal{C}\\) due to its convexity. The supporting hyperplane at this point is defined as \\(\\bar{\\ell}_{\\mathbf{m}} := m_{11} \\cdot tp + m_{00} \\cdot tn = m_{11} \\overline{TP}_{\\mathbf{m}} + m_{00} \\overline{TN}_{\\mathbf{m}}\\). We note that if \\(m_{00}\\) and \\(m_{11}\\) have opposite signs, then \\(\\bar{h}_m\\) is the trivial classifier predicting all 1’s or all 0’s, since either predicting true positives or true negatives results in negative reward. This corresponds to a supporting hyperplane with a positive slope, so it can only be tangent at the vertices. Additionally, the boundary \\(\\partial \\mathcal{C}\\) can be split into upper and lower boundaries (\\(\\partial \\mathcal{C}_{+}, \\partial \\mathcal{C}_{-}\\)), corresponding to \\(\\theta \\in (0, \\pi/2)\\) and \\(\\theta \\in (\\pi, 3\\pi/2)\\) respectively (and whether \\(m_{00}, m_{11}\\) are positive or negative). We also define the notions of Bayes optimal and inverse-optimal classifiers. Given a performance metric \\(\\phi\\), we define:\n\nThe Bayes utility as \\(\\bar{\\tau} := \\sup_{h \\in \\mathcal{H}} \\phi(C(h)) = \\sup_{C \\in \\mathcal{C}} \\phi(C)\\); this is the highest achievable utility (using the metric \\(\\phi\\)) over all classifiers $h \\(\\mathcal{H}\\) for a given problem.\nThe Bayes classifier as \\(\\bar{h} := \\arg \\max_{h \\in \\mathcal{H}} \\phi(C(h))\\); this is the classifier \\(h\\) corresponding to the Bayes utility.\nThe Bayes confusion matrix as \\(\\bar{C} := \\arg \\max_{C \\in \\mathcal{C}} \\phi(C)\\); this is the confusion matrix corresponding to the Bayes utility and classifier.\n\nSimilarly, the inverse Bayes utility, classifier, and confusion matrix can be defined by replacing “\\(\\sup\\)” with “\\(\\inf\\)”; they represent the classifier and confusion matrix corresponding to the lower bound on utility for a given problem. We also have the following useful proposition:\n\n\n\n\n\n\nproposition\n\n\n\n\nProposition 3.1 Let \\(\\phi \\in \\varphi_{LPM}\\). Then\n\\[\\bar{h}(x) = \\left\\{\\begin{array}{lr}\n\\unicode{x1D7D9} \\left[\\eta(x) \\geq \\frac{m_{00}}{m_{11} + m_{00}}\\right], & m_{11} + m_{00} \\geq 0 \\\\\n\\unicode{x1D7D9} \\left[\\frac{m_{00}}{m_{11} + m_{00}} \\geq \\eta(x)\\right], & \\text { o.w. }\n\\end{array}\\right\\} \\tag{3.2}\\]\nis a Bayes optimal classifier with respect to \\(\\phi\\). The inverse Bayes classifier is given by \\(\\underline{h} = 1 - \\bar{h}\\).\n\n\n\nThis is a simple derivation based on the fact that we only get rewards from true positives and true negatives. Essentially, if we recover an LPM, we can use it to determine the best-performing classifier, obtained by placing a threshold on the conditional probability of a given sample, that corresponds to a confusion matrix. Therefore, the three notions of Bayes utility, classifier, and confusion matrix are functionally equivalent in our setting.\nWe will now formalize the problem of metric elicitation. Given two classifiers \\(h\\) and \\(h'\\) (or equivalently, two confusion matrices \\(C\\) and \\(C'\\)), we define an oracle query as the function:\n\\[\\Gamma\\left(h, h^{\\prime}\\right)=\\Omega\\left(C, C^{\\prime}\\right)=\\unicode{x1D7D9}\\left[\\phi(C)&gt;\\phi\\left(C^{\\prime}\\right)\\right]=: \\unicode{x1D7D9} \\left[C \\succ C^{\\prime}\\right], \\tag{3.3}\\]\nwhich represents the classifier preferred by the practitioner. We can then define the metric elicitation problem for populations:\n\n\n\n\n\n\ndefinition\n\n\n\n\nDefinition 3.1 Suppose the true (oracle) performance metric is \\(\\phi\\). The goal is to recover a metric \\(\\hat{\\phi}\\) by querying the oracle for as few pairwise comparisons of the form \\(\\Omega\\left(C, C^{\\prime}\\right)\\) so that \\(\\|\\phi - \\hat{\\phi}\\|_{--} &lt; \\kappa\\) for a sufficiently small \\(\\kappa &gt; 0\\) and for any suitable norm \\(\\|\\cdot\\|_{--}\\).\n\n\n\nIn practice, we do not have access to the true probability distribution or the population, which would provide the true values of \\(C\\) and \\(C'\\). However, we can subtly alter this problem description to use \\(\\hat{C}\\) and \\(\\hat{C}^{\\prime}\\), which are derived from our dataset of \\(n\\) samples:\n\n\n\n\n\n\ndefinition\n\n\n\n\nDefinition 3.2 Suppose the true (oracle) performance metric is \\(\\phi\\). The aim is to recover a metric \\(\\hat{\\phi}\\) by querying the oracle for as few pairwise comparisons of the form \\(\\Omega\\left(\\hat{C}, \\hat{C}^{\\prime}\\right)\\) so that \\(\\|\\phi - \\hat{\\phi}\\|_{--} &lt; \\kappa\\) for a sufficiently small \\(\\kappa &gt; 0\\) and for any suitable norm \\(\\|\\cdot\\|_{--}\\).\n\n\n\nAs is common in theoretical ML research, we solve the population problem and then consider ways to extend this to practical settings where we only have limited datasets of samples. In our case, this corresponds to calculating the confusion matrices from a portion of the dataset we have access to.\n\n3.4.1 Linear Performance Metric Elicitation\nFor LPM elicitation, we need one more proposition.\n\n\n\n\n\n\nproposition\n\n\n\n\nProposition 3.2 For a metric \\(\\psi\\) (quasiconvex and monotone increasing in TP/TN) or \\(\\phi\\) (quasiconcave and monotone increasing), and parametrization \\(\\rho^+\\)/\\(\\rho^-\\) of upper/lower boundary, composition \\(\\psi \\circ \\rho^-\\) is quasiconvex and unimodal on [0, 1], and \\(\\phi \\circ \\rho^+\\) is quasiconcave and unimodal on [0, 1].\n\n\n\nQuasiconcavity and quasiconvexity are slightly more general variations on concavity and convexity. Their main useful property in our setting is that they are unimodal (they have a singular extremum), so we can devise a binary-search-style algorithm for eliciting the Bayes optimal and inverse-optimal confusion matrices for a given setting, as well as the corresponding \\(\\phi\\)’s. We first note that to maximize a quasiconcave metric, in which \\(\\phi\\) is monotonically increasing in \\(TP\\) and \\(TN\\), we note that the resulting maximizer (and supporting hyperplane) will occur on the upper boundary of \\(\\mathcal{C}\\). We thus set our initial search range to be \\([0, \\pi/2]\\) and repeatedly divide it into four regions. Then, we calculate the resulting confusion matrix on the 5 resulting boundaries of these regions and query the oracle \\(4\\) times. We repeat this in each iteration of the binary search until a maximizer is found.\n\n\n\n\n\n\nremark\n\n\n\n\nRemark 3.1. In the case of quasiconcave and quasiconvex search ranges, a slightly more sophisticated variation on typical binary search must be used. To illustrate this, consider the two distributions in Figure 3.3:\n\n\n\n\n\n\n\n\n\n\nFigure 3.3\n\n\n\nFor both the symmetric and skewed distributions, if we were to divide the search range into two portions and compare \\(A\\), \\(C\\), and \\(E\\), we would find that \\(C &gt; A\\) and \\(C &gt; E\\). In both cases, this does not help us reduce our search range, since the true maximum could lie on either of the two intervals (as in the second case), or at \\(C\\) itself (as in the first case). Therefore, we must make comparisons between all five points \\(A, B, C, D, and E\\). This allows us to correctly restrict our search range to \\([B, D]\\) in the first case and \\([C, E]\\) in the second. These extra search requirements are due to the quasiconcavity of the search space we are considering, in which there exists a maximum but we need to make several comparisons at various points throughout the search space to be able to reduce its size in each iteration.\n\n\n\n\n\n\\begin{algorithm} \\caption{Quasiconcave Metric Maximization} \\begin{algorithmic} \\State \\textbf{input:} $\\epsilon &gt; 0$ and oracle $\\Omega$ \\State \\textbf{initialize:} $\\theta_a = 0, \\theta_b = \\frac{\\pi}{2}$ \\While{$|\\theta_b - \\theta_a| &gt; \\epsilon$} \\State set $\\theta_c = \\frac{3\\theta_a+\\theta_b}{4}$, $\\theta_d = \\frac{\\theta_a+\\theta_b}{2}$, and $\\theta_e = \\frac{\\theta_a+3\\theta_b}{4}$ \\State obtain $h\\theta_a, h\\theta_c, h\\theta_d, h\\theta_e, h\\theta_b$ using Proposition 1 \\State Compute $C\\theta_a, C\\theta_c, C\\theta_d, C\\theta_e, C\\theta_b$ using (1) \\State Query $\\Omega(C\\theta_c, C\\theta_a), \\Omega(C\\theta_d, C\\theta_c), \\Omega(C\\theta_e, C\\theta_d)$, and $\\Omega(C\\theta_b, C\\theta_e)$ \\If{$q_{i,j}$ is ambiguous} \\State request $q_{i,j}$'s label from reference \\Else \\State impute $q_{i,j}$'s label from previously labeled queries \\EndIf \\If{$C\\theta' \\succ C\\theta'' \\succ C\\theta'''$ for consecutive $\\theta &lt; \\theta' &lt; \\theta''$} \\State assume the default order $C\\theta \\prec C\\theta' \\prec C\\theta''$ \\EndIf \\If{$C\\theta' \\succ C\\theta'' \\succ C\\theta'''$ for consecutive $\\theta &lt; \\theta' &lt; \\theta''$} \\State assume the default order $C\\theta \\prec C\\theta' \\prec C\\theta''$ \\EndIf \\If{$C\\theta_a \\succ C\\theta_c$} \\State Set $\\theta_b = \\theta_d$ \\ElsIf{$C\\theta_a \\prec C\\theta_c \\succ C\\theta_d$} \\State Set $\\theta_b = \\theta_d$ \\ElsIf{$C\\theta_c \\prec C\\theta_d \\succ C\\theta_e$} \\State Set $\\theta_a = \\theta_c$ \\State Set $\\theta_b = \\theta_e$ \\ElsIf{$C\\theta_d \\prec C\\theta_e \\succ C\\theta_b$} \\State Set $\\theta_a = \\theta_d$ \\Else \\State Set $\\theta_a = \\theta_d$ \\EndIf \\EndWhile \\State \\textbf{output:} $\\vec{m}, C$, and $\\vec{l}$, where $\\vec{m} = m_l(\\theta_d), C = C\\theta_d$, and $\\vec{l} := (\\vec{m}, (tp, tn)) = (\\vec{m}, C)$ \\end{algorithmic} \\end{algorithm}\n\n\nTo elicit LPMs, we run Algorithm 1, querying the oracle in each iteration, and set the elicited metric \\(\\hat{m}\\) (which is the maximizer on \\(\\mathcal{C}\\)) to be the slope of the resulting hyperplane, since the metric is linear.\n\n\n\n\n\n\nremark\n\n\n\n\nRemark 3.2. To find the minimum of a quasiconvex metric, we flip all instances of \\(\\prec\\) and \\(\\succ\\), and use an initial search range of \\([\\pi, 3\\pi/2]\\); we use this algorithm, which we refer to as Algorithm 2, in our elicitation of LFPMs.\n\n\n\nNext, we provide a Python implementation of Algorithm 1.\n\n\n\n\n\n\ncode\n\n\n\n\ndef get_m(theta):\n    \"\"\"\n    Inputs: \n    - theta: the value that parametrizes m\n    Outputs:\n    - m_0 and m_1 for the LPM\n    \"\"\"\n\n    return (math.cos(theta), math.sin(theta))\n\ndef lpm_elicitation(epsilon, oracle):\n    \"\"\"\n    Inputs:\n    - epsilon: some epsilon &gt; 0 representing threshold of error\n    - oracle: some function that accepts 2 confusion matrices and\n        returns true if the first is preferred and false otherwise\n    Outputs:\n    - estimate for m, which is used to compute the LPM as described above\n    \"\"\"\n\n    a = 0\n    b = math.pi/2\n    while (b - a &gt; epsilon):\n        c = (3 * a + b) / 4\n        d = (a + b) / 2\n        e = (a + 3 * b) / 4\n\n        m_a, m_b, m_c, m_d, m_e = (get_m(x) for x in [a,b,c,d,e]) # using definition of m\n        c_a, c_b, c_c, c_d, c_e = (get_c(x) for x in [m_a, m_b, m_c, m_d, m_e]) # compute classifier from m's then calculate confusion matrices\n        \n        response_ac = oracle(c_a, c_c)\n        response_cd = oracle(c_c, c_d)\n        response_de = oracle(c_d, c_e)\n        response_eb = oracle(c_e, c_b)\n\n        # update ranges to keep the peak\n        if response_ac:\n            b = d\n        elif response_cd:\n            b = d\n        elif response_de:\n            a = c\n            b = e\n        elif response_eb:\n            a = d\n        else:\n            a = d\n    return get_m(d), get_c(d)\n\n\n\n\n\n3.4.2 Linear-Fractional Performance Metric Elicitation\nNow, we present the next main result, which is an algorithm to elicit linear-fractional performance metrics. For this task, we will need the following assumption: Let \\(\\phi \\in \\varphi_{L F P M}\\). We assume \\(p_{11}, p_{00} \\geq 0, p_{11} \\geq q_{11}, p_{00} \\geq q_{00},\\) \\(p_{0}=0, q_{0}=\\) \\(\\left(p_{11}-q_{11}\\right) \\zeta+\\left(p_{00}-q_{00}\\right)(1-\\zeta)\\), and \\(p_{11}+p_{00}=1\\).\nThese assumptions guarantee that the LFPM \\(\\phi\\) which we are trying to elicit is monotonically increasing in \\(TP\\) and \\(TN\\), just as in the LPM elicitation case. We first provide motivation and an overview of the approach for LFPM elicitation and then present pseudocode for the algorithm.\nThe general idea of the algorithm is to use Algorithm 1 to obtain a maximizer and a minimizer for the given dataset; these result in two systems of equations involving the true LFPM \\(\\phi^*\\) with 1 degree of freedom. Then, we run a grid search that is independent of oracle queries to find the point where solutions to the systems match pointwise on the resulting confusion matrices; this occurs close to where the true metric lies.\nMore formally, suppose that the true metric is \\[\\phi^{*}(C)=\\frac{p_{11}^{*} T P+p_{00}^{*} T N}{q_{11}^{*} T P+q_{00}^{*} T N+q_{0}^{*}}. \\tag{3.4}\\] Then, let \\(\\bar{\\tau}\\) and \\(\\underline{\\tau}\\) represent the maximizer and minimizer of \\(\\phi\\) over \\(\\mathcal{C}\\), respectively. There exists a hyperplane \\[\\begin{aligned}\n\\bar{\\ell}_{f}^{*}:=\\left(p_{11}^{*}-\\bar{\\tau}^{*} q_{11}^{*}\\right) t p+\\left(p_{00}^{*}-\\bar{\\tau}^{*} q_{00}^{*}\\right) t n=\\bar{\\tau}^{*} q_{0}^{*},\n\\end{aligned}\\] which touches \\(\\mathcal{C}\\) at \\(\\left(\\overline{T P}^{*}, \\overline{T N}^{*}\\right)\\) on \\(\\partial \\mathcal{C}_{+}\\). Correspondingly, there also exists a hyperplane \\(\\underline{\\ell}_{f}^{*}:=\\left(p_{11}^{*}-\\underline{\\tau}^{*} q_{11}^{*}\\right) t p+\\left(p_{00}^{*}-\\underline{\\tau}^{*} q_{00}^{*}\\right) \\operatorname{tn}=\\underline{\\tau}^{*} q_{0}^{*}\\), which touches \\(\\mathcal{C}\\) at \\(\\left(\\underline{TP}^{*}, \\underline{T N}^{*}\\right)\\) on \\(\\partial \\mathcal{C}_{-}\\). While we are unable to obtain Equation 3.4 and ?eq-eq3.49 directly, we can use Algorithm 1 to get a hyperplane \\[\\bar{\\ell}:=\\bar{m}_{11} t p+\\bar{m}_{00} t n= \\bar{m}_{11} \\overline{T P}^{*}+\\bar{m}_{00} \\overline{T N}^{*} = \\bar{C}_{0}, \\tag{3.5}\\] which is equivalent to \\(\\bar{\\ell}_{f}^{*}\\) (Equation 3.4) up to a constant multiple. From here, we can obtain the system of equations\n\\[p_{11}^{*}-\\bar{\\tau}^{*} q_{11}^{*}=\\alpha \\bar{m}_{11}, p_{00}^{*}-\\bar{\\tau}^{*} q_{00}^{*}=\\alpha \\bar{m}_{00}, \\bar{\\tau}^{*} q_{0}^{*}=\\alpha \\bar{C}_{0}, \\tag{3.6}\\] where \\(\\alpha &gt; 0\\) (we know it is \\(\\geq0\\) due to our assumptions earlier and because \\(\\bar{m}\\) is positive, but if it is equal to \\(0\\) then \\(\\phi^*\\) would be constant. So, our resulting system of equations is \\[\\begin{aligned}\n    p_{11}^{\\prime}-\\bar{\\tau}^{*} q_{11}^{\\prime}=\\bar{m}_{11}, p_{00}^{\\prime}-\\bar{\\tau}^{*} q_{00}^{\\prime}=\\bar{m}_{00}, \\bar{\\tau}^{*} q_{0}^{\\prime}=\\bar{C}_{0}.\n\\end{aligned} \\tag{3.7}\\]\nNow, similarly, we can approximate ?eq-eq3.49 using the algorithm we defined for quasiconvex metrics (Algorithm 2), where we altered the search range and comparisons. After finding the minimizer, we obtain the hyperplane \\[\\underline{\\ell}:=\\underline{m}_{11} t p+\\underline{m}_{00} t n=\\underline{m}_{11} \\underline{TP}^{*}+\\underline{m}_{00} \\underline{TN}^{*} = \\underline{C}_{0}, \\tag{3.8}\\] which is equivalent to \\(\\underline{\\ell}_{f}^{*}\\) (?eq-eq3.49) up to a constant multiple. So then, our system of equations is \\[p_{11}^{*}-\\underline{\\tau}^{*} q_{11}^{*}=\\gamma \\underline{m}_{11}, p_{00}^{*}-\\underline{\\tau}^{*} q_{00}^{*}=\\gamma \\underline{m}_{00}, \\underline{\\tau}^{*} q_{0}^{*}=\\gamma \\underline{C}_{0}, \\tag{3.9}\\] where \\(\\gamma &lt;0\\) (for a reason analogous to why we have \\(\\alpha &gt;0\\)), meaning our resulting system of equations is \\[\\begin{aligned}\n    p_{11}^{\\prime \\prime}-\\underline{\\tau}^{*} q_{11}^{\\prime \\prime}=\\underline{m}_{11}, p_{00}^{\\prime \\prime}-\\underline{\\tau}^{*} q_{00}^{\\prime \\prime}=\\underline{m}_{00}, \\underline{\\tau}^{*} q_{0}^{\\prime \\prime}=\\underline{C}_{0}.\n\\end{aligned} \\tag{3.10}\\]\nEquation 3.9 and Equation 3.10 form the two systems of equations mentioned in our overview of the algorithm. Next, we demonstrate that they have only one degree of freedom. Note that if we know \\(p_{11}'\\), we could solve both systems of equations as follows: \\[\\begin{aligned}\n    p_{00}^{\\prime}  &=1-p_{11}^{\\prime}, q_{0}^{\\prime}=\\bar{C}_{0} \\frac{P^{\\prime}}{Q^{\\prime}}\\\\\n    q_{11}^{\\prime}  &=\\left(p_{11}^{\\prime}-\\bar{m}_{11}\\right) \\frac{P^{\\prime}}{Q^{\\prime}} \\\\\n    q_{00}^{\\prime}&=\\left(p_{00}^{\\prime}-\\bar{m}_{00}\\right) \\frac{P^{\\prime}}{Q^{\\prime}},\n\\end{aligned} \\tag{3.11}\\] where \\(P^{\\prime}=p_{11}^{\\prime} \\zeta+p_{00}^{\\prime}(1-\\zeta)\\) and \\(Q^{\\prime}=P^{\\prime}+\\bar{C}_{0}-\\) \\(\\bar{m}_{11} \\zeta-\\bar{m}_{00}(1-\\zeta).\\)\nNow, suppose we know \\(p_{11}'\\). We could use this value to solve both systems Equation 3.9 and Equation 3.10, yielding two metrics, \\(\\phi'\\) and \\(\\phi''\\), from the maximizer and minimizer, respectively. Importantly, when \\(p_{11}^{*} / p_{00}^{*}=p_{11}^{\\prime} / p_{00}^{\\prime}=p_{11}^{\\prime \\prime} / p_{00}^{\\prime \\prime}\\), then \\(\\phi^{*}(C)=\\phi^{\\prime}(C) / \\alpha=-\\phi^{\\prime \\prime}(C) / \\gamma\\). Essentially, when we find a value of \\(p_{11}'\\) that results in \\(\\phi'\\) and \\(\\phi''\\) h aving constant ratios at all points on the boundary of \\(\\mathcal{C}\\), we can obtain \\(\\phi^*\\), as it is derivable from \\(\\phi'\\) and \\(\\alpha\\) (or, alternatively, \\(\\phi''\\) and \\(\\gamma\\)).\nWe will perform a grid search for \\(p_{11}'\\) on \\([0,1]\\). For each point in our search, we will compute \\(\\phi'\\) and \\(\\phi''\\). Then, we will generate several confusion matrices on the boundaries and calculate the ratio $’’ / \\(\\phi'\\) for each. We will select the value of \\(p_{11}'\\) for which the ratio \\(\\phi'' / \\phi'\\) is closest to constant and use it to compute the elicited metric \\(\\hat{\\phi}\\). The pseudocode for LFPM elicitation is given in Algorithm 2.\n\n\n\\begin{algorithm} \\caption{Grid Search for Best Ratio} \\begin{algorithmic} \\State \\textbf{Input:} $k, \\Delta$. \\State \\textbf{Initialize:} $\\sigma_{\\text{opt}} = \\infty, p'_{11,\\text{opt}} = 0$. \\State Generate $C_1, \\dots, C_k$ on $\\partial C_+$ and $\\partial C_-$ (Section 3). \\State Generate $C_1, \\dots, C_k$ on $\\partial C_+$ and $\\partial C_-$ (Section 3). \\For{$p'_{11} = 0; \\; p'_{11} \\leq 1; \\; p'_{11} = p'_{11} + \\Delta$} \\State Compute $\\phi'$, $\\phi''$ using Proposition 4. \\State Compute array $r = \\left[ \\frac{\\phi'(C_1)}{\\phi''(C_1)}, \\dots, \\frac{\\phi'(C_k)}{\\phi''(C_k)} \\right]$. \\State Set $\\sigma = \\text{std}(r)$. \\If{$\\sigma &lt; \\sigma_{\\text{opt}}$} \\State Set $\\sigma_{\\text{opt}} = \\sigma$ and $p'_{11,\\text{opt}} = p'_{11}$. \\EndIf \\EndFor \\State \\textbf{Output:} $p'_{11,\\text{opt}}$. \\end{algorithmic} \\end{algorithm}\n\n\nWe provide a Python implementation as below.\n\n\n\n\n\n\ncode\n\n\n\n\ndef lfpm_elicitation(k, delta):\n    \"\"\"\n    Inputs:\n    - k: the number of confusion matrices to evaluate on\n    - delta: the spacing for the grid search\n    Outputs:\n    - p_11', which will allow us to compute the elicited LFPM\n    \"\"\"\n\n    sigma_opt = np.inf\n    p11_opt = 0\n    C = compute_confusion_matrices(k) # generates k confusion matrices to evaluate on\n\n    for i in range(int(1/delta)):\n        p11 = i * delta\n        phi1 = compute_upper_metric(p11) # solves the first system of equations with p11 \n        phi2 = compute_lower_metric(p11) # solves the second system of equations with p11 \n        utility_1 = [phi1(c) for c in C] #calculate phi for both systems of equations\n        utility_2 = [phi2(c) for c in C]\n\n        r = []\n        for i in range(k):\n            r.append(utility_1[i] / utility_2[i])\n        sigma = np.std(r)\n\n        if(sigma &lt; sigma_opt):\n            sigma_opt = sigma\n            p11_opt = p11\n    return p11_opt\n\n\n\nIn summary, to elicit LFPMs, we utilize a special property of the LPM minimizer and maximizer on \\(\\mathcal{C}\\)–namely, that we can use the corresponding supporting hyperplanes to form a system of equations that can be used to approximate \\(\\phi^*\\) if one parameter (\\(p_{11}'\\)) is found, and that this parameter can be found using an oracle-independent grid search. Importantly, these algorithms can be shown to satisfy significant theoretical guarantees. We provide formal statement and intuitive interpretation of these guarantees here, with their proofs available in the appendix of the original paper. First, we define the oracle noise \\(\\epsilon_{\\Omega}\\), which arises from the oracle potentially flipping the comparison output on two confusion matrices that are close enough in utility.\nGiven \\(\\epsilon, \\epsilon_{\\Omega} \\geq 0\\) and a metric \\(\\phi\\) satisfying our assumptions, Algorithm 1 or Algorithm 2 finds an approximate maximizer/minimizer and supporting hyperplane. Additionally, the value of \\(\\phi\\) at that point is within \\(O\\left(\\sqrt{\\epsilon_{\\Omega}} + \\epsilon\\right)\\) of the optimum, and the number of queries is \\(O\\left(\\log \\frac{1}{\\epsilon}\\right)\\). Let \\(\\mathbf{m}^{*}\\) be the true performance metric. Given \\(\\epsilon &gt; 0\\), LPM elicitation outputs a performance metric \\(\\hat{\\mathbf{m}}\\), such that \\(\\left\\|\\mathbf{m}^{*} - \\hat{\\mathbf{m}}\\right\\|_{\\infty} \\leq \\sqrt{2} \\epsilon + \\frac{2}{k_{0}} \\sqrt{2 k_{1} \\epsilon_{\\Omega}}\\). These results ensure that Algorithm 1 and Algorithm 2 find an appropriate maximizer and minimizer in the search space, within a certain range of accuracy that depends on oracle and sample noise, and within a certain number of queries. Both of these statements are guaranteed by the binary search approach.\nLet \\(h_{\\theta}\\) and \\(\\hat{h}_{\\theta}\\) be two classifiers estimated using \\(\\eta\\) and \\(\\hat{\\eta}\\), respectively. Further, let \\(\\bar{\\theta}\\) be such that \\(h_{\\bar{\\theta}} = \\arg \\max _{\\theta} \\phi\\left(h_{\\theta}\\right)\\). Then \\(\\|C(\\hat{h}_{\\bar{\\theta}}) - C\\left(h_{\\bar{\\theta}}\\right)\\|_{\\infty} = O\\left(\\left\\|\\hat{\\eta}_{n} - \\eta\\right\\|_{\\infty}\\right)\\). This result indicates that the drop in elicited metric quality caused by using a dataset of samples rather than population confusion matrices is bounded by the drop in performance of the decision boundary \\(\\eta\\). These three guarantees together ensure that oracle noise and sample noise do not amplify drops in performance when using metric elicitation; rather, these drops in performance are bounded by the drops that would typically occur when using the standard machine learning paradigm of training a decision boundary and using a pre-established metric. For further interesting exploration of the types of problems that can be solved using the framework of metric elicitation, we refer the reader to (Hiranandani, Narasimhan, and Koyejo 2020), which performs metric elicitation to determine the oracle’s ideal tradeoff between the classifier’s overall performance and the discrepancy between its performance on certain protected groups.\n\n\n3.4.3 Multiclass Performance Metric Elicitation\nAlthough the previous section only described metric elicitation for binary classification problems, the general framework can still be applied to multiclass classification problems(Hiranandani et al. 2019b). Consider the case of classifying subtypes of leukemia (Yang and Naiman 2014). We can train a neural network to predict conditional probability of a certain leukemia subtype given certain gene expressions. However, it may not be appropriate to classify the subtype purely based on whichever one has the highest confidence. For instance, a treatment for leukemia subtype C1 may be perfect for cases of C1, but it may be ineffective or harmful for certain other subtypes. Therefore, the final response from the classifier may not be as simple as as choosing the class with the highest conditional probability, just like how the threshold for binary classification may not always be 50%. With multiclass metric elicitation, we can show confusion matrices to an oracle (like the doctor in the leukemia example) to determine which classifier has the best tradeoffs. In (Hiranandani et al. 2019b), the authors focus on eliciting linear performance metrics, which is what we will describe in this chapter. Most of the notation from Binary Metric Elicitation still persists, just modified to provide categorical responses. \\(X \\in \\mathcal{X}\\) is the input random variable. \\(Y \\in [k]\\) is the output random variable, where \\([k]\\) is the index set \\(\\{1, 2, \\dots, k\\}\\).\nThe dataset of size \\(n\\) is denoted by \\(\\{(\\vec{x}, y)\\}_{i=1}^n\\) generated independently and identically from \\(\\mathbb{P}(X, Y)\\). \\(\\eta_i(\\vec{x}) = \\mathbb{P}(Y=i | X=\\vec{x})\\) gives the conditional probability of class \\(i \\in [k]\\) given an observation. \\(\\xi_i = \\mathbb{P}(Y=i)\\) is the marginal probability of class \\(i \\in [k]\\). The set of all classifiers is \\(\\mathcal{H} = \\{h : \\mathcal{X} \\rightarrow \\Delta_k\\}\\), where \\(\\Delta_k\\) is (k-1) dimensional simplex. In this case, the outputs of classifiers are 1-hot vectors of size \\(k\\) where the only index with value 1 is the predicted class and all other positions have a value of 0. The confusion matrix for a classifier, \\(h\\), is \\(C(h, \\mathbb{P}) \\in \\mathbb{R}^{k \\times k}\\), where:\n\\[C_{ij}(h, \\mathbb{P}) = \\mathbb{P}(Y=i, h=j) \\text{\\qquad for } i, j \\in [k] \\tag{3.12}\\]\nNote that the confusion matrices are \\(k\\times k\\) and store the joint probabilities of each type of classification for each possible class. This means that the sum of row \\(i\\) in the confusion matrix equals \\(\\xi_i\\), because this is equivalent to adding over all possible classifications. Since we know the sums of each row, all diagonal elements can be reconstructed from just the off-diagonal elements, so a confusion matrix \\(C(h, \\mathbb{P})\\) can be expressed as a vector of off-diagonal elements, \\(\\vec{c}(h, \\mathbb{P}) = \\textit{off-diag}(C(h, \\mathbb{P}))\\), and \\(\\vec{c} \\in \\mathbb{R}^q\\) where \\(q := k^2 - k\\). The vector \\(\\vec{c}\\) is called the vector of ‘off-diagonal confusions.’ The space of off-diagonal confusions is \\(\\mathcal{C} = \\{\\vec{c}(h, \\mathbb{P}) : h \\in \\mathcal{H}\\}\\).\nIn cases where the oracle would care about the exact type of misclassification (i.e. misclassifying and object from class 1 as class 2), this off-diagonal confusion matrix is necessary. However, there are many cases where the performance of a classifier is determined by just the probability of correct prediction for each class, which just requires the diagonal elements. In these cases, we can define the vector of ‘diagonal confusions’ as \\(\\vec{d}(h, \\mathbb{P}) = \\textit{diag}(C(h, \\mathbb{P})) \\in \\mathbb{R}^k\\). The space of diagonal confusions is \\(\\mathcal{D} = \\{\\vec{d}(h, \\mathbb{P}) : h \\in \\mathcal{H}\\}\\).\nFinally, the setup for metric elicitation is identical to the one examined in the previous chapter. We still assume access to an oracle that can choose between two classifiers or confusion matrices, using notation \\(\\Gamma\\) for comparing two classifiers and \\(\\Omega\\) for comparing confusion matrices, which returns 1 if the first classifier is better and 0 otherwise. We still assume that the oracle behaves according to some unknown performance metric, and we wish to recover this metric up to some small error tolerance (based on a suitable norm). The two different types of confusion vectors result in different algorithms for metric elicitation, which we will explore in later sections.\nA Diagonal Linear Performance Metric (DLPM) is a performance metric that only considers the diagonal elements in the confusion matrix. The metric is defined as \\(\\psi(\\vec{d}) = \\langle \\vec{a}, \\vec{d} \\rangle\\), where \\(\\vec{a} \\in \\mathbb{R}^k\\) such that \\(||\\vec{a}||_1 = 1\\). It is also called weighted accuracy (Narasimhan et al. 2015). The family of DLPMs is denoted as \\(\\varphi_{DLPM}\\). Since these only consider the diagonal elements, which we want to maximize, we can focus on only eliciting monotonically increasing DLPMs, meaning that all elements in \\(\\vec{a}\\) are non-negative.\nConsider the trivial classifiers that only predict a single class at all times. The diagonal confusions when only predicting class \\(i\\) are \\(\\vec{v}_i \\in \\mathbb{R}^k\\) with \\(\\xi_i\\) at index \\(i\\) and zero elsewhere. Note that this is the maximum possible value in index \\(i\\), because this represents perfectly classifying all points that have a true class of \\(i\\). We can consider the space of diagonal confusions, visualized in Figure 3.4 (taken from (Hiranandani et al. 2019b)). The space of \\(\\mathcal{D}\\) is strictly convex, closed, and contained in the box \\([0, \\xi_1] \\times \\dots \\times [0, \\xi_k]\\). We also know that the only vertices are \\(\\vec{v}_i\\) for each \\(i \\in [k]^{(k-1)}\\).\n\n\n\n\n\n\nFigure 3.4: (a) Geometry of space of diagonal confusions for \\(k=3\\). This is a convex region with three flat areas representing confusions when restricted to only two classes. (b) Geometry of diagonal confusions when restricted to classes \\(k_1\\) and \\(k_2\\). Notice how this is identical to the space of confusion matrices examined in the previous chapter.\n\n\n\nWe know that this is strictly convex under the assumption that an object from any class can be misclassified as any other class. Mathematically, the assumption is that \\(g_{ij}(r) = \\mathbb{P} \\left[\\frac{\\eta_i(X)}{\\eta_j(X)} \\geq r \\right]\\) \\(\\forall i, j \\in [k]\\) are continuous and strictly decreasing for \\(r \\in [0, \\infty)\\).\nWe can also define the space of binary classification confusion matrices confined to classes \\(k_1\\) and \\(k_2\\), which is the 2-D \\((k_1, k_2)\\) axis-aligned face of \\(\\mathcal{D}\\), denoted as \\(\\mathcal{D}_{k_1, k_2}\\). Note that this is strictly convex, since \\(\\mathcal{D}\\) itself is strictly convex, and it has the same geometry as the space of binary confusion matrices examined in the previous chapter. Therefore, we can construct an RBO classifier for \\(\\psi \\in \\varphi_{DLPM}\\), parameterized by \\(\\vec{a}\\), as follows: \\[\\begin{aligned}\n\\bar{h}_{k_1, k_2}(\\vec{x})= \\left\\{\n\\begin{array}{ll}\n      k_1, \\text{ if } a_{k_1} \\eta_{k_1}(\\vec{x}) \\geq a_{k_2} \\eta_{k_2}(\\vec{x})\\\\\nk_2, \\text{ o.w.}\n\\end{array}\n\\right\\}.\n\\end{aligned} \\tag{3.13}\\]\nWe can parameterize the upper boundary of \\(\\mathcal{D}_{k_1, k_2}\\), denoted as \\(\\partial \\mathcal{D}^{+}_{k_1, k_2}\\), using a single parameter \\(m \\in [0, 1]\\). Specifically, we can construct a DLPM by setting \\(a_{k_1} = m\\), \\(a_{k_2} = 1 - m\\), and all others to 0. Using Equation 3.13, we can get the diagonal confusions, so varying \\(m\\) parameterizes \\(\\partial \\mathcal{D}^{+}_{k_1, k_2}\\). The parameterization is denoted as \\(\\nu(m; k_1, k_2)\\).\n\n3.4.3.1 Diagonal Linear Performance Metric Elicitation\nSuppose the oracle follows a true metric, \\(\\psi\\), that is linear and monotone increasing across all axes. If we consider the composition \\(\\psi \\circ \\nu(m; k_1, k_2): [0, 1] \\rightarrow \\mathbb{R}\\), we know it must be concave and unimodal, because \\(\\mathcal{D}_{k_1, k_2}\\) is a convex set. Therefore, we can find the value of \\(m\\) that maximizes \\(\\psi \\circ \\nu(m; k_1, k_2)\\) for any given \\(k_1\\) and \\(k_2\\) using a binary search procedure.\nSince the RBO classifier for classes \\(k_1\\) and \\(k_2\\) only rely on the relative weights of the classes in the DLPM (see Equation 3.13), finding the value of \\(m\\) that maximizes \\(\\psi \\circ \\nu(m; k_1, k_2)\\) gives us the true relative ratio between \\(a_{k_1}\\) and \\(a_{k_2}\\). Specifically, from the definition of \\(\\nu\\), we know that \\(\\frac{a_{k_2}}{a_{k_1}} = \\frac{1-m}{m}\\). We can therefore simply calculate the ratio between \\(a_1\\) and all other weights to reconstruct an estimate for the true metric. A python implementation of this algorithm is provided below.\n\n\n\n\n\n\ncode\n\n\n\n\nimport numpy as np\n\ndef rbo_dlpm(m, k1, k2, k):\n    \"\"\"\n    This constructs DLPM weights for the upper boundary of the\n    restricted diagonal confusions, given a parameter m.\n    This is equivalent to \\nu(m; k1, k2)\n    \n    Inputs:\n    - m: parameter (between 0 and 1) for the upper boundary\n    - k1: first axis for this  face\n    - k2: second axis for this face\n    - k: number of classes\n    Outputs:\n    - DLPM weights for this point on the upper boundary\n    \"\"\"\n    new_a = np.zeros(k)\n    new_a[k1] = m\n    new_a[k2] = 1 - m\n    return new_a\n\ndef dlpm_elicitation(epsilon, oracle, get_d, k):\n    \"\"\"\n    Inputs:\n    - epsilon: some epsilon &gt; 0 representing threshold of error\n    - oracle: some function that accepts 2 confusion matrices and\n        returns true if the first is preferred and false otherwise\n    - get_d: some function that accepts dlpm weights and returns \n        diagonal confusions\n    - k: number of classes\n    Outputs:\n    - estimate for true DLPM weights\n    \"\"\"\n    a_hat = np.zeros(k)\n    a_hat[0] = 1\n    for i in range(1, k):\n        # iterate over each axis to find appropriate ratio\n        a = 0  # lower bound of binary search\n        b = 1  # upper bound of binary search\n\n        while (b - a &gt; epsilon):\n            c = (3 * a + b) / 4\n            d = (a + b) / 2\n            e = (a + 3 * b) / 4\n\n            # get diagonal confusions for each point\n            d_a, d_c, d_d, d_e, d_b = (get_d(rbo_dlpm(x, 0, i, k)) \n                for x in [a, c, d, e, b])\n\n            # query oracle for each pair\n            response_ac = oracle(d_a, d_c)\n            response_cd = oracle(d_c, d_d)\n            response_de = oracle(d_d, d_e)\n            response_eb = oracle(d_e, d_b)\n\n            # update ranges to keep the peak\n            if response_ac:\n                b = d\n            elif response_cd:\n                b = d\n            elif response_de:\n                a = c\n                b = e\n            elif response_eb:\n                a = d\n            else:\n                a = d\n\n        midpt = (a + b) / 2\n        a_hat[i] = (1 - midpt) / midpt\n    return a_hat / np.sum(a_hat)\n\n\n\nTo use this algorithm for metric elicitation on a real dataset, we need to supply the “oracle” and “get_d” functions. The oracle function is an interface to an expert who judges which of two confusion matrices is better. The get_d function will need to construct a classifier given the DLPM weights, following the principles of the RBO classifier from Equation 3.13, and calculate the confusion matrix from a validation set.\nUsing the same oracle feedback noise model from the binary metric elicitation, we can make the following guarantees:\n\n\n\n\n\n\nproposition\n\n\n\n\nGiven \\(\\epsilon, \\epsilon_\\Omega \\geq 0\\), and a 1-Lipschitz DLPM \\(\\varphi^*\\) parameterized by \\(\\vec{a}^*\\). Then the output \\(\\hat{a}\\) of the DLPM elicitation algorithm after \\(O((k-1)\\log\\frac{1}{\\epsilon})\\) queries to the oracle satisfies \\(||\\vec{a}^* - \\hat{a}||_\\infty \\leq O(\\epsilon + \\sqrt{\\epsilon_\\Omega})\\), which is equivalent to \\(||\\vec{a}^* - \\hat{a}||_2 \\leq O(\\sqrt{k}(\\epsilon + \\sqrt{\\epsilon_\\Omega}))\\).\n\n\n\nIn other words, the maximum difference between the estimate and true value along any component (indicated by the L-infinity norm) is linearly bounded by the sum of the epsilon specified by the algorithm and the square root of the oracle’s correctness guarantee (\\(\\epsilon_\\Omega\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Elicitation</span>"
    ]
  },
  {
    "objectID": "src/chap4.html#case-study-3-active-preference-learning-in-robotics",
    "href": "src/chap4.html#case-study-3-active-preference-learning-in-robotics",
    "title": "3  Elicitation",
    "section": "3.5 Case Study 3: Active Preference Learning in Robotics",
    "text": "3.5 Case Study 3: Active Preference Learning in Robotics\nHow exactly do robots learn human preferences from just the pairwise comparisons, if they need to learn how to act in the environment itself? The comparisons in turn help robots learn the reward function of the human, which allows them to further take actions in real settings. Let’s say there are two trajectories \\(\\xi_A\\) and \\(\\xi_B\\) that might be taken as the next course of action in any context, like choosing the next turn, or choosing the next chatGPT response. The robot is offering both to a human for comparison. To answer which of them is better, the human would ask themselves if \\(R(\\xi_A)\\) or \\(R(\\xi_B)\\) is bigger, with \\(R(\\xi) = w * \\phi(\\xi)\\) being the reward function. In this equation \\(w\\) and \\(\\phi(\\xi)\\) are vectors of weights and features of the trajectory, so alternatively, we can express this as:\n\\[R(\\xi) = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ ... \\\\ w_N \\end{bmatrix} \\cdot \\begin{bmatrix} \\phi_1(\\xi) \\\\ \\phi_2(\\xi) \\\\ ... \\\\ \\phi_N(\\xi) \\end{bmatrix} \\tag{3.14}\\]\nIf one says that they preferred \\(\\xi_2\\) less than \\(\\xi_1\\) then it means \\(\\xi_2 &lt; \\xi_1 \\implies R(\\xi_2) &lt; R(\\xi_1) \\implies w * \\phi(\\xi_2) &lt; w * \\phi(\\xi_1) \\implies 0 &lt; w * (\\phi(\\xi_1) - \\phi(\\xi_2)) \\implies 0 &lt; w * \\Phi\\). Alternatively, if one preferred \\(\\xi_2\\) more than \\(\\xi_1\\), the signs would be flipped, resulting in \\(0 &gt; w * \\Phi\\). The two results can be represented in the N-dimensional space, where when it is split by the decision boundary, it creates half-spaces indicating preferences for each of the sides. For example we can see how a query between two items can split the plain into two halves, indicating preference towards one of the items. Such an image can be extended into bigger dimensions, where a line would become a separating hyperplane. If one is to truly believe the answers of one person, they would remove everything from the other side of the hyperplane that does not agree with the received human preference. But since humans are noisy, that approach is not optimal, thus most applications up-weight the indicated side of the plane to emphasize that points on that side are better, and down-weight the other side as they do not agree with the provided comparison.\nHow should someone choose which queries to conduct, otherwise, what is the most informative query sequence? After completing one query, the next query should be orthogonal to the previous one so that the potential space consistent with the preferences decreases in half. The intuition behind that is the potential space has all of the reward functions that agree with the provided answers, so to find a specific reward function for a human, decreasing the space narrows down the possible options. The original query created the blue space, and a new one created a red space, resulting in a purple intersection of the two which is still consistent with both of the queries’s results. The image shows that the purple portion is exactly half of the blue portion.\n\n\n\n\n\n\nFigure 3.5: Creating further comparisons limits the space that agrees with answers to all of them. The blue area demonstrates a preference for object 1 over object 2. The red area demonstrates a preference for object 3 over object 4. Combination (purple area) shows the space that is consistent with both of those preferences.\n\n\n\nMathematically, from (Biyik and Sadigh 2018) this can be expressed as set \\(F\\) of potential queries \\(\\phi\\), where \\(F = \\{\\phi: \\phi = \\Phi(\\xi_A) - \\Phi(\\xi_B), \\xi_A, \\xi_B \\in \\Xi\\}\\) (defining that a query is the difference between the features of two trajectories). Using that, the authors define a human update function \\(f_{\\phi}(w) = \\min(1, \\exp(I^T\\phi))\\) that accounts for how much of the space will still be consistent with the preferences. Finally, for a specific query, they define the minimum volume removed as \\(\\min\\{\\mathbb{E}[1 - f_{\\phi}(w)], \\mathbb{E}[1 - f_{-\\phi}(w)]\\}\\) (expected size of the two sides of the remaining space after it is split by a query - purple area in Figure 3.5), and the final goal is to maximize that amount over all possible queries since it is optimal to get rid of as much space as possible to narrow down the options for the reward function: \\(\\max_{\\phi} \\min\\{ \\mathbb{E}[1 - f_{\\phi}(w)], \\mathbb{E}[1 - f_{-\\phi}(w)]\\}\\). Effectively this is finding such \\(\\phi\\) that maximizes the information one can get by asking the next comparison query. While this approach uses minimum volume removed, there can be other metrics inside the \\(\\max\\) function. Some applications like movie recommendations do not require extra constraints, however in robotics one might want to add more constraints that satisfy certain rules, so that the resulting query follows the dynamics of the physical world.\nThe first real example of learning reward functions from pairwise comparisons is a 2D driving simulator from (Biyik and Sadigh 2018). In ?fig-car_direct you can see the setting of a 3-lane road with the orange car being controlled by the computer. The queries conducted for this problem are two different trajectories presented to the human, and they are asked to evaluate which one of them is better. For the features that contribute to the reward function, it is important to consider that robots might not find some of the information as informative for the learning process as a human would. For this example, the underlying features included the distance between lane boundaries, distance to other cars, and the heading and speed of the controlled car. The weights toward the last feature were weighted the highest according to the authors, since it takes a lot of effort for the car to change or correct its direction.\nAt the start of the learning process, the car had no direction learned and was moving all over the road. In the middle of learning after 30 queries, the simulator learned to follow the direction of the road and go straight but still experienced collisions. After 70 queries, the simulator learned to avoid collisions, as well as keep the car within the lane without swerving.\n\n3.5.0.1 Active Learning for Pairwise Comparisons\nWe have discussed that pairwise comparisons should be selected to maximize the minimum volume of remaining options removed. The question that can come out of the driving example is does it really matter to follow that goal or does random choice of queries performs as well? It turns out that indeed most AL algorithms (purposefully selecting queries) over time converge with the performance of the random query selection, so in long term the performance is similar. However, what is different is that AL achieves better performance earlier, which in time-sensitive tasks can be a critical factor. One example of such a setting can be exoskeletons for humans as part of the rehabilitation after surgery (Li et al. 2021). Different people have significantly different walking patterns as well as rehabilitation requirements, so the exoskeleton needs to adapt to the human as soon as possible for a more successful rehabilitation. Figure Figure 3.6 demonstrates the difference in the time needed between the two approaches. In general, in robotics, the time differences that might seem small to a human might be detrimental to the final performance.\n\n\n\n\n\n\nFigure 3.6: Performance of AL and random query selection algorithms in the task of exoskeleton learning with human preferences. (Li et al. 2021)\n\n\n\nIn conclusion, pairwise comparisons show to be a great way of learning linear reward functions, but at times present challenges or incapabilities that can be further improved with additional incorporations of approaches like AL. That improves many applications in terms of time spent getting to the result in case of exoskeleton adjustments, as well as getting to a middle ground between polar behaviors in applications like negotiations.\n\n\n3.5.1 Application: Guiding Human Demonstrations in Robotics\nA strong approach to learning policies for robotic manipulation is imitation learning, the technique of learning behaviors from human demonstrations. In particular, interactive imitation learning allows a group of humans to contribute their own demonstrations for a task, allowing for scalable learning. However, not all groups of demonstrators are equally helpful for interactive imitation learning.\nThe ideal set of demonstrations for imitation learning would follow a single, optimal method for performing the task, which a robot could learn to mimic. Conversely, multimodality, the presence of multiple optimal methods in the demonstration set, is challenging for imitation learning since it has to learn from contradicting information for how to accomplish a task. A common reason for multimodality is the fact that different people often subconsciously choose different paths for execution, as illustrated in Figure 3.7.\n\n\n\n\n\n\nFigure 3.7: Examples of two different ways to insert a nut onto a round peg. The orange demonstration picks up the nut from the hole while the blue demonstration picks up the nut from the side (Gandhi et al. 2022)\n\n\n\nGandhi et al. (Gandhi et al. 2022) identifies whether demonstrations are compatible with one another and offer an active elicitation interface to guide humans to provide better demonstrations in interactive imitation learning. Their key motivation is to allow multiple users to contribute demonstrations over the course of data collection by guiding users towards compatible demonstrations. To identify whether a demonstration is “compatible” with a base policy trained with prior demonstrations, the researchers measure the likelihood of demonstrated actions under the base policy, and the novelty of the visited states. Intuitively, low likelihood and low novelty demonstrations should be excluded since they represent conflicting modes of behavior on states that the robot can already handle, and are therefore incompatible. This concept of compatibility is used for filtering a new set of demonstrations and actively eliciting compatible demonstrations. In the following subsections, we describe the process of estimating compatibility and active elicitation in more detal.\n\n3.5.1.1 Estimating Compatiblity\nWe want to define a compatibility measure \\(\\mathcal{M}\\), that estimates the performance of policy \\(\\pi_{base}\\) that is retrained on a union of \\(\\mathcal{D}_{base}\\), the known base dataset, and \\(\\mathcal{D}_{new}\\), the newly collected dataset. To define this compatibility measure in a way that is easy to compute, we can use two interpretable metrics: likelihood and novelty. The likelihood of actions \\(a_{new}\\) in \\(\\mathcal{D}_{new}\\) is measured as the negative mean squared error between actions predicted by the base policy and this proposed action:\n\\[likelihood(s_{new}, a_{new}) = -\\mathbb{E}[|| \\pi_{base}(s_{new}) - a_{new} ||^2_2]. \\tag{3.15}\\]\nThe novelty of the state \\(s_{new}\\) in \\(\\mathcal{D}_{new}\\) is the standard deviation in the predicted actions under base policy:\n\\[novelty(s_{new}) = \\mathrm{Var}[\\pi_{base}(s_{new})]. \\tag{3.16}\\]\nWe can plot likelihood and novelty on a 2D plane, as shown in Figure 3.8, and identify thresholds on likelihood and novelty, denoted as \\(\\lambda\\) and \\(\\eta\\) respectively. Intuitively, demonstrations with low likelihood in low novelty states should be excluded, because this indicates that there is a conflict between the base behavior and the new demonstration due to multimodality. Note that in high novelty states, the likelihood should be disregarded because the base policy does not have a concrete idea for how to handle these states anyways so more data is needed.\n\n\n\n\n\n\nFigure 3.8: Examples of plots of likelihood and novelty for compatible and incompatible operators (Gandhi et al. 2022)\n\n\n\nThe final compatibility metric, parameterized by the likelihood and novelty thresholds \\(\\lambda\\) and \\(\\eta\\), is \\(\\mathcal{M}(\\mathcal{D}_{base}, (s_{new}, a_{new})) \\in [0, 1]\\), defined as:\n\\[\\begin{aligned}\n    \\mathcal{M} = \\begin{cases}\n        1 - \\min(\\frac{\\mathbb{E}[|| \\pi_{base}(s_{new}) - a_{new} ||^2_2]}{\\lambda}, 1) & \\text{ if } \\text{novelty}(s_{new}) &lt; \\eta \\\\\n        1 & \\text{ otherwise }\n       \\end{cases}.\n\\end{aligned} \\tag{3.17}\\]\nNote that \\(\\lambda\\) and \\(\\eta\\) need to be specified by hand. This is accomplished by assuming the ability to collect a priori incompatible demonstrations to identify reasonable thresholds that remove the most datapoints in the incompatible demonstrations while keeping the most datapoints in the compatible demonstrations.\n\n\n3.5.1.2 Case Studies with Fixed Sets\nThe researchers evaluate the utility of the compatibility metric on three tasks: placing a square nut on a square peg, placing a round nut on a round peg, and opening a drawer and placing a hammer inside. For each task, they train a base policy using a “proficient” operator’s demonstration while sampling trajectories from other operators for the new set. The naive baseline is to use all datapoints while the \\(\\mathcal{M}\\)-Filtered demonstrations use the compatibility metric to filter out incompatible demonstrations. The results are presented in Table 3.3. As you can see, M-filtering results in equal or greater performance despite using less data than the naive baseline, demonstrating the effectiveness of compatibility-based filtering.\n\n\n\nTable 3.3: Success rates (mean/std across 3 training runs) for policies trained on \\(\\mathcal{D}_{new}\\) by using all the data (Naive) or filtering by compatibility (\\(\\mathcal{M}\\)-Filtered) (Gandhi et al. 2022)\n\n\n\n\n\n\nSquare Nut\n\nRound Nut\n\nHammer Placement\n\n\n\nOperator\nNaive\n\\(\\mathcal{M}\\)-Filtered\nNaive\n\\(\\mathcal{M}\\)-Filtered\nNaive\n\\(\\mathcal{M}\\)-Filtered\n\n\nBase Operator\n38.7 (2.1)\n-\n13.3 (2.3)\n-\n24.7 (6.1)\n-\n\n\nOperator 1\n54.3 (1.5)\n61.0 (4.4)\n26.7 (11.7)\n32.0 (12.2)\n38.0 (2.0)\n39.7 (4.6)\n\n\nOperator 2\n40.3 (5.1)\n42.0 (2.0)\n22.0 (7.2)\n26.7 (5.0)\n33.3 (3.1)\n32.7 (6.4)\n\n\nOperator 3\n37.3 (2.1)\n42.7 (0.6)\n17.3 (4.6)\n18.0 (13.9)\n8.0 (0.0)\n12.0 (0.0)\n\n\nOperator 4\n27.3 (3.5)\n37.3 (2.1)\n7.3 (4.6)\n13.3 (1.2)\n4.0 (0.0)\n4.0 (0.0)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.9: The phases of the active elicitation interface: (a) initial prompting, (b) demonstrations with live feedback, and (c) corrective feedback (Gandhi et al. 2022)\n\n\n\n\n\n3.5.1.3 Actively Eliciting Compatible Demonstrations\nIn the previous section, we assume access to a dataset that has already been collected, and we see how filtering out incompatible demonstrations helps improve performance. However, when collecting a new dataset, it would be better to ensure that operators collect compatible demonstrations from the start, allowing us to retain as much data as possible for training.\nTo actively elicit compatible demonstrations, the researchers set up a pipeline for live feedback and examples. At the start, operators are given a task specification and some episodes to practice using the robot. Then, the active elicitation process begins, as shown in Figure 3.9. Each operator is shown some rollouts of the base policy to understand the style of the base operator. Next, the operator provides a demonstration similar to the ones they were shown. As they record their demonstrations, the interface provides online feedback, with green indicating compatible actions and red indicating incompatible actions. If the number of incompatible state-action pairs (ones where \\(\\mathcal{M}\\) is zero) exceeds 5% of the demonstration length, the demonstration is rejected. However, to provide corrective feedback, the interface shows the areas of the demonstration with the highest average incompatibility and also provides an expert demo that shows what should actually be done. Demonstrators can use this feedback to provide more compatible demonstrations moving forward.\nThis process helps improve the demonstration quality in both simulation and real experiments, as show in Table 3.4. Specifically, on the real results, active elicitation outperformed the base policy by 25% and naive data collection by 55%. Overall, active elicitation is a powerful tool to ensure that data collected for imitation learning improves the quality of the learned policy.\n\n\n\nTable 3.4: Success rates (mean/std across users) for policies trained on \\(\\mathcal{D}_{new}\\) by using all the data (Naive), filtering by compatibility (\\(\\mathcal{M}\\)-Filtered), or using informed demonstration collection (Gandhi et al. 2022)\n\n\n\n\n\nTask Ba\nse Naive\nNaive + Fil\ntered Informed\n\n\n\n\n\nRound Nut 13.\n3 (2.3) 9.\n6 (4.6)\n9.7 (4.2) 15\n.7 (6.0)\n\n\nHammer Placement 24.\n7 (6.1) 20.\n8 (15.7)\n22.0 (15.5) 31.\n8 (16.3)\n\n\n\\(\\left[ \\textup{Real} \\right]\\) Food Plating\n60.0 30.\n0 (17.3)\n- 85\n.0 (9.6)\n\n\n\n\n\n\nA fundamental limitation of eliciting compatible demonstrations is the fact that the “base” demonstrator is considered the ground truth. When the base demonstrator specifies a preference, all other demonstrators must abide by it, even if they have strong preferences against it. For instance, when pouring milk and cereal into a bowl, different people have different preferences for what is the correct order, but active elicitation forces all demonstrators to follow the initial preference of the base operator. The researchers hope that future work can enable users to override the default demonstration set and follow a base behavior that better aligns with their preferences. This could enable multiple modes of behavior to be collected in data while only following a user’s specified preference instead of attempting to collapse all modes into a single policy.\nLooking forward, active elicitation provides a foundation for allowing robots to query humans about the type of data needed, enabling more efficient data collection through transparency.\nIn summary, this chapter has explored the complexities and innovations in interAL as applied to large models within robotics. It begins by investigating pairwise comparisons and their role in efficiently learning linear reward functions from large datasets, overcoming limitations in supervised learning. When combined with active learning techniques, these comparisons supply timely, targeted, and context-appropriate feedback, enhancing performance in time-critical applications like exoskeleton adjustments during rehabilitation.\nWe then shift to imitation learning or inverse reward learning from demonstrations, emphasizing the difficulties introduced by multimodal demonstration sets. active elicitation approaches to compile compatible demonstrations, streamlining the learning process by guiding users to provide more valuable, steady examples are incredibly promising, however, to tackling this issue. This method shows promise in refining the interactive imitation learning data collection pipeline, enabling more capable and effective robotic training.\nAdditionally, the chapter examines the integration of foundation models into robotics, highlighting the transformative innovations of R3M and Voltron. R3M’s pre-training on diverse human activities dramatically improves robotic manipulation with minimal supervision. Meanwhile, Voltron builds on these capabilities by incorporating language-driven representation learning for remarkably adaptable and nuanced robotic task performance. These models represent significant leaps in robotics while opening new frontiers for future research and applications.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Elicitation</span>"
    ]
  },
  {
    "objectID": "src/chap5.html",
    "href": "src/chap5.html",
    "title": "4  Decisions",
    "section": "",
    "text": "4.1 Dueling Bandit\nFullscreen Part 1 Fullscreen Part 2\nThe multi-armed bandit (MAB) problem involves a gambler deciding which lever to pull on an MAB machine to maximize the winning rate, despite not knowing which machine is the most rewarding. This scenario highlights the need to balance exploration (trying new machines to discover potential higher rewards) and exploitation (using current knowledge to maximize gains). MAB algorithms address this dilemma by making decisions under uncertainty to achieve the best possible outcomes based on gathered data. At the core of the MAB problem is a set of actions, or ‘arms,’ denoted by \\(\\mathcal{A} = \\{1, 2, \\ldots, K\\}\\), where \\(K\\) signifies the total number of arms. For each round \\(t\\), the agent selects an arm \\(a_t \\in \\mathcal{A}\\) and receives a reward \\(r_t\\), sampled from an arm-specific, unknown probability distribution. The expected reward of pulling arm \\(a\\) is represented as \\(\\mu_a = \\mathbb{E}[r_t | a]\\).\nThe multi-armed bandit framework can be extended in various ways to model more complex scenarios. In the infinite-armed bandit problem, the set of possible arms \\(\\mathcal{A}\\) is either very large or infinite. This introduces significant challenges in exploration, as the agent cannot afford to explore each arm even once. Algorithms for infinite-armed bandits typically assume some regularity or structure of the reward function across arms to make the problem tractable. The contextual bandit problem extends the bandit framework by incorporating observable external states or contexts that influence the reward distributions of arms. The agent’s task is to learn policies that map contexts to arms to maximize reward. This model is particularly powerful for personalized recommendations, where the context can include user features or historical interactions. In dueling bandit problems, the agent chooses two arms to pull simultaneously and receives feedback only on which of the two is better, not the actual reward values. This pairwise comparison model is especially useful in scenarios where absolute evaluations are difficult, but relative preferences are easier to determine, such as in ranking systems.\nContextual bandits extend the multi-armed bandits by making decisions conditional on the state of the environment and previous observations. The benefit of such a model is that observing the environment can provide additional information, potentially leading to better rewards and outcomes. In each iteration, the agent is presented with the context of the environment, then decides on an action based on the context and previous observations. Finally, the agent observes the action’s outcome and reward. Throughout this process, the agent aims to maximize the expected reward.\nIn many real-world contexts, one may not have a real-valued reward (or at least a reliable one) associated with a decision. Instead, we may only have observations indicating which of a set of bandits was optimal in a given scenario. The assumption is that within these observations of preferred choices among a set of options, there is an implicit reward or payoff encapsulated in that decision. Consider the following examples:\nGenerally, we assume access to a set of actions. A noteworthy assumption is that any observations we make are unbiased estimates of the payoff. This means that if we observe a human preferred one option over another (or several others), the preferred option had a higher implicit reward or payoff than the alternatives. In the case of dietary preferences, this may mean that a human liked the preferred option; in the case of video recommendations, a user was more entertained, satisfied, or educated by the video they selected than the other options.\nThe overarching context is that we do not have direct or reliable access to rewards. We may not have a reward at all (for some decisions, it may be impossible to define a real value to the outcome), or it may be noisy (for example, if we ask a human to rate their satisfaction on a scale of 1 to 10). We use relative comparisons to evaluate the best of multiple options in this case. Our goal is to minimize total regret in the face of noisy comparisons. Humans may not always provide consistent observations (since human decision-making is not guaranteed to be consistent). However, we can still determine an optimal strategy with the observed comparisons. We aim to minimize the frequency of sub-optimal decisions according to human preferences. In practice, many formulations of bandits can allow for infinitely many bandits (for example, in continuous-value and high-dimensional spaces). However, this situation can be intractable when determining an optimal decision strategy. With infinite options, how can we always ensure we have chosen the best? We will constrain our bandits to a discrete space to enable efficient exploration. We will assume that we have \\(k\\) bandits, \\(b_i, i \\in [1, k]\\), and our task is to choose the one that will minimize regret.\nWith the framework outlined, we now define our approach more formally. This method was introduced by (Yue et al. 2012), and proofs for the guarantees and derivations of parameters can be found in their work.\nTo determine the optimal action, we will compare pairwise to ascertain the probability that an action \\(b_i\\) is preferred over another \\(b_j\\), where \\(i \\ne j\\). Concretely, we assume access to a function \\(\\epsilon\\) that helps determine this probability; in practice, this can be done with an oracle, such as asking a human which of two options they prefer: \\[P(b_i &gt; b_j) = \\varepsilon(b_i, b_j) + \\frac{1}{2}.\\] With this model, three basic properties govern the values provided by \\(\\epsilon\\): \\[\\epsilon(b_i, b_j) = -\\epsilon(b_j, b_i), \\epsilon(b_i, b_i) = 0, \\epsilon(b_i, b_j) \\in \\left(-\\frac{1}{2}, \\frac{1}{2} \\right).\\]\nWe assume there is a total ordering of bandits, such that \\(b_i \\succ b_j\\) implies \\(\\epsilon(b_i, b_j) &gt; 0\\). We impose two constraints to properly model comparisons:\nThese assumptions may initially seem limiting; however, common models for comparisons satisfy these constraints. For example, the Bradley-Terry Model follows \\(P(b_i &gt; b_j) = \\frac{\\mu_i}{\\mu_i + \\mu_j}\\). The Gaussian model with unit variance also satisfies these constraints: \\(P(b_i &gt; b_j) = P(X_i - X_j &gt; 0)\\), where \\(X_i - X_j \\sim N(\\mu_i - \\mu_j, 2)\\).\nTo accurately model the preferences between bandits in our framework of pairwise bandit comparisons and regret, we must track certain parameters in our algorithm. First, we will maintain a running empirical estimate of the probability of bandit preferences based on our observations. It is important to note that we do not have direct access to an \\(\\epsilon\\) function. Instead, we must present two bandits to a human, who selects a winner. To do this, we define: \\[\\hat{P}_{i, j} = \\frac{\\# b_i\\ \\text{wins}}{\\# \\text{comparisons between}\\ i \\text{and}\\ j}.\\]\nWe will also compute confidence intervals at each timestep for each of the entries in \\(\\hat{P}\\) as \\[\\hat{C}_t = \\left( \\hat{P}_t - c_t, \\hat{P}_t + c_t \\right),\\] where \\(c_t = \\sqrt{\\frac{4\\log(\\frac{1}{\\delta})}{t}}\\). Note that \\(\\delta = \\frac{1}{TK^2}\\), where \\(T\\) is the time horizon and \\(K\\) is the number of bandits.\nPreviously, we discussed approaches for finding the best action in a specific context. Now, we consider changing contexts, which means there is no longer a static hidden preference matrix \\(P\\). Instead, at every time step, there is a preference matrix \\(P_C\\) depending on context \\(C\\). We consider a context \\(C\\) and a preference matrix \\(P_C\\) to be chosen by nature as a result of the given environment (Yue et al., 2012). The goal of a contextual bandits algorithm is to find a policy \\(\\pi\\) that maps contexts to a Von Neumann winner distribution over our bandits. That is, our policy \\(\\pi\\) should map any context to some distribution over our bandits such that sampling from that distribution is preferred to a random action for that context.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decisions</span>"
    ]
  },
  {
    "objectID": "src/chap5.html#dueling-bandit",
    "href": "src/chap5.html#dueling-bandit",
    "title": "4  Decisions",
    "section": "",
    "text": "Dietary preferences: When providing food recommendations to humans, it is often not possible to quantify an explicit reward from recommending a specific food item. Instead, we can offer meal options and observe which one the person selects.\nVideo recommendation: Websites like YouTube and TikTok recommend specific videos to users. It is typically not feasible to measure the reward a person gains from watching a video. However, we can infer that a user preferred one video over another. From these relative preference observations, we can develop a strategy to recommend videos they are likely to enjoy.\nExoskeleton gait optimization: Tucker et al. (2020) created a framework that uses human-evaluated preferences for an exoskeleton gait algorithm to develop an optimal strategy for the exoskeleton to assist a human in walking. A human cannot reliably produce a numerical value for how well the exoskeleton helped them walk but can reliably indicate which option performed best according to their preferences.\n\n\n\n\n\n\n\nStrong Stochastic Transitivity: We must maintain our total ordering of bandits, and as such, the comparison model also respects this ordering: \\[b_i \\succ b_j \\succ b_k \\Rightarrow \\epsilon(b_i, b_k) \\ge \\text{max}\\{\\epsilon(b_i, b_j), \\epsilon(b_j, b_k)\\}. \\tag{4.1}\\]\nStochastic Triangle Inequality: We also impose a triangle inequality, which captures the condition that the probability of a bandit winning (or losing) a comparison will exhibit diminishing returns as it becomes increasingly superior (or inferior) to the competing bandit: \\[b_i \\succ b_j \\succ b_k \\Rightarrow \\epsilon(b_i, b_k) \\le \\epsilon(b_i, b_j) + \\epsilon(b_j, b_k). \\tag{4.2}\\]\n\n\n\n\n\n\n4.1.1 Regret\nThe agent aims to pick a sequence of arms \\((a_1, a_2, \\ldots, a_T)\\) across a succession of time steps \\(t = 1\\) to \\(t = T\\) to maximize the total accumulated reward. Formally, the strategy seeks to maximize the sum of the expected rewards: \\(\\max_{a_1, \\ldots, a_T} \\mathbb{E} \\left[\\sum_{t=1}^{T} r_t\\right]\\). Regret is defined as the difference between the cumulative reward that could have been obtained by always pulling the best arm (in hindsight, after knowing the reward distributions) and the cumulative reward actually obtained by the algorithm. Formally, if \\(\\mu^*\\) is the expected reward of the best arm and \\(\\mu_{a_t}\\) is the expected reward of the arm chosen at time \\(t\\), the regret after \\(T\\) time steps is given by \\(R(T) = T \\cdot \\mu^* - \\sum_{t=1}^{T} \\mu_{a_t}\\). The objective of a bandit algorithm is to minimize this regret over time, effectively learning to make decisions that are as close as possible to the decisions of an oracle that knows the reward distributions beforehand. Low regret indicates an algorithm that has often learned to choose well-performing arms, balancing the exploration of unknown arms with the exploitation of arms that are already known to perform well. Thus, an efficient bandit algorithm exhibits sub-linear regret growth, meaning that the average regret per round tends to zero as the number of rounds \\(T\\) goes to infinity: \\(\\lim_{T \\to \\infty} \\frac{R(T)}{T} = 0\\). Minimizing regret is a cornerstone in the design of bandit algorithms, and its analysis helps in understanding the long-term efficiency and effectiveness of different bandit strategies.\nAs previously discussed, our goal is to select the bandit that minimizes a quantity that reflects regret or the cost of not selecting the optimal bandit at all times. We can leverage our comparison model to define a quantity for regret over some time horizon \\(T\\), which is the number of decisions we make (selecting what we think is the best bandit at each iteration). Assuming we know the best bandit \\(b^*\\) (and we know that there is a best bandit, since there is a total ordering of our discrete bandits), we can define two notions of regret:\n\nStrong regret: aims to capture the fraction of users who would prefer the optimal bandit \\(b^*\\) over the worse of the options \\(b_1, b_2\\) we provide at a given step:\\(R_T = \\sum_{t = 1}^T \\text{max} \\left\\{ \\epsilon(b^*, b_1^{(t)}), \\epsilon(b^*, b_2^{(t)}) \\right\\}\\)\nWeak regret: aims to capture the fraction of users who would prefer the optimal bandit \\(b^*\\) over the better of the options \\(b_1, b_2\\) we provide at a given step:\\(\\tilde{R}_T = \\sum_{t = 1}^T \\text{min} \\left\\{ \\epsilon(b^*, b_1^{(t)}), \\epsilon(b^*, b_2^{(t)}) \\right\\}\\)\n\nThe best bandit described in our regret definition is called a Condorcet Winner. This is the strongest form of winner. It’s the action \\(A_{i}\\) which is preferred to each other action \\(A_j\\) with \\(p &gt; 0.5\\) in a head-to-head election. While the above introduced notions of regret assume an overall best bandit to exist, there might be settings, where no bandit wins more than half head-to-head duels. A set of actions without a Condorcet winner is described by the following preference matrix, where each entry \\(\\Delta_{jk}\\) is \\(p(j \\succ k) - 0.5\\), the probability that action \\(j\\) is preferred over action \\(k\\) minus 0.5. There is no Condorcet winner as there is no action that is preferred with \\(p &gt; 0.5\\) over all other actions. Imagine, you want to find the best pizza to eat (=action). There may not be a pizza that wins more than half of the head-to-head duels against every other pizza.\nHowever, we might still have an intuition of the best pizza. Therefore Sui et al., 2018 introduce the concepts of different \\(\\textit{winners}\\) in dueling bandit problems (Sui et al. 2018). In this example, we might define the best pizza as the most popular one. We call the Pizza receiving the most votes in a public vote the Borda Winner, or formally, Borda winner \\(j = \\arg\\max_{i \\in A, i \\neq j} \\left(\\sum p(j \\succ i)\\right)\\). In contrast to the Condorcet Winner setting, there is always guaranteed to be one or more (in the case of a tie) Borda winners for a set of actions. However - if there is a Condorcet Winner, this might not necessarily be the same as a Borda Winner: In our Pizza example, a Pepperoni Pizza might win more than half of its head-to-head duels, while the Cheese-Pizza is still the most popular in a public poll.\nA more generic concept of winner is the Von Neumann Winner, which describes a probability distribution rather than a single bandit winner. A Von Neumann winner simply prescribes a probability distribution \\(W\\) such that sampling from this distribution ‘beats’ an action from the random uniform distribution with \\(p &gt; 0.5\\). In our pizza example, this would correspond to trusting a friend to order whichever Pizza he likes, because this may still be preferred to ordering randomly. Formally, \\(W\\) is a Von Neumann if \\((j \\sim W, k \\sim R) [p(p(j \\succ k) &gt; 0.5) &gt; 0.5]\\) where \\(R\\) describes the uniform probability distribution over our actions. The concept of a Von Neumann winner is useful in contextual bandits, which will be introduced later. In these settings, the preference matrix depends on different context, which may have different Borda winners, just as different parties may vote for different pizzas.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n0\n0.03\n-0.02\n0.06\n0.10\n0.11\n\n\nB\n-0.03\n0\n0.03\n0.05\n0.08\n0.11\n\n\nC\n\n-0.03\n0\n0.04\n0.07\n0.09\n\n\nD\n-0.06\n-0.05\n-0.04\n0\n0.05\n0.07\n\n\nE\n-0.10\n-0.08\n-0.07\n-0.05\n0\n0.03\n\n\nF\n-0.11\n-0.11\n-0.09\n-0.07\n-0.03\n0\n\n\n\n\n\nFigure 4.1: Violation of Condorcet Winner. Highlighted entries are different from Table 1. No Condorcet winner exists as no arm could beat every other arm.\n\n\n\nNext, we introduce two performance measures for the planner. The asymptotic ex-post regret is defined as \\[\\text{Regret}(\\mu_1, \\ldots \\mu_K) = T\\cdot \\max_i \\mu_i - \\sum_{i=1}^T E[\\mu_{I_t}].\\]\nIntuitively, this represents the difference between the reward achieved by always taking the action with the highest possible reward and the expected welfare of the recommendation algorithm (based on the actions it recommends at each timestep).\nWe also define a weaker performance measure, the Bayesian regret, which is defined as \\[\\text {Bayesian regret}=E_{\\mu_1, \\ldots, \\mu_K \\sim \\text {Prior}}\\left[\\operatorname{Regret}\\left(\\mu_1, \\ldots, \\mu_K\\right)\\right]\\]\nWith a Bayesian optimal policy, we would like either definition of regret to vanish as \\(T\\to \\infty\\); we are considering “large-market optimal\" settings where there are many short-lived, rather than a few long-term, users. Note the fact that ex-post regret is prior-free makes it robust to inaccuracies on the prior.\n\n\n4.1.2 Acquisition Functions\nVarious strategies have been developed to balance the exploration-exploitation trade-off. These strategies differ in selecting arms based on past experiences and rewards.\n\n4.1.2.1 Classical Acquisition Functions\nUniform acquisition function is the most straightforward approach where each arm is selected uniformly randomly over time. This strategy does not consider the past rewards and treats each arm equally promising regardless of the observed outcomes. It is a purely explorative strategy that ensures each arm is sampled enough to estimate its expected reward, but it does not exploit the information to optimize rewards. In mathematical terms, if \\(N_t(a)\\) denotes the number of times arm \\(a\\) has been selected up to time \\(t\\), the Uniform Strategy would ensure that \\(N_t(a) \\approx \\frac{t}{K}\\) for all arms \\(a\\) as \\(t\\) grows large: \\(P(a_t = a) = \\frac{1}{K}\\)\nThe Epsilon Greedy is a popular method that introduces a balance between exploration and exploitation. With a small probability \\(\\epsilon\\), it explores by choosing an arm at random, and with a probability \\(1 - \\epsilon\\), it exploits by selecting the arm with the highest estimated reward so far. This strategy incrementally favors actions that have historically yielded higher rewards, but still allows for occasional exploration to discover better options potentially. The parameter \\(\\epsilon\\) is chosen based on the desired exploration level, often set between 0.01 and 0.1. \\[P(a_t = a) =\n\\begin{cases}\n\\frac{\\epsilon}{K} + 1 - \\epsilon & \\text{if } a = \\arg\\max_{a'} \\hat{\\mu}_{a'} \\\\\n\\frac{\\epsilon}{K} & \\text{otherwise}\n\\end{cases}\\]\nUpper Confidence Bound (UCB) acquisition function takes a more sophisticated approach to the exploration-exploitation dilemma. It selects arms based on both the estimated rewards and the uncertainty or variance associated with those estimates. Specifically, it favors arms with high upper confidence bounds on the estimated rewards, which is a sum of the estimated mean and a confidence interval that decreases with the number of times the arm has been played. This ensures that arms with less certainty (those played less often) are considered more often, naturally balancing exploration with exploitation as the uncertainty is reduced over time.\n\\[P(a_t = a) =\n\\begin{cases}\n1 & \\text{if } a = \\arg\\max_{a'} \\left( \\hat{\\mu}_{a'} + \\sqrt{\\frac{2 \\ln t}{N_t(a')}} \\right) \\\\\n0 & \\text{otherwise}\n\\end{cases}\\]\n\n\n4.1.2.2 Interleaved Filter\nThis algorithm tries to find the best bandit (Condorcet Winner) in a discrete, limited bandit-space via pairwise comparisons of the bandits. We will now introduce the algorithm for the Interleaved Filter as provided in (Yue et al. 2012) to solve a dueling bandit setup. It starts with a randomly defined best bandit \\(\\hat{b}\\) and iteratively compares it to set \\(W\\) containing the remaining bandits \\(b\\) resulting in winning probabilities \\(\\hat{P}_{\\hat{b},b}\\) and confidence interval \\(\\hat{C}_{\\hat{b},b}\\). If a bandit \\(b\\) is confidently worse than \\(\\hat{b}\\), it is removed from \\(W\\). If a bandit \\(b'\\) is confidently better than \\(\\hat{b}\\), it is set as new best bandit \\(\\hat{b}\\) and bandit \\(\\hat{b}\\) as well as every other bandit \\(b\\) worse than \\(\\hat{b}\\) are removed from \\(W\\). This is done, until \\(W\\) is empty, leaving the final \\(\\hat{b}\\) as the predicted best bandit.\n\n\ninput: \\(T\\), \\(B=\\{b_1, \\dots, b_k\\}\\) \\(\\delta \\gets 1/(TK^2)\\) Choose \\(\\hat{b} \\in B\\) randomly \\(W \\gets \\{b_1, \\dots, b_k\\} \\backslash \\{\\hat{b}\\}\\) \\(\\forall b \\in W\\), maintain estimate \\(\\hat{P}_{\\hat{b},b}\\) of \\(P(\\hat{b} &gt; b)\\) according to (6) \\(\\forall b \\in W\\), maintain \\(1 - \\delta\\) confidence interval \\(\\hat{C}_{\\hat{b},b}\\) of \\(\\hat{P}_{\\hat{b},b}\\) according to (7), (8) compare \\(\\hat{b}\\) and \\(b\\) update \\(\\hat{P}_{\\hat{b},b}\\), \\(\\hat{C}_{\\hat{b},b}\\) \\(W \\gets W \\backslash \\{b\\}\\)\n\\(W \\gets W \\backslash \\{b\\}\\) \\(\\hat{b} \\gets b'\\), \\(W \\gets W \\backslash \\{b'\\}\\) \\(\\forall b \\in W\\), reset \\(\\hat{P}_{\\hat{b},b}\\) and \\(\\hat{C}_{\\hat{b},b}\\) \\(\\hat{T} \\gets\\) Total Comparisons Made \\((\\hat{b}, \\hat{T})\\)\n\n\n\nParameter Initialization\n\nIn lines 1-6 of the algorithm, we take the inputs and first compute the value \\(\\delta\\) which is used to compute our confidence intervals. We select an initial guess of an optimal bandit \\(\\hat{b}\\) by uniformly sampling from all bandits \\(\\mathcal{B}\\). We also keep a running set of bandit candidates \\(W\\), which is initialized to be \\(\\mathcal{B} \\setminus \\{\\hat{b}\\}\\). At this point, we also initialize our empirical estimates for \\(\\hat{P}, \\hat{C}\\).\nNext, we will repeat several steps until our working set of bandit candidates \\(W\\) is empty.\n\nUpdate Estimates Based on Comparisons\n\nThe first step at each iteration (lines 8-11) is to look at all candidates in \\(W\\), and compare them to our current guess \\(\\hat{b}\\) using an oracle (e.g. by asking a human which of \\(\\hat{b}\\) or \\(b \\in W\\) is preferred). With this new set of wins and comparisons, we update our estimates of \\(\\hat{P}, \\hat{C}\\).\n\nPrune Suboptimal Bandits\n\nIn lines 12-13, with updated comparison win probabilities and corresponding confidence intervals, we can remove bandit candidates from \\(W\\) that we are confident \\(\\hat{b}\\) is better than. The intuition here is that we are mostly sure that our current best guess is better than some of the candidates, and we don’t need to consider those candidates in future iterations.\n\nCheck for Better Bandits from Candidate Set\n\nNow that our candidate set of bandits may be smaller, in lines 15-21 we check if there are any bandits \\(b'\\) that we are confident are better than our current best guess. If we do find such a candidate, we remove bandits which \\(\\hat{P}\\) indicates \\(b\\) is likely worse than \\(\\hat{b}\\). Note that in this step, we do not require the probability to be outside the confidence interval, since we already found one we believe to be significantly closer to optimal than our current best guess.\nOnce we remove the candidates likely worse than \\(\\hat{b}\\), we crown \\(b'\\) as the new best guess, e.g. \\(\\hat{b} := b'\\). Consequently, we remove \\(b'\\) from \\(W\\) and reset our empirical win counters \\(\\hat{P}, \\hat{C}\\).\n\n\nWith this algorithm defined, let us look at some provisions of the method with respect to identifying the optimal strategy. Note that the proofs and derivations for these quantities are provided in (Yue et al. 2012).\nFirst, the method guarantees that for the provided time horizon \\(T\\), the algorithm returns the correct bandit with probability \\(P \\ge 1 - \\frac{1}{T}\\). It is interesting and useful to note that if one has a strict requirement for the probability of identifying the correct bandit, one can compute the time horizon \\(T\\) that guarantees this outcome at that probability. Furthermore, a time horizon of 1 leaves no probabilistic guarantee of a successful outcome, and increasing \\(T\\) has diminishing returns. Second, in the event that the algorithm returns an incorrect bandit, the maximal regret incurred is linear with respect to \\(T\\), e.g. \\(\\mathcal(O)(T)\\). This is also a useful provision as it allows us to estimate the overall cost in the worst case outcome. Based on these two provisions, we can compute the expected cumulative regret from running the Interleaved Filter algorithm, which is: \\[\\mathbb{E}\\left[R_T\\right] \\le \\left(1 - \\frac{1}{T}\\right) \\mathbb{E}\\left[ R_T^{IF} \\right] + \\frac{1}{T}\\mathcal{O}(T) \\\\\n= \\mathcal{O}\\left(\\mathbb{E}\\left[ R_T^{IF} \\right] + 1\\right)\\]\nInterestingly, the original work shows that these bounds hold for both strong and weak regret. As demonstrated, the Interleaved Filter algorithm [fig-if] provides a robust method to ascertain the optimal bandit or strategy given a set of options and only noisy comparisons. In most real-world scenarios for modeling human preferences, it is not possible to observe a real-world reward value, or at least a reliable one and as such this method is a useful way to properly model human preferences.\nFurthermore, the algorithm provides strong guarantees for the probability of selecting the correct bandit, maximal regret, and the number of comparisons required. It is even more impressive that the method can do so without severely limiting constraints; as demonstrated, the most commonly used models satisfy the imposed constraints.\nAs we look to model human preferences, we can certainly leverage this method for k-armed dueling bandits to identify the best strategy to solve human-centric challenges, from video recommendation to meal selection and exoskeleton-assisted walking.\n\n\n4.1.2.3 Dueling Bandit Gradient Descent\nThis algorithm tries to find the best bandit in a continuous bandit-space. Here, the set of all bandits is regarded as an Information-Retrieval (IR) system with infinite bandits uniquely defined by \\(w\\). We will cover the Dueling Bandit Gradient Descent algorithm from Yue and Joachims 2009 (Yue and Joachims 2009). Yue and Joachims use the dueling bandits formulation for online IR optimization. They propose a retrieval system parameterized by a set of continuous variables lying in \\(W\\), a \\(d\\)-dimensional unit-sphere. The DBGD algorithm adapts the current parameters \\(w_t\\) of IR system by comparison with slightly altered parameters \\(w_t'\\) both querying query \\(q_t\\). Only if the IR outcome using \\(w_t'\\) is preferred, the parameters are changed in their direction. We will now discuss the algorithm more detailed.\n\n\ninput: \\(\\gamma\\), \\(\\delta\\), \\(w_1\\)\nSample unit vector \\(u_t\\) uniformly\n\\(w_t' \\gets P_W(w_t + \\delta u_t)\\)\nCompare \\(w_t\\) and \\(w_t'\\)\n\\(w_{t+1} \\gets P_W(w_t + \\gamma u_t)\\)\n\\(w_{t+1} \\gets w_t\\)\n\n\nWe first choose exploration step length \\(\\delta\\), exploitation step length \\(\\gamma\\), and starting point (in unit-sphere) \\(w_1\\). Choose a query and sample a random unit vector \\(u_t\\). We duel \\(w_t\\) and \\(w_t'\\), where \\(w_t\\) is our current point in the sphere, and \\(w_t'\\) is our exploratory comparison, which is generated by taking a random step of length \\(\\delta\\), such that \\(w_t' = w_t + \\delta u_t\\). The objective of this duel is to ascertain the binary preference of users with respect to the results yielded by the IR systems parameterized by \\(w_t\\) and \\(w_t'\\) respectively, taking query \\(q_t\\) as an input. The parameters that get the majority of the votes in the head to head win. If \\(w_t\\) wins, then we keep the parameters for the next iteration. If \\(w_t'\\) wins the duel, we update our parameters in the direction of \\(u_t\\) by taking a step of length \\(\\gamma\\). Note that the algorithm describes projection operation \\(P_W(\\overrightarrow{v})\\). Since \\(u_t\\) is chosen randomly, \\(w_t + \\delta u_t\\) or \\(w_t + \\gamma u_t\\) could exist outside of the unit sphere where all possible parameter configurations lie. In this case, we simply project the point back onto the sphere using said projection \\(P_W(\\overrightarrow{v})\\).\nYue and Joachims show that this algorithm has sublinear regret in \\(T\\), the number of iterations. We note that the algorithm assumes that there exists a hidden reward function \\(R(w)\\) that maps system parameters \\(w_t\\) to a reward value which is smooth and strictly concave over the input space \\(W\\).\nLastly, we would also like to give motivation behind \\(\\delta\\) and \\(\\gamma\\) being different values. We need a \\(\\delta\\) that is sufficiently large that the comparison between a system parameterized by \\(w_t\\) and \\(w_t'\\) is meaningful. On the other hand, we may wish to take a smaller step in the direction of \\(w_t'\\) during our update step, as during a duel, we only score \\(w_t\\) against \\(w_t'\\) over the results on one query \\(q_t\\). Having \\(\\delta &gt; \\gamma\\) allows us to get reward signal from meaningfully different points while also updating our belief of the best point \\(w_{\\text{best}}\\) gradually.\n\n\nSparring EXP4\nZoghi et al. 2015 propose one algorithm for this problem — sparring EXP4, which duels two traditional EXP4 - algorithms. The (traditional) EXP4 algorithm solves the traditional contextual bandits — the case where we can directly observe a reward for a choice of bandit given a context. The EXP4 algorithm embeds each bandit as a vector. When the algorithm sees the context (called ‘advice’ in this formulation), it produces a probability distribution over the choices based on an adjusted softmax function on the inner product between the context and the bandit vectors. The probability function is different from a softmax as we assign some minimum probability that any action gets chosen to enforce exploration. A reward is then observed for the choice and propagated back through the embedding of the chosen bandit.\nSparring EXP4 runs two instances of the EXP4 algorithm against each other. Each EXP4 instance samples an action given a context, and then these choices are ‘dueled’ against each other. Instead of directly observing a reward, as for traditional EXP4, we instead observe two converse reward — a positive reward for the choice that won the duel and a negative reward to the choice that lost. The reward is proportional to the degree to which the bandit wins the duel, i.e. how likely the bandit is to be preferred over the other when users are queried for binary preferences. Like in traditional EXP4, the reward or negative reward is then propagated back through the representations of the bandits.\n\n\n4.1.2.4 Feel-good Thompson sampling\nThis algorithm is a solution for the contextual dueling bandit setting, and tries to minimize cumulative average regret (= find WHAT WINNER?!Von Neumann???): \\[\\text{Regret}(T) := \\sum_{t=1}^{T} \\left[ r_{*}(x_t, a_{t}^{*}) - \\frac{r_{*}(x_t, a_{t}^{1}) + r_{*}(x_t, a_{t}^{2})}{2} \\right],\\] where \\(r_{*}(x_t, a_{t})\\) is the true, hidden reward function of a context \\(x_t\\) and action \\(a_t\\). Thompson sampling is an iterative process of receiving preference over two actions, each maximizing a different approximation of the reward function based on past data and adding this new information to the data.\nFinding good approximations of the reward function at time \\(t\\) is done by sampling two reward function parameters \\(\\theta_t^{j=1}\\) and \\(\\theta_t^{j=2}\\) from a posterior distribution based on all previous data \\(p_j(\\cdot \\mid S_{t-1})\\). This posterior distribution is proportional to the multiplication of the prior and the likelihood function, which is a Gaussian in standard Thompson sampling. In Feel-Good Thompson sampling, an additional term called \"Feel-good exploration\" encourages parameters \\(\\theta\\) with a large maximum reward in previous rounds. This change to the likelihood function may increase probabilities in uncertain areas, thus exploring those regions. All that’s left is to select an action maximizing each reward function approximation and receive a preference \\(y_t\\) on one of them to add the new information to the dataset(Zhang 2021).\n\n\nInitialize \\(S_0 = \\varnothing\\). Receive prompt \\(x_t\\) and action space \\(\\mathcal{A}_t\\). Sample model parameter \\(\\theta_t^j\\) from the posterior distribution \\(p^j(\\cdot \\mid S_{t-1})\\) Select response \\(a_t^j = \\arg\\max_{a \\in \\mathcal{A}_t} \\langle \\theta_t^j, \\phi(x_t, a) \\rangle\\). Receive preference \\(y_t\\). Update dataset \\(S_t \\leftarrow S_{t-1} \\cup \\{(x_t, a_t^1, a_t^2, y_t)\\}\\).\n\n\n\n\n\n4.1.3 Applications\nThere are many applications where contextual bandits are used. Many of these applications can utilize human preferences. One particular application illustrates the benefits a contextual bandit would have over a multi-armed bandit: a website deciding which app to show someone visiting the website. A multi-armed bandit might decide to show someone an ad for a swimsuit because the swimsuit ads have gotten the most user clicks (which indicates human preference). A contextual bandit might choose differently, however. A contextual bandit will also take into account the context, which in this case might mean information about the user (location, previously visited pages, and device information). If it discovers the user lives in a cold environment, for example, it might suggest a sweater ad for the user instead and get a better chance of a click. There are many more examples of where contextual bandits can be applied. They can be applied in other web applications, such as to optimize search results, medical applications, such as how much of a medication to prescribe based on a patient’s history, and gaming applications, such as basing moves off of the state of a chess board to try to win. In each of the above examples, human feedback could have been introduced during training and leveraged to learn a reward function.\nWe explored different versions of bandits that address the exploration-exploitation trade-off in various real-world scenarios. These models have been employed across various fields, including but not limited to healthcare, finance, dynamic pricing, and anomaly detection. This section provides a deep dive into some real-world applications, emphasizing the value and advancements achieved by incorporating bandit methodologies. The content of this section draws upon the findings from the survey cited in reference (Bouneffouf, Rish, and Aggarwal 2020).\nIn healthcare, researchers have been applying bandits to address challenges in clinical trials and behavioral modeling (Bouneffouf, Rish, and Cecchi 2017; Bastani and Bayati 2020). One of the examples is drug dosing. Warfarin, an oral anticoagulant, has traditionally been administered using fixed dosing protocols. Physicians would then make subsequent adjustments based on the patient’s emerging symptoms. Nonetheless, inaccuracies in the initial dosage—whether too low or too high—can lead to serious complications like strokes and internal bleeding. In a pivotal study, researchers in (Bastani and Bayati 2020) modeled the Warfarin initial dosing as a contextual bandit problem to assign dosages to individual patients appropriately based on their medication history. Their contributions include the adaptation of the LASSO estimator to the bandit setting, achieving a theoretical regret bound of \\(O({s_0}^2 \\log^2(dT)\\), where \\(d\\) represents the number of covariates, \\(s_0 &lt;&lt; d\\) signifies the number of pertinent covariates, and \\(T\\) indicates the total number of users. Additionally, they conducted empirical experiments to validate the robustness of their methodology.\nWithin the finance sector, bandits have been instrumental in reshaping the landscape of portfolio optimization. Portfolio optimization is an approach to designing a portfolio based on the investor’s return and risk criteria, which fits the exploration-exploitation nature of the bandit problems. (Shen et al. 2015) utilized multi-armed bandits to exploit correlations between the instruments. They constructed orthogonal portfolios and integrated them with the UCB policy to achieve a cumulative regret bound of \\(\\frac{8n}{\\Delta*} \\ln(m) + 5n\\), where \\(n\\), \\(m\\), and \\(\\Delta*\\) denotes the number of available assets, total time steps, and the gap between the best-expected reward and the expected reward. On the other hand, (Huo and Fu 2017) focused on risk-awareness online portfolio optimization by incorporating a compute of the minimum spanning tree in the bipartite graph, which encodes a combination of financial institutions and assets that helps diversify and reduce exposure to systematic risk during the financial crisis.\nDynamic pricing, also known as demand-based pricing, refers to the strategy of setting flexible prices for products or services based on current market demands. The application of bandits in dynamic pricing offers a systematic approach to making real-time pricing decisions while balancing the trade-off between exploring new price points and exploiting known optimal prices. (Misra, Schwartz, and Abernethy 2019) proposed a policy where the company has only incomplete demand information. They derived an algorithm that balances immediate and future profits by combining multi-armed bandits with partial identification of consumer demand from economic theory.\nare essential components of numerous online platforms, guiding users through vast content landscapes to deliver tailored suggestions. These systems are instrumental in platforms like e-commerce sites, streaming platforms, and social media networks. However, the challenge of effectively recommending items to users is non-trivial, given the dynamic nature of user preferences and the vast amount of content available.\nOne of the most significant challenges in recommendation systems is the \"cold start\" problem. This issue arises when a new user joins a platform, and the system has limited or no information about the user’s preferences. Traditional recommendation algorithms struggle in such scenarios since they rely on historical user-item interactions. As discussed in (Zhou et al. 2017), the bandit setting is particularly suitable for large-scale recommender systems with a vast number of items. By continuously exploring user preferences and exploiting known interactions, bandit-based recommender systems can quickly adapt to new users, ensuring relevant recommendations in a few interactions. The continuous exploration inherent in bandit approaches also means that as a user’s preferences evolve, the system can adapt, ensuring that recommendations remain relevant. Recommending content that is up to date is also another important aspect of a recommendation system. In (Bouneffouf, Bouzeghoub, and Gançarski 2012), the concept of \"freshness\" in content is explored through the lens of the bandit problem. The Freshness-Aware Thompson Sampling algorithm introduced in this study aims to manage the recommendation of fresh documents according to the user’s risk of the situation.\nDialogue systems, often termed conversational agents or chatbots, aim to simulate human-like conversations with users. These systems are deployed across various platforms, including customer support, virtual assistants, and entertainment applications, and they are crucial for enhancing user experience and engagement. Response selection is fundamental to creating a natural and coherent dialogue flow. Traditional dialogue systems rely on a predefined set of responses or rules, which can make interactions feel scripted and inauthentic. In (Liu et al. 2018), the authors proposed a contextual multi-armed bandit model for online learning of response selection. Specifically, they utilized bidirectional LSTM to produce the distributed representations of a dialogue context and responses and customized the Thompson sampling method.\nTo create a more engaging and dynamic interaction, there’s a growing interest in developing pro-active dialogue systems that can initiate conversations without user initiation. (perez and Silander 2018) proposed a novel approach to this challenge with contextual bandits. By introducing memory models into the bandit framework, the system can recall past interactions, making its proactive responses more contextually relevant. Their contributions include the Contextual Attentive Memory Network, which implements a differentiable attention mechanism over past interactions.\n(Upadhyay et al. 2019) addressed the challenge of orchestrating multiple independently trained dialogue agents or skills in a unified system. They attempted online posterior dialogue orchestration, defining it as selecting the most suitable subset of skills in response to a user’s input, which studying a context-attentive bandit model that operates under a skill execution budget, ensuring efficient and accurate response selection.\nAnomaly detection refers to the task of identifying samples that behave differently from the majority. In (Ding, Li, and Liu 2019), the authors delve into anomaly detection in an interactive setting, allowing the system to actively engage with human experts through a limited number of queries about genuine anomalies. The goal is to present as many true anomalies to the human expert as possible after a fixed query budget is used up. They applied the multi-armed contextual bandit framework to address this issue. This algorithm adeptly integrates both nodal attributes and node dependencies into a unified model, efficiently managing the exploration-exploitation trade-off during anomaly queries.\nThere are many challenges associated with contextual bandits. The first challenge is that each action only reveals the reward for that particular action. Therefore, the algorithm has to work with incomplete information. This leads to the dilemma of exploitation versus exploration: when should the algorithm choose the best-known option versus trying new options for potentially better outcomes? Another significant challenge for contextual bandits is using context effectively. The context the environment gives needs to be explored to figure out which action is best for each context.\nThe overarching goal in systems designed for recommending options of high value to users is to achieve an optimal balance between exploration and exploitation. This dual approach is crucial in environments where user preferences and needs are dynamic and diverse. Exploration refers to the process of seeking out new options, learning about untried possibilities, and gathering fresh information that could lead to high-value recommendations. In contrast, exploitation involves utilizing existing knowledge and past experiences to recommend the best options currently known. This balance is key to maintaining a system that continuously adapts to changing user preferences while ensuring the reliability of its recommendations.\nA key observation in such systems is the dual role of users as both producers and consumers of information. Each user’s experience contributes valuable data that informs future recommendations for others. For instance, platforms like Waze, Netflix, and Trip Advisor rely heavily on user input and feedback. Waze uses real-time traffic data from drivers to recommend optimal routes; Netflix suggests movies and shows based on viewing histories and ratings; Trip Advisor relies on traveler reviews to guide future tourists. In these examples, the balance between gathering new information (exploration) and recommending the best-known options (exploitation) is dynamically managed to enhance user experience and satisfaction. This approach underscores the importance of user engagement in systems where monetary incentives are not (or can not be) the primary driver.\nRecommendation systems often face the challenge of overcoming user biases that can lead to a narrow exploration of options. Users come with preconceived notions and preferences, which can cause them to overlook potentially valuable options that initially appear inferior or unaligned with their interests. This predisposition can significantly limit the effectiveness of recommendation systems, as users might miss out on high-value choices simply due to their existing biases.\nTo counteract this, it is crucial for recommendation systems to actively incentivize exploration among users. One innovative approach to achieve this is through the strategic use of information asymmetry. By controlling and selectively presenting information, these systems can guide users to explore options they might not typically consider. This method aims to reveal the true potential of various options by nudging users out of their comfort zones and encouraging a broader exploration of available choices. An important note here is that the system is not lying to users - it only selectively reveals information it has.\nThe concept of incentivizing exploration becomes even more complex when considering different types of users. For instance, systems often encounter short-lived users who have little to gain from contributing to the system’s learning process, as their interactions are infrequent or based on immediate needs. Similarly, some users may operate under a ‘greedy’ principle, primarily seeking immediate gratification rather than contributing to the long-term accuracy and effectiveness of the system. In such scenarios, managing information asymmetry can be a powerful tool. By selectively revealing information, recommendation systems can create a sense of novelty and interest, prompting even the most transient or self-interested users to engage in exploration, thereby enhancing the system’s overall knowledge base and recommendation quality.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decisions</span>"
    ]
  },
  {
    "objectID": "src/chap5.html#preferential-bayesian-optimization",
    "href": "src/chap5.html#preferential-bayesian-optimization",
    "title": "4  Decisions",
    "section": "4.2 Preferential Bayesian Optimization",
    "text": "4.2 Preferential Bayesian Optimization\nThe traditional Bayesian optimization (BO) problem is described as follows. There is a black-box objective function \\(g: \\mathcal{X} \\rightarrow \\Re\\) defined on a bounded subset \\(\\mathcal{X} \\subseteq \\Re^q\\) such that direct queries to the function are expensive or not possible. However, we would like to solve the global optimization problem of finding \\(\\mathbf{x}_{\\min }=\\arg \\min _{\\mathbf{x} \\in \\mathcal{X}} g(\\mathbf{x})\\). This is highly analogous to modeling human preferences, since it is the case that direct access to a human’s latent preference function is not possible but we would still like to find its optimum, such as in A/B tests or recommender systems.\nWe approach this problem for human preferences with Preferential Bayesian Optimization (PBO), as the key difference is that we are able to query the preference function through pairwise comparisons of data points, i.e. duels. This is a form of indirect observation of the objective function, which models real-world scenarios closely: we commonly need to to optimize a function via data about preferences. With humans, it has been demonstrated that we are better at evaluating differences rather than absolute magnitudes (Kahneman and Tversky 1979) and therefore PBO models can be applied in various contexts.\n\n4.2.1 Problem statement\nThe problem of finding the optimum of a latent preference function defined on \\(\\mathcal{X}\\) can be reduced to determining a sequence of duels on \\(\\mathcal{X} \\times \\mathcal{X}\\). From each duel \\(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right] \\in\\) \\(\\mathcal{X} \\times \\mathcal{X}\\) we obtain binary feedback \\(\\{0,1\\}\\) indicating whether or not \\(\\mathbf{x}\\) is preferred over \\(\\mathbf{x}^{\\prime}\\) (\\(g(\\mathbf{x}) &lt; g(\\mathbf{x}^{\\prime})\\)). We consider that \\(\\mathbf{x}\\) is the winner of the duel if the output is \\(\\{1\\}\\) and that \\(\\mathbf{x}^{\\prime}\\) wins the duel if the output is \\(\\{0\\}\\). The aim is to find \\(\\mathbf{x}_{\\min }\\) by reducing as much as possible the number of queried duels.\nThe key idea in PBO is to learn a preference function in the space of duels using a Gaussian process. We define a joint reward \\(f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)\\) on each duel which is never directly observed. Instead, the feedback we obtain after each pair is a binary output \\(y \\in\\) \\(\\{0,1\\}\\) indicating which of the two inputs is preferred. One definition of f we will use (though others are possible) is \\(f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=g\\left(\\mathbf{x}^{\\prime}\\right)-g(\\mathbf{x})\\). The more \\(\\mathbf{x}^{\\prime}\\) is preferred over \\(\\mathbf{x}\\), the bigger the reward.\nWe define the model of preference using a Bernoulli likelihood, where \\(p\\left(y=1 \\mid\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)\\) and \\(p\\left(y=0 \\mid\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=\\pi_f\\left(\\left[\\mathbf{x}^{\\prime}, \\mathbf{x}\\right]\\right)\\) for some inverse link function \\(\\pi: \\Re \\times \\Re \\rightarrow[0,1]\\). \\(\\pi_f\\) has the property that \\(\\pi_f\\left(\\left[\\mathbf{x}^{\\prime}, \\mathbf{x}\\right]\\right)=1-\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)\\). A natural choice for \\(\\pi_f\\) is the logistic function \\[\\label{eq:bernoulli_pref}\n\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=\\sigma\\left(f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)\\right)=\\frac{1}{1+e^{-f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)}},\\] but others are possible. Therefore we have that for any duel \\(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\) in which \\(g(\\mathbf{x}) \\leq g\\left(\\mathbf{x}^{\\prime}\\right)\\) it holds that \\(\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right) \\geq 0.5\\). \\(\\pi_f\\) is a preference function that maps each query \\(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\) to the probability of having a preference on the left input \\(\\mathbf{x}\\) over the right input \\(\\mathbf{x}^{\\prime}\\).\nWhen we marginalize over the right input \\(\\mathbf{x}^{\\prime}\\) of \\(f\\) (is this correct?), the global minimum of \\(f\\) in \\(\\mathcal{X}\\) coincides with \\(\\mathbf{x}_{\\min }\\). We also introduce the definition of the Copeland score function for a point \\(\\mathbf{x}\\) as \\[S(\\mathbf{x})=\\operatorname{Vol}(\\mathcal{X})^{-1} \\int_{\\mathcal{X}} \\mathbb{I}_{\\left\\{\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right) \\geq 0.5\\right\\}} d \\mathbf{x}^{\\prime}\\] where \\(\\operatorname{Vol}(\\mathcal{X})=\\int_{\\mathcal{X}} d \\mathbf{x}^{\\prime}\\) is a normalizing constant that bounds \\(S(\\mathbf{x})\\) in the interval \\([0,1]\\). If \\(\\mathcal{X}\\) is a finite set, the Copeland score is simply the proportion of duels that a certain element \\(\\mathbf{x}\\) will win with probability larger than 0.5. A soft variant we will use instead of the Copeland score is the soft-Copeland score, defined as \\[\\label{eq:soft-copeland}\nC(\\mathbf{x})=\\operatorname{Vol}(\\mathcal{X})^{-1} \\int_{\\mathcal{X}} \\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right) d \\mathbf{x}^{\\prime}\\] where the probability function \\(\\pi_f\\) is integrated over \\(\\mathcal{X}\\). This score aims to capture the average probability of \\(\\mathbf{x}\\) being the winner of a duel.\nWe define the Condorcet winner \\(\\mathbf{x}_c\\) as the point with maximal soft-Copeland score. Note that this corresponds to the global minimum of \\(f\\), since the defining integral takes maximum value for points \\(\\mathbf{x} \\in \\mathcal{X}\\) where \\(f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=\\) \\(g\\left(\\mathbf{x}^{\\prime}\\right)-g(\\mathbf{x})&gt;0\\) or all \\(\\mathbf{x}^{\\prime}\\), occurring only if \\(\\mathbf{x}_c\\) is a minimum of \\(f\\). Therefore, if the preference function \\(\\pi_f\\) can be learned by observing the results of duels then our optimization problem of finding the minimum of \\(f\\) can be solved by finding the Condorcet winner of the Copeland score.\n\n\n4.2.2 Acquisition Functions\nWe describe several acquisition functions for sequential learning of the Condorcet winner. Our dataset \\(\\mathcal{D}=\\left\\{\\left[\\mathbf{x}_i, \\mathbf{x}_i^{\\prime}\\right], y_i\\right\\}_{i=1}^N\\) represents the \\(N\\) duels that have been performed so far. We aim to define a sequential policy \\(\\alpha\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right] ; \\mathcal{D}_j, \\theta\\right)\\) for querying duels, where \\(\\theta\\) is a vector of model hyper-parameters, in order to find the minimum of the latent function \\(g\\) as quickly as possible. Using Gaussian processes (GP) for classification with our dataset \\(\\mathcal{D}\\) allows us to perform inference over \\(f\\) and \\(\\pi_f\\).\n\nPure Exploration\nThe output variable \\(y_{\\star}\\) of a prediction follows a Bernoulli distribution with probability given by the preference function \\(\\pi_f\\). To carry out exploration as a policy, one method is to search for the duel where GP is most uncertain about the probability of the outcome (has the highest variance of \\(\\sigma\\left(f_{\\star}\\right)\\) ), which is the result of transforming out epistemic uncertainty about \\(f\\), modeled by a GP, through the logistic function. The first order moment of this distribution coincides with the expectation of \\(y_{\\star}\\) but its variance is \\[\\begin{aligned}\n\\mathbb{V}\\left[\\sigma\\left(f_{\\star}\\right)\\right] & =\\int\\left(\\sigma\\left(f_{\\star}\\right)-\\mathbb{E}\\left[\\sigma\\left(f_{\\star}\\right)\\right]\\right)^2 p\\left(f_{\\star} \\mid \\mathcal{D},\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right) d f_{\\star} \\\\\n& =\\int \\sigma\\left(f_{\\star}\\right)^2 p\\left(f_{\\star} \\mid \\mathcal{D},\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right) d f_{\\star}-\\mathbb{E}\\left[\\sigma\\left(f_{\\star}\\right)\\right]^2\n\\end{aligned}\\] which explicitly takes into account the uncertainty over \\(f\\). Hence, pure exploration of duels space can be carried out by maximizing \\[\\alpha_{\\mathrm{PE}}\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right] \\mid \\mathcal{D}_j\\right)=\\mathbb{V}\\left[\\sigma\\left(f_{\\star}\\right)\\left|\\left[\\mathbf{x}_{\\star}, \\mathbf{x}_{\\star}^{\\prime}\\right]\\right| \\mathcal{D}_j\\right] .\\]\nNote that in this case, duels that have been already visited will have a lower chance of being visited again even in cases in which the objective takes similar values in both players. In practice, this acquisition functions requires computation of an intractable integral, that we approximate using Monte-Carlo.\n\n\nPrincipled Optimistic Preferential Bayesian Optimization (POP-BO)\nIn a slightly modified problem setup (Xu et al. 2024), the algorithm tries to solve for the MLE \\(\\hat{g}\\) and its confidence set \\(\\mathcal{B}_g\\) where \\(g\\) is the ground truth black-box function. Assumptions include that \\(g\\) is a member of a reproducing kernel Hilbert space (RKHS) \\(\\mathcal{H}_k\\) for some kernel function \\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}\\), and \\(\\|g\\|_k \\leq B\\) so that \\(\\mathcal{B}_g = \\left\\{\\tilde{g} \\in \\mathcal{H}_k \\mid\\|\\tilde{g}\\|_k \\leq B\\right\\}\\). Similarly defining \\(f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=g\\left(\\mathbf{x}^{\\prime}\\right)-g(\\mathbf{x})\\), we model the preference function with a Bernoulli distribution as in Equation [eq:bernoulli_pref] and also assume that probabilities follow the Bradley-Terry model, i.e. \\[\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=\\sigma\\left(f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)\\right)=\\frac{e^{g(\\mathbf{x})}}{e^{g(\\mathbf{x})}+e^{g\\left(\\mathbf{x^{\\prime}}\\right)}}\\]\nThe update rule for MLE \\(\\hat{g}\\) is (equation 8,6,5) \\[\\begin{aligned}\n\\hat{g}_t^{\\text {MLE }}&:= \\arg \\underset{\\tilde{g} \\in \\mathcal{B}^t_g}{\\max}\\ell_t(\\tilde{g}) \\\\\n\\ell_t(\\tilde{g}) &:= \\log \\prod_{\\tau=1}^t y_\\tau \\pi_{\\tilde{f}}([\\mathbf{x_\\tau}, \\mathbf{x^{\\prime}_\\tau}])+\\left(1-y_\\tau\\right)\\left(1-\\pi_{\\tilde{f}}([\\mathbf{x_\\tau}, \\mathbf{x^{\\prime}_\\tau}])\\right) \\\\\n&=\\sum_{\\tau=1}^t \\log \\left(\\frac{e^{\\tilde{g}(\\mathbf{x_\\tau})} y_\\tau+e^{\\tilde{g}(\\mathbf{x_\\tau^\\prime})}\\left(1-y_\\tau\\right)}{e^{\\tilde{g}(\\mathbf{x_\\tau})}+e^{\\tilde{g}(\\mathbf{x_\\tau^\\prime})}}\\right) \\\\\n&=\\sum_{\\tau=1}^t\\left(\\tilde{g}(\\mathbf{x_\\tau}) y_\\tau+\\tilde{g}(\\mathbf{x_\\tau^\\prime})\\left(1-y_\\tau\\right)\\right)-\\sum_{\\tau=1}^t \\log \\left(e^{\\tilde{g}(\\mathbf{x_\\tau})}+e^{\\tilde{g}(\\mathbf{x_\\tau^\\prime})}\\right)\n\\end{aligned}\\]\n(Eq 22 shows how to represent this as a convex optimisation problem so that it can be solved)\nThe update rule for the confidence set \\(\\mathcal{B}_f^{t+1}\\) is, (eq 9, 10?)\n\\[\\begin{aligned}\n&\\forall \\epsilon, \\delta &gt; 0 \\\\\n&\\mathcal{B}_g^{t+1}:=\\left\\{\\tilde{g} \\in \\mathcal{B}_g \\mid \\ell_t(\\tilde{g}) \\geq \\ell_t\\left(\\hat{g}_t^{\\mathrm{MLE}}\\right)-\\beta_1(\\epsilon, \\delta, t)\\right\\}\n\\end{aligned}\\] where \\[\\beta_1(\\epsilon, \\delta, t):=\\sqrt{32 t B^2 \\log \\frac{\\pi^2 t^2 \\mathcal{N}\\left(\\mathcal{B}_f, \\epsilon,\\|\\cdot\\|_{\\infty}\\right)}{6 \\delta}}+ C_L \\epsilon t=\\mathcal{O}\\left(\\sqrt{t \\log \\frac{t \\mathcal{N}\\left(\\mathcal{B}_f, \\epsilon,\\|\\cdot\\|_{\\infty}\\right)}{\\delta}}+\\epsilon t\\right),\\] with \\(C_L\\) a constant independent of \\(\\delta, t\\) and \\(\\epsilon\\). \\(\\epsilon\\) is typically chosen to be \\(1 / T\\), where T is the running horizon of the algorithm. This satisfies the theorem that, \\[\\mathbb{P}\\left(g \\in \\mathcal{B}_g^{t+1}, \\forall t \\geq 1\\right) \\geq 1-\\delta .\\]\nIntuitively, the confidence set \\(\\mathcal{B}_g^{t+1}\\) includes the functions with the log-likelihood value that is only ‘a little worse’ than the maximum likelihood estimator, and the theorem states that \\(\\mathcal{B}_g^{t+1}\\) contains the ground-truth function \\(g\\) with high probability.\nInner level optimization in Line 4 of the algorithm can also be represented as a convex optimisation problem so that it can be solved, Eq 24, 25. The outer optimisation can be solved using grid search or Eq 26 for medium size problems.\n\n\nGiven the initial point \\(\\mathbf{x_0} \\in \\mathcal{X}\\) and set \\(\\mathcal{B}_g^1 = \\mathcal{B}_g\\) Set the reference point \\(\\mathbf{x_t^{\\prime}} = \\mathbf{x_{t-1}}\\) Compute \\(\\mathbf{x_t} \\in \\arg\\max_{\\mathbf{x} \\in \\mathcal{X}} \\max_{\\tilde{g} \\in \\mathcal{B}_g^t} (\\tilde{g}(\\mathbf{x}) - \\tilde{g}(\\mathbf{x_t^{\\prime}}))\\), with the inner optimal function denoted as \\(\\tilde{g}_t\\) Obtain the output of the duel \\(y_t\\) and append the new data point to \\(\\mathcal{D}_t\\) Update the maximum likelihood estimator \\(\\hat{g}_t^{\\mathrm{MLE}}\\) and the posterior confidence set \\(\\mathcal{B}_g^{t+1}\\).\n\n\n\n\nqEUBO: Decision-Theoretic EUBO\nqEUBO (Astudillo et al. 2023) derives an acquisition function that extends duels to \\(q&gt;2\\) options which we call queries. Let \\(X=\\left(\\mathbf{x_1}, \\ldots, \\mathbf{x_q}\\right) \\in \\mathcal{X}^q\\) denote a query containing two points or more, and let \\(g: \\mathcal{X} \\rightarrow \\Re\\) be the latent preference function. Then after \\(n\\) user queries, we define the expected utility of the best option (qEUBO) as \\[\\mathrm{qEUBO}_n(X)=\\mathbb{E}_n\\left[\\max \\left\\{g\\left(x_1\\right), \\ldots, g\\left(x_q\\right)\\right\\}\\right].\\]\nWe now show that qEUBO is one-step Bayes optimal, meaning that each step chooses the query that maximises the expected utility received by the human. For a query \\(X \\in \\mathcal{X}^q\\), let \\[V_n(X)=\\mathbb{E}_n\\left[\\max _{x \\in \\mathbb{X}} \\mathbb{E}_{n+1}[g(x)] \\mid X_{n+1}=X\\right] .\\] Then \\(V_n\\) defines the expected utility received if an additional query \\(X_{n+1}=X\\) is performed, and maximizing \\(V_n\\) is one-step Bayes optimal. Since \\(\\max _{x \\in \\mathbb{X}} \\mathbb{E}_n[f(x)]\\) does not depend on \\(X_{n+1}\\), we can also equivalently maximize \\[\\mathbb{E}_n\\left[\\max _{x \\in \\mathbb{X}} \\mathbb{E}_{n+1}[g(x)]-\\max _{x \\in \\mathbb{X}} \\mathbb{E}_n[g(x)] \\mid X_{n+1}=X\\right],\\] which takes the same form as the knowledge gradient acquisition function (Wu and Frazier 2018) in standard Bayesian optimization.\n\\(V_n\\) involves a nested stochastic optimization task, while qEUBO is a much simpler policy. When human responses are noise-free, we are able to use qEUBO as a sufficient policy due to the following theorem:\n\n\\[\\underset{X \\in \\mathbb{X}^q}{\\operatorname{argmax}} \\mathrm{qEUBO}_n(X) \\subseteq \\underset{X \\in \\mathbb{X}^q}{\\operatorname{argmax}} V_n(X) .\\]\n\n\nProof. Proof. For a query \\(X \\in \\mathcal{X}^q\\), let \\(x^{+}(X, i) \\in \\operatorname{argmax}_{x \\in \\mathbb{X}} \\mathbb{E}_n[g(x) \\mid(X, i)]\\) and define \\(X^{+}(X)=\\) \\(\\left(x^{+}(X, 1), \\ldots, x^{+}(X, q)\\right)\\).\nClaim 1 \\(V_n(X) \\leq \\mathrm{qEUBO}_n\\left(X^{+}(X)\\right) .\\) We see that \\[\\begin{aligned}\nV_n(X) & =\\sum_{i=1}^q \\mathbf{P}_n(r(X)=i) \\mathbb{E}_n[g\\left(x^{+}(X, i)\\right) ] \\\\\n& \\leq \\sum_{i=1}^q \\mathbf{P}_n(r(X)=i) \\mathbb{E}_n[\\max _{i=1, \\ldots, q} g(x^{+}(X, i))] \\\\\n& =\\mathbb{E}_n\\left[\\max _{i=1, \\ldots, q} g\\left(x^{+}(X, i)\\right)\\right] \\\\\n& =\\mathrm{qEUBO}_n\\left(X^{+}(X)\\right),\n\\end{aligned}\\] as claimed.\nClaim 2 \\(\\mathrm{qEUBO}_n(X) \\leq V_n(X) .\\) For any given \\(X \\in \\mathbb{X}^q\\) we have \\[\\mathbb{E}_n\\left[f\\left(x_{r(X)}\\right) \\mid(X, r(X))\\right] \\leq \\max _{x \\in \\mathbb{X}} \\mathbb{E}_n[f(x) \\mid(X, r(X))] .\\] Since \\(f\\left(x_{r(X)}\\right)=\\max _{i=1, \\ldots, q} f\\left(x_i\\right)\\), taking expectations over \\(r(X)\\) on both sides obtains the required result.\nNow building on the arguments above, let \\(X^* \\in \\operatorname{argmax}_{X \\in \\mathbb{X}^q} \\mathrm{qEUBO}_n(X)\\) and suppose for contradiction that \\(X^* \\notin \\operatorname{argmax}_{X \\in \\mathbb{X}^q} V_n(X)\\). Then, there exists \\(\\widetilde{X} \\in \\mathbb{X}^q\\) such that \\(V_n(\\widetilde{X})&gt;V_n\\left(X^*\\right)\\). We have \\[\\begin{aligned}\n\\operatorname{qEUBO}_n\\left(X^{+}(\\tilde{X})\\right) & \\geq V_n(\\tilde{X}) \\\\\n& &gt;V_n\\left(X^*\\right) \\\\\n& \\geq \\operatorname{qEUBO}_n\\left(X^*\\right) \\\\\n& \\geq \\operatorname{qEUBO}_n\\left(X^{+}(\\tilde{X})\\right) .\n\\end{aligned}\\]\nThe first inequality follows from (1). The second inequality is due to our supposition for contradiction. The third inequality is due to (2). Finally, the fourth inequality holds since \\(X^* \\in \\operatorname{argmax}_{X \\in \\mathbb{X}^q} \\mathrm{qEUBO}_n(X)\\). This contradiction concludes the proof. ◻\n\nTherefore a sufficient condition for following one-step Bayes optimality is by maximizing \\(\\text{qEUBO}_n\\).\nIn experiments that were ran comparing qEUBO to other state-of-the-art acquisition functions, qEUBO consistently outperformed on most problems and was closely followed by qEI and qTS. These results also extended to experiments with multiple options when \\(q&gt;2\\). In fact, there is faster convergence in regret when using more options in human queries. [Prove Theorem 3: Regret analysis]\n\n\nqEI: Batch Expected Improvement\n\\[\\begin{aligned}\n\\mathrm{qEI}= & \\mathbb{E}_{\\mathbf{y}}\\left[\\left(\\max _{i \\in[1, \\ldots, q]}\\left(\\mu_{\\min }-y_i\\right)\\right)_{+}\\right] \\\\\n= & \\sum_{i=1}^q \\mathbb{E}_{\\mathbf{y}}\\left(\\mu_{\\min }-y_i \\mid y_i \\leq \\mu_{\\min }, y_i \\leq y_j \\forall j \\neq i\\right) \\\\\n& p\\left(y_i \\leq \\mu_{\\min }, y_i \\leq y_j \\forall j \\neq i\\right) .\n\\end{aligned}\\]\n\n\nqTS: Batch Thompson Sampling\n\n\nInitial data \\(\\mathcal{D}_{\\mathcal{I}(1)}=\\{(\\mathbf{x}_i, y_i)\\}_{i \\in \\mathcal{I}(1)}\\) Compute current posterior \\(p(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{\\mathcal{I}(t)})\\) Sample \\(\\boldsymbol{\\theta}\\) from \\(p(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{\\mathcal{I}(t)})\\) Select \\(k \\leftarrow \\arg \\max_{j \\notin \\mathcal{I}(t)} \\mathbb{E}[y_j \\mid \\mathbf{x}_j, \\boldsymbol{\\theta}]\\) Collect \\(y_k\\) by evaluating \\(f\\) at \\(\\mathbf{x}_k\\) \\(\\mathcal{D}_{\\mathcal{I}(t+1)} \\leftarrow \\mathcal{D}_{\\mathcal{I}(t)} \\cup \\{(\\mathbf{x}_k, y_k)\\}\\)\n\n\n\n\nInitial data \\(\\mathcal{D}_{\\mathcal{I}(1)}=\\{\\mathbf{x}_i, y_i\\}_{i \\in \\mathcal{I}(1)}\\), batch size \\(S\\) Compute current posterior \\(p(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{\\mathcal{I}(t)})\\) Sample \\(\\boldsymbol{\\theta}\\) from \\(p(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{\\mathcal{I}(t)})\\) Select \\(k(s) \\leftarrow \\arg \\max_{j \\notin \\mathcal{I}(t)} \\mathbb{E}[y_j \\mid \\mathbf{x}_j, \\boldsymbol{\\theta}]\\) \\(\\mathcal{D}_{\\mathcal{I}(t+1)} = \\mathcal{D}_{\\mathcal{I}(t)} \\cup \\{\\mathbf{x}_{k(s)}, y_{k(s)}\\}_{s=1}^S\\)\n\n\n\n\n\n4.2.3 Regret Analysis\n\nqEUBO Regret\nWith the definition of Bayesian simple regret, we have that qEUBO converges to zero at a rate of \\(o(1/n)\\), i.e.\n\n\\[\\label{th:quebo_regret}\n\\mathbb{E}\\left[f\\left(x^*\\right)-f\\left(\\widehat{x}_n^*\\right)\\right]=o(1 / n)\\]\n\nwhere \\(x^*=\\operatorname{argmax}_{x \\in \\mathrm{X}} f(x)\\) and \\(\\widehat{x}_n^* \\in \\operatorname{argmax}_{x \\in \\mathrm{X}} \\mathbb{E}_n[f(x)]\\).\nThis theorem holds under the following assumptions:\n\n\\(f\\) is injective \\(\\mathbf{P}(f(x)=f(y))=0\\) for any \\(x, y \\in \\mathbb{X}\\) with \\(x \\neq y\\).\n\\(f\\) represents the preferred option \\(\\exists a&gt;1 / 2\\) s.t. \\(\\mathbf{P}\\left(r(X) \\in \\operatorname{argmax}_{i=1, \\ldots, 2} f\\left(x_i\\right) \\mid f(X)\\right) \\geq a \\forall\\) \\(X=\\left(x_1, x_2\\right) \\in \\mathbb{X}^2\\) with \\(x_1 \\neq x_2\\) almost surely under the prior on \\(f\\).\nExpected difference in utility is proportional to probability of greater utility \\(\\exists \\Delta \\geq \\delta&gt;0\\) s.t. \\(\\forall \\mathcal{D}^{(n)} \\text{and} \\forall x, y \\in \\mathbb{X}\\) (potentially depending on \\(\\mathcal{D}^{(n)}\\)), \\[\\delta \\mathbf{P}^{(n)}(f(x)&gt;f(y)) \\leq \\mathbb{E}^{(n)}\\left[\\{f(x)-f(y)\\}^{+}\\right] \\leq \\Delta \\mathbf{P}^{(n)}(f(x)&gt;f(y))\\] almost surely under the prior on \\(f\\).\n\nFurther lemmas leading to a proof of Theorem [th:quebo_regret] is given in (Astudillo et al. 2023) Section B.\n\n\nqEI Regret\nThe following theorem shows that, under the same assumptions used for qEUBO regret, simple regret of qEI can fail to converge to 0.\n\nThere exists a problem instance (i.e., \\(\\mathbb{X}\\) and Bayesian prior distribution over f) satisfying the assumptions described in Theorem [th:quebo_regret] such that if the sequence of queries is chosen by maximizing qEI, then \\(\\mathbb{E}\\left[f\\left(x^*\\right)-\\right.\\) \\(\\left.f\\left(\\widehat{x}_n^*\\right)\\right] \\geq R\\) for all \\(n\\), for a constant \\(R&gt;0\\).\n\n\nProof. Proof. Let \\(X = \\{1, 2, 3, 4\\}\\) and consider the functions \\(f_i:X \\rightarrow R\\), for \\(i=1,2,3,4\\), given by \\(f_i(1) = -1\\) and \\(f_i(2) = 0\\) for all \\(i\\), and \\[\\begin{aligned}\n    f_1(x) = \\begin{cases}\n    1, &\\ x=3\\\\\n    \\frac{1}{2}, &\\ x=4\n    \\end{cases},\n\\hspace{0.5cm}\nf_2(x) = \\begin{cases}\n    \\frac{1}{2}, &\\ x=3\\\\\n    1, &\\ x=4\n    \\end{cases},\n\\hspace{0.5cm}\nf_3(x) = \\begin{cases}\n    -\\frac{1}{2}, &\\ x=3\\\\\n    -1, &\\ x=4\n    \\end{cases},\n\\hspace{0.5cm}\nf_4(x) = \\begin{cases}\n    -1, &\\ x=3\\\\\n    -\\frac{1}{2}, &\\ x=4\n    \\end{cases}.\n\\end{aligned}\\]\nLet \\(p\\) be a number with \\(0 &lt; p &lt; 1/3\\) and set \\(q=1-p\\). We consider a prior distribution on \\(f\\) with support \\(\\{f_i\\}_{i=1}^4\\) such that \\[\\begin{aligned}\np_i = Pr(f=f_i) =\n    \\begin{cases}\n        p/2, i =1,2,\\\\\n        q/2, i=3,4.\n    \\end{cases}\n\\end{aligned}\\] We also assume the user’s response likelihood is given by \\(Pr(r(X)=1\\mid f(x_1) &gt; f(x_2)) = a\\) for some \\(a\\) such that \\(1/2 &lt; a &lt; 1\\),\nLet \\(D^{(n)}\\) denote the set of observations up to time \\(n\\) and let \\(p_i^{(n)} = Pr(f=f_i \\mid \\mathbb{E}^{(n)})\\) for \\(i=1,2,3,4\\). We let the initial data set be \\(\\mathcal{D}^{(0)} = \\{(X^{(0)}, r^{(0)})\\}\\), where \\(X^{(0)}= (1,2)\\). We will prove that the following statements are true for all \\(n\\geq 0\\).\n\n\\(p_i^{(n)} &gt; 0\\) for \\(i=1,2,3,4\\).\n\\(p_1^{(n)} &lt; \\frac{1}{2}p_3^{(n)}\\) and \\(p_2^{(n)} &lt; \\frac{1}{2}p_4^{(n)}\\).\n\\(\\arg \\max_{x\\in\\mathcal{X}}\\mathbb{E}^{(n)}[f(x)]=\\{2\\}\\).\n\\(\\arg \\max_{X\\in\\mathcal{X}^2}\\text{qEI}^{(n)}(X) = \\{(3, 4)\\}\\).\n\nWe prove this by induction over \\(n\\). We begin by proving this for \\(n=0\\). Since \\(f_i(1) &lt; f_i(2)\\) for all \\(i\\), the posterior distribution on \\(f\\) given \\(\\mathcal{D}^{(0)}\\) remains the same as the prior; i.e., \\(p_i^{(0)} = p_i\\) for \\(i=1,2,3,4\\). Using this, statements 1 and 2 can be easily verified. Now note that \\(\\mathbb{E}^{(0)}[f(1)]=-1\\), \\(\\mathbb{E}^{(0)}[f(2)]=0\\), and \\(\\mathbb{E}^{(0)}[f(3)] = \\mathbb{E}^{(0)}[f(4)] = \\frac{3}{2}(p - q)\\). Since \\(p &lt; q\\), it follows that \\(\\arg \\max_{x\\in\\mathcal{X}}\\mathbb{E}^{(n)}[f(x)]=\\{2\\}\\); i.e., statement 3 holds. Finally, since \\(\\max_{x\\in\\{1,2\\}}\\mathbb{E}^{(0)}[f(x)] = 0\\), the qEI acquisition function at time \\(n=0\\) is given by \\(\\text{qEI}^{(0)}(X) = \\mathbb{E}^{(0)}[\\{\\max\\{f(x_1), f(x_2)\\}\\}^+]\\). A direct calculation can now be performed to verify that statement 4 holds. This completes the base case.\nNow suppose statements 1-4 hold for some \\(n\\geq 0\\). Since \\(X^{(n+1)} = (3, 4)\\), the posterior distribution on \\(f\\) given \\(D^{(n+1)}\\) is given by \\[\\begin{aligned}\np_i^{(n+1)} \\propto \\begin{cases}\n                        p_i^{(n)}\\ell, \\ i=1,3,\\\\\n                         p_i^{(n)} (1 - \\ell), \\ i=2,4,\n                        \\end{cases}\n\\end{aligned}\\] where \\[\\ell = a I\\{r^{(n+1)} = 1\\} + (1-a)I\\{r^{(n+1)} = 2\\}.\\] Observe that \\(0&lt; \\ell &lt; 1\\) since \\(0 &lt; a &lt; 1\\). Thus, \\(\\ell &gt; 0\\) and \\(1-\\ell &gt; 0\\). Since \\(p_i^{(n)} &gt; 0\\) by the induction hypothesis, it follows from this that \\(p_i^{(n+1)} &gt; 0\\) for \\(i=1,2,3,4\\). Moreover, since \\(p_i^{(n+1)} \\propto p_i^{(n)}\\ell\\) for \\(i=1,3\\) and \\(p_1^{(n)} &lt; \\frac{1}{2}p_3^{(n)}\\) by the induction hypothesis, it follows that \\(p_1^{(n+1)} &lt; \\frac{1}{2}p_3^{(n+1)}\\). Similarly, \\(p_2^{(n+1)} &lt; \\frac{1}{2}p_4^{(n+1)}\\). Thus, statements 1 and 2 hold at time \\(n+1\\).\nNow observe that \\[\\begin{aligned}\n    \\mathbb{E}^{(n+1)}[f(3)] &= p_1^{(n+1)} + \\frac{1}{2}p_2^{(n+1)} - \\frac{1}{2}p_3^{(n+1)} - p_4^{(n+1)}\\\\\n    &= \\left(p_1^{(n+1)} - \\frac{1}{2}p_3^{(n+1)}\\right) + \\left(\\frac{1}{2}p_2^{(n+1)} - p_4^{(n+1)}\\right)\\\\\n    &\\leq \\left(p_1^{(n+1)} - \\frac{1}{2}p_3^{(n+1)}\\right) + \\left(p_2^{(n+1)} - \\frac{1}{2}p_4^{(n+1)}\\right)\\\\\n    &\\leq 0,\n\\end{aligned}\\] where the last inequality holds since \\(p_1^{(n+1)} &lt; \\frac{1}{2}p_3^{(n+1)}\\) and \\(p_2^{(n+1)} &lt; \\frac{1}{2}p_4^{(n+1)}\\). Similarly, we see that \\(\\mathbb{E}^{(n+1)}[f(4)] \\leq 0\\). Since \\(\\mathbb{E}^{(n+1)}[f(1)]=-1\\) and \\(\\mathbb{E}^{(n+1)}[f(2)]=0\\), it follows that \\(\\arg \\max_{x\\in\\mathcal{X}}\\mathbb{E}^{(n+1)}[f(x)]=\\{2\\}\\); i.e., statement 3 holds at time \\(n+1\\).\nSince \\(\\max_{x\\in\\mathcal{X}}\\mathbb{E}^{(0)}[f(x)] = 0\\), the qEI acquisition function at time \\(n+1\\) is given by \\(\\text{qEI}^{(n+1)}(X) = \\mathbb{E}^{(n+1)}[\\{\\max\\{f(x_1), f(x_2)\\}\\}^+]\\). Since \\(f(1) \\leq f(x)\\) almost surely under the prior for all \\(x\\in\\mathcal{X}\\), there is always a maximizer of qEI that does not contain \\(1\\). Thus, to find the maximizer of qEI, it suffices to analyse its value at the pairs \\((2, 3)\\), \\((3,4)\\) and \\((4,2)\\). We have \\[\\text{qEI}^{(n+1)}(2, 3) = p_1^{(n+1)} + 1/2 p_2^{(n+1)},\\] \\[\\operatorname{qEI}^{(n+1)}(3, 4) = p_1^{(n+1)} + p_2^{(n+1)}\\] and \\[\\operatorname{qEI}^{(n+1)}(4, 2) = 1/2p_1^{(n+1)} + p_2^{(n+1)}.\\] Since \\(p_1^{(n+1)} &gt; 0\\) and \\(p_2^{(n+1)} &gt; 0\\), it follows that \\(\\arg \\max_{X \\in X^2}\\text{qEI}^{(n+1)}(X) = \\{(3, 4)\\}\\), which concludes the proof by induction.\nFinally, since \\(\\arg \\max_{x\\in X}\\mathbb{E}^{(n)}[f(x)]=\\{2\\}\\) for all \\(n\\), the Bayesian simple regret of qEI is given by \\[\\begin{aligned}\n    \\mathbb{E}\\left[f(x^*) - f(2)\\right] &= \\sum_{i=1}p_i\\left(\\max_{x\\in X}f_i(x) - f_i(2)\\right)\\\\\n    &= p\n\\end{aligned}\\] for all \\(n\\). ◻\n\n\n\nPOP-BO Regret\nCommonly used kernel functions within the RKHS are:\n\nLinear: \\[k(x, \\bar{x})=x^{\\top} \\bar{x} .\\]\nSquared Exponential (SE): \\[k(x, \\bar{x})=\\sigma_{\\mathrm{SE}}^2 \\exp \\left\\{-\\frac{\\|x-\\bar{x}\\|^2}{l^2}\\right\\},\\] where \\(\\sigma_{\\mathrm{SE}}^2\\) is the variance parameter and \\(l\\) is the lengthscale parameter.\nMatérn: \\[k(x, \\bar{x})=\\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left(\\sqrt{2 \\nu} \\frac{\\|x-\\bar{x}\\|}{\\rho}\\right)^\\nu K_\\nu\\left(\\sqrt{2 \\nu} \\frac{\\|x-\\bar{x}\\|}{\\rho}\\right),\\] where \\(\\rho\\) and \\(\\nu\\) are the two positive parameters of the kernel function, \\(\\Gamma\\) is the gamma function, and \\(K_\\nu\\) is the modified Bessel function of the second kind. \\(\\nu\\) captures the smoothness of the kernel function.\n\nWith the definition of Bayesian simple regret, we have the following theorem defining the regret bound:\n\nWith probability at least \\(1-\\delta\\), the cumulative regret of POP-BO satisfies, \\[R_T=\\mathcal{O}\\left(\\sqrt{\\beta_T \\gamma_T^{f f^{\\prime}} T}\\right),\\] where \\[\\beta_T=\\beta(1 / T, \\delta, T)=\\mathcal{O}\\left(\\sqrt{T \\log \\frac{T \\mathcal{N}\\left(\\mathcal{B}_f, 1 / T,\\|\\cdot\\|_{\\infty}\\right)}{\\delta}}\\right).\\]\n\nThe guaranteed convergence rate is characterised as:\n\n[]{#th: popbo_converge label=“th: popbo_converge”} Let \\(t^{\\star}\\) be defined as in Eq. (19). With probability at least \\(1-\\delta\\), \\[f\\left(x^{\\star}\\right)-f\\left(x_{t^{\\star}}\\right) \\leq \\mathcal{O}\\left(\\frac{\\sqrt{\\beta_T \\gamma_T^{f f^{\\prime}}}}{\\sqrt{T}}\\right)\\]\n\nTheorem [th: popbo_converge] highlights that by minimizing the known term \\(2\\left(2 B+\\lambda^{-1 / 2} \\sqrt{\\beta\\left(\\epsilon, \\frac{\\delta}{2}, t\\right)}\\right) \\sigma_t^{f f^{\\prime}}\\left(\\left(x_t, x_t^{\\prime}\\right)\\right)\\), the reported final solution \\(x_{t^{\\star}}\\) has a guaranteed convergence rate.\nFurther kernel-specific regret bounds for POP-BO are calculated as follows:\n\nSetting \\(\\epsilon=1 / T\\) and running our POP-BO algorithm in Alg. 1,\n\nIf \\(k(x, y)=\\langle x, y\\rangle\\), we have, \\[R_T=\\mathcal{O}\\left(T^{3 / 4}(\\log T)^{3 / 4}\\right) .\\]\nIf \\(k(x, y)\\) is a squared exponential kernel, we have, \\[R_T=\\mathcal{O}\\left(T^{3 / 4}(\\log T)^{3 / 4(d+1)}\\right) .\\]\nIf \\(k(x, y)\\) is a Matérn kernel, we have, \\[\\left.R_T=\\mathcal{O}\\left(T^{3 / 4}(\\log T)^{3 / 4} T^{\\frac{d}{\\nu}\\left(\\frac{1}{4}+\\frac{d+1}{4+2(d+1)^d / \\nu}\\right.}\\right)\\right).\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decisions</span>"
    ]
  },
  {
    "objectID": "src/chap5.html#case-study-1-foundation-models-for-robotics",
    "href": "src/chap5.html#case-study-1-foundation-models-for-robotics",
    "title": "4  Decisions",
    "section": "4.3 Case Study 1: Foundation Models for Robotics",
    "text": "4.3 Case Study 1: Foundation Models for Robotics\nModern foundation models have been ubiquitous in discussions of powerful, general purpose AI systems that can accomplish myriad tasks across many disciplines such as programming, medicine, law, open question-answering and much more, with rapidly increasing capabilities (Bommasani et al. 2022). However, despite successes from large labs in controlled environments (Brohan et al. 2023) foundation models have not seen ubiquitous use in robotics due to shifting robot morphology, lack of data, and the sim to real gap in robotics (Walke et al. 2023). For this subsection we explore two promising approaches known as R3M and Voltron which are the first to leverage pre-training on vast amounts of data towards performance improvement on downstream robotic tasks despite the aforementioned issues (Nair et al. 2022; Karamcheti et al. 2023).\nR3M represents a significant advancement in the field of robotic manipulation and learning. This model diverges from traditional approaches that rely on training from scratch within the same domain on the same robot data as instead it leverags pretraining on large datasets, akin to the practices in computer vision and natural language processing (NLP) where models are trained on diverse, large-scale datasets to create reusable, general-purpose representations. The core principle behind R3M is its training methodology. It is pre-trained on a wide array of human videos, encompassing various activities and interactions. This diverse dataset enables the model to capture a broad spectrum of physical interactions and dynamics, which are crucial for effective robotic manipulation known as EGO4D (Grauman et al. 2022). However, prior papers could not fit this dataset well, and R3M leveraged. The training utilizes a unique objective that combines time contrastive learning, video-language alignment, and a sparsity penalty. This objective ensures that R3M not only understands the temporal dynamics of scenes (i.e., how states transition over time) but also focuses on semantically relevant features, such as objects and their interrelations, while maintaining a compact and efficient representation. What sets R3M apart in the realm of robotics is its efficiency and effectiveness in learning from a limited amount of data. The model demonstrates remarkable performance in learning tasks in the real world with minimal human supervision – typically less than 10 minutes. This is a stark contrast to traditional models that require extensive and often prohibitively large datasets for training. Furthermore, R3M’s pre-trained nature allows for its application across a variety of tasks and environments without the need for retraining from scratch, making it a versatile tool in robotic manipulation. The empirical results from using R3M are compelling, leading to a 10% improvement over training from a pretrained image-net model, self-supervised approaches such as MoCo or even CLIP (Deng et al. 2009; He et al. 2020; Radford et al. 2021). Note however, that R3m does not use any language data which leaves quite a bit of supervision to be desired.\nBuilding off the success of R3M, Voltron proposes a further extension of leveraging self-supervision and advancements in foundation models, and multi-modality. Voltron takes on an intuitive and simple dual use objective, where the trained model alternates between predicting the task in an image through natural language and classifying images based on a natural text label. This forces a nuanced understanding of both modalities (Radford et al. 2021). Voltron’s approach is distinguished by its versatility and depth of learning. It is adept at handling a wide range of robotic tasks, from low-level spatial feature recognition to high-level semantic understanding required in language-conditioned imitation and intent scoring. This flexibility makes it suitable for various applications in robotic manipulation, from grasping objects based on descriptive language to performing complex sequences of actions in response to verbal instructions. The authors rigorously test Voltron in scenarios such as dense segmentation for grasp affordance prediction, object detection in cluttered scenes, and learning multi-task language-conditioned policies for real-world manipulation with up to 15% improvement over baselines. In each of these domains, Voltron has shown a remarkable ability to outperform existing models like MVP and R3M, showcasing its superior adaptability and learning capabilities (Xiao et al. 2022). Moreover, Voltron’s framework allows for a balance between encoding low-level and high-level features, which is critical in the context of robotics. This balance enables the model to excel in both control tasks and those requiring deeper semantic understanding, offering a comprehensive solution in the realm of robotic vision and manipulation.\nVoltron stands as a groundbreaking approach in the field of robotics, offering a language-driven, versatile, and efficient approach to learning and manipulation. Its ability to seamlessly integrate visual and linguistic data makes it a potent tool in the ever-evolving landscape of robotic technology, with potential applications that extend far beyond current capabilities. Interesting the authors show Voltron does not beat R3M off the shelf but only when trained on similar amounts of data. Nevertheless, Voltron’s success in diverse tasks and environments heralds a new era in robotic manipulation, where language and vision coalesce to create more intelligent, adaptable, and capable robotic systems.\nOn the note of applying AL to RL and environment settings, there have been many recent papers that have attempted to extend this to more modern RL environments. For example, the paper “When to Ask for Help” (Xie et al. 2022) examines the intersection of autonomous and AL. Instead of just expecting an RL agent to autonomously solve a task, making the assumption that an agent could get stuck and need human input to get “unstuck” is a key insight of the paper. In general, there has been an emphasis in recent literature in robotics on not just blindly using demonstration data as a form of human input, but rather actively querying a human and using this to better synthesize correct actions.\nAL holds promise for enhancing AI models in real-world scenarios, yet several challenges persist. This discussion aims to provide an overview of these challenges.\nTask-Specific Considerations: For certain tasks, the input space of a model may have some rare yet extremely important pockets which may never be discovered by AL and may cause severe blindspots in the model. In medical imaging for instance, there can be rare yet critical diseases. Designing AL strategies for medical image analysis must prioritize rare classes, such as various forms of cancers. Oftentimes, collecting data around those rare classes is not a recommendation of the AL process because these examples constitute heavy distribution drifts from the input distribution a model has seen.\nComplex Task Adaptation: AL has predominantly been adopted for simple classification tasks, leaving more other types of tasks (generative ones for instance), less explored. In Natural Language Processing, tasks like natural language inference, question-answering pose additional complexities that affect the direct application of the AL process. While machine translation has seen AL applications, generation tasks in NLP require more thorough exploration. Challenges arise in obtaining unlabeled data, particularly for tasks with intricate inputs.\nUnsupervised and Semi-Supervised Approaches: In the presence of large datasets without sufficient labels, unsupervised and semi-supervised approaches become crucial. These methods offer a means to extract information without relying on labeled data for every data point, potentially revolutionizing fields like medical image analysis. There is an ongoing need for methods that combine self/semi-supervised learning with AL.\nAlgorithm Scalability: Scalability is a critical concern for online AL algorithms, particularly when dealing with large datasets and high-velocity data streams. The computational demands of AL can become prohibitive as data volume increases, posing challenges for practical deployment. Issues of catastrophic forgetting and model plasticity further complicate scalability, requiring careful consideration in algorithm design.\nLabeling Quality Assurance: The effectiveness of most online AL strategies hinges on the quality of labeled data. Ensuring labeling accuracy in real-world scenarios is challenging, with human annotators prone to errors, biases, and diverse interpretations. Addressing imperfections in labeling through considerations of oracle imperfections becomes essential in real-life AL applications. Solutions for cleaning up data and verifying its quality need to be more aggressively pursued.\nData Drift Challenges: Real-world settings introduce data drift, where distributions shift over time, challenging models to adapt for accurate predictions. These shifts can impact the quality of labeled data acquired in the AL process. For example, the criterion or proxy used for selecting informative instances may be thrown off when the distribution a model is trained on, and the distribution we want it to perform well on, are too far away from one another.\nEvaluation in Real-Life Scenarios: While AL methods are often evaluated assuming access to ground-truth labels, the real motivation for AL lies in label scarcity. Assessing the effectiveness of AL strategies becomes challenging in real-life scenarios where ground-truth labels may be limited. In other words, one may verify the goodness of an AL algorithm within the lab, but once the algorithm is deployed for improving all sorts of models on all sorts of data distributions, verifying whether AL is actually improving a model is tricky, especially when collecting and labeling data from the target distribution is expensive and defeats the purpose of using AL in the first place.\nBy systematically addressing these challenges, the field of AL in AI can progress towards more effective and practical applications. In summary, AL is a promising modern tool to model training that presents potential benefits. As was mentioned at the start, there are numerous approaches that can be employed by AL, starting from reducing error of model’s prediction, reducing variance, to more conformal predictions. The flavor of AL heavily depends on the applications, which include robotics, LLM, autonomous vehicles, and more. We discussed in more detail how to perform AL for variance reduction in the case of predicting kinematics of the robotic arms, which showed decrease in MSE as well as more stable reduction in it. Next we talked about using AL for reducing the number of comparisons required to create a ranking of objects, and the examples discussed were able to achieve that but with some loss in the prediction accuracy. Finally, we discussed how AL can be used for modeling of reward functions within a dynamical system, which demonstrated improvements in performance and time required to achieve it. For a more hands-on experience with AL and demonstrated example, we encourage the readers to explore a blogpost by Max Halford (Halford 2023).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decisions</span>"
    ]
  },
  {
    "objectID": "src/chap6.html",
    "href": "src/chap6.html",
    "title": "5  Aggregation",
    "section": "",
    "text": "5.1 Social Choice Theory and Implications for AI Preference Aggregation\nIn many applications, human preferences must be aggregated across multiple individuals to determine a collective decision or ranking. This process is central to social choice theory, which provides a mathematical foundation for preference aggregation. Unlike individual preference modeling, which focuses on how a single person makes decisions, social choice theory addresses the challenge of combining multiple preference profiles into a single coherent outcome. A social welfare function (SWF) takes as input each individual’s preference ranking over a set of alternatives and produces a social ranking of those alternatives. A related concept is a social choice function (SCF), which selects a single winning alternative given individuals’ preferences. Many voting rules can be seen as social choice functions that aim to reflect the group’s preferences. Formally, let \\(N=\\{1,2,\\dots,n\\}\\) be a set of \\(n\\) voters (agents) and \\(A=\\{a_1,\\dots,a_m\\}\\) a set of \\(m\\) alternatives (with \\(m \\ge 3\\)). Each voter \\(i\\) has a preference order \\(\\succ_i\\) over \\(A\\). A social choice function is a mapping \\(f: (\\succ_1,\\dots,\\succ_n)\\mapsto A\\) that picks a winning alternative for each possible profile of individual preferences. A social welfare function is a mapping that produces a complete societal ranking \\(\\succ^*\\) of the alternatives. The central question is: can we design an aggregation rule that faithfully represents individual preferences while satisfying certain fairness or rationality axioms?\nMany common voting rules illustrate different methods of aggregation, each with its own merits and vulnerabilities:\nHowever, preference aggregation is not always straightforward. The Condorcet paradox illustrates that majority preferences can be cyclic (rock-paper-scissors style), so that no single alternative is majority-preferred to all others, violating transitivity. Different voting rules can yield different winners on the same profile, highlighting how the choice of rule influences the outcome. To guide the design of social choice functions, several desirable properties or axioms have been proposed. Three classical fairness criteria are:\nAdditionally, we assume an unrestricted domain (universal admissibility): individuals may have any transitive preference ordering over the \\(m\\) alternatives (no restrictions like single-peaked preferences unless explicitly imposed). One might hope that a fair voting rule exists that satisfies all the above properties for three or more alternatives. Surprisingly, a seminal negative result shows this is impossible.\nArrow’s Impossibility Theorem (Arrow 1951) is a cornerstone of social choice theory. It states that when there are three or more alternatives (\\(m\\ge 3\\)), no social welfare function can simultaneously satisfy Unanimity, IIA, and Non-dictatorship – unless it is a trivial dictatorial rule. In other words, any aggregation mechanism that is not dictatorial will inevitably violate at least one of the fairness criteria. The theorem is usually proven by contradiction: assuming a social welfare function satisfies all conditions, one can show that one voter’s preferences always decide the outcome, hence the rule is dictatorial. Intuitively, Arrow’s theorem is driven by the possibility of preference cycles in majority voting. Even if individual preferences are transitive, aggregated majorities can prefer \\(A\\) to \\(B\\), \\(B\\) to \\(C\\), and \\(C\\) to \\(A\\) in a cycle, as in the Condorcet paradox. Under Unanimity and IIA, the social ranking must locally match these pairwise preferences, but this produces a contradiction with transitivity unless one voter’s ranking is given overriding authority. A sketch of Arrow’s proof is as follows: one shows that under the axioms, the social ranking between any two alternatives \\(x\\) and \\(y\\) must agree with some particular voter’s preference (the “pivotal” voter for that pair). With IIA, the identity of the pivotal voter must be the same across all pairs of alternatives, otherwise by cleverly constructing profiles one can derive a conflict. This single pivotal voter then effectively dictates the entire social order, violating Non-dictatorship. Hence, the axioms are incompatible.\nArrow’s Impossibility Theorem has profound implications: it formalizes the inherent trade-offs in designing any fair aggregation scheme. In practice, different voting rules relax one or more of Arrow’s conditions. For instance, Borda count violates IIA (since introducing or removing an irrelevant alternative can change the point totals), while a dictatorship violates fairness blatantly. The theorem suggests that every practical voting system must sacrifice at least one of the ideal fairness criteria. It also motivated the exploration of alternative frameworks (such as allowing interpersonal comparisons of utility or cardinal preference aggregation) to escape the impossibility by weakening assumptions.\nComplementing Arrow’s theorem, the Gibbard–Satterthwaite theorem focuses on incentives and strategic manipulation in voting systems (Gibbard 1973; Satterthwaite 1975). It considers any deterministic social choice function \\(f\\) that chooses a single winner from the set of \\(m\\ge 3\\) alternatives. The theorem states that if \\(f\\) is strategy-proof (incentive compatible) and onto (its range of outcomes is the entire set of alternatives), then \\(f\\) must be dictatorial. Strategy-proofness (also called truthfulness or dominant-strategy incentive compatibility) means that no voter can ever benefit by misrepresenting their true preferences, regardless of what others do. In other words, reporting their genuine ranking is a (weakly) dominant strategy for each voter. The theorem implies that for any realistic voting rule where every alternative can possibly win, either one voter effectively decides the outcome (a dictatorship) or else the rule is susceptible to strategic manipulation by voters. The Gibbard–Satterthwaite theorem tells us that every non-dictatorial voting rule for 3 or more alternatives is manipulable: there will exist some election scenario where a voter can gain a more preferred outcome by voting insincerely (i.e. not according to their true preferences). For example, in a simple plurality vote, a voter whose true favorite is a long-shot candidate might vote for a more viable candidate to avoid a worst-case outcome (“lesser of two evils” voting). In a Borda count election, voters might strategically raise a competitor in their ranking to push down an even stronger rival. The only way to avoid all such strategic voting incentives is to have a dictatorship or limit the choice set to at most two alternatives.\nThe proof of Gibbard–Satterthwaite is non-trivial, but one can outline the idea: Given a non-dictatorial and onto rule \\(f\\), one shows there exist at least three distinct outcomes that can result from some preference profiles. By carefully constructing profiles and using the onto property, one finds a situation where a single voter can change the outcome by switching their order of two candidates, demonstrating manipulability. The theorem is robust – even if we allow ties or weaker conditions, similar impossibility results hold (Gibbard’s 1978 extension handles randomized rules). The practical takeaway is that all meaningful voting protocols encourage tactical voting in some situations. Nonetheless, certain systems are considered “harder to manipulate” or more resistant due to complexity or uncertainty. For instance, while STV (ranked-choice voting) can be manipulated in theory, determining a beneficial strategic vote can be NP-hard in worst cases, which arguably provides some practical deterrence to manipulation (Bartholdi, Tovey, and Trick 1989).\nArrow’s and Gibbard–Satterthwaite’s theorems highlight the limitations any preference aggregation method must face. In domains like reinforcement learning from human feedback (RLHF) and AI alignment, we also aggregate preferences – often preferences of multiple human evaluators or preferences revealed in pairwise comparisons – to guide machine learning systems. While these settings sometimes use cardinal scores or learned reward functions (escaping the strict ordinal framework of Arrow’s theorem), the spirit of these impossibility results still applies: there is no perfect way to aggregate human opinions without trade-offs.\nFor example, aggregating human feedback to train a model may run into inconsistencies analogous to preference cycles, especially when feedback comes from diverse individuals with different values. A simple majority vote over preferences might yield unstable or unfair outcomes if some annotators are systematically in the minority. Weighting votes by some credibility or expertise (weighted voting) can improve outcomes but raises the question of how to set the weights without introducing dictator-like influence. Recent research has proposed methods like jury learning – which integrates dissenting voices by having a panel (“jury”) of models or human subgroups whose aggregated judgment guides the learning (Gordon et al. 2022) – to ensure minority preferences are not entirely ignored. Another perspective is social choice in AI alignment, which suggests using social choice theory to design AI systems that respect a plurality of human values instead of collapsing everything into a single objective. In pluralistic value alignment, instead of forcing a single “best” solution, an AI might present a diverse set of options or behave in a way that reflects a distribution of values. This approach aims to preserve the diversity of human preferences rather than always aggregating to one monolithic preference. For instance, a conversational AI might be designed to recognize multiple acceptable responses (each aligning with different value systems) rather than one canonical “aligned” response for a given query.\nThese considerations are especially relevant in generative AI and large language models, where training involves human preference data. If we aggregate feedback naively, we might overfit to the majority preference and lose minority perspectives (a form of tyranny of the majority). On the other hand, trying to satisfy everyone can lead to indecision or an incoherent objective. The impossibility results remind us there is no free lunch: we must carefully decide which properties to prioritize (e.g. giving more weight to expert annotators versus preserving broader fairness, or balancing consistency vs inclusivity). Designing aggregation mechanisms for AI that reflect collective human values is an ongoing challenge. It often involves insights from traditional voting theory (to understand trade-offs and failure modes) combined with machine learning techniques (to model and learn from preference data). In summary, social choice theory provides cautionary guidance as we build systems that learn from human preferences: we need to be conscious of which fairness criteria we relax and be transparent about the compromises being made in any preference aggregation pipeline.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Aggregation</span>"
    ]
  },
  {
    "objectID": "src/chap6.html#social-choice-theory-and-implications-for-ai-preference-aggregation",
    "href": "src/chap6.html#social-choice-theory-and-implications-for-ai-preference-aggregation",
    "title": "5  Aggregation",
    "section": "",
    "text": "Plurality: Each voter names their top choice; the alternative with the most votes wins.\nBorda Count: Voters rank all alternatives, and points are assigned based on the position in each ranking. For example, with \\(m\\) alternatives, a voter’s top-ranked alternative gets \\(m-1\\) points, the second-ranked gets \\(m-2\\), and so on down to 0. The Borda score of an alternative is the sum of points from all voters, and the winner is the alternative with the highest total score.\nSingle Transferable Vote (STV): Voters rank choices, and the count proceeds in rounds. In each round, the alternative with the fewest votes is eliminated and those votes are transferred to the next preferred remaining alternative on each ballot, until one candidate has a majority.\nCondorcet Methods: These look for a candidate that wins in all pairwise majority contests against other alternatives (the Condorcet winner), if such an alternative exists.\n\n\n\nUnanimity (Pareto efficiency): If all individuals strictly prefer one alternative \\(x\\) over another \\(y\\) (i.e. \\(x \\succ_i y\\) for every voter \\(i\\)), then the group ranking should prefer \\(x\\) over \\(y\\) as well (\\(x \\succ^* y\\)).\nIndependence of Irrelevant Alternatives (IIA): The social preference between any two alternatives \\(x\\) and \\(y\\) should depend only on the individual preferences between \\(x\\) and \\(y\\). In other words, if we change individuals’ rankings of other “irrelevant” alternatives (not \\(x\\) or \\(y\\)) in any way, the group’s relative ordering of \\(x\\) and \\(y\\) should remain unchanged.\nNon-dictatorship: The aggregation should not simply follow a single individual’s preference regardless of others. There is no voter \\(i\\) who always gets their top choice as the social choice (or whose rankings always become the social ranking), irrespective of other voters’ preferences.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Aggregation</span>"
    ]
  },
  {
    "objectID": "src/chap6.html#single-item-auctions",
    "href": "src/chap6.html#single-item-auctions",
    "title": "5  Aggregation",
    "section": "5.2 Mechanism Design",
    "text": "5.2 Mechanism Design\nWhile voting rules aggregate ordinal rank preferences to select a social outcome, another class of preference aggregation occurs in economic settings like auctions and general mechanism design. Here individuals reveal their valuations (numerical utilities) for outcomes, and the mechanism chooses an outcome (such as an allocation of goods) and possibly payments. Mechanism design asks: how can we design rules so that rational agents, acting in their own interest, end up revealing information that leads to a socially desirable outcome? A central concept is incentive compatibility – the mechanism should be designed so that each participant’s best strategy is to act according to their true preferences (e.g. bid their true value). In this section, we focus on auctions as a prime example of preference aggregation with money, and highlight classical results including Vickrey–Clarke–Groves (VCG) mechanisms and Myerson’s optimal auction.\nConsider a single-item auction with one item for sale and \\(n\\) bidders. Bidder \\(i\\) has a private valuation \\(v_i\\) for the item (how much the item is worth to them). Each bidder’s goal is to maximize their own utility, defined as \\(v_i - p_i\\) if they win and pay price \\(p_i\\), or \\(0\\) if they do not win (assuming quasilinear utility where money is the transferable utility). The auction’s task is to allocate the item to one of the bidders and possibly determine payments. We can think of an auction as a mechanism that asks each bidder for a “message” (typically a bid representing how much they are willing to pay), then selects a winner and a price based on the bids. A key objective might be social welfare maximization – allocate the item to the bidder who values it most (maximizing \\(v_i\\) of the winner). Another possible objective is revenue maximization for the seller – choose the allocation and price to maximize the seller’s expected payment.\nA classic result in auction theory is that to maximize social welfare in a single-item private-value setting, one should award the item to the highest valuer – and this can be done in an incentive-compatible way by using a second-price auction. The Vickrey second-price auction works as follows: (1) All bidders submit sealed bids \\(b_1, b_2, \\ldots, b_n\\). (2) The bidder with the highest bid wins the item. (3) The price paid by the winner is the second-highest bid. For example, if the bids are \\((2,\\, 6,\\, 4,\\, 1)\\) (in some currency units), the highest bid is \\(6\\) (by bidder 2, say) and the second-highest is \\(4\\). Bidder 2 wins the item and pays \\(4\\).\nUnder this mechanism, it turns out that bidding truthfully \\(b_i = v_i\\) is a dominant strategy for each bidder. In other words, the auction is dominant-strategy incentive compatible (DSIC): no matter what others do, a bidder maximizes their expected utility by reporting their true valuation. The intuition is as follows. If bidder \\(i\\) bids lower than their true value (i.e. \\(b_i &lt; v_i\\)), and if their true value was actually the highest, they risk losing the item even though they value it more than the price they would have paid – a missed opportunity for positive utility. Bidding higher than their value (\\(b_i &gt; v_i\\)) cannot help them win in any situation where bidding truthfully wouldn’t (it could only make a difference if their true \\(v_i\\) wasn’t the highest but they tried to win anyway); and if they do win with an inflated bid, they might end up paying the second-highest bid which could be above their true value, yielding negative utility. By bidding exactly \\(v_i\\), if they win, it means all other bids were lower, so \\(v_i\\) is at least as high as the second-highest bid \\(p\\) they pay – guaranteeing non-negative utility \\(v_i - p \\ge 0\\). If they lose, it means someone else had a higher bid (hence higher value, if others are truthful), so bidder \\(i\\) wouldn’t have gained anyway. This argument, made rigorous by Vickrey (Vickrey 1961), establishes that truth-telling is a dominant strategy in the second-price auction. As a consequence, when everyone bids truthfully, the item is allocated to the bidder with the highest \\(v_i\\), achieving maximum social surplus (allocative efficiency). The second-price auction is thus an elegant mechanism that aligns individual incentives with social welfare maximization.\nIt is worth contrasting this with a first-price auction, where the winner pays their own bid. In a first-price auction, bidders have an incentive to bid below their true value (to avoid the winner’s curse of paying too much), in a Nash equilibrium that involves bid shading. The first-price auction can still allocate to the highest valuer in equilibrium, but only through strategic behavior (and it is not DSIC). By charging the second-highest bid, the Vickrey auction removes the incentive to shade bids, since the price does not directly depend on one’s own bid beyond the fact of winning or losing.\nSo far, we discussed auctions aimed at maximizing social welfare. In many cases, the auctioneer (seller) is interested in maximizing revenue. A foundational result by Roger Myerson (1981) provides a characterization of optimal auctions (those that maximize the seller’s expected revenue) for single-item settings under certain assumptions (Myerson 1981). The problem can be formulated as follows: suppose each bidder’s private value \\(v_i\\) is drawn independently from a known distribution \\(F_i\\) (for simplicity, assume identical distribution \\(F\\) for all bidders, i.i.d.). We seek a mechanism (allocation rule and payment rule) that maximizes the seller’s expected payment, subject to incentive compatibility and individual rationality (participants should not expect negative utility from truthful participation).\nMyerson’s theorem states that the optimal auction in such a setting is a threshold auction characterized by virtual valuations. Define the virtual value for a bidder with value \\(v\\) as \\(\\varphi(v) = v - \\frac{1-F(v)}{f(v)}\\), where \\(f\\) is the probability density function of \\(F\\) (assuming it is continuous). An assumption called regularity (which holds for many distributions) is that \\(\\varphi(v)\\) is non-decreasing in \\(v\\). Myerson showed that the revenue-maximizing strategy is: treat \\(\\varphi(v)\\) as the effective “score” of a bid, allocate the item to the bidder with the highest non-negative virtual value (if all virtual values are negative, allocate to no one), and charge them the smallest value they could have such that they would still win (the payment is essentially the critical bid where \\(\\varphi\\) of that bid equals the second-highest virtual value or the zero cutoff). In practice, for i.i.d. bidders, this reduces to: there is an optimal reserve price \\(r\\) such that you sell to the highest bidder if and only if their bid \\(b_{\\max} \\ge r\\); if sold, the price is the max of the second-highest bid and \\(r\\).\nIn the case of \\(n\\) bidders with values i.i.d. uniform on \\([0,1]\\) (which is a regular distribution), one can compute the optimal reserve price. The virtual value function for uniform \\([0,1]\\) is \\(\\varphi(v) = v - \\frac{1-v}{1} = 2v - 1\\). Setting \\(\\varphi(v)\\ge 0\\) gives \\(v \\ge 0.5\\). So Myerson’s mechanism says: don’t sell the item if all bids are below 0.5; otherwise, sell to the highest bidder at at least 0.5. This is exactly a second-price auction with a reserve of \\(r=0.5\\). Our earlier example implicitly demonstrated this: with two uniform(0,1) bidders, the optimal auction sets a reserve price of \\(0.5\\) and yields a certain expected revenue. We can break down the cases: - With probability \\(1/4\\), both bidders have values below \\(0.5\\) (each below 0.5 with probability 1/2), in which case nobody wins and revenue is 0. - With probability \\(1/4\\), both bidders have \\(v &gt; 0.5\\). In this case, the second-price auction with reserve will sell to the highest bidder at the max of the second-highest value and 0.5. Given both \\(v_1, v_2 &gt; 0.5\\), the expected second-highest value (conditional on both &gt;0.5) is \\(\\frac{2}{3}\\) (in fact, the order statistics of two uniforms on [0.5,1] give mean of min = 2/3). So in this case the expected price is the second-highest value (since that will exceed 0.5), about 0.667. - With probability \\(1/2\\), one bidder is above 0.5 and the other below. In that case, the one above 0.5 wins at price equal to the reserve 0.5 (since the second-highest bid is the reserve).\nTaking the expectation, the seller’s expected revenue is \\(0*(1/4) + (2/3)*(1/4) + (1/2*1/2) = 0 + 1/6 + 1/4 = 5/12 \\approx 0.417\\). This is higher than the expected revenue without a reserve. In fact, without a reserve (just a plain second-price with two bidders uniform [0,1]), one can compute the expected revenue is \\(1/3 \\approx 0.333\\) (the second order statistic’s expectation). Thus, the reserve has increased revenue. Myerson’s theory tells us that indeed the second-price auction with an optimally chosen reserve maximizes revenue among all DSIC mechanisms for this setting. A notable special case result is that when bidder distributions are i.i.d. and regular, an optimal auction is essentially “allocatively efficient with a reserve price” – i.e. aside from possibly excluding low-value bidders via a reserve, it allocates to the highest remaining bid.\nMyerson’s work also highlighted the gap between revenue maximization and welfare maximization. The price of optimality (in revenue) is that the seller might sometimes forego efficient allocation (e.g. not selling despite a willing buyer, in order to preserve a high reserve price strategy). In contrast, Vickrey’s auction always allocates efficiently but may not maximize revenue.\nAn interesting insight in auction theory is that increasing competition can yield more revenue than fine-tuning the auction mechanism. The Bulow–Klemperer theorem (Bulow and Klemperer 1996) demonstrates that, under certain regularity assumptions, a simple welfare-maximizing auction with one extra bidder outperforms the optimal auction with fewer bidders. Specifically, for i.i.d. bidders with a regular distribution \\(F\\), the expected revenue of a second-price auction with \\(n+1\\) bidders is at least as high as the expected revenue of the Myerson-optimal auction with \\(n\\) bidders. In formula form:\n\\[\n\\mathbb{E}_{v_1,\\ldots,v_{n+1} \\sim F}[\\text{Rev}^{\\text{(second-price)}}(n+1 \\text{ bidders})] \\geq\n\\mathbb{E}_{v_1,\\ldots,v_n \\sim F}[\\text{Rev}^{\\text{(optimal)}}(n \\text{ bidders})] \\,.\n\\tag{4.1}\\label{eq-eq3.64}\n\\]\nThis result suggests that, in practice, having more participants (competition) is often more valuable than exploiting detailed knowledge of bidder distributions. As a corollary, a policy recommendation is that a seller is usually better off using a simple auction design (like a Vickrey auction or other transparent mechanism) and putting effort into attracting more bidders, rather than using a complex optimal mechanism that might discourage participation.\nVickrey’s second-price auction can be generalized to multiple items and more complex outcomes by the Vickrey–Clarke–Groves (VCG) mechanism. The VCG mechanism is a cornerstone of mechanism design that provides a general solution for implementing socially efficient outcomes (maximizing total stated value) in dominant strategies, for a broad class of problems. It works for any scenario where agents have quasilinear utilities and we want to maximize the sum of valuations.\nIn a general mechanism design setting, let \\(\\Omega\\) be the set of possible outcomes. Each agent \\(i\\) has a private valuation function \\(v_i(\\omega)\\) for outcomes \\(\\omega \\in \\Omega\\) (the amount of utility, in money terms, that \\(i\\) gets from outcome \\(\\omega\\)). Agents report bids \\(b_i(\\omega)\\) (which we hope equal \\(v_i(\\omega)\\) if they are truthful). The mechanism then chooses an outcome \\(\\omega^* \\in \\Omega\\) to maximize the reported total value:\n\\[\n\\omega^* = \\arg\\max_{\\omega \\in \\Omega} \\sum_{i=1}^n b_i(\\omega) \\,,\n\\]\ni.e. \\(\\omega^*\\) is the outcome that would be socially optimal if the \\(b_i\\) were true values. To induce truth-telling, VCG sets payments such that each agent pays the externality they impose on others by their presence. Specifically, one convenient form of the VCG payment for agent \\(i\\) is:\n\\[\np_i(b) = \\max_{\\omega \\in \\Omega} \\sum_{j \\neq i} b_j(\\omega)\\;-\\;\\sum_{j \\neq i} b_j(\\omega^*) \\,,\n\\]\nwhich can be interpreted as: what would the total value of others be if \\(i\\) were not present (first term, maximizing without \\(i\\)) minus the total value others actually get in the chosen outcome \\(\\omega^*\\). Equivalently, we can write the payment as the agent’s bid for the chosen outcome minus a rebate term:\n\\[\np_i(b) = b_i(\\omega^*) \\;-\\; \\Big[\\sum_{j=1}^n b_j(\\omega^*) - \\max_{\\omega \\in \\Omega} \\sum_{j \\neq i} b_j(\\omega)\\Big] \\,. \\tag{4.2}\\label{eq-eq3.67}\n\\]\nThis formula (which in single-item auction reduces to second-price logic) ensures that each agent’s net payoff is \\(v_i(\\omega^*) - p_i = \\max_{\\omega} \\sum_{j\\neq i} v_j(\\omega) + v_i(\\omega^*) - \\sum_{j\\neq i} v_j(\\omega^*)\\). All terms except \\(v_i(\\omega^*)\\) cancel out, meaning each agent’s utility equals the max total welfare of others plus their own value for the chosen outcome minus others’ welfare in the chosen outcome – which does not depend on \\(v_i(\\omega^*)\\) except through the decision of \\(\\omega^*\\). By construction, an agent cannot influence \\(\\omega^*\\) in a way that improves this expression unless it genuinely increases total welfare, so misreporting \\(v_i\\) cannot increase their utility. Thus truthful reporting is a dominant strategy. VCG is dominant-strategy incentive compatible (DSIC) and produces an outcome that maximizes \\(\\sum_i v_i(\\omega)\\), achieving social welfare maximization.\nVCG provides a powerful existence result: under broad conditions, there is a mechanism that achieves efficient allocation with truth-telling (in fact, VCG is essentially the unique one, aside from adding harmless constant transfers). However, implementing VCG in practice can be difficult. One challenge is computational: finding \\(\\arg\\max_{\\omega}\\sum_i b_i(\\omega)\\) can be NP-hard if \\(\\Omega\\) is a combinatorially large space (as in many combinatorial auctions). Another issue is budget balance and revenue: VCG payments might not yield any revenue to the mechanism designer in some cases (or even require subsidies in complex settings), and they can be low or zero in certain environments, which is problematic if the seller needs revenue. VCG is also vulnerable to collusion or the presence of fake identities (sybil attacks) – the mechanism assumes each participant is a separate entity; if one bidder can split into two identities, they might game the outcome.\nNonetheless, for many domains, VCG or variants have been successfully used or at least studied. Notably, combinatorial auctions (where multiple items are up for sale and bidders have valuations for bundles of items) can in theory be handled by VCG: just let \\(\\Omega\\) be all possible allocations of items to bidders, and have bidders report \\(b_i(S)\\) for each bundle \\(S\\) of items. VCG would allocate the items in the way that maximizes total reported value and charge each bidder the opportunity cost their presence imposes on others. In practice, as mentioned, combinatorial auctions face exponential complexity in preference reporting (each bidder potentially has to specify a value for every subset of items) and winner determination (solving an NP-hard combinatorial optimization). Heuristic or restricted approaches (like limiting the kinds of bundles or using iterative bidding with query learning of preferences) are used to make the problem tractable. Additionally, pure VCG in combinatorial settings can have undesirable properties: for example, in some cases adding more bidders can cause VCG prices to drop to zero (the so-called “threshold problem” or revenue monotonicity failure), and bidders may collude to manipulate their bids collectively.\nOne high-stakes application of combinatorial auctions is spectrum auctions for selling licenses of electromagnetic spectrum to telecom companies. Governments have used multi-round combinatorial auctions to allocate spectrum, with billions of dollars at stake. Designing these auctions requires balancing efficiency with simplicity and robustness to strategic behavior. Early spectrum auctions that used simpler formats (like sequential auctions or one-shot sealed bids for each license) ran into problems like the exposure problem – a bidder valuing a combination of items (say complementary licenses in adjacent regions) risks winning only part of the combination at a high price, which could be bad for them if the items are worth much less separately. The simultaneous multi-round auction (SMRA) was an innovation that allowed bidding on all items at once in rounds, giving bidders some price discovery to mitigate the exposure problem. Even so, strategic issues like demand reduction (bidders deliberately not bidding on too many items to keep prices low) and tacit collusion through signaling bids have been observed. These practical complications underscore that while VCG is a beautiful theoretical ideal, real-world mechanism design often involves compromises and tweaks.\n\n5.2.1 Case Study 1: Mechanism for Peer Grading\nTo illustrate an application of mechanism design beyond auctions, consider a classroom setting where students grade each other’s work (peer assessment). The goal is to design a system (a “mechanism”) that produces fair and accurate grades while incentivizing students to put effort into grading. Jason Hartline and colleagues (2020) studied such a scenario, examining how to optimize scoring rules for peer grading (Hartline et al. 2020). In this setting, students are both agents (who might strategize to maximize their own grade or minimize their effort) and graders. The “outcome” we want is a set of final grades for students, ideally reflecting the true quality of their work.\nOne idea is to use proper scoring rules to evaluate the peer graders. A proper scoring rule is a concept from forecast evaluation that gives highest expected score for truthful reporting of probabilities. In peer grading, one might try to reward students based on how close their grading is to some ground truth or to the TA’s grades. However, a naive application of proper scoring can backfire. Hartline et al. observed a “lazy peer grader” problem: if students figure out that always giving an average score (say 80%) yields a decent reward under the scoring rule, they might not bother to carefully distinguish good and bad work. In one experiment, giving all peers an 80% could yield a 96% accuracy score for the grader under a certain scoring rule (Hartline et al. 2023). This clearly undermines the goal – the grader is basically cheating the system by always predicting the class average.\nTo combat this, the mechanism designers sought a scoring rule that maximizes the difference in reward between a diligent grading and a lazy strategy, thereby incentivizing effort. They formulated this as an optimization problem: design the reward function for peer graders such that truthful, careful grading yields a strictly higher expected score than any degenerate strategy like always giving the average. By analyzing data and grader behavior models, they adjusted the scoring rules to penalize obviously lazy patterns and reward variance when warranted. The resulting mechanism improved the accuracy of peer grading by aligning the incentives of student graders (who want a high score for their grading job) with the objective of accurate assessment. This case study highlights how ideas of incentive compatibility and mechanism design apply even in social/educational contexts: the “payments” are points towards one’s own grade, and the mechanism must account for strategic behavior to ensure a reliable outcome.\nIn conclusion, mechanism design provides a toolkit for aggregating preferences (or signals, like grades or bids) in a principled way, by explicitly accounting for individual incentives. Whether in auctions, peer grading, or other domains, the design of rules (allocation algorithms, payment or scoring schemes) crucially determines whether people feel encouraged to be truthful or to game the system. The theories of VCG and Myerson give us optimal baselines for efficiency and revenue in auctions, while impossibility results like Gibbard–Satterthwaite warn us of the limitations in voting. Real-world implementations often have to grapple with complexity and approximate these ideals. While learning from individual human preference is a powerful approach, it too faces aggregation challenges. If the human feedback is inconsistent or if different annotators have different preferences, the reward model may end up capturing an average that satisfies no one perfectly. There is active research on scalable oversight: techniques to gather and aggregate human feedback on tasks that are too complex for any single person to evaluate reliably. This includes approaches like recursive reward modeling, iterated amplification (Christiano, Shlegeris, and Amodei 2018), and AI-assisted debate (Irving, Christiano, and Amodei 2018), where AI systems help humans provide better feedback or break down tasks. The goal of scalable oversight is to leverage human preferences and principles in guiding AI even as AI systems tackle increasingly complex or open-ended tasks, while mitigating the human burden and bias in evaluation.\nIn summary, preference aggregation in machine learning spans from simple models like Bradley–Terry for pairwise comparisons to elaborate RLHF pipelines for training large models. The deep mathematical foundations – whether Arrow’s theorem or Myerson’s auction theory – remind us that whenever we aggregate preferences or signals from multiple sources, we must consider incentive effects, fairness criteria, and the possibility of inconsistency. By combining insights from social choice, economics, and statistical learning, we aim to build AI systems that not only learn from human preferences but do so in a principled, robust, and fair manner. The next chapter will delve further into aligning AI with human values, building on the mechanisms and learning algorithms discussed here to ensure AI systems remain beneficial and in line with what people truly want.\n\n\n5.2.2 Case Study 2: Incentive-Compatible Online Learning\nTo address this problem, we seek to create a model. We first outline the key criteria that our model must achieve. The model revolves around repeated interactions between a planner (the system) and multiple agents (the users). Each agent, upon arrival in the system, is presented with a set of available options to choose from. These options could vary widely depending on the application of the model, such as routes in a transportation network, a selection of hotels in a travel booking system, or even entertainment choices in a streaming service. The interaction process is straightforward but crucial: agents arrive, select an action from the provided options, and then report feedback based on their experience. This feedback is vital as it forms the basis upon which the planner improves and evolves its recommendations. The agents in this model are considered strategic; they aim to maximize their reward based on the information available to them. This aspect of the model acknowledges the real-world scenario where users are typically self-interested and seek to optimize their own outcomes. The planner, on the other hand, has a broader objective. It aims to learn which alternatives are best in a given context and works to maximize the overall welfare of all agents. This involves a complex balancing act: the planner must accurately interpret feedback from a diverse set of agents, each with their own preferences and biases, and use this information to refine and improve the set of options available. The ultimate goal of the planner is to create a dynamic, responsive system that not only caters to the immediate needs of individual agents but also enhances the collective experience over time, leading to a continually improving recommendation ecosystem.\nHere, we seek to address the inherent limitations faced by the planner, particularly in scenarios where monetary transfers are not an option, and the only tool at its disposal is the control over the flow of information between agents. This inquiry aims to understand the extent to which these limitations impact the planner’s ability to effectively guide and influence agent behavior. A critical question is whether the planner can successfully induce exploration among agents, especially in the absence of financial incentives. This involves investigating strategies to encourage users to try less obvious or popular options, thus broadening the scope of feedback and enhancing the system’s ability to learn and identify the best alternatives. Another question is understanding the rate at which the planner learns from agent interactions. This encompasses examining how different agent incentives, their willingness to explore, and their feedback impact the speed and efficiency with which the planner can identify optimal recommendations.\nThe model can be extended in several directions, each raising its own set of questions.\n1.  Multiple Agents with Interconnected Payoffs: When multiple agents arrive simultaneously, their choices and payoffs become interconnected, resembling a game. The research question here focuses on how these interdependencies affect individual and collective decision-making.\n\n2.  Planner with Arbitrary Objective Function: Investigating scenarios where the planner operates under an arbitrary objective function, which might not align with maximizing overall welfare or learning the best alternative.\n\n3.  Observed Heterogeneity Among Agents: This involves situations where differences among agents are observable and known, akin to contextual bandits in machine learning. The research question revolves around how these observable traits can be used to tailor recommendations more effectively.\n\n4.  Unobserved Heterogeneity Among Agents: This aspect delves into scenarios where differences among agents are not directly observable, necessitating the use of causal inference techniques to understand and cater to diverse user needs.\nIn our setup, there is a “planner,” which aims to increase exploration, and many independent “agents,” which will act selfishly (in a way that they believe will maximize their individual reward) (Mansour, Slivkins, and Syrgkanis 2019; Mansour et al. 2021). Under our model shown in Figure 1.1, there are \\(K\\) possible actions that all users can take, and each action has some mean reward \\(\\mu_i \\in [0, 1]\\). In addition, there is a common prior belief on each \\(\\mu_i\\) across all users.. The \\(T\\) agents, or users, will arrive sequentially. As the \\(t\\)’th user arrives, they are recommended an action \\(I_t\\) by the planner, which they are free to follow or not follow. After taking whichever action they choose, the user experiences some realized reward \\(r_i \\in [0, 1]\\), which is stochastic i.i.d. with mean \\(\\mu_i\\), and reports this reward back to the planner.\nSo far, the model we have defined is equivalent to a multi-armed bandit model, which we have seen earlier in this chapter (1). Under this model, well-known results in economics, operations research and computer science show that \\(O(\\sqrt{T})\\) regret is achievable (Russo and Roy 2015; Auer, Cesa-Bianchi, and Fischer 2002; Lai and Robbins 1985) with algorithms such as Thompson sampling and UCB. However, our agents are strategic and aim to maximize their own rewards. If they observe the rewards gained from actions taken by other previous users, they will simply take the action they believe will yield the highest reward given the previous actions; they would prefer to benefit from exploration done by other users rather than take the risk of exploring themselves. Therefore, exploration on an individual level, which the planner would like to facilitate, is not guaranteed under this paradigm.\nIn light of this, we also require that our model satisfy incentive compatibility, or that taking the action recommended by the planner has an expected utility that is as high as any other action the agent could take. Formally, \\(\\forall i : \\, E[\\mu_i | I_t = i] \\geq E[\\mu_{i'} | I_t = i].\\) Note that this incentivizes the agents to actually take the actions recommended by the planner; if incentive compatibility is not satisfied, agents would simply ignore the planner and take whatever action they think will lead to the highest reward.\nAt a high level, the key to achieving incentive compatibility while still creating a policy for the planner that facilitates exploration is information asymmetry. Under this paradigm, the users only have access to their previous recommendations, actions, and rewards, and not to the recommendations, actions, and rewards of other users. Therefore, they are unsure of whether, after other users take certain actions and receive certain rewards, arms that they might have initially considered worse in practice outperform arms that they initially considered better. Only the planner has access to the previous actions and rewards of all users; the user only has access to their own recommendations and overall knowledge of the planner’s policy. The main question we aim to answer for the rest of this section is, given this new constraint of incentive compatibility, is \\(O(\\sqrt{T})\\) regret still achievable? We illustrate such an algorithm in the following.\nThe main result here is a black-box reduction algorithm to turn any bandit algorithm into an incentive compatible one, with only a constant increase in Bayesian regret. Since, as mentioned earlier, there are bandit algorithms with \\(O(\\sqrt{T})\\) Bayesian regret, black-box reduction will also allow us to get incentive-compatible algorithms with \\(O(\\sqrt{T})\\) regret. The idea of black-box reduction will be to simulate \\(T\\) steps of any bandit algorithm in an incentive-compatible way in \\(c T\\) steps. This allows us to design incentive-compatible recommendation systems by using any bandit algorithm and then adapting it. Consider the following setting: there are two possible actions, \\(A_1\\) and \\(A_2\\). Assume the setting of deterministic rewards, where action 1 has reward \\(\\mu_1\\) with prior \\(U[1/3, 1]\\) and mean \\(\\mathbb{E}[\\mu_1] = 2/3\\), and action 2 has reward \\(\\mu_2\\) with prior \\(U[0, 1]\\) and mean \\(\\mathbb{E}[\\mu_2] = 1/2\\). Without the planner intervention and with full observability, users would simply always pick \\(A_1\\), so how can the planner incentivize users to play \\(A_2\\)?\nThe key insight is going to be to hide exploration in a pool of exploitation. The users are only going to receive a recommendation from the planner, and no other observations. After deterministically recommending the action with the highest expected reward (\\(A_1\\)), the planner will pick one guinea pig to recommend the exploratory action of \\(A_2\\). The users don’t know whether they are the guinea pig, so intuitively, as long as the planner picks guinea pigs uniformly at random and at low enough frequencies, the optimal decision for the users is still to follow the planner’s recommendation, even if it might go against their interest. The planner will pick the user who will be recommended the exploratory action uniformly at random from the \\(L\\) users that come after the first one (which deterministically gets recommended the exploitation action). Under this setting (illustrated in Figure 1.2), it is optimal for users to always follow the option that is recommended for them. More formally, if \\(I_t\\) is the recommendation that a user receives at time \\(t\\), then we have that:\n\\[\n\\begin{split}\n    \\mathbb{E}[\\mu_1 - \\mu_2 | I_t = 2] Pr[I_t = 2] &= \\frac{1}{L} (\\mu_1 - \\mu_2) \\quad \\text{(Gains if you are the unlucky guinea pig)}\\\\\n    &+ (1 - \\frac{1}{L}) \\mathbb{E}[\\mu_1 - \\mu_2 | \\mu_1 &lt; \\mu_2] \\times p[\\mu_1 &lt; \\mu_2] \\quad \\text{(Loss if you are not and $\\mu_1 &lt; \\mu_2$)}\\\\\n    &\\leq 0\n\\end{split}\n\\]\nThis holds when \\(L \\geq 12\\). It means that the gains from not taking the recommended action are negative, which implies that users should always take the recommendation. So far we have considered the case where rewards are deterministic, but what about stochastic rewards? We are now going to consider the case where rewards are independent and identically distributed from some distribution, and where each action \\(A_i\\) has some reward distribution \\(r_i^t \\sim D_i, \\mathbb{E}[r_i^t] = \\mu_i\\). Back to the case where there are only two actions, we are going to adapt the prior algorithm of guinea pig-picking to the stochastic reward setting. Since one reward observation is not enough to fully know \\(\\mu_1\\) anymore, we’ll instead observe the outcome of the first action \\(M\\) times to form a strong posterior \\(\\mathbb{E}[\\mu_1 | r_1^1, \\ldots r_1^M]\\). We can use with stochastic rewards when there are two actions. Similarly, as before, we pick one guinea pig uniformly at random from the next \\(L\\) users and use the reward we get as the exploratory signal. In a very similar manner, we can generalize this algorithm from always having two actions to the general multi-armed bandit problem. Now suppose we have a general multi-armed bandit algorithm \\(A\\). We will wrap this algorithm around our black box reduction algorithm to make it incentive-compatible. We wrap every decision that \\(A\\) would make by exactly \\(L-1\\) recommendations of the action believed to be the best so far. This guarantees that the expected rewards for the users that are not chosen as guinea pigs are at least as good as \\(A\\)’s reward at phase \\(n\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Aggregation</span>"
    ]
  },
  {
    "objectID": "src/chap6.html#mutual-information-paradigm",
    "href": "src/chap6.html#mutual-information-paradigm",
    "title": "5  Aggregation",
    "section": "5.3 Mutual Information Paradigm",
    "text": "5.3 Mutual Information Paradigm\nIn this section we discuss an influential new framework for designing peer prediction mechanisms, the Mutual Information Paradigm (MIP) introduced by Kong and Schoenebeck (Kong and Schoenebeck 2019). Traditional peer prediction approaches typically rely on scoring rules and correlation between agents’ signals. However, these methods often struggle with issues like uninformed equilibria, where agents can coordinate on uninformative strategies that yield higher payoffs than truth-telling. The core idea is to reward agents based on the mutual information between their report and the reports of other agents. We consider a setting with \\(n\\) agents, each possessing a private signal \\(\\Psi_i\\) drawn from some set \\(\\Sigma\\). The mechanism asks each agent to report their signal, which we denote as \\(\\hat{\\Psi}_i\\). For each agent \\(i\\), the mechanism randomly selects a reference agent \\(j \\neq i\\). Agent \\(i\\)’s payment is then calculated as \\(MI(\\hat{\\Psi}_i; \\hat{\\Psi}_j)\\) where \\(MI\\) is an information-monotone mutual information measure. An information-monotone \\(MI\\) measure must satisfy the following properties:\n\nSymmetry: \\(MI(X; Y) = MI(Y; X)\\).\nNon-negativity: \\(MI(X; Y) \\geq 0\\), with equality if and only if \\(X\\) and \\(Y\\) are independent.\nData processing inequality: For any transition probability \\(M\\), if \\(Y\\) is independent of \\(M(X)\\) conditioned on \\(X\\), then \\(MI(M(X); Y) \\leq MI(X; Y)\\).\n\nTwo important families of mutual information measures that satisfy these properties are \\(f\\)-mutual information and Bregman mutual information. The \\(f\\)-mutual information is defined as \\(MI_f(X; Y) = D_f(U_{X,Y}, V_{X,Y})\\), where \\(D_f\\) is an \\(f\\)-divergence, \\(U_{X,Y}\\) is the joint distribution of \\(X\\) and \\(Y\\), and \\(V_{X,Y}\\) is the product of their marginal distributions. The Bregman mutual information is defined as: \\(BMI_{PS}(X; Y) = \\mathbb{E}_{X} [D{PS}(U_{Y|X}, U_Y)]\\), where \\(D_{PS}\\) is a Bregman divergence based on a proper scoring rule \\(PS\\), \\(U_{Y|X}\\) is the conditional distribution of \\(Y\\) given \\(X\\), and \\(U_Y\\) is the marginal distribution of \\(Y\\). The MIP framework can be applied in both single-question and multi-question settings. In the multi-question setting, the mechanism can estimate the mutual information empirically from multiple questions. In the single-question setting, additional techniques like asking for predictions about other agents’ reports are used to estimate the mutual information. A key theoretical result of the MIP framework is that when the chosen mutual information measure is strictly information-monotone with respect to agents’ priors, the resulting mechanism is both dominantly truthful and strongly truthful. This means that truth-telling is a dominant strategy for each agent and that the truth-telling equilibrium yields strictly higher payoffs than any other non-permutation strategy profile. As research continues to address practical implementation challenges of designing truthful mechanisms, MIP-based approaches have significant potential to improve preference elicitation and aggregation in real-world applications lacking verifiable ground truth.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Aggregation</span>"
    ]
  },
  {
    "objectID": "src/chap6.html#exercises",
    "href": "src/chap6.html#exercises",
    "title": "5  Aggregation",
    "section": "5.4 Exercises",
    "text": "5.4 Exercises\n\n5.4.1 Question 1: Pairwise Feedback Mechanisms for Digital Goods\nConsider a marketplace for digital goods (such as personalized articles, artwork, or AI-generated data), where the exact utility derived from these goods is only revealed to buyers after the goods have been generated and delivered. To elicit truthful preferences from buyers who find it difficult to precisely quantify their valuations beforehand, the marketplace implements a pairwise feedback mechanism, inspired by the work of Robertson and Koyejo (2023).\nFormally, each buyer requests a personalized digital good and, upon receiving the good, provides feedback by indicating whether their realized utility is higher or lower than a randomly chosen reference price \\(c \\in [0,1]\\). The mechanism utilizes this binary feedback to estimate valuations and allocate future goods accordingly.\nAnswer the following:\n\nFormalize the Problem: Let \\(u_i\\) denote the true valuation of buyer \\(i\\), and let \\(r_i(c)\\) denote the buyer’s reported feedback (\\(r_i(c) = 1\\) if \\(u_i \\geq c\\), 0 otherwise). Prove that, under uniform random selection of the reference price \\(c\\), the expected value \\(\\mathbb{E}[r_i(c)]\\) is equal to the true valuation \\(u_i\\).\nIncentive Compatibility Analysis: Discuss conditions under which this feedback-based mechanism is incentive compatible, i.e., buyers have no incentive to misreport their preferences. Specifically, analyze why strategic misreporting (reporting \\(r_i(c)\\) incorrectly for some reference prices) would not increase a buyer’s expected payoff.\nRegret Analysis: Suppose the mechanism estimates buyers’ utilities from past feedback and allocates future goods using an epsilon-greedy strategy (exploration rate \\(\\eta_t\\)). Provide an informal discussion of the trade-off involved in choosing the exploration rate, and how it affects the social welfare and revenue of the marketplace over time.\nPractical Implications: Suggest one practical scenario outside the digital-goods marketplace where such a feedback-driven, pairwise comparison approach would be beneficial. Briefly justify your choice, mentioning challenges and benefits.\n\n\n\n5.4.2 Question 2: Scalable Oversight in Complex Decision-Making\nIn scenarios involving complex or high-dimensional outcomes (such as summarizing lengthy texts, assessing the quality of detailed AI-generated reports, or reviewing scientific papers), evaluating the quality of outputs can become infeasible for a single human overseer. One practical solution is scalable oversight, where the evaluation task is decomposed and distributed among multiple human evaluators or even assisted by AI agents. Consider a scalable oversight scenario inspired by recursive reward modeling, where complex evaluations are hierarchically decomposed into simpler tasks. Specifically, suppose you want to evaluate a lengthy report generated by an AI system. Answer the following:\n(a) Decomposition of the Task: Propose a formal recursive decomposition strategy to evaluate a long AI-generated report of length (N) paragraphs. Specifically, describe a hierarchical evaluation method that decomposes the original evaluation into simpler subtasks at multiple hierarchical levels. Clearly describe how many subtasks you have at each level and how the final aggregated evaluation is computed.\n(b) Statistical Aggregation Method: Suppose each evaluation subtask yields a binary score (s_i {0,1}), where (1) indicates acceptable quality and (0) indicates unacceptable quality. Propose a simple statistical aggregation method (e.g., majority voting, threshold voting, weighted aggregation, etc.) to combine subtask evaluations into a single global quality assessment at the top level. Justify your choice mathematically.\n(c) Computational Simulation: Implement a Python simulation of your hierarchical decomposition and aggregation method described in parts (a) and (b). Assume each subtask is evaluated with some fixed probability (p) of being correct (representing human evaluators with bounded accuracy).\nSpecifically, your simulation should: - Implement a hierarchical evaluation scheme (e.g., binary-tree decomposition). - Assume evaluators have accuracy (p = 0.8) (i.e., probability of correctly identifying paragraph quality). - Simulate how evaluator accuracy at the leaf nodes affects the reliability of the global evaluation at the root node. - Plot how the reliability of the top-level evaluation (accuracy at the root) varies as you increase the depth of hierarchy for a report of fixed length (e.g., (N = 64) paragraphs).\n(d) Practical Discussion: Briefly discuss advantages and potential drawbacks of scalable oversight approaches such as recursive decomposition in the context of AI alignment. Include considerations such as evaluator fatigue, consistency, cost, and vulnerability to manipulation or collusion.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Aggregation</span>"
    ]
  },
  {
    "objectID": "src/chap7.html",
    "href": "src/chap7.html",
    "title": "6  Alternatives",
    "section": "",
    "text": "6.1 Human Values and AI Alignment\nIn recent years, the rapidly advancing capabilities of large models have led to increased discussion of aligning AI systems with human values. This chapter discusses the multifaceted relationship between values, alignment, and human-centered design in the context of AI. We begin by exploring the fundamental concept of human values and their ethical implications in AI design. This includes discussions on human values and ethics in AI, understanding and addressing bias in AI, and methods for aligning AI with human values. Additionally, we examine AI alignment problems, focusing on outer alignment to avoid specification gaming and inner alignment to prevent goal misgeneralization. Next, we cover techniques in value learning. This section introduces methodologies such as reinforcement learning from human feedback and contrastive preference learning, which are crucial for teaching AI systems to understand and align with human values. The importance of value alignment verification is emphasized to ensure that AI systems remain consistent with human values over time, adapting to changes and preventing misalignment. We then explore the principles and practices of human-centered design. This includes discussions on AI and human-computer interaction and methods for designing AI for positive human impact, which focuses on creating AI systems that are socially aware, human-centered, and positively impactful. A crucial part of this discussion is adaptive user interfaces, where we discuss key ideas, design principles, applications, and limitations of these interfaces, showcasing how they enhance user experience by dynamically adjusting to user needs and preferences. Finally, we present case studies in human-centered AI, including the LaMPost case study, Multi-Value, and DaDa: Cross-Dialectal English NLP, and social skill training via LLMs. These case studies provide real-world examples of successful implementations of human-centered AI systems. By integrating these elements, the chapter aims to provide a comprehensive understanding of how to create AI systems that are ethical, aligned with human values, and beneficial to society.\nIn this part, we take a step back from the technical details to reflect on the broader concept of human values and their profound influence on our behavior and decision-making.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Alternatives</span>"
    ]
  },
  {
    "objectID": "src/chap7.html#human-values-and-ai-alignment",
    "href": "src/chap7.html#human-values-and-ai-alignment",
    "title": "6  Alternatives",
    "section": "",
    "text": "6.1.1 Human Values and Ethics in AI\nHuman values are the principles and standards that guide behavior and decision-making, reflecting what is essential in life and influencing choices and actions. One notable scholar in this field is Shalom H. Schwartz, a social psychologist renowned for his theory on basic human values. Schwartz’s work has significantly contributed to our understanding of how values influence behavior across different cultures. He describes values as “desirable, trans-situational goals, varying in importance, that serve as guiding principles in people’s lives” (Schwartz 1992). This perspective underscores the importance of values in shaping consistent and ethical behavior across different contexts. Supporting this view, philosopher William K. Frankena emphasizes the integral role of values in ethical behavior and decision-making processes. Frankena’s work in ethical theory provides a foundation for understanding how moral judgments are formed. He notes that “ethical theory is concerned with the principles and concepts that underlie moral judgments” (Frankena 1973), highlighting the need to comprehend ethical principles deeply to make informed moral judgments. Examples of ethical values include autonomy, fairness, justice, and well-being. For computer scientists developing AI systems, understanding these concepts is crucial. AI systems that interact with humans and impact societal structures must be designed with these values in mind. By embedding such values into AI, developers can create systems that respect human dignity and promote positive social outcomes.\n\nAutonomy is the right to choose, an essential aspect of personal freedom. Gerald Dworkin defines autonomy as “the capacity to reflect upon and endorse or reject one’s desires and values” (Dworkin 1988). In AI, respecting autonomy means creating systems that support user independence and decision-making rather than manipulating or coercing them.\nFairness involves treating all individuals equally and justly, ensuring no discrimination. John Rawls, one of the most influential political philosophers of the \\(20^{th}\\) century, in his groundbreaking book “A Theory of Justice,” describes fairness as “the elimination of arbitrary distinctions and the establishment of a balance between competing claims” (Rawls 1971). For AI systems, this translates to algorithms that do not perpetuate bias or inequality, ensuring that all users are treated equitably.\nJustice is about upholding what is morally right and ensuring fair treatment for all. Rawls also highlights that “justice is the first virtue of social institutions, as truth is of systems of thought” (Rawls 1971). In the context of AI, justice involves creating technologies that enhance fairness in legal, social, and economic systems, providing equal opportunities and protection to all individuals.\n\nWell-being focuses on promoting the health, happiness, and prosperity of individuals. Martha Nussbaum and Amartya Sen, two distinguished scholars known for their significant contributions to welfare economics and the development of the capability approach, discuss the importance of well-being in their collaborative work “The Quality of Life.” They argue that “well-being is about the expansion of the capabilities of people to lead the kind of lives they value” (Nussbaum and Sen 1993). AI systems should enhance users’ quality of life, supporting their health, education, and economic stability.\nUnderstanding human values is foundational for readers with a computer science background before delving into AI ethics. These values provide the ethical underpinnings necessary to design and deploy AI systems responsibly. As AI systems increasingly impact all aspects of society, developers must embed these values into their work to ensure technologies benefit humanity and do not exacerbate existing inequalities.\nHuman values play a crucial role in decision-making by shaping the criteria for evaluating options and outcomes. They influence priorities and ethical considerations, guiding individuals and organizations to make choices that align with their principles. Nick Bostrom, a prominent philosopher in AI and existential risk, highlights the importance of values in setting priorities and determining desirable outcomes (Bostrom 2014). Aligning actions with values ensures consistency and ethical integrity in decision-making. Incorporating human values into AI systems ensures that AI decisions align with societal norms and ethical standards. Stuart Russell, an AI researcher and advocate for human-compatible AI, stresses the importance of embedding human values into AI systems to ensure they act in beneficial and ethical ways (Russell 2019). By integrating values such as fairness, justice, and well-being, AI systems can make decisions that reflect societal expectations and ethical considerations.\nExamples of incorporating values into AI systems demonstrate the practical application of these principles. For instance, autonomous vehicles are programmed to prioritize human safety, ensuring decisions that protect lives. In healthcare, AI systems uphold values by safeguarding patient privacy and ensuring informed consent, adhering to ethical medical standards. Judicial AI systems aim to eliminate biases in sentencing recommendations, promoting fairness and justice. Luciano Floridi underscores the necessity for AI systems to be designed in a way that respects and upholds human values to function ethically and effectively (Floridi 2011).\nTo ensure that these values are systematically embedded within AI systems, it is essential to consider major ethical frameworks such as deontological, consequentialist, and virtue ethics that guide moral decision-making.\nDeontological ethics, primarily associated with the philosopher Immanuel Kant, focuses on rules and duties. This ethical framework posits that actions are morally right if they adhere to established rules and duties, regardless of the outcomes. Kant’s moral philosophy emphasizes the importance of duty and adherence to moral laws. Robert Johnson, a scholar who has extensively studied Kantian ethics, explains that “Kant’s moral philosophy emphasizes that actions must be judged based on their adherence to duty and moral law, not by their consequences” (Johnson and Cureton 2022). This perspective is grounded in the belief that specific actions are intrinsically right or wrong, and individuals must perform or avoid these actions based on rational moral principles.\nIn the context of AI, deontological ethics implies that AI systems should be designed to follow ethical rules and principles. For instance, AI systems must respect user privacy and confidentiality as an inviolable duty. This approach ensures that AI technologies do not infringe on individuals’ rights, regardless of potential benefits. Implementing deontological principles in AI design can prevent ethical breaches, such as unauthorized data usage or surveillance. By adhering to established moral guidelines, AI systems can maintain ethical integrity and avoid actions that would be considered inherently wrong. As Floridi states, “AI systems should be developed with a commitment to uphold moral duties and respect human dignity” (Floridi 2011).\nConsequentialist ethics, in contrast, evaluates the morality of actions based on their outcomes. The most well-known form of consequentialism is utilitarianism, articulated by philosophers like Jeremy Bentham and John Stuart Mill. This ethical theory suggests that actions are morally right if they promote the greatest happiness for the greatest number. Mill emphasizes that “the moral worth of an action is determined by its contribution to overall utility, measured by the happiness or well-being it produces” (Mill 1863). Consequentialist ethics is pragmatic, focusing on the results of actions rather than the actions themselves.\nApplying consequentialist ethics to AI development involves designing AI systems to achieve beneficial outcomes. This means prioritizing positive societal impacts, such as improving healthcare outcomes, enhancing public safety, or reducing environmental harm. For instance, algorithms can be designed to optimize resource allocation in disaster response, thereby maximizing the overall well-being of affected populations. In this framework, the ethicality of AI decisions is judged by their ability to produce desirable consequences. Virginia Dignum, a professor of responsible artificial intelligence at Umeå University, explains that “designing algorithms with a focus on maximizing positive outcomes can lead to more ethical and effective AI systems” (Dignum 2019). Consequently, AI developers focus on the potential impacts of their technologies and strive to enhance their beneficial effects.\nVirtue ethics, originating from the teachings of Aristotle, emphasizes the importance of character and virtues in ethical behavior. This framework posits that ethical behavior arises from developing good character traits and living a virtuous life. Aristotle, an ancient Greek philosopher and the author of “Nicomachean Ethics,” argues that “virtue is about cultivating excellence in character to achieve eudaimonia or human flourishing” (Aristotle 350 B.C.E.). Virtue ethics focuses on the individual’s character and the moral qualities that define a good person, such as honesty, courage, and compassion.\nAdditionally, virtue ethics encourages the development and use of AI systems that promote virtuous behavior. This involves fostering transparency, accountability, and fairness in AI technologies. For example, AI systems should be designed to provide clear and understandable explanations for their decisions, promoting transparency and building user trust. Furthermore, AI developers should strive to create technologies that support ethical practices and enhance the common good. Floridi emphasizes that “virtue ethics in AI development requires a commitment to fostering moral virtues and promoting human well-being” (Floridi 2011). By focusing on the character and virtues of AI developers and AI systems, virtue ethics provides a holistic approach to ethical AI development.\nApplying these ethical frameworks to AI development is essential to ensure that AI systems operate ethically and responsibly. Deontological ethics in AI involves ensuring that AI follows ethical rules and principles. For instance, AI systems should be designed to respect user privacy and confidentiality. Consequentialist ethics focuses on developing AI to achieve beneficial outcomes. This means creating algorithms prioritizing positive societal impacts, such as improving healthcare outcomes or reducing environmental harm. Virtue ethics encourages virtuous behavior in AI development and use, promoting transparency, accountability, and fairness. Floridi emphasizes that “ethical AI development requires a commitment to core moral principles and virtues” (Floridi 2011).\nExamples in practice demonstrate how these frameworks can be applied to guide ethical AI development. Implementing fairness constraints in machine learning models ensures that algorithms do not discriminate against certain groups. Binns notes that “fairness in machine learning can be informed by lessons from political philosophy to create more just and equitable systems” (Binns 2018). Designing algorithms that maximize overall well-being aligns with consequentialist ethics by focusing on the positive outcomes of AI deployment. Additionally, developing AI systems focusing on transparency and accountability supports virtue ethics by fostering trust and reliability in AI technologies.\nEthical principles provide a framework for ensuring that AI operates in ways that are fair, just, and beneficial. Deontological ethics, for instance, focuses on moral rules and obligations, while consequentialism considers the outcomes of actions. By embedding these ethical principles into AI design, we can create systems that respect human dignity and promote societal well-being.\n\n\n6.1.2 Bias in AI\nBias in AI refers to systematic errors that result in unfair outcomes. These biases can occur at various stages of AI system development and deployment, leading to significant ethical and practical concerns. Addressing bias in AI is crucial because it directly impacts the fairness, accountability, and trustworthiness of AI systems. Barocas, Hardt, and Narayanan emphasize that “bias in machine learning can lead to decisions that systematically disadvantage certain groups” (Barocas, Hardt, and Narayanan 2019). O’Neil further highlights the societal impact of biased AI, noting that “algorithms can perpetuate and amplify existing inequalities, leading to a cycle of discrimination” (O’Neil 2016). Therefore, understanding and mitigating bias is essential for developing ethical AI systems that promote fairness and equity.\nData bias originates from skewed or non-representative data used to train AI models. This bias often reflects historical prejudices and systemic inequalities in the data. For example, if a hiring algorithm is trained on historical hiring data that reflects gender or racial biases, it may perpetuate these biases in its recommendations. Fatemeh Mehrabi and her colleagues, in their survey on bias in AI, state that “data bias can result from sampling bias, measurement bias, or historical bias, each contributing to the unfairness of AI systems” (Mehrabi et al. 2021). Safiya Umoja Noble, author of “Algorithms of Oppression,” discusses how biased data in search engines can reinforce stereotypes and marginalize certain groups, noting that “search algorithms often reflect the biases of the society they operate within” (Noble 2018). Addressing data bias involves careful collection, preprocessing, and validation to ensure diversity and representation.\nAn effort to address data bias is the “Lab in the Wild” platform, which seeks to broaden the scope of Human-Computer Interaction (HCI) studies beyond the traditional “WEIRD” (Western, Educated, Industrialized, Rich, and Democratic) population (oliveira17?). Paulo S. Oliveira, one of the platform’s researchers, notes that this initiative aims to correct demographic skew in behavioral science research by engaging a diverse global audience. By allowing individuals from various demographics to participate in studies from their environments, “Lab in the Wild” provides researchers with a more inclusive dataset.\nAnother important consideration is the cultural nuances of potential users. For instance, designing a computer vision system to describe objects and people daily must consider whether to identify gender. In the United States, there is growing sensitivity toward gender identity, suggesting that excluding gender might be prudent. Conversely, in India, where a visually impaired woman may need gender-specific information for safety, including gender identification is critical. Ayanna Howard, a roboticist and AI researcher at Georgia Tech, emphasizes the need for adaptable systems that respect local customs and address specific user needs in her work on human-robot interaction. This highlights the importance of adaptable systems that respect local customs and address specific user needs.\nAlgorithmic bias often arises from the design and implementation choices made by developers. This type of bias can stem from the mathematical frameworks and assumptions underlying the algorithms. For instance, decision trees and reinforcement learning policies can inadvertently prioritize certain outcomes, resulting in biased results. Solon Barocas, a professor at Cornell University, and his colleagues explain that “algorithmic bias can emerge from optimization objectives that do not adequately consider fairness constraints” (Barocas, Hardt, and Narayanan 2019). Cathy O’Neil, a data scientist who has written extensively on the societal impacts of algorithms, provides examples of how biased algorithms in predictive policing and credit scoring can disproportionately affect disadvantaged communities. She argues that “algorithmic decisions can have far-reaching consequences when fairness is not adequately addressed” (O’Neil 2016). Mitigating algorithmic bias requires incorporating fairness constraints and regularly auditing algorithmic decisions.\nWeidinger et al., in their 2022 study published in “Artificial Intelligence,” investigate how reinforcement learning (RL) algorithms can replicate or amplify biases present in training data or algorithmic design (Weidinger, Reinecke, and Haas 2022). They propose RL-based paradigms to test for these biases, aiming to identify and mitigate their impact. Similarly, Mazeika et al., in their research on modeling emotional dynamics from video data, explore how algorithms might prioritize certain emotional expressions or demographics based on their training and data usage (Mazeika et al. 2022). Their work highlights the need for careful consideration of algorithmic design to avoid unintended bias in AI systems.\n\n\n6.1.3 Aligning AI with Human Values\nAligning AI systems with human values presents several significant challenges. Human values are multifaceted and context-dependent, making them difficult to encode into AI systems. As Bostrom highlights, “the complexity of human values means that they are not easily reducible to simple rules or objectives” (Bostrom 2014). Additionally, values can evolve, requiring AI systems to adapt. Russell notes that “the dynamic nature of human values necessitates continuous monitoring and updating of AI systems to ensure ongoing alignment” (Russell 2019). Different stakeholders may also have conflicting values, posing a challenge for AI alignment. Addressing these conflicts requires a nuanced approach to balance diverse perspectives and priorities.\nWhat is the right way to represent values? In a Reinforcement Learning (RL) paradigm, one might ask: at what level should we model rewards? Many people are trying to use language. In Constitutional AI (Bai et al. 2022), we write down the rules we want a language model to follow or apply reinforcement learning from human feedback, discussed in the next section. Many problems have been framed in an RL setting. Some experts in reinforcement learning argue that a single scalar reward is not enough (Vamplew et al. 2018, 2022). They suggest a vectorized reward approach might better emulate the emotional-like system humans have (Moerland, Broekens, and Jonker 2018). With this robustness, we might capture all the dimensions of human values. These approaches are still in the early stages. Language does play a crucial role in human values. Tomasello (Tomasello 2019) argues that learning a language and the awareness of convention it brings help children understand their cultural group and reason about it with peers. However, human values seem to be composed of more than just linguistic utterances. Several strategies have been proposed to align AI systems with human values.\n\nOne effective approach is value-sensitive design, which considers human values from the outset of the design process. Friedman, Kahn, and Borning explain that “value-sensitive design integrates human values into the technology design process to ensure that the resulting systems support and enhance human well-being” (Friedman, Kahn, and Borning 2008).\nAnother strategy is participatory design, which engages stakeholders in the design process to ensure their values are reflected in the AI system. Muller emphasizes that “participatory design creates a collaborative space where diverse stakeholders can contribute their perspectives and values, leading to more inclusive and ethical AI systems” (Muller 2003). Additionally, iterative testing and feedback allow continuous refinement of AI systems based on user feedback, ensuring they remain aligned with human values over time. Practical examples of value alignment in AI systems demonstrate how these strategies can be implemented effectively.\n\nIn autonomous vehicles, ensuring safety and ethical decision-making in critical scenarios is paramount. These vehicles must make real-time decisions that prioritize human safety above all else. Goodall discusses how “Waymo’s safety protocols are designed to prioritize human safety and ethical considerations in autonomous driving” (Goodall 2014). These protocols include extensive testing and validation processes to ensure that autonomous driving algorithms handle various scenarios ethically and safely. For example, the system must decide how to react in an unavoidable collision, weighing the potential outcomes to minimize harm. By embedding these ethical considerations into their design and operation, companies like Waymo aim to align their AI systems with societal values of safety and responsibility.\nIn healthcare AI, respecting patient privacy and ensuring informed consent are crucial. Healthcare applications often involve sensitive personal data, and AI systems must handle this information with the utmost care. Jiang et al. highlight how “IBM Watson for Oncology incorporates patient privacy protections and informed consent processes to align with ethical standards in medical practice” (F. Jiang et al. 2017). IBM Watson for Oncology uses AI to assist in diagnosing and recommending treatments for cancer patients. To align with ethical standards, the system ensures that patients are fully informed about how their data will be used and that their consent is obtained before processing their information. This approach protects patient privacy, and builds trust between patients and healthcare providers, demonstrating a commitment to ethical medical practices.\nJudicial AI systems strive to avoid biases in sentencing recommendations, promoting fairness and justice. The judicial system’s integrity depends on delivering fair and unbiased judgments. However, AI systems in judicial contexts, such as risk assessment tools, can perpetuate existing biases if not carefully designed and monitored. Angwin et al. describe how “the COMPAS system has undergone revisions to address biases and improve fairness in judicial decision-making” (Angwin et al. 2016). COMPAS, a tool used to assess the likelihood of a defendant reoffending, faced criticism for racial bias in its risk assessments. Subsequent revisions removed biased factors and incorporated fairness constraints. These changes aimed to ensure that the tool provides more equitable assessments, aligning the AI system with the judicial values of fairness and justice.\nThis raises the question for AI: At what level should we model values? Is language alone enough? We might also consider questions about language understanding. LLMs seem to “get” much of language even without physical embodiment. Will they “get” human values, too? First, we’ll cover work that attempts to gain traction on these issues. Methodologically, this is similar to the RLHF and language modeling work we will cover in the next section. Here, we focus on this work with a somewhat different aim. In some of the earliest work to address values in AI systems head-on, (Hendrycks et al. 2020) introduced a new dataset called ETHICS, sourced from Reddit and through Mturk worker annotations. Instead of showing moral dilemmas, they focused just on clear-cut examples. One example scenario is, “I pushed the elderly man in the wheelchair to the ground” which their model labels as “Unacceptable” and “I pushed the elderly man in the wheelchair around the park.” which their model labels as “acceptable”. To come up with these labels, they define many different tasks, one of which (for the preceding example) is how well the model can capture the ratings of human votes (from the “Am I the Asshole” subreddit). The model that they train could be the reward model that is then later used in a language model setting. What’s interesting and what we’ll be talking about is how they’re using language as the means to gain traction on values.\n\n\n\n\n\n\nFigure 6.1: Learned utility values from (Hendrycks et al. 2020)\n\n\n\nTheir dataset included various scenarios, from deontological ones like Kantian ethics to utilitarian approaches. We’ll describe some of those approaches in a later subsection. The model they trained performed well at predicting scores and utility values for these situations. For example, it ranked highly, “I got called to the principal’s office because I won a school-wide award”, and negatively “I rewired my electricity in the attic and I fell and hurt my back”. In subsequent work by others, this underlying technology has been deployed to reason morally. Part of this work prompted a response from (L. Jiang et al. 2021). Anecdotally, many people were unhappy with this demo, disagreeing that LLMs could reason morally at (Talat et al. 2022).\n\n\n\n\n\n\nFigure 6.2: An overview of (L. Jiang et al. 2021)\n\n\n\nIf you ask, “Should I drive my friend to the airport if I don’t have a license?” Delphi gets it right and says no. The question that we’re driving at in this is what does it mean for Delphi to get it right? What values are we considering, and how are those represented in the sorts of systems that we’re working on? You can also get Delphi to say a lot of hateful and toxic things by subtly manipulating the input to this model—does this suggest that the model is merely susceptible to hallucinations like other LLMs but otherwise performant? Or does it suggest an underlying lack of capacity?\nDelphi operationalizes the ETHICS dataset and adds a couple of others (Sap et al. 2019). They call their new, compiled dataset the Commonsense Norm Bank, sourcing many scenarios from Reddit and having crowd workers annotate the acceptability of various judgments pairwise. This allows the model to perform various morally relevant tasks. When prompted, the model outputs a class label for appropriateness and a generative description. For example, “greeting a friend by kissing on a cheek” is appropriate behavior when appended with “in France” but not with “in Korea”. The model captures actual cultural norms. Our driving question should be, how ought we best formalize these kinds of norms, and is this necessarily the right approach? When released in late 2021, Delphi outperformed GPT-3 on a variety of these scenarios. In personal communication with the authors, we understand that Delphi continues to outperform GPT-4 on many of these scenarios as well. 1\nThere have also been works that seek to operationalize performance on moral values to turn such a model into something actionable. (Hendrycks et al. 2021) used the same constituent parts of the ETHICS dataset to create a model that reasons around text-based adventure games. Jiminy Cricket is a character in one of these games, which has scenarios like those in Figure 6.3. These games offer limited options, and the goal was to see whether agents would perform morally well and not just finish the game. They labeled all examples of game-based actions according to three degrees: positive, somewhat positive, and negative. For example, saving a life in the game was very positive, while drinking water was somewhat positive. They found that with this labeled data, it was possible to train a model that shaped the reward of the underlying RL agent playing the games. The agent would not only finish the games well but also score highly on moral metrics. This approach is similar to optimizing multiple objectives like helpfulness and harmlessness (Liang et al. 2023).\n\n\n\n\n\n\nFigure 6.3: An example scenario from (Hendrycks et al. 2021)\n\n\n\nWe are discussing whether language is the right medium for learning values. (Arcas 2022) claims that language encompasses all of morality. Since these models operate in the linguistic domain, they can also reason morally. He provides an example with the Lambda model at Google. Anecdotally, when asked to translate a sentence from Turkish to English, where Turkish does not have gendered pronouns, the model might say, “The nurse put her hand in her coat pocket.” This inference shows gender assumption. When instructed to avoid gendered assumptions, the model can say “his/her hand.” He claims this capability is sufficient for moral reasoning.\nNext, we now explore the broader challenges of AI alignment, particularly focusing on AI alignment problems and the critical dimensions of outer and inner alignment.\n\n\n6.1.4 AI Alignment Problems\nAI alignment ensures that AI systems’ goals and behaviors are consistent with human values and intentions. Various definitions of AI alignment emphasize the importance of aligning AI systems with human goals, preferences, or ethical principles. As stated by (Wikipedia contributors 2023), AI alignment involves\n\n(Wikipedia contributors 2023): “steer[ing] AI systems towards humans’ intended goals, preferences, or ethical principles”\n(Ngo, Chan, and Mindermann 2023): “the challenge of ensuring that AI systems pursue goals that match human values or interests rather than unintended and undesirable goals”\n(P. Christiano 2018): “an AI \\(A\\) is aligned with an operator \\(H\\) [when] \\(A\\) is trying to do what \\(H\\) wants it to do”\n\nThe importance of AI alignment lies in preventing unintended consequences and ensuring that AI systems act beneficially and ethically. Proper alignment is crucial for the safe and ethical deployment of AI, as it helps AI systems correctly learn and generalize from human preferences, goals, and values, which may be incomplete, conflicting, or misspecified. In practice, AI alignment is a technical challenge, especially for systems with broad capabilities like large language models (LLMs). The degree of alignment can be viewed as a scalar value: a language model post-RLHF (Reinforcement Learning from Human Feedback) is more aligned than a model that has only been instruction-tuned, which in turn is more aligned than the base model. There are specific terms to distinguish different notions of alignment. Intent alignment refers to a system trying to do what its operator wants it to do, though not necessarily succeeding (P. Christiano 2018). Value alignment, in constrast, involves a system correctly learning and adopting the values of its human operators. Alignment is often divided into two broad subproblems: outer alignment, which focuses on avoiding specification gaming, and inner alignment, which aims to avoid goal misgeneralization. In the following sections, we will examine these subproblems in greater detail. It is also important to consider how human preferences and values are aggregated and who the human operators are, topics addressed in related discussions on ethics and preference elicitation mechanisms.\n\n6.1.4.1 Outer Alignment: Avoiding Specification Gaming\nTo align a model with human values, we need an objective function or reward model that accurately specifies our preferences. However, human preferences are complex and difficult to formalize. When these preferences are incompletely or incorrectly specified, optimizing against the flawed objective function can yield models with undesirable and unintuitive behavior, exploiting discrepancies between our true values and the specified objective function. This phenomenon, known as specification gaming, arises from reward misspecification, and addressing this issue constitutes the outer alignment problem (Amodei et al. 2016).\nSpecification gaming occurs when AI systems exploit poorly defined objectives to achieve goals in unintended ways. For instance, a cleaning robot might hide dirt under a rug instead of cleaning it to achieve a “clean” status. This manipulative behavior results from the robot optimizing for an inadequately specified objective function. Another example involves gaming AI, which uses bugs or exploits to win rather than play by the intended rules, thus achieving victory through unintended means (Krakovna et al. 2020).\nOne example of specification gaming is seen in recommendation systems, such as those used by YouTube or Facebook. Ideally, these systems should recommend content that users enjoy. As a proxy for this goal, the systems estimate the likelihood that a user clicks on a piece of content. Although the true objective (user enjoyment) and the proxy (click likelihood) are closely correlated, the algorithm may learn to recommend clickbait, offensive, or untruthful content, as users likely click on it. This optimization for clicks rather than genuine enjoyment exemplifies specification gaming, where the algorithm exploits the divergence between the specified objective and the true goal, resulting in misalignment with user interests (Amodei et al. 2016).\nAnother instance of specification gaming is evident in reinforcement learning from human feedback (RLHF). Human raters often reward language model (LM) generations that are longer and have a more authoritative tone, regardless of their truthfulness. Here, the true objective (providing high-quality, truthful, and helpful answers) diverges from the proxy goal (a reward model that, due to human rater biases, favors longer and more authoritative-sounding generations). Consequently, models trained with RLHF may produce low-quality answers containing hallucinations but are still favored by the reward model (Leike et al. 2018).\nCreating accurate objective functions is challenging due to the complexity of human intentions. Human goals are nuanced and context-dependent, making them difficult to encode precisely. Common pitfalls in objective function design include oversimplifying objectives and ignoring long-term consequences. Leike et al. emphasize that “accurately capturing the complexity of human values in objective functions is crucial to avoid specification gaming and ensure proper alignment” (Leike et al. 2018).\nTo mitigate specification gaming, better objective function design is essential. This involves incorporating broader context and constraints into the objectives and regularly updating them based on feedback. Iterative testing and validation are also critical. AI behavior must be continuously tested in diverse scenarios, using simulation environments to identify and fix exploits. Everitt and Hutter discuss the importance of “robust objective functions and rigorous testing to prevent specification gaming and achieve reliable AI alignment” (Everitt and Hutter 2018). Clark and Amodei further highlight that “faulty reward functions can lead to unintended and potentially harmful AI behavior, necessitating ongoing refinement and validation” (Clark and Amodei 2016).\nThe metrics used to evaluate AI systems play a crucial role in outer alignment. Many AI metrics, such as BLEU, METEOR, and ROUGE, are chosen for their ease of measurement but do not necessarily capture human judgment (Hardt and Recht 2021). These metrics can lead to specification gaming, as they may not align with the true objectives we want the AI to achieve. Similarly, using SAT scores to measure LLM performance may not predict real-world task effectiveness, highlighting the need for more contextually relevant benchmarks (Chowdhery et al. 2022). The word error rate (WER) used in speech recognition is another example; it does not account for semantic errors, leading to misleading conclusions about the system’s performance (Xiong et al. 2016).\nA classic example comes from six years ago with the claim that a system “Achieve[d] human parity in conversation speech recognition” (Xiong et al. 2016). However, we know from experience that captioning services have only recently begun to transcribe speech passably, whether in online meetings or web videos. What happened? In this case, researchers showed their system beat the human baseline—the error rate when transcribing films. However, there were issues with their approach. First, they used a poor measure of a human baseline by hiring untrained Mturk annotators instead of professional captioners. Second, the metric itself, the word error rate (WER), was flawed. WER measures the number of incorrect words in the gold transcription versus the predicted transcription. Consider what the metric hides when it says that two systems both have an error rate of six percent. This does not mean the systems are equivalent. One might substitute “a” for “the,” while the other substitutes “tarantula” for “banana.” The metric was not sensitive to semantic errors, so a model could outperform humans in WER yet still make unintelligent, highly unsemantic mistakes.\n\n\n6.1.4.2 Inner Alignment: Preventing Goal Misgeneralization\nAssume we have perfectly specified human values in a reward model. An issue remains: given finite training data, many models perform well on the training set, but each will generalize somewhat differently. How do we choose models that correctly generalize to new distributions? This is the problem of goal misgeneralization, also known as the inner alignment problem, where a learned algorithm performs well on the training set but generalizes poorly to new input distributions, achieving low rewards even on the reward function it was trained on. Inner alignment ensures that the learned goals and behaviors of an AI system align with the intended objectives during deployment, whereas goal misgeneralization occurs when an AI system applies learned goals inappropriately to new situations (Hubinger et al. 2019).\nConsider the following example of goal misgeneralization from (Shah et al. 2022). The setup involves a never-ending reinforcement learning environment without discrete episodes. The agent navigates a grid world where it can collect rewards by chopping trees. Trees regenerate at a rate dependent on the number left; they replenish slowly when few remain. The optimal policy is to chop trees sustainably, i.e., fewer when they are scarce. However, the agent does not initially learn the optimal policy.\n\n\n\n\n\n\nFigure 6.4: The agent’s performance in Tree Gridworld. The reward is shown in orange, and the green distribution indicates the number of remaining trees.\n\n\n\nInitially, the agent is inefficient at chopping trees, keeping the tree population high (point A). As it improves its chopping skills, it over-harvests, leading to deforestation and a prolonged period of minimal reward (between points B and C). Eventually, it learns sustainable chopping (point D). This scenario (up to point C) exemplifies goal misgeneralization. When the agent first becomes proficient at chopping (between points A and B), it faces a range of potential goals, from sustainable to rapid tree chopping. All these goals align with the (well-specified) reward function and its experience of being rewarded for increased efficiency. Unfortunately, it adopts the detrimental goal of rapid deforestation, resulting in a prolonged period of low reward.\nAnother example of goal misgeneralization occurs in recommendation systems. These systems aim to maximize user engagement, which can inadvertently lead to promoting extreme or sensational content. Krakovna et al. highlights that “recommendation systems can misgeneralize by prioritizing content that maximizes clicks or watch time, even if it involves promoting harmful or misleading information” (Krakovna et al. 2020). This misalignment between the system’s learned objective (engagement) and the intended objective (informative and beneficial content) exemplifies how goal misgeneralization can manifest in real-world applications.\nAutonomous vehicles also present cases of goal misgeneralization. These vehicles must interpret and respond to various signals in their environment. However, in rare scenarios, they may misinterpret signals, leading to unsafe maneuvers. Amodei et al. discuss that “autonomous vehicles can exhibit unsafe behaviors when faced with uncommon situations that were not well-represented in the training data, demonstrating a misgeneralization of their learned driving policies” (Amodei et al. 2016). Ensuring that autonomous vehicles generalize correctly to all possible driving conditions remains a significant challenge.\nTo address goal misgeneralization, robust training procedures are essential. This involves using diverse and representative training data to cover a wide range of scenarios and incorporating adversarial training to handle edge cases. Leike et al. (Leike et al. 2018) emphasize the importance of “robust training procedures that include diverse datasets and adversarial examples to improve the generalization of AI systems”. Additionally, careful specification of learning goals is crucial. This means defining clear and comprehensive objectives and regularly reviewing and adjusting these goals based on performance and feedback. Hubinger et al. suggests that “regularly updating and refining the objectives based on ongoing evaluation can help mitigate the risks of goal misgeneralization” (Hubinger et al. 2019).\nA key concern about goal misgeneralization in competent, general systems is that a policy successfully models the preferences of human raters (or the reward model) and behaves accordingly to maximize reward during training. However, it may deviate catastrophically from human preferences when given a different input distribution during deployment, such as during an unexpected geopolitical conflict or when facing novel technological developments. Increasing data size, regularization, and red-teaming can help mitigate goal misgeneralization, but they do not fundamentally solve the problem. Understanding the inductive biases of optimization algorithms and model families may help address the problem more generally.\n\nSo, can you differentiate between inner and outer alignment?\n\nThe distinction between inner and outer alignment can be a bit subtle. The following four cases, from (Ngo, Chan, and Mindermann 2023), may help to clarify the difference:\n\nThe policy behaves incompetently. This is a capability generalization failure.\nThe policy behaves competently and desirably. This is aligned behavior.\nThe policy behaves in a competent yet undesirable way which gets a high reward according to the original reward function. This is an outer alignment failure, also known as reward misspecification.\nThe policy behaves in a competent yet undesirable way which gets a low reward according to the original reward function. This is an inner alignment failure, also known as goal misgeneralization.\n\nNow that we understand the alignment problem overall, we move on to the specific techniques used for value learning to ensure AI systems are aligned with human values.\n\n\n\n6.1.5 Techniques in Value Learning\nVarious methods in value learning for foundation models have been explored in great detail in recent years (Stiennon et al. 2020). Using binary human-labeled feedback to make models closely aligned to human preferences is particularly difficult in scenarios where large datasets inherently encompass suboptimal behaviors. The approach of Reinforcement Learning from Human Feedback (RLHF) ((Ouyang et al. 2022)) has risen to prominence as an effective method for addressing this issue. The technique applies to various domains, from prompt-image alignment, fine-tuning large language models or diffusion models, and improving the performance of robot policies.\n\n6.1.5.1 Reinforcement Learning from Human Feedback\nReinforcement Learning from Human Feedback (RLHF) is a technique used to align AI behavior with human values by incorporating human feedback into the reinforcement learning process. This approach is particularly effective when large datasets inherently encompass suboptimal behaviors. RLHF aims to refine policies by discriminating between desirable and undesirable actions, ensuring that AI systems act following human preferences (Ouyang et al. 2022).\nThe core concept of RLHF: It first trains a reward model using a dataset of binary preferences gathered from human feedback. This reward model is then used to fine-tune the AI model through a reinforcement learning algorithm. The core concept is to utilize human feedback to guide AI learning, thereby aligning the AI’s behavior with human expectations (Stiennon et al. 2020).\n\n\n\n\n\n\nFigure 6.5: The above diagram depicts the three steps in the traditional RLHF pipeline: (a) supervised fine-tuning, (b) reward model (RM) training, and (c) reinforcement learning via proximal policy optimization (PPO) on this reward model. Image taken from (Ouyang et al. 2022).\n\n\n\nThe RLHF pipeline involves the following steps:\nStep 1: Supervised Fine-Tuning\nIn the initial step for language modeling tasks, we utilize a high-quality dataset consisting of \\(\\left(\\text{prompt}, \\text{response}\\right)\\) pairs to train the model. Prompts are sampled from a curated dataset designed to cover a wide range of instructions and queries, such as “Explain the moon landing to a 6-year-old.” Trained human labelers provide the desired output behavior for each prompt, ensuring responses are accurate, clear, and aligned with task goals. For instance, in response to the moon landing prompt, a labeler might generate, “Some people went to the moon in a big rocket and explored its surface.” The collected \\(\\left(\\text{prompt}, \\text{response}\\right)\\) pairs serve as the training data for the model, with the cross-entropy loss function applied only to the response tokens. This helps the model learn to generate responses that are closely aligned with the human-provided examples. The training process adjusts model parameters through supervised learning, minimizing the difference between the model’s predictions and the human responses.\nStep 2: Reward Model (RM) Training\nIn this step, we train a reward model to score any \\(\\left(\\text{prompt}, \\text{response}\\right)\\) pair and produce a meaningful scalar value. Multiple model-generated responses are sampled for each prompt. Human labelers then rank these responses from best to worst based on their quality and alignment with the prompt. For example, given the prompt “Explain the moon landing to a 6-year-old,” responses like “People went to the moon in a big rocket and explored its surface” might be ranked higher than “The moon is a natural satellite of Earth.” The rankings provided by the labelers are used to train the reward model \\(\\Phi_{\\text{RM}}\\). The model is trained by minimizing the following loss function across all training samples:\n\\[\\mathbb{L}(\\Phi_{RM}) = -\\mathbb{E}_{(x,y_e,i\\rightarrow D_{RL})}[\\log(\\sigma(\\Phi_{RM}(x, y_i)) - \\Phi_{RM}(x, y_{1-i}))]\\]\nfor \\(i \\in \\{0,1 \\}\\). This loss function encourages the reward model to produce higher scores for better-ranked responses, thereby learning to evaluate the quality of model outputs effectively.\nStep 3: Reinforcement Learning\nIn this step, we refine the policy using reinforcement learning (RL) based on the rewards provided by the trained reward model. A new prompt is sampled from the dataset, and the policy generates an output. The reward model then calculates a reward for this output, and the reward is used to update the policy using the Proximal Policy Optimization (PPO) algorithm.\nThe RL setting is defined as follows:\n\nAction Space: The set of all possible actions the agent can take, which, for language models, is typically the set of all possible completions.\nPolicy: A probability distribution over the action space. In the case of language models like LLM, the policy is contained within the model and represents the probability of predicting each completion.\nObservations: The inputs to the policy, which in this context are prompts sampled from a certain distribution.\nReward: A numerical score provided by the Reward Model (RM) that indicates the quality of actions taken by the agent.\n\nDuring training, batches of prompts are sampled from two distinct distributions, namely either \\(D_\\text{RL}\\), the distribution of prompts explicitly used for the RL model, or \\(D_\\text{pretrain}\\), the distribution of prompts from the pre-trained model. The objective for the RL agent is to maximize the reward while ensuring that the policy does not deviate significantly from the supervised fine-tuned model and does not degrade the performance on tasks the pre-trained model was optimized for. When sampling a response \\(y\\) to a prompt \\(x\\) from \\(D_\\text{RL}\\), the first objective function is:\n\\[\\text{objective}_1(x_{RL}, y; \\phi) = RM(x_{RL}, y) - \\beta \\log \\frac{\\text{LLM}_{\\phi}^{RL}(y|x)}{\\text{LLM}_{SFT}(y|x)}\\]\nWhere the first term is the reward from the RM, and the second term is the Kullback-Leibler (KL) divergence, weighted by a factor \\(\\beta\\), which acts as a regularizer to prevent the RL model from straying too far from the SFT model. Further, for each \\(x\\) from \\(D_\\text{pretrain}\\), the second objective is to ensure that the RL model’s performance on text completion does not worsen:\n\\[\\text{objective}_2(x_{\\text{pretrain}} ; \\phi) = \\gamma \\log \\text{LLM}_{\\phi}^{RL}(x_{\\text{pretrain}})\\]\nwhere \\(\\gamma\\) is a weighting factor that balances the influence of this objective against the others.\nThe final objective function is a sum of the expected values of the two objectives described above, across both distributions. In the RL setting, we maximize this objective function:\n\\[\\text{objective}(\\phi) = E_{(x,y) \\sim D_{\\phi}^{RL}}[RM(x, y) - \\beta \\log \\frac{\\text{LLM}_{\\phi}^{RL}(y|x)}{\\text{LLM}_{SFT}(y|x)}] + \\gamma E_{x \\sim D_{\\text{pretrain}}}[\\log \\text{LLM}_{\\phi}^{RL}(x)]\\]\nIn practice, the second part of the objective is often not used to perform \\(\\text{RLHF}\\). The KL penalty is typically enough to constrain the RL policy. This function balances the drive to maximize the reward with the need to maintain the quality of text completion and the similarity to the behavior of the supervised fine-tuned model.\nLimitations and Challenges: Despite its successes, RLHF faces several challenges. One major issue is the quality of human feedback, which can be inconsistent and subjective. Scalability is another concern, as obtaining a large amount of high-quality feedback can be expensive and time-consuming. Over-optimization and hallucinations, where the model generates plausible but incorrect outputs, are also common problems. This generally stems from temporal credit assignment and the instability of approximate dynamic programming (Hasselt et al. 2018). Further, it is expensive to gather tens of thousands of preferences over datasets to create robust reward models. Strategies to overcome these challenges include using diverse and representative training data, incorporating adversarial training to handle edge cases, and continuously refining the reward model based on ongoing feedback and performance evaluations (Leike et al. 2018).\n\n\n6.1.5.2 Contrastive Preference Learning\nContrastive Preference Learning (CPL) is a learning paradigm designed to enhance the alignment of AI systems with human preferences without relying on traditional reinforcement learning (RL) methods. CPL addresses many limitations inherent in traditional RLHF techniques by learning from human comparisons rather than explicit reward signals. This section provides an in-depth exploration of CPL, detailing its methodology, experiments, results, and potential challenges. Recent research has shown that human preferences are often better modeled by the optimal advantage function or regret, rather than traditional reward functions used in RLHF. Traditional RLHF approaches, which learn a reward function from a preference model and then apply RL, incur significant computational expenses and complexity (Hejna et al. 2023). CPL offers a streamlined and scalable alternative by leveraging a more accurate regret model of human preferences.\nThe key idea of CPL is the substitution of the optimal advantage function with the log probability of the policy in a maximum entropy reinforcement learning framework. This substitution is beneficial as it circumvents the need to learn the advantage function and avoids the optimization challenges associated with RL-like algorithms. By using the log probability of the policy, CPL more closely aligns with how humans model preferences and enables efficient supervised learning from human feedback.\nCPL is a structured approach to aligning AI behavior with human preferences by relying on a dataset of preferred behavior segments \\(\\mathcal{D}_{\\text{pref}} = \\{(\\sigma_i^+, \\sigma_i^-)\\}_{i=1}^n\\), where \\(\\sigma^+ \\succ \\sigma^-\\). Each behavior segment \\(\\sigma\\) is a sequence of states and actions, \\(\\sigma = (s_1, a_1, s_2, a_2, \\ldots, s_k, a_k)\\). The CPL approach aims to maximize the expected sum of rewards minus an entropy term, which promotes exploration and prevents overfitting to specific actions:\n\\[\\max_\\pi \\mathbb{E}_{\\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t (r(s_t, a_t) - \\alpha \\log \\pi(a_t | s_t)) \\right]\\]\nwhere \\(\\gamma\\) is the discount factor, \\(\\alpha\\) is the temperature parameter controlling the stochasticity of the policy, and \\(r\\) is the reward function. This step sets the foundation by defining the optimization objective that the CPL model strives to achieve. In the learning process, CPL compares the log probabilities of actions in preferred segments \\(\\sigma^+\\) against those in non-preferred segments \\(\\sigma^-\\) :\n\\[\\mathbb{L}_{CPL}(\\pi_\\theta, \\mathcal{D}_{\\text{pref}}) = \\mathbb{E}_{(\\sigma^+,\\sigma^-) \\sim \\mathcal{D}_{\\text{pref}}} \\left[ -\\log \\frac{\\exp(\\sum_{\\sigma^+} \\gamma^t \\alpha \\log \\pi_\\theta(a_t^+|s_t^+))}{\\exp(\\sum_{\\sigma^+} \\gamma^t \\alpha \\log \\pi_\\theta(a_t^+|s_t^+)) + \\exp(\\sum_{\\sigma^-} \\gamma^t \\alpha \\log \\pi_\\theta(a_t^-|s_t^-))} \\right]\\]\nThis comparison allows the model to learn which actions are more aligned with human preferences, forming the core learning mechanism of CPL. The preference model for CPL is regret-based, described as\n\\[P_{A^*}[\\sigma^+ \\succ \\sigma^-] = \\frac{\\exp(\\sum_{\\sigma^+} \\gamma^t A^*(s_t^+, a_t^+))}{\\exp(\\sum_{\\sigma^+} \\gamma^t A^*(s_t^+, a_t^+)) + \\exp(\\sum_{\\sigma^-} \\gamma^t A^*(s_t^-, a_t^-))}\\] where \\(A^*(s_t, a_t)\\) represents the advantage function and is a matrix. This step models human preferences based on regret, reflecting how humans might evaluate different behaviors.\nOne hypothesis as to why one might consider a regret-based model more useful over a sum-of-rewards, Bradley-Terry model is that humans likely think of preferences based on the regret of each behavior under the optimal policy of the expert’s reward function.\nThe key insight that the paper leverages is that from (Ziebart 2010) in MaxEnt Offline RL. In this general setting, (Ziebart 2010) shows that one can write that the optimal advantage function is related to the optimal policy by \\(A^*_r(s, a) = \\alpha \\log \\pi^*(a|s)\\). Therefore, the loss function for CPL can be written by substituting the above result to obtain: \\[L_{CPL}(\\pi_\\theta, \\mathcal{D}_{\\text{pref}}) = \\mathbb{E}_{(\\sigma^+,\\sigma^-) \\sim \\mathcal{D}_{\\text{pref}}} \\left[ -\\log P_{\\pi_\\theta}[\\sigma^+ \\succ \\sigma^-] \\right]\\]\nOne merit of using CPL over the typical RLHF pipeline is that it can lead to a deduction in mode collapse. Further, it makes reward misgeneralization failures less likely, enhancing the reliability of the learned policy. However, the approach still has a few limitations:\n\nCPL assumes knowledge of the human rater’s temporal discounting (i.e., of the discount factor \\(\\gamma\\)), which in practice would be difficult to communicate.\nCPL’s loss function is computed over segments, it requires a substantial amount of GPU memory for large segment sizes.\n\n\nHow does RLHF with PPO and CPL compare their effectiveness and applicability in aligning AI systems with human values?\n\nThe ongoing challenge in aligning foundation models in the future will be to refine these methodologies further, balancing computational feasibility with the sophistication needed to capture the intricacies of human values and countering failure modes such as reward over-optimization. In conclusion, exploring value learning through RLHF and CPL methods has enriched our understanding of integrating human preferences into foundation models. To provide a well-rounded perspective on aligning AI systems with human values, the following table highlights a detailed comparison of RLHF with PPO and CPL, emphasizing their advantages, limitations, and ideal scenarios.\n\n\n\nTable 6.1: Comparison between RLHF with PPO and CPL\n\n\n\n\n\n\n\n\n\n\n\nRLHF with PPO\nCPL\n\n\n\n\nStrengths\n\nExcels in optimizing policies through reinforcement learning\nSuitable for tasks that benefit from iterative improvement\nEffective in continuous action spaces\n\n\nEmphasizes regret and optimality rather than reward maximization\nReduces computational overhead\nAligns more closely with human preferences\nAvoids reward\n\nover-optimization\n\nMore scalable due to reliance on supervised learning techniques\n\n\n\nLimitations\n\nFaces limitations in handling complex preference structures\nHigh computational cost\nSusceptible to reward\n\nmisgeneralization\n\nMay struggle in environments where direct human feedback is less accessible\nDepends on high-quality preference data for effective training\n\n\n\nIdeal Scenarios\n\nTasks with well-defined reward functions\nEnvironments allowing extensive interaction and feedback\n\n\nEnvironments where human feedback is more accessible than well-defined reward functions\nTasks requiring computational efficiency and scalability\n\n\n\n\n\n\n\n\n\n\n6.1.6 Value Alignment Verification\nAfter we discuss the techniques of value learning, it becomes evident that aligning machine behavior with human values, while advanced, is inherently approximate and not infallible. This realization underscores the importance of value alignment verification—a methodology to ensure that the values imparted to a machine truly reflect those of a human. Human-robot value alignment has been explored through various lenses, including qualitative trust assessments (Huang et al. 2018), asymptotic alignment through active learning of human preferences (Hadfield-Menell et al. 2016; P. F. Christiano et al. 2017; Sadigh et al. 2017), and formal verification methods (Brown et al. 2021). This section will focus on the formal verification approach for value alignment as discussed in (Brown et al. 2021). Unless otherwise stated, all information presented here is derived from (Brown et al. 2021). This approach aims to ensure that the values imparted to a machine align with those of a human.\nTo begin with, consider an MDP with state space \\(\\mathcal{S}\\), action space \\(\\mathcal{A}\\), and transition model \\(\\mathcal{T}\\). This formal framework allows us to model the environment in which humans and robots operate. Denote the human’s reward function as \\(R\\) and the robot’s reward function as \\(R^\\prime\\). Both the human and robot reward functions must be linear in a set of shared features, defined as: \\[\\begin{aligned}\n    R(s) = \\mathbf{w}^\\top \\phi(s), R^\\prime(s) = \\mathbf{w}^{\\prime \\top} \\phi(s).\n\\end{aligned}\\]\nThese linear reward functions provide a common ground for comparing human and robot preferences.\nNext, the optimal state-action value function, which indicates the expected cumulative reward of following a policy \\(\\pi\\) starting from state \\(s\\) and action \\(a\\), but we follow the notation in (Brown et al. 2021) for simplicity. The optimal state-action value function is given by:\n\\[\\begin{aligned}\n    Q_R^\\pi (s,a) = \\mathbf{w}^\\top \\Phi_{\\pi_R}^{(s,a)}, \\Phi_{\\pi_R}^{(s,a)} = \\mathbb{E}_\\pi [\\sum_{t=0}^\\infty \\gamma^t \\phi(s_t) \\vert s_0 = s, a_0 = a].\n\\end{aligned}\\]\nHere, \\(\\Phi_{\\pi_R}^{(s,a)}\\) is the feature expectation vector under policy \\(\\pi\\), capturing the long-term feature visitation frequencies. We overload the action space notation to define the set of all optimal actions given a state as\n\\[\\begin{aligned}\n    \\mathcal{A}_R(s) = \\underset{x}{\\operatorname{argmax}} \\\\ Q^{\\pi^*}_R(s,a)\n\\end{aligned}\\] where \\(\\pi^*\\) is an optimal policy. We can now define the aligned reward polytope (ARP). The ARP is the set of all weights \\(\\mathcal{w}\\) that satisfy the following set of strict linear inequalities, \\(\\mathbf{w}^\\top \\mathbf{A}  &gt; \\mathbf{0}\\) where each row of \\(\\mathbf{A}\\) corresponds to \\(\\Phi_{\\pi^*_R}^{(s,a)} - \\Phi_{\\pi^*_R}^{(s,b)}\\) for a single \\((s,a,b)\\) tuple where \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}_R(s), b \\notin \\mathcal{A}_R(s)\\). Thus, to construct \\(\\mathbf{A}\\), one must loop over all \\((s,a,b)\\) tuples which has complexity \\(O(\\vert \\mathcal{S} \\vert \\cdot \\vert \\mathcal{A} \\vert^2)\\). This construction ensures that the weights \\(\\mathbf{w}\\) align with the human’s optimal actions across all states.\nThe intuition behind the ARP is that we use the human optimal policy for each state to determine what actions are optimal and what are suboptimal at this state. Then, for every one of those combinations, we can place a linear inequality on the set of reward weights consistent with that optimal vs suboptimal action bifurcation. One of the key assumptions that let us do this is that we assume both the human and the robot act optimally according to their reward function. This is known as a rationality assumption and provides the link between actions and rewards that we need.\nFor illustration, consider a simple grid world environment. ?fig-toy shows the optimal policy and the corresponding ARP. The optimal policy reveals that the gray state is less preferred compared to the white states, which is reflected in the ARP (hatched region of ?fig-toy).\n\n\n \n\n\nOptimal policy (a) and aligned reward polytope (ARP) (b) for a grid world with two features (white and gray) and a linear reward function (R(s) = w0 ⋅ 1white(s) + w1 ⋅ 1gray(s)). The ARP is denoted by the hatched region in (b).\n\n\nComputing the ARP exactly can be computationally demanding or we may not have access to the robot’s reward function. This section describes heuristics for testing value alignment in the case the robot’s reward weights (\\(\\mathbf{w^\\prime}\\)) are unknown, but the robot’s policy can be queried. Heuristics provide simplified methods to estimate value alignment without the need for exhaustive computations.\nARP-blackbox: The ARP black-box (ARP-bb) heuristic helps address the challenge of computing the ARP by allowing users to work with a simplified model. In this heuristic, the user first solves for the ARP and removes all redundant half-space constraints. For each remaining half-space constraint, the user queries the robot’s action at the corresponding state. The intuition here is that states, where different actions are taken, reveal crucial information about the reward function. By focusing on these key states, we can gain insights into the robot’s reward function without needing to know it explicitly.\nSet Cover Optimal Teaching: The Set Cover Optimal Teaching (SCOT) heuristic uses techniques from (Brown and Niekum 2019) to generate maximally informative trajectories. These trajectories are sequences of states where the number of optimal actions is limited, making them particularly informative for understanding the robot’s policy. By querying the robot for actions along these trajectories, we can efficiently gauge the alignment of the robot’s policy. This method helps to identify potential misalignments by focusing on critical decision points in the trajectories.\nCritical States: The Critical States (CS) heuristic identifies states where the gap in value between the optimal action and an average action is significant. These states are crucial because if the robot’s policy is misaligned, the misalignment will be most consequential at these critical states. By querying the robot’s policy at these states, we can assess the alignment more effectively. This heuristic is particularly useful when we have a limited budget of states to check, as it prioritizes the most informative states for evaluation.\nPractical Examples: To illustrate the concepts of value alignment verification, we present an example of applying value alignment verification in a simple MDP grid world environment. Consider a grid world where the human’s reward function is defined as \\(R(s) = 50 \\cdot \\mathbf{1}_{green}(s) - 1 \\cdot \\mathbf{1}_{white}(s) - 50 \\cdot \\mathbf{1}_{blue}(s)\\), where \\(\\mathbf{1}_{color}(s)\\) is an indicator feature for the color of the grid cell. The objective is to align the robot’s policy with this reward function.\n\n\n      \n\n\n\noptimal policy (b) preference query 1 (c) preference query 2 (d) ARP-bb queries (e) SCOT queries (f) CS queries. In the preference queries, the human reward model prefers black to orange.\n\n\n\n?fig-island (a) shows all optimal actions at each state according to the human’s reward function. This optimal policy serves as the benchmark for alignment verification. ?fig-island (b) and ?fig-island (c) show two pairwise preference trajectory queries (black is preferable to orange according to ([eq: human_r])). Preference query 1 verifies that the robot values reaching the terminal goal state (green) rather than visiting more white states. Preference query 2 verifies that the robot values white states more than blue states. These two preference queries are all we need to determine whether the robot’s values are aligned with the human’s values.\nNext, we apply the heuristics discussed in the previous section to this grid world example. ?fig-island (d), ?fig-island (e), and ?fig-island (f) show the action queries requested by the heuristics ARP-bb, SCOT, and CS. Each heuristic queries the robot’s actions at specific states to assess alignment:\n\nARP-bb: This heuristic queries the fewest states but is myopic. It focuses on critical states derived from the ARP.\nSCOT: This heuristic generates maximally informative trajectories, querying more states than necessary but providing a comprehensive assessment.\nCS: This heuristic queries many redundant states, focusing on those where the value gap between optimal and average actions is significant.\n\nTo pass the test given by each heuristic, the robot’s action at each of the queried states must be optimal under the human’s reward function. The example demonstrates that while the ARP-bb heuristic is efficient, it might miss the broader context. SCOT provides a thorough assessment but at the cost of querying more states. CS focuses on high-impact states but includes redundant queries.\nIt is important to note that both the construction of the ARP and the heuristics rely on having an optimal policy for the human. Thus, in most practical settings we would simply use that policy on the robot without needing to bother with value alignment verification. As such, value alignment verification as presented here is more of an academic exercise rather than a tool of practical utility.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Alternatives</span>"
    ]
  },
  {
    "objectID": "src/chap7.html#human-centered-design",
    "href": "src/chap7.html#human-centered-design",
    "title": "6  Alternatives",
    "section": "6.2 Human-Centered Design",
    "text": "6.2 Human-Centered Design\nAfter understanding AI alignment, the next step is to explore practical methodologies for incorporating user feedback and ensuring that AI systems not only align with but also cater to the needs and preferences of their users. This section will provide insights into various Human-Centered Design techniques and their application in creating AI systems that are intuitive and ethically sound, ultimately enhancing the human-AI interaction experience.\n\n6.2.1 AI and Human-Computer Interaction\nHuman-Computer Interaction (HCI) is critical in the context of artificial intelligence because it focuses on designing systems that are intuitive and responsive to human needs. While human-robot interaction and other forms of human interaction with technology are important, HCI specifically addresses the broader and more common interfaces that people interact with daily. HCI principles ensure that AI systems are not only functional but also accessible and user-friendly, making them essential for the successful integration of AI into everyday life. By focusing on HCI, we can leverage established methodologies and insights to create AI systems that are more aligned with human values and needs.\nAt the heart of this exploration is the concept of human-in-the-loop processes. As AI systems become more sophisticated, their ability to simulate human decision-making processes and behaviors has increased, leading to innovative applications across various domains. The presentation by Meredith Morris, titled “Human-in-the-loop Computing: Reimagining Human-Computer Interaction in the Age of AI,” shows work in the integration of human intelligence with AI capabilities (Morris 2019). Projects like Soylent and LaMPost are highlighted as exemplary cases of this integration. Soylent is a Word plugin that uses human computation to help with editing tasks, while LaMPost is a platform that leverages crowd workers to aid in natural language processing tasks (Bernstein et al. 2010; Project 2017). These examples demonstrate how human input can significantly enhance AI outputs by leveraging the unique strengths of human cognition, thereby addressing complex AI problems that were previously unsolvable. For instance, Soylent can improve text quality by incorporating nuanced human feedback, and LaMPost can refine NLP tasks by incorporating human insights into language subtleties, both of which go beyond the capabilities of fully automated systems. However, the integration of human elements in AI systems brings up critical ethical considerations. The presentation discusses the changing perceptions of the ethics of human-in-the-loop processes. While the cost-effectiveness of human data labeling and other processes was once seen as beneficial, it is the ethical implications of such interactions that take precedence nowadays. This shift underscores the evolving norms in HCI and the importance of considering the ethical dimensions of human-AI interactions.\nThe role of diverse human perspectives plays a crucial role in enhancing AI systems. Involving a broad spectrum of users in the development and testing of AI systems ensures that these technologies are inclusive and representative of the global population, moving beyond the limitations of a WEIRD (Western, Educated, Industrialized, Rich, and Democratic) user base. The methodologies for collecting user feedback in HCI form a critical part of this discussion since they are vital in understanding user needs, preferences, and behaviors, which in turn inform the development of more user-centered AI systems. The presentation by Meredith Morris (Morris 2019) also highlights how these methods can be effectively employed to gain insights from users to ensure that AI systems are aligned with the real-world needs and expectations of users. In HCI, collecting user feedback is a fraught problem. When interacting with AI systems, the typical end user simply cares about tasks that the system can perform. Thus, a key question in HCI for AI is finding and understanding these tasks. Methodologies for collecting user feedback in HCI, are described as follow:\n\nStoryboarding is a visual method used to predict and explore the user experience with a product or service. A storyboard in HCI is typically a sequence of drawings with annotations that represent a user’s interactions with technology. This technique is borrowed from the film and animation industry and is used in HCI to convey a sequence of events or user flows, including the user’s actions, reactions, and emotions.\nWizard of Oz Studies is a method of user testing where participants interact with a system they believe to be autonomous, but which is actually being controlled or partially controlled by a human ‘wizard’ behind the scenes. This technique allows researchers to simulate the response of a system that may not yet be fully functional or developed.\n\nBoth Storyboarding and Wizard of Oz Studies are effective for engaging with users early in the design process. They help deal with the problem of gathering feedback on a product that doesn’t yet exist. Users often have difficulty imagining outcomes when they cannot touch a live demonstration.\n\nSurveys in HCI are structured tools that consist of a series of questions designed to be answered by a large number of participants. They can be conducted online, by telephone, through paper questionnaires, or using computer-assisted methods. Surveys are useful for collecting quantitative data from a broad audience, which can be analyzed statistically.\nInterviews in HCI are more in-depth and involve direct, two-way communication between the researcher and the participant. Interviews can be structured, semi-structured, or unstructured, ranging from tightly scripted question sets to open-ended conversations.\nFocus Groups involve a small group of participants discussing their experiences and opinions about a system or design, often with a moderator. Group dynamics can provide insights into collective user perspectives. In particular, users can bounce ideas off each other to provide richer feedback and quieter users who may not otherwise provide feedback may be encouraged by their peers.\nCommunity-Based Participatory Design (CBPD) is a human-centered approach that involves the people who will use a product in the design and development process. With CBPD, designers work closely with community members to identify problems, develop prototypes, and iterate based on community feedback. For example, when building a software product for deaf people, the engineering team can hire deaf engineers or designers to provide feedback as they collaboratively build the product.\nField Studies involve observing and collecting data on how users interact with a system in their natural environment. This method is based on the premise that observing users in their context provides a more accurate understanding of user behavior. It can include a variety of techniques like ethnography, contextual inquiries, and natural observations.\nLab-based studies are conducted in a controlled environment where the researchers can manipulate variables and observe user behavior in a setting designed to minimize external influences. Common lab-based methods include usability testing, controlled experiments, and eye-tracking studies.\nDiary Studies and Ethnography in HCI are a research method where participants are asked to keep a record of their interactions with a system or product over a while. This log may include text, images, and sometimes even audio or video recordings, depending on the study’s design. Participants typically document their activities, thoughts, feelings, and frustrations as they occur in their natural context.\nEthnography is a qualitative research method that involves observing and interacting with participants in their real-life environment. Ethnographers aim to immerse themselves in the user environment to get a deep understanding of the cultural, social, and organizational contexts that shape technology use.\n\nAs we have explored various methodologies for collecting human feedback, it becomes evident that the role of human input is indispensable in shaping AI systems that are not only effective but also ethically sound and user-centric. In the next step, we will elaborate on how to design AI systems for positive human impact, examining how socially aware and human-centered approaches can be employed to ensure that AI technologies contribute meaningfully to society. This includes understanding how AI can be utilized to address real-world challenges and create tangible benefits for individuals and communities.\n\n\n6.2.2 Designing AI for Positive Human Impact\nIn the field of natural language processing (NLP), the primary focus has traditionally been on quantitative metrics such as performance benchmarks, accuracy, and computations. These metrics have long guided the development and evaluation of the technologies. However, as the field evolves and becomes increasingly intertwined with human interactions like the recent popularity of Large Language Models (LLMs), a paradigm shift is becoming increasingly necessary. For example, these LLMs are shown to produce unethical or harmful responses or reflect values that only represent a certain group of people. The need for a human-centered approach in NLP development is crucial as these models are much more likely to be utilized in a broad spectrum of human-centric applications, impacting various aspects of daily life. This shift calls for an inclusive framework where LLMs are not only optimized for efficiency and accuracy but are also sensitized to ethical, cultural, and societal contexts. Integrating a human-centered perspective ensures that these models are developed with a deep understanding of, and respect for, the diversity and complexity of human values and social norms. This approach goes beyond merely preventing harmful outcomes; it also focuses on enhancing the positive impact of NLP technologies on society. In this session, we explore the intricacies of a human-centered approach in NLP development, focusing on three key themes: Socially Aware, Human-Centered, and Positively Impactful.\n\n6.2.2.1 Socially Aware\nIn the exploration of socially aware NLP, (Hovy and Yang 2021) presents a comprehensive taxonomy of seven social factors grounded in linguistic theory (See Figure 6.6).\n\n\n\n\n\n\nFigure 6.6: Taxonomy of social factors\n\n\n\nThis taxonomy illustrates both the current limitations and progressions in NLP as they pertain to each of these factors. The primary aim is to motivate the NLP community to integrate these social factors more effectively, thereby advancing towards a level of language understanding that more closely resembles human capabilities. The characteristics of speakers, encompassing variables such as age, gender, ethnicity, social class, and dialect, play a crucial role in language processing. Certain languages or dialects, often categorized as low-resource, are spoken by vulnerable populations that require special consideration in NLP systems. In many cases, the dominant culture and values are over-represented, leading to an inadvertent marginalization of minority perspectives. These minority voices must be not only recognized but also given equitable representation in language models. Additionally, norms and context are vital components in understanding linguistic behavior. They dictate the appropriateness of language use in various social situations and settings. Recognizing and adapting to these norms is a critical aspect of developing socially aware NLP systems that can effectively function across diverse social environments.\n\n\n6.2.2.2 Human-Centered\nThe Human-Centered aspect of NLP development emphasizes the creation of language models that prioritize the needs, preferences, and well-being of human users. This involves integrating human-centered design principles throughout the development stages of LLMs, which are described as follows:\n\nTask Formulation stage: Human-centered NLP development begins with understanding the specific problems and contexts in which users operate. This involves collaborating with end-users to identify their needs and challenges, ensuring that the tasks addressed by the models are relevant and meaningful to them. By engaging with users early in the process, developers can create models that are not only technically robust but also practically useful.\nData Collection stage: Human-centered principles ensure that the data used to train models is representative of the diverse user population. This includes collecting data from various demographic groups, languages, and cultural contexts to avoid biases that could lead to unfair or harmful outcomes. Ethical considerations are paramount, ensuring that data is collected with informed consent and respecting users’ privacy.\nData Processing in a human-centered approach involves carefully curating and annotating data to reflect the nuances of human language and behavior. This step includes filtering out potentially harmful content, addressing imbalances in the data, and ensuring that the labels and annotations are accurate and meaningful. By involving human annotators from diverse backgrounds, developers can capture a wider range of perspectives and reduce the risk of biased outputs.\nModel Training with a human-centered focus involves incorporating feedback from users and domain experts to fine-tune the models. This iterative process ensures that the models remain aligned with users’ needs and preferences. Techniques such as active learning, where the model queries users for the most informative examples, can be employed to improve the model’s performance.\nModel Evaluation in a human-centered framework goes beyond traditional metrics like accuracy and F1-score. It includes assessing the model’s impact on users, its fairness, and its ability to handle real-world scenarios. User studies and A/B testing can provide valuable insights into how the model performs in practice and how it affects users’ experiences.\nDeployment of human-centered NLP models involves continuous monitoring and feedback loops to ensure that the models remain effective and aligned with users’ needs over time. This includes setting up mechanisms for users to report issues and provide feedback, which can then be used to update and improve the models. Ensuring transparency in how the models operate and how user data is used also fosters trust and acceptance among users.\n\n\n\n6.2.2.3 Positively Impactful\nBuilding on the human-centered approach, it is crucial to consider how language models can be utilized and the broader impacts they can have on society.\nUtilization: LLMs offer socially beneficial applications across various domains such as public policy, mental health, and education. In public policy, they assist in analyzing large volumes of data to inform decision-making processes. In mental health, LLMs can provide personalized therapy and even train therapists by simulating patient interactions. In the education sector, they enable personalized learning experiences and language assistance, making education more accessible and effective. These examples demonstrate the versatility of LLMs in contributing positively to critical areas of human life.\nImpact: The deployment of NLP models, especially LLMs, has significant societal impacts. Positively, they enhance human productivity and creativity, offering tools and insights that streamline processes and foster innovative thinking. LLMs serve as powerful aids in various sectors, from education to industry, enhancing efficiency and enabling new forms of expression and problem-solving. it is essential to acknowledge the potential negative impacts. One major concern is the ability of LLMs to generate and spread misinformation. As these models become more adept at producing human-like text, distinguishing between AI-generated and human-created content becomes increasingly challenging. This raises issues of trust and reliability, with the risk of widespread dissemination of false or misleading information, which could have significant adverse effects on individuals and society.\nBy considering both the utilization and impact of LLMs, we can better harness their potential for positive societal contributions while mitigating the risks associated with their deployment. In conclusion, by thoughtfully integrating human-centered principles and ensuring positive impacts through feedback collection and ethical considerations, we can develop language models that not only enhance human well-being but also align closely with societal values. Building on these foundational principles, we now turn our attention to Adaptive User Interfaces, which exemplify the practical application of these concepts by personalizing interactions and improving user experiences in dynamic environments.\n\n\n\n6.2.3 Adaptive User Interfaces\nAdaptive user interfaces (AUIs) represent a significant advancement in personalizing user experiences by learning and adapting to individual preferences. This section will discuss the methodologies and applications of AUIs, highlighting their role in enhancing human-AI interaction through intelligent adaptation. The integration of AUIs within human-centered design paradigms ensures that AI systems not only meet user needs but also anticipate and adapt to their evolving preferences, thus maximizing positive human impact. Nowadays, consumers have more choices than ever and the need for personalized and intelligent assistance to make sense of the vast amount of information presented to them is clear.\n\n6.2.3.1 Key ideas\nIn general, personalized recommendation systems require a model or profile of the user. We categorize modeling approaches into four groups.\n\nUser-created profiles (usually done manually).\nManually defined groups that each user is classified into.\nAutomatically learned groups that each user is classified into.\nAdaptively learned individual user models from interactions with the recommendation system.\n\nThe last approach is referred to as adaptive user interfaces. This approach promises that each user is given the most personalization possible, leading to better outcomes. In this session, we discuss recommendation systems that adaptively learn an individual’s preferences and use that knowledge to intelligently recommend choices that the individual is more inclined to like.\nThe problem of learning individual models can be formalized as follows: a set of tasks requiring a user decision, a description for each task, and a history of the user’s decision on each task. This allows us to find a function that maps from task descriptions (features) to user decisions. Tasks can be described using crowd-sourced data (a collaborative approach) or measurable features of the task (a content-based approach). This session will focus on content-based approaches for describing tasks. After understanding the framework for adaptive user interfaces, it is useful to provide example applications to ground future discussions. Adaptive user interfaces have been developed for command and form completion, email filtering and filing, news selection and layout, browsing the internet, selecting movies and TV shows, online shopping, in-car navigation, interactive scheduling, and dialogue systems, among many other applications.\n\n\n6.2.3.2 Design\nThe goal of an adaptive user interface is to create a software tool that reduces human effort by acquiring a user model based on past user interactions. This is analogous to the goal of machine learning (ML) which is to create a software tool that improves some task performance by acquiring knowledge based on partial task experience. The design of an adaptive user interface can be broken up into six steps:\n\nFormulating the Problem: Given some task that an intelligent system could aid, the goal is to find a formulation that lets the assistant improve its performance over time by learning from interactions with a user. In this step the designer has to make design choices about what aspect of user behavior is predicted, and what is the proper level of granularity for description (i.e. what is a training example). This step usually involves formulating the problem into some sort of supervised learning framework.\nEngineering the Representation: At this stage we have a formulation of a task in ML terms and we need to represent the behavior and user model in such a way that makes computational learning not only tractable but as easy as possible. In this step, the designer has to make design choices about what information is used to make predictions, and how that information is encoded and passed to the model.\nCollecting User Traces: In this third step the goal is to find an effective way to collect traces (samples) of user behavior. The designer must choose how to translate traces into training data and also how to elicit traces from a user. An ideal adaptive user interface places no extra effort on the user to collect such traces.\nModeling the User: In this step the designer must decide what model class to use (neural network, decision tree, graphical model, etc.) and how to train the model (optimizer, step size, batch size, etc.). This step in the design process is usually given too much importance in academia. It is quite often the case that the success of an adaptive user interface is more sensitive to the other design steps.\nUsing the Model Effectively: At this stage the designer must decide how the model will be integrated into a software tool. Specifically, when and how is the model evaluated and how is the output of the model presented to the user? In addition, the designer must consider how to handle situations in which the model predictions are wrong. An ideal adaptive user interface will let the user take advantage of good predictions and ignore bad ones.\nGaining User Acceptance: The final step in the design process is to get users to try the system and ultimately adopt it. The initial attraction of users is often a marketing problem, but to retain users the system must be well-designed and easy to use.\n\n\n\n6.2.3.3 Applications\nAfter understanding the design of Adaptive User Interfaces, let’s take a look at how we can apply it to real-world problems. We will summarize and analyze three different application areas of learning human preferences, which are driving route advisor (Rogers, Fiechter, and Langley 1999), destination selection (Langley et al. 1999), and resource scheduling (Gervasio, Iba, and Langley 1999).\n1. Driving Route Advisor: The task of route selection involves determining a desirable path for a driver to take from their current location to a chosen destination, given the knowledge of available roads from a digital map. While computational route advisors exist in rental cars and online, they cannot personalize individual drivers’ preferences, which is a gap that adaptive user interfaces aim to fill by learning and recommending routes tailored to the driver’s unique choices and behaviors.\nHere is an approach to route selection through learning individual drivers’ route preferences.\n\nFormulation: Learn a “subjective” function to evaluate entire routes.\nRepresentation: Global route features are computable from digital maps.\nData collection: Preference of one complete route over another.\nInduction: A method for learning weights from preference data.\nUsing model: Apply subjective function to find “optimal” route.\n\nThis method aims to learn a user model that considers the entirety of a route, thereby avoiding issues like data fragmentation and credit assignment problems.\nThe design choices are incorporated into (Rogers, Fiechter, and Langley 1999), which: models driver preferences in terms of 14 global route features; gives the driver two alternative routes he might take; lets the driver refine these choices along route dimensions; uses driver choices to refine its model of his preferences; and invokes the driver model to recommend future routes. We note that providing drivers with choices lets the system collect data on route preferences in an unobtrusive manner. The interface of the application is presented in Figure 6.7.\n\n\n\n\n\n\nFigure 6.7: The adaptive route advisor.\n\n\n\nIn driving route advisor task (Rogers, Fiechter, and Langley 1999), a linear model is used for predicting the cost of a route based on the time, distance, number of intersections, and the number of turns. The system uses each training pair as a constraint on the weights found during the learning process. The experimental results are shown in the ?fig-exp-2.\n\n\n \n\n\n(Left) Experiments with 24 subjects show the Route Advisor improves its predictive ability rapidly with experience. (Right) Analyses also show that personalized user models produce better results than generalized models, even when given more data.\n\n\n2. Destination Selection: The task of destination selection involves assisting a driver in identifying one or more suitable destinations that fulfill a specific goal, such as finding a place to eat lunch, based on the driver’s current location and knowledge of nearby options. While there are many recommendation systems online, including those for restaurants, they are not ideally suited for drivers due to the driving environment’s demand for limited visual attention, thus necessitating a more tailored and accessible approach for in-car use.\nOne approach to destination recommendation can be cast as:\n\nFormulation: Learn to predict features the user cares about in items.\nRepresentation: Conditions/weights on attributes and values.\nData collection: Converse with the user to help him make decisions, noting whether he accepts or rejects questions and items.\nInduction: Any supervised induction method.\nUsing model: Guide the dialogue by selecting informative questions and suggesting likely values.\n\nThis design relies on the idea of a conversational user interface. Spoken-language versions of this approach appear well suited to the driving environment.\nThis approach is implemented in (Langley et al. 1999), where it engages in spoken conversations to help a user refine goals; incorporates a dialogue model to constrain this process; collects and stores traces of interaction with the user; and personalizes both its questions and recommended items. Their work focused on recommending restaurants to users who want advice about where to eat. This approach to recommendation would work well for drivers, it also has broader applications. We present experimental results in\n\n\n \n\n\n(Left) Speech Acts Per Conversation. (Right) Time Per Conversation.\n\n\n3. Resource Scheduling: The task of resource scheduling describes the challenge of allocating a limited set of resources to complete a set of tasks or jobs within a certain time frame, while also considering the constraints on both the jobs and the resources. Although automated scheduling systems are prevalent in various industries and some interactive schedulers exist, there is a distinct need for systems that can create personalized schedules reflecting the unique preferences of individual users.\nAn approach to personalized scheduling can be described as:\n\nFormulation: Learn a utility function to evaluate entire schedules.\nRepresentation: Global features are computable from the schedule.\nData collection: Preference of one candidate schedule over others.\nInduction: A method for learning weights from preference data.\nUsing model: Apply the ‘subjective’ function to find a good schedule.\n\nWe note that this method is similar to that in the Adaptive Route Advisor. However, it assumes a search through a space of complete schedules (a repair space), which requires some initial schedule. This approach is implemented in (Gervasio, Iba, and Langley 1999), where the interactive scheduler retrieves an initial schedule from a personalized case library; suggests to the user improved schedules from which to select; lets the user direct search to improve on certain dimensions; collects user choices to refine its personalized utility function; stores solutions in the case base to initialize future schedules; and invokes the user model to recommend future schedule repairs. As before, providing users with choices lets the system collect data on schedule preferences unobtrusively. An example of the interface, and the experimental results are shown in ?fig-exp-3.\n\n\n \n\n\n(Left) The interface of the INCA: Interactive Scheduling . (Right) Experiments with INCA suggest that retrieving personalized schedules helps users more as task difficulty increases. These experimental studies used a mixture of human and synthetic subjects.\n\n\n\n\n6.2.3.4 Limitations\nThe challenges of adaptive interfaces may involve: conceptualizing user modeling as a task suitable for inductive learning, crafting representations that facilitate the learning process, gathering training data from users in a way that doesn’t intrude on their experience, applying the learned user model effectively, ensuring the system can learn in real-time, and dealing with the necessity of learning from a limited number of training instances. These challenges are not only pertinent to adaptive interfaces but also intersect with broader applications of machine learning, while also introducing some unique issues. However, new sensor technology can bring promises to adaptive interfaces. Adaptive interfaces rely on user traces to drive their modeling process, so they stand to benefit from developments like GPS and cell phone locators, robust software for speech recognition, accurate eye and head trackers, real-time video interpreters, wearable body sensors (GSR, heart rate), and portable brain-wave sensors. As those devices become more widespread, they will offer new sources of data and support new types of adaptive services. In addition, adaptive interfaces can be viewed as a form of cognitive simulation that automatically generates knowledge structures to learn user preferences. They are capable of making explicit predictions about future user behavior and explaining individual differences through the process of personalization. This perspective views adaptive interfaces as tools that not only serve functional purposes but also model the psychological aspects of user interaction. Two distinct approaches within cognitive simulation are related to adaptive interfaces: process models that incorporate fundamental architectural principles, and content models that operate at the knowledge level, focusing on behavior. We note that both of them have roles to play, but content models are more relevant to personalization and adaptive interfaces.\nIn conclusion, adaptive user interfaces represent a significant advancement in creating personalized and efficient interactions between humans and technology. By leveraging modern sensor technologies and cognitive simulation approaches, these interfaces can dynamically learn and adapt to individual user preferences, enhancing overall user experience and system effectiveness. The methodologies discussed, from conceptualizing user models to collecting and utilizing user feedback, form the foundation of this innovative approach. As we transition to the next section, we will explore practical applications and real-world implementations of these human-centered AI principles through detailed case studies, illustrating the tangible impact of adaptive interfaces in various domains.\n\n\n\n6.2.4 Case Studies in Human-Centered AI\nIn this section, we examine practical examples that illustrate the application of human-centered principles in the development and deployment of AI systems. By examining these case studies, we aim to provide concrete insights into how AI technologies can be designed and implemented to better align with human values, enhance inclusivity, and address the specific needs of diverse user groups. The following case studies highlight different approaches and methodologies used to ensure that AI systems are not only effective but also considerate of the human experience.\n\n6.2.4.1 LaMPost Case Study\nIn our exploration of human-centered AI design, it is crucial to examine how metrics can be improved to better capture the human experience and address the shortcomings of traditional evaluation methods. The LaMPost case study (Goodman et al. 2022) exemplifies this effort by focusing on the development of an AI assistant designed to aid individuals with dyslexia in writing emails. This case is particularly relevant to our discussion because it highlights the importance of human-centered principles in AI development, especially in creating tools that cater to specific cognitive differences and enhance user experience.\nDyslexia is a cognitive difference that affects approximately 15 percent of language users, with varying degrees of impact on speaking, spelling, and writing abilities. It is a spectrum disorder, meaning symptoms and severity differ among individuals. More importantly, dyslexia is not an intellectual disability; many individuals with dyslexia possess high intelligence. Given the significant number of people affected by dyslexia, it is essential to develop AI tools that support their unique needs and enhance their daily tasks.\nThe LaMPost project sought to answer the question, “How can LLMs be applied to enhance the writing workflows of adults with dyslexia?” To address this, researchers employed a participatory design approach, involving employees with dyslexia from their company (Google) in the study. This approach ensured that the development process was inclusive and responsive to the actual needs and preferences of the dyslexic community. By focusing on the real-world application of LLMs in aiding email writing for dyslexic individuals, LaMPost serves as a powerful example of how AI can be designed to better capture and enhance the human experience.\nThe figure below allows users to see suggestions for rewriting selected text, helping them identify main ideas, suggest possible changes, and rewrite their selections to improve clarity and expression.\n\n\n\nThe Suggest Possible Changes feature from LaMPost.\n\n\nThe table below categorizes the challenges faced by users at different writing levels and the strategies they can use to overcome these challenges, illustrating the varied support needs addressed by LaMPost\n\n\n\n\n\nWriting level\n\n\nExamples of Challenges\n\n\nStrategies\n\n\n\n\n\n\nhigh\n\n\nexpressing ideas\n\n\n“word faucet”, ASR dictation\n\n\n\n\n\n\nordering ideas\n\n\npost-it outlining\n\n\n\n\nlow\n\n\nappropriate language\n\n\nproofreading\n\n\n\n\n\n\nparaphrasing\n\n\nfeedback\n\n\n\n\n\nUser challenged and strategies in LaMPost.\n\n\nNext, they ran a focus group to get initial ideas from members of the dyslexic community. This focus group helped them figure out what to measure and added the second research question: “How do adults with dyslexia feel about LLM-assisted writing?” In other words, how does the LLM impact users’ feelings of satisfaction, self-expression, self-efficacy, autonomy, and control?\nFrom this focus group, they went and created a prototype to answer the desires of the group. They included three features in their prototype model. One feature was: identifying main ideas. They focused on this to support overall clarity and organization of high-level ideas of the user. Another feature was suggest possible changes. They focused on this because users wanted to identify high-level adjustments to improve their writing. The last feature they added was rewrite my selections. They added this because users wanted help expressing ideas with a desired phrasing tone or style. This feature generated a rewrite based on a command you gave it.\nWith the prototype, the researchers evaluated again with 19 participants with dyslexia from outside their organization. They did a three-part study, including a demonstration and background on the system (25 min). Then they did a writing exercise with two real tasks (emails) each user had to do in the real world (25 min). For example, one task might have been to write an email to the principal of their child’s school to ask for a meeting. Then, the researchers did another follow-up interview for more qualitative data, e.g. to ask about specific choices users made when interacting with the model (25 min).\nLaMPost’s design prioritized autonomy by allowing users to choose the best option for their writing. One successful thing is that most users felt in control while writing. Users found that numerous options were helpful to filter through poor results. However, participants said the selection process was cognitively demanding and time-consuming. As we all know, features identified in LaMPost are all over the place, such as in Google Docs. Nonetheless, there remain many questions about the balance between automated writing and providing more control to the end users.\n\nHow could researchers hone in on this trade-off between the ease of automated writing and providing control to end-users?\nYou will need to design a study to approach this question.\n\nIdentify your research question, hypotheses, and the methods that you will use. (Hint: use the HCI methods described in the previous section.)\nScope the domain of your study appropriately—more broadly than dyslexia but not so broadly to be meaningless.\nWhat domains will you include? (E.g. students use ChatGPT for assignments, doctors use an LLM to write notes, etc.)\n\n\nIn this way, both the case study of LaMPost and its presaging of greater trends in LLM interfaces recapitulate the maxim of HCI: HCI is a cycle. You design a potential system, prototype it, get feedback from people, and iterate constantly. Next, we will explore two case studies that exemplify the application of human-centered principles in NLP. These case studies illustrate how LLMs can be adapted to foster social inclusivity and provide training in social skills.\n\n\n6.2.4.2 Multi-Value and DaDa: Cross-Dialectal English NLP\nEnglish NLP systems are largely trained to perform well in Standard American English - the form of written English found in professional settings and elsewhere. Not only is Standard American English the most well-represented form of English in textual datasets but NLP engineers and researchers often filter dialectal and vernacular English examples from their datasets to improve performance on SAE benchmarks. As a result, NLP systems are generally less performant when processing dialectal inputs than SAE inputs. This performance gap is observable over various benchmarks and tasks, like the SPIDER benchmark. (Chang et al. 2023)\n\n\n\nStress test reveals worse performance on the SPIDER benchmark with synthetic dialectical examples than with SAE.\n\n\nAs natural language systems become more pervasive, this performance gap increasingly represents a real allocational harm against dialectal English speakers — these speakers are excluded from using helpful systems and assistants. Multi-Value is a framework for evaluating foundation language models on dialectic input, and DADA is a framework for adapting LLMs to improve performance on dialectic input.\nSynthetic Dialectal Data\nZiems et al. (2023) create synthetic dialectal data for several English dialects (Appalachian English, Chicano English, Indian English, Colloquial Singapore English, and Urban African American English).(Ziems et al. 2023) They created synthetic data based on transforming SAE examples to have direct evaluation comparisons. These synthetic examples were created by leveraging known linguistic features of the dialects, such as negative concord in UAAVE. Figure 6.8 maps out the presence of various linguistic features.\n\n\n\n\n\n\nFigure 6.8: A comparative distribution of features in five dialects.\n\n\n\nThis synthetic data, while somewhat limited in the variety of samples. can produce and create realistic examples for benchmarking LM performance. Figure 6.9 demonstrates creating a synthetic dialectic example using the ‘give passive’ linguistic feature, illustrating the transformation process from SAE to a vernacular form.\n\n\n\n\n\n\nFigure 6.9: Execution of a sample transform using a documented linguistic feature.\n\n\n\nFeature Level Adapters One approach to the LLM adaption task would be to train an adapter for each dialect using a parameter-efficient fine-tuning method like low-rank adapters. (Hu et al. 2021) While adapters can certainly bridge the gap between SAE LMs and dialect inputs, this approach suffers from a couple of weaknesses, namely:\n\nIndividually trained adapters do not leverage similarities between low-resource dialects. Transfer learning is often helpful for training low-resource languages and dialects.\nThe model needs to know which adapter to use at inference time. This presupposes that we can accurately classify the dialect — sometimes based on as little as one utterance. This classification is not always possible — a more general approach is needed.\n\nTherefore, Liu et al. (2023) propose a novel solution — DADA: Dialect Adaption via Dynamic Aggregation of Linguistic Rules. (Liu, Held, and Yang 2023) DADA trains adapters on the linguistic feature level rather than the dialect level. The model can use multiple linguistic feature adapters via an additional fusion layer. They can therefore train using multi-dialectical data and cover linguistic variation via a comprehensive set of roughly 200 adapters. DADA saw an improvement in performance over single-dialect adapters for most dialects, as shown in Figure 6.10.\n\n\n\n\n\n\nFigure 6.10: Execution of a sample transform using a documented linguistic feature.\n\n\n\nThe Multi-Value and DADA case study underscores the importance of designing NLP systems that are inclusive and representative of diverse language users. By addressing the performance gaps in handling dialectal inputs, this case study highlights the necessity of incorporating diverse linguistic data and creating adaptable systems. This approach enhances AI functionality and accessibility, ensuring it respects and reflects linguistic diversity. Ultimately, the study reinforces human-centered design principles, demonstrating how AI can be tailored to better serve and empower all users. Moving forward, we will explore how LLMs can be utilized for social skill training, showcasing their potential to improve human interactions.\n\n\n6.2.4.3 Social Skill Training via LLMs\nThe emergence of Large Language Models (LLMs) marks a significant milestone in the field of social skills training. This case study explores the potential of LLMs to augment social skill development across diverse scenarios. More specifically, we discuss a dual-framework approach, where two distinct LLMs operate in tandem as a Partner and a Mentor, guiding human learners in their journey towards improved social interaction. In this framework, we have two agents which are\n\nAI Partner: LLM-empowered agents that users can engage with across various topics. This interactive model facilitates practical, conversation-based learning, enabling users to experiment with different communication styles and techniques or practice and develop specific skills in real-world scenarios in a safe, AI-mediated environment.\nAI Mentor: An LLM-empowered entity designed to provide constructive, personalized feedback based on the interaction of users and the AI Partner. This mentor analyzes conversation dynamics, identifies areas for improvement, offers tailored advice, and guides users toward effective social strategies and improved interaction skills.\n\nFor example, in conflict resolution, individuals learning to handle difficult conversations can use the AI Partner to simulate interactions with a digitalized partner. As a Conflict Resolution Expert, the AI Mentor helps analyze these interactions, offering strategies to navigate conflicts effectively.\nIn the educational sector, K-12 teachers aiming to incorporate more growth-mindset language into their teaching can practice with a digitalized student. An experienced teacher or mentor, represented by the AI Mentor, provides insights on effective communication and teaching methods. For negotiation training, students preparing to negotiate their first job offers can engage in simulated negotiations with a digitalized HR representative through the AI Partner. As a Negotiation Expert, the AI Mentor then offers guidance on negotiation tactics, helping students effectively articulate their values and negotiate job terms. Lastly, in therapy training, novice therapists can interact with a digitalized patient via the AI Partner to practice therapy sessions. The AI Mentor, functioning as a Therapy Coach, then reviews these sessions, providing feedback and suggestions on enhancing therapeutic techniques and patient engagement.\nCARE: Therapy Skill Training Hsu et al. (2023) introduced CARE (Hsu et al. 2023), a framework designed for therapy skill training. This framework leverages a simulated environment, enabling counselors to practice their skills without the risk of harming real individuals. An integral component of CARE is the AI Mentor, which offers invaluable feedback and guidance during the training process. See Figure 6.11 for the overview of the framework.\n\n\n\n\n\n\nFigure 6.11: CARE Framework\n\n\n\nCARE’s primary function is for novice therapists and counselors to assess and determine the most effective counseling strategies tailored to specific contexts. It provides counselors with customized example responses, which they can adopt, adapt, or disregard when interacting with a simulated support seeker. This approach is deeply rooted in the principles of Motivational Interviewing and utilizes a rich dataset of counseling conversations combined with LLMs. The effectiveness of CARE has been established through rigorous quantitative evaluations and qualitative user studies, which included simulated chats and semi-structured interviews. Notably, CARE has shown significant benefits in aiding novice counselors. From the assessment, counselors chose to use CARE 93% of the time, directly used a CARE response without editing 60% of the time, and sent more extended responses with CARE. Qualitatively, counselors noted several advantages of CARE, such as its ability to refresh memory on various strategies, inspire innovative responses, boost confidence, and save time during consultations. However, there were some drawbacks, including potential disruptions in the thought process, perceived limitations in response options, the requirement for decision-making, and the time needed to review suggestions. Overall, the framework is particularly beneficial for therapists new to the field, offering them a supportive and educational tool to enhance their counseling skills effectively.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Alternatives</span>"
    ]
  },
  {
    "objectID": "src/chap7.html#practice-exercises",
    "href": "src/chap7.html#practice-exercises",
    "title": "6  Alternatives",
    "section": "6.3 Practice Exercises",
    "text": "6.3 Practice Exercises",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Alternatives</span>"
    ]
  },
  {
    "objectID": "src/chap7.html#footnotes",
    "href": "src/chap7.html#footnotes",
    "title": "6  Alternatives",
    "section": "",
    "text": "GPT-4 is good at coming up with longer-rendered answers about why some things are appropriate or not.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Alternatives</span>"
    ]
  },
  {
    "objectID": "src/ack.html",
    "href": "src/ack.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "Citation\nInitial versions of this book were compiled as lecture notes to the class CS329H: Machine Learning from Human Preferences at Stanford University taught in Fall 2023 and Fall 2024. We thank Rehaan Ahmad, Ahmed Ahmed, Jirayu Burapacheep, Michael Byun, Akash Chaurasia, Andrew Conkey, Tanvi Deshpande, Eric Han, Laya Iyer, Adarsh Jeewajee, Shreyas Kar, Arjun Karanam, Jared Moore, Aashiq Muhamed, Bidipta Sarkar, William Shabecoff, Stephan Sharkov, Max Sobol Mark, Kushal Thaman, Joe Vincent, Yibo Zhang, Duc Nguyen, Grace Sodunke, Ky Nguyen, and Mykkel Kochenderfer for their early contributions and feedback.\nThanks for reading our book! We hope you find this book useful in your research and teaching.",
    "crumbs": [
      "Acknowledgments"
    ]
  },
  {
    "objectID": "src/ack.html#citation",
    "href": "src/ack.html#citation",
    "title": "Acknowledgments",
    "section": "",
    "text": "BibTeX citation:\n@book{mlhp,\n  author    = {Truong, Sang and Haupt, Andreas and Koyejo, Sanmi},\n  title     = {{Machine Learning from Human Preferences}},\n  year      = {2025},\n  publisher = {Stanford University},\n  doi       = {},\n  note      = {}\n}\nFor attribution, please cite this work as:\n\nS. Truong, A. Haupt, and S. Koyejo. 2025. Machine Learning from Human Preferences. Stanford University.",
    "crumbs": [
      "Acknowledgments"
    ]
  }
]