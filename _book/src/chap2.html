<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Background – Machine Learning from Human Preferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../src/chap3.html" rel="next">
<link href="../index.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-07ba0ad10f5680c660e360ac31d2f3b6.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8b864f0777c60eecff11d75b6b2e1175.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-fa3d1c749edcb96cd5cb7d620f3e5237.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-6f24586c8b15e78d85e3983c622e3e8a.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/monaco-editor@0.46.0/min/vs/editor/editor.main.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous" referrerpolicy="no-referrer">
  
<style type="text/css">
.monaco-editor pre {
  background-color: unset !important;
}

.qpyodide-editor-toolbar {
  width: 100%;
  display: flex;
  justify-content: space-between;
  box-sizing: border-box;
}

.qpyodide-editor-toolbar-left-buttons, .qpyodide-editor-toolbar-right-buttons {
  display: flex;
}

.qpyodide-non-interactive-loading-container.qpyodide-cell-needs-evaluation, .qpyodide-non-interactive-loading-container.qpyodide-cell-evaluated {
  justify-content: center;
  display: flex;
  background-color: rgba(250, 250, 250, 0.65);
  border: 1px solid rgba(233, 236, 239, 0.65);
  border-radius: 0.5rem;
  margin-top: 15px;
  margin-bottom: 15px;
}

.qpyodide-r-project-logo {
  color: #2767B0; /* R Project's blue color */
}

.qpyodide-icon-status-spinner {
  color: #7894c4;
}

.qpyodide-icon-run-code {
  color: #0d9c29
}

.qpyodide-output-code-stdout {
  color: #111;
}

.qpyodide-output-code-stderr {
  color: #db4133;
}

.qpyodide-editor {
  border: 1px solid #EEEEEE;
}

.qpyodide-editor-toolbar {
  background-color: #EEEEEE;
  padding: 0.2rem 0.5rem;
}

.qpyodide-button {
  background-color: #EEEEEE;
  display: inline-block;
  font-weight: 400;
  line-height: 1;
  text-decoration: none;
  text-align: center;
  color: #000;
  border-color: #dee2e6;
  border: 1px solid rgba(0,0,0,0);
  padding: 0.375rem 0.75rem;
  font-size: .9rem;
  border-radius: 0.25rem;
  transition: color .15s ease-in-out,background-color .15s ease-in-out,border-color .15s ease-in-out,box-shadow .15s ease-in-out;
}

.qpyodide-button:hover {
  color: #000;
  background-color: #d9dce0;
  border-color: #c8ccd0;
}

.qpyodide-button:disabled,.qpyodide-button.disabled,fieldset:disabled .qpyodide-button {
  pointer-events: none;
  opacity: .65
}

.qpyodide-button-reset {
  color: #696969; /*#4682b4;*/
}

.qpyodide-button-copy {
  color: #696969;
}


/* Custom styling for RevealJS Presentations*/

/* Reset the style of the interactive area */
.reveal div.qpyodide-interactive-area {
  display: block;
  box-shadow: none;
  max-width: 100%;
  max-height: 100%;
  margin: 0;
  padding: 0;
} 

/* Provide space to entries */
.reveal div.qpyodide-output-code-area pre div {
  margin: 1px 2px 1px 10px;
}

/* Collapse the inside code tags to avoid extra space between line outputs */
.reveal pre div code.qpyodide-output-code-stdout, .reveal pre div code.qpyodide-output-code-stderr {
  padding: 0;
  display: contents;
}

.reveal pre div code.qpyodide-output-code-stdout {
  color: #111;
}

.reveal pre div code.qpyodide-output-code-stderr {
  color: #db4133;
}


/* Create a border around console and output (does not effect graphs) */
.reveal div.qpyodide-console-area {
  border: 1px solid #EEEEEE;
  box-shadow: 2px 2px 10px #EEEEEE;
}

/* Cap output height and allow text to scroll */
/* TODO: Is there a better way to fit contents/max it parallel to the monaco editor size? */
.reveal div.qpyodide-output-code-area pre {
  max-height: 400px;
  overflow: scroll;
}
</style>
<script type="module">
// Document level settings ----

// Determine if we need to install python packages
globalThis.qpyodideInstallPythonPackagesList = [''];

// Check to see if we have an empty array, if we do set to skip the installation.
globalThis.qpyodideSetupPythonPackages = !(qpyodideInstallPythonPackagesList.indexOf("") !== -1);

// Display a startup message?
globalThis.qpyodideShowStartupMessage = true;

// Describe the webR settings that should be used
globalThis.qpyodideCustomizedPyodideOptions = {
  "indexURL": "https://cdn.jsdelivr.net/pyodide/v0.27.2/full/",
  "env": {
    "HOME": "/home/pyodide",
  }, 
  stdout: (text) => {qpyodideAddToOutputArray(text, "out");},
  stderr: (text) => {qpyodideAddToOutputArray(text, "error");}
}

// Store cell data
globalThis.qpyodideCellDetails = [{"code":"import numpy as np\nnp.random.seed(0)\n\ndef ackley(X, a=20, b=0.2, c=2*np.pi):\n    \"\"\"\n    Compute the Ackley function.\n    Parameters:\n      X: A NumPy array of shape (n, d) where each row is a n-dimensional point.\n      a, b, c: Parameters of the Ackley function.\n    Returns:\n      A NumPy array of shape (n,) of function values\n    \"\"\"\n    X = np.atleast_2d(X)\n    d = X.shape[1]\n    sum_sq = np.sum(X ** 2, axis=1)\n    term1 = -a * np.exp(-b * np.sqrt(sum_sq / d))\n    term2 = -np.exp(np.sum(np.cos(c * X), axis=1) / d)\n    return term1 + term2 + a + np.e","id":1,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"import matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nccmap = LinearSegmentedColormap.from_list(\"ackley\", [\"#f76a05\", \"#FFF2C9\"])\nplt.rcParams.update({\n    \"font.size\": 14,\n    \"axes.labelsize\": 16,\n    \"xtick.labelsize\": 14,\n    \"ytick.labelsize\": 14,\n    \"legend.fontsize\": 14,\n    \"axes.titlesize\": 16,\n})\n\ndef draw_surface():\n    inps = np.linspace(-2, 2, 100)\n    X, Y = np.meshgrid(inps, inps)\n    grid = np.column_stack([X.ravel(), Y.ravel()])\n    Z = ackley(grid).reshape(X.shape)\n    \n    plt.figure(figsize=(6, 5))\n    contour = plt.contourf(X, Y, Z, 50, cmap=ccmap)\n    plt.contour(X, Y, Z, levels=15, colors='black', linewidths=0.5, alpha=0.6)\n    plt.colorbar(contour, label=r'$f(x)$', ticks=[0, 3, 6])\n    plt.xlim(-2, 2)\n    plt.ylim(-2, 2)\n    plt.xticks([-2, 0, 2])\n    plt.yticks([-2, 0, 2])\n    plt.xlabel(r'$x_1$')\n    plt.ylabel(r'$x_2$')","id":2,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"d = 2\nn = 800\nitems = np.random.randn(n, d)*0.5 + np.ones((n, d))*0.5\nutilities = ackley(items)\ny = (utilities > utilities.mean())\ndraw_surface()\nplt.scatter(items[:, 0], items[:, 1], c=y, cmap='coolwarm', alpha=0.5)\nplt.show()","id":3,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"n_pairs = 10000\npair_indices = np.random.randint(0, n, size=(n_pairs, 2))\n# Exclude pairs where both indices are the same\nmask = pair_indices[:, 0] != pair_indices[:, 1]\npair_indices = pair_indices[mask]\n\nscores = np.zeros(n, dtype=int)\nwins = utilities[pair_indices[:, 0]] > utilities[pair_indices[:, 1]]\n\n# For pairs where the first item wins:\n#   - Increase score for the first item by 1\n#   - Decrease score for the second item by 1\nnp.add.at(scores, pair_indices[wins, 0], 1)\nnp.add.at(scores, pair_indices[wins, 1], -1)\n\n# For pairs where the second item wins or it's a tie:\n#   - Decrease score for the first item by 1\n#   - Increase score for the second item by 1\nnp.add.at(scores, pair_indices[~wins, 0], -1)\nnp.add.at(scores, pair_indices[~wins, 1], 1)\n\n# Determine preferred and non-preferred items based on scores\npreferred = scores > 0\nnon_preferred = scores < 0\n\ndraw_surface()\nplt.scatter(items[preferred, 0], items[preferred, 1], c='blue', label='Preferred', alpha=0.5)\nplt.scatter(items[non_preferred, 0], items[non_preferred, 1], c='purple', label='Non-preferred', alpha=0.5)\nplt.legend()\nplt.show()","id":4,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"import numpy as np\n\n# Define utilities for each group\ngroup1_utilities = {'A': 1.0, 'B': 2.0, 'C': 3.0}\ngroup2_utilities = {'A': 3.0, 'B': 2.0, 'C': 1.0}\n\n# Group weights\nalpha1 = 0.5\nalpha2 = 0.5\n\ndef softmax(utilities):\n    exp_vals = np.exp(list(utilities.values()))\n    total = np.sum(exp_vals)\n    return {k: np.exp(v) / total for k, v in utilities.items()}\n\n# Compute mixed logit probabilities over all 3 options\np1_full = softmax(group1_utilities)\np2_full = softmax(group2_utilities)\np_mix_full = {k: alpha1 * p1_full[k] + alpha2 * p2_full[k] for k in group1_utilities}\n\n# Compute ratio A/B in full model\nratio_full = p_mix_full['A'] / p_mix_full['B']\n\n# Now remove option C\nreduced_utils1 = {'A': group1_utilities['A'], 'B': group1_utilities['B']}\nreduced_utils2 = {'A': group2_utilities['A'], 'B': group2_utilities['B']}\np1_reduced = softmax(reduced_utils1)\np2_reduced = softmax(reduced_utils2)\np_mix_reduced = {k: alpha1 * p1_reduced[k] + alpha2 * p2_reduced[k] for k in reduced_utils1}\n\n# Compute ratio A/B in reduced model\nratio_reduced = p_mix_reduced['A'] / p_mix_reduced['B']\n\n# Show violation of IIA\nprint(f\"Ratio A/B with all options: {ratio_full:.4f}\")\nprint(f\"Ratio A/B after removing C: {ratio_reduced:.4f}\")","id":5,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Define two Gaussian distributions\nmu1, sigma1 = 0, 1\nmu2, sigma2 = 4, 1\nx = np.linspace(-4, 8, 1000)\n\n# Evaluate the PDFs\npdf1 = norm.pdf(x, mu1, sigma1)\npdf2 = norm.pdf(x, mu2, sigma2)\n\n# Mixture: equal weight\nmixture_pdf = 0.5 * pdf1 + 0.5 * pdf2\n\n# Plot\nplt.plot(x, pdf1, label='N(0, 1)', linestyle='--')\nplt.plot(x, pdf2, label='N(4, 1)', linestyle='--')\nplt.plot(x, mixture_pdf, label='Mixture 0.5*N(0,1) + 0.5*N(4,1)', linewidth=2)\nplt.title(\"Mixture of Two Gaussians is Not Gaussian\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.grid(True)\nplt.show()","id":6,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"import numpy as np\n\n# Deterministic utilities\nv_car = 1.0\nv_bus = 2.0  # Initially a single bus alternative\n\n# Logit choice probabilities (before splitting bus)\ndef softmax(utilities: np.ndarray) -> np.ndarray:\n    exp_util = np.exp(utilities)\n    return exp_util / np.sum(exp_util)\n\n# Before splitting: two alternatives\nutilities_before = np.array([v_car, v_bus])\nprobs_before = softmax(utilities_before)\nprint(\"Before splitting (Car, Bus):\", probs_before)\n\n# After splitting: three alternatives\nv_red_bus = v_bus\nv_blue_bus = v_bus\nutilities_after = np.array([v_car, v_red_bus, v_blue_bus])\nprobs_after = softmax(utilities_after)\nprint(\"After splitting (Car, Red Bus, Blue Bus):\", probs_after)\nprint(\"After splitting, total bus share:\", probs_after[1] + probs_after[2])","id":7,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\ndef likelihood(p: float) -> float:\n    \"\"\"\n    Computes the likelihood of 9 heads and 3 tails, assuming p_heads is p.\n\n    Args:\n    p (float): A value between 0 and 1 representing the probability of heads.\n\n    Returns:\n    float: The likelihood value at p_heads=p. Return 0 if p is outside the range [0, 1].\n    \"\"\"\n    # YOUR CODE HERE (~1-3 lines)\n    pass\n    # END OF YOUR CODE\n\n\ndef propose(x_current: float, sigma: float) -> float:\n    \"\"\"\n    Proposes a new sample from the proposal distribution Q.\n    Here, Q is a normal distribution centered at x_current with standard deviation sigma.\n\n    Args:\n    x_current (float): The current value in the Markov chain.\n    sigma (float): Standard deviation of the normal proposal distribution.\n\n    Returns:\n    float: The proposed new sample.\n    \"\"\"\n    # YOUR CODE HERE (~1-3 lines)\n    pass\n    # END OF YOUR CODE\n\n\ndef acceptance_probability(x_current: float, x_proposed: float) -> float:\n    \"\"\"\n    Computes the acceptance probability A for the proposed sample.\n    Since the proposal distribution is symmetric, Q cancels out.\n\n    Args:\n    x_current (float): The current value in the Markov chain.\n    x_proposed (float): The proposed new value.\n\n    Returns:\n    float: The acceptance probability\n    \"\"\"\n    # YOUR CODE HERE (~4-6 lines)\n    pass\n    # END OF YOUR CODE\n\n\ndef metropolis_hastings(N: int, T: int, x_init: float, sigma: float) -> np.ndarray:\n    \"\"\"\n    Runs the Metropolis-Hastings algorithm to sample from a posterior distribution.\n\n    Args:\n    N (int): Total number of iterations.\n    T (int): Burn-in period (number of initial samples to discard).\n    x_init (float): Initial value of the chain.\n    sigma (float): Standard deviation of the proposal distribution.\n\n    Returns:\n    list: Samples collected after the burn-in period.\n    \"\"\"\n    samples = []\n    x_current = x_init\n\n    for t in range(N):\n        # YOUR CODE HERE (~7-10 lines)\n        # Use the propose and acceptance_probability functions to get x_{t+1} and store it in samples after the burn-in period T\n        pass\n        # END OF YOUR CODE\n\n    return samples\n\n\ndef plot_results(samples: np.ndarray) -> None:\n    \"\"\"\n    Plots the histogram of MCMC samples along with the true Beta(10, 4) PDF.\n\n    Args:\n    samples (np.ndarray): Array of samples collected from the Metropolis-Hastings algorithm.\n\n    Returns:\n    None\n    \"\"\"\n    # Histogram of the samples from the Metropolis-Hastings algorithm\n    plt.hist(samples, bins=50, density=True, alpha=0.5, label=\"MCMC Samples\")\n\n    # True Beta(10, 4) distribution for comparison\n    p = np.linspace(0, 1, 1000)\n    beta_pdf = beta.pdf(p, 10, 4)\n    plt.plot(p, beta_pdf, \"r-\", label=\"Beta(10, 4) PDF\")\n\n    plt.xlabel(\"p_heads\")\n    plt.ylabel(\"Density\")\n    plt.title(\"Metropolis-Hastings Sampling of Biased Coin Posterior\")\n    plt.legend()\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    # MCMC Parameters (DO NOT CHANGE!)\n    N = 50000  # Total number of iterations\n    T = 10000  # Burn-in period to discard\n    x_init = 0.5  # Initial guess for p_heads\n    sigma = 0.1  # Standard deviation of the proposal distribution\n\n    # Run Metropolis-Hastings and plot the results\n    samples = metropolis_hastings(N, T, x_init, sigma)\n    plot_results(samples)","id":8,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"import torch\nimport torch.distributions as dist\nimport math\nfrom tqdm import tqdm\nfrom typing import Tuple\n\ndef make_data(\n    true_p: torch.Tensor, true_weights_1: torch.Tensor, true_weights_2: torch.Tensor, num_movies: int, feature_dim: int\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Generates a synthetic movie dataset according to the CardinalStreams model.\n\n    Args:\n        true_p (torch.Tensor): Probability of coming from Group 1.\n        true_weights_1 (torch.Tensor): Weights for Group 1.\n        true_weights_2 (torch.Tensor): Weights for Group 2.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: A tuple containing the dataset and labels.\n    \"\"\"\n    # Create movie features\n    first_movie_features = torch.randn((num_movies, feature_dim))\n    second_movie_features = torch.randn((num_movies, feature_dim))\n\n    # Only care about difference of features for Bradley-Terry\n    dataset = first_movie_features - second_movie_features\n\n    # Get probabilities that first movie is preferred assuming Group 1 or Group 2\n    weight_1_probs = torch.sigmoid(dataset @ true_weights_1)\n    weight_2_probs = torch.sigmoid(dataset @ true_weights_2)\n\n    # Probability that first movie is preferred overall can be viewed as sum of conditioning on Group 1 and Group 2\n    first_movie_preferred_probs = (\n        true_p * weight_1_probs + (1 - true_p) * weight_2_probs\n    )\n    labels = dist.Bernoulli(first_movie_preferred_probs).sample()\n    return dataset, labels\n\n\ndef compute_likelihoods(\n    dataset: torch.Tensor,\n    labels: torch.Tensor,\n    p: torch.Tensor,\n    w_1: torch.Tensor,\n    w_2: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"\n    Computes the likelihood of each datapoint. Use your calculation from part (a) to help.\n\n    Args:\n        dataset (torch.Tensor): The dataset of differences between movie features.\n        labels (torch.Tensor): The labels where 1 indicates the first movie is preferred, and 0 indicates preference of the second movie.\n        p (torch.Tensor): The probability of coming from Group 1.\n        w_1 (torch.Tensor): Weights for Group 1.\n        w_2 (torch.Tensor): Weights for Group 2.\n\n    Returns:\n        torch.Tensor: The likelihoods for each datapoint. Should have shape (dataset.shape[0], )\n    \"\"\"\n    # YOUR CODE HERE (~6-8 lines)\n    pass\n    # END OF YOUR CODE\n\ndef compute_prior_density(\n    p: torch.Tensor, w_1: torch.Tensor, w_2: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"\n    Computes the prior density of the parameters.\n\n    Args:\n        p (torch.Tensor): The probability of preferring model 1.\n        w_1 (torch.Tensor): Weights for model 1.\n        w_2 (torch.Tensor): Weights for model 2.\n\n    Returns:\n        torch.Tensor: The prior densities of p, w_1, and w_2.\n    \"\"\"\n    # Adjusts p to stay in the range [0.3, 0.7] to prevent multiple equilibria issues at p=0 and p=1\n    p_prob = torch.tensor([2.5]) if 0.3 <= p <= 0.7 else torch.tensor([0.0])\n\n    def normal_pdf(x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Computes the PDF of the standard normal distribution at x.\"\"\"\n        return (1.0 / torch.sqrt(torch.tensor(2 * math.pi))) * torch.exp(-0.5 * x**2)\n\n    weights_1_prob = normal_pdf(w_1)\n    weights_2_prob = normal_pdf(w_2)\n\n    # Concatenate the densities\n    concatenated_prob = torch.cat([p_prob, weights_1_prob, weights_2_prob])\n    return concatenated_prob\n\n\ndef metropolis_hastings(\n    dataset: torch.Tensor,\n    labels: torch.Tensor,\n    sigma: float = 0.01,\n    num_iters: int = 30000,\n    burn_in: int = 20000,\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]:\n    \"\"\"\n    Performs the Metropolis-Hastings algorithm to sample from the posterior distribution.\n    DO NOT CHANGE THE DEFAULT VALUES!\n\n    Args:\n        dataset (torch.Tensor): The dataset of differences between movie features.\n        labels (torch.Tensor): The labels indicating which movie is preferred.\n        sigma (float, optional): Standard deviation for proposal distribution.\n            Defaults to 0.01.\n        num_iters (int, optional): Total number of iterations. Defaults to 30000.\n        burn_in (int, optional): Number of iterations to discard as burn-in.\n            Defaults to 20000.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]: Samples of p,\n        w_1, w_2, and the fraction of accepted proposals.\n    \"\"\"\n    feature_dim = dataset.shape[1]\n\n    # Initialize random starting parameters by sampling priors\n    curr_p = 0.3 + 0.4 * torch.rand(1)\n    curr_w_1 = torch.randn(feature_dim)\n    curr_w_2 = torch.randn(feature_dim)\n\n    # Keep track of samples and total number of accepted proposals\n    p_samples = []\n    w_1_samples = []\n    w_2_samples = []\n    accept_count = 0 \n\n    for T in tqdm(range(num_iters)):\n        # YOUR CODE HERE (~3 lines)\n        pass # Sample proposals for p, w_1, w_2\n        # END OF YOUR CODE\n\n        # YOUR CODE HERE (~4-6 lines)\n        pass # Compute likehoods and prior densities on both the proposed and current samples\n        # END OF YOUR CODE\n\n        # YOUR CODE HERE (~2-4 lines)\n        pass # Obtain the ratios of the likelihoods and prior densities between the proposed and current samples \n        # END OF YOUR CODE \n\n        # YOUR CODE HERE (~1-2 lines)\n        pass # Multiply all ratios (both likelihoods and prior densities) and use this to calculate the acceptance probability of the proposal\n        # END OF YOUR CODE\n\n        # YOUR CODE HERE (~4-6 lines)\n        pass # Sample randomness to determine whether the proposal should be accepted to update curr_p, curr_w_1, curr_w_2, and accept_count\n        # END OF YOUR CODE \n\n        # YOUR CODE HERE (~4-6 lines)\n        pass # Update p_samples, w_1_samples, w_2_samples if we have passed the burn in period T\n        # END OF YOUR CODE \n\n    fraction_accepted = accept_count / num_iters\n    print(f\"Fraction of accepted proposals: {fraction_accepted}\")\n    return (\n        torch.stack(p_samples),\n        torch.stack(w_1_samples),\n        torch.stack(w_2_samples),\n        fraction_accepted,\n    )\n\n\ndef evaluate_metropolis(num_sims: int, num_movies: int, feature_dim: int) -> None:\n    \"\"\"\n    Runs the Metropolis-Hastings algorithm N times and compare estimated parameters\n    with true parameters to obtain success rate. You should attain a success rate of around 90%. \n\n    Note that there are two successful equilibria to converge to. They are true_weights_1 and true_weights_2 with probabilities\n    p and 1-p in addition to true_weights_2 and true_weights_1 with probabilities 1-p and p. This is why even though it may appear your\n    predicted parameters don't match the true parameters, they are in fact equivalent. \n\n    Args:\n        num_sims (int): Number of simulations to run.\n\n    Returns:\n        None\n    \"\"\"\n    \n    success_count = 0\n    for _ in range(num_sims):\n        # Sample random ground truth parameters\n        true_p = 0.3 + 0.4 * torch.rand(1)\n        true_weights_1 = torch.randn(feature_dim)\n        true_weights_2 = torch.randn(feature_dim)\n\n        print(\"\\n---- MCMC Simulation ----\")\n        print(\"True parameters:\", true_p, true_weights_1, true_weights_2)\n\n        dataset, labels = make_data(true_p, true_weights_1, true_weights_2, num_movies, feature_dim)\n        p_samples, w_1_samples, w_2_samples, _ = metropolis_hastings(dataset, labels)\n\n        p_pred = p_samples.mean(dim=0)\n        w_1_pred = w_1_samples.mean(dim=0)\n        w_2_pred = w_2_samples.mean(dim=0)\n\n        print(\"Predicted parameters:\", p_pred, w_1_pred, w_2_pred)\n\n        # Do casework on two equilibria cases to check for success\n        p_diff_case_1 = torch.abs(p_pred - true_p)\n        p_diff_case_2 = torch.abs(p_pred - (1 - true_p))\n\n        w_1_diff_case_1 = torch.max(torch.abs(w_1_pred - true_weights_1))\n        w_1_diff_case_2 = torch.max(torch.abs(w_1_pred - true_weights_2))\n\n        w_2_diff_case_1 = torch.max(torch.abs(w_2_pred - true_weights_2))\n        w_2_diff_case_2 = torch.max(torch.abs(w_2_pred - true_weights_1))\n\n        pass_case_1 = (\n            p_diff_case_1 < 0.1 and w_1_diff_case_1 < 0.5 and w_2_diff_case_1 < 0.5\n        )\n        pass_case_2 = (\n            p_diff_case_2 < 0.1 and w_1_diff_case_2 < 0.5 and w_2_diff_case_2 < 0.5\n        )\n        passes = pass_case_1 or pass_case_2\n\n        print(f'Result: {\"Success\" if passes else \"FAILED\"}')\n        if passes:\n            success_count += 1\n    print(f'Success rate: {success_count / num_sims}')\n\n\nif __name__ == \"__main__\":\n    evaluate_metropolis(num_sims=10, num_movies=30000, feature_dim=10)","id":9,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}}];


</script>
<script type="module">
// Declare startupMessageqpyodide globally
globalThis.qpyodideStartupMessage = document.createElement("p");

// Function to set the button text
globalThis.qpyodideSetInteractiveButtonState = function(buttonText, enableCodeButton = true) {
  document.querySelectorAll(".qpyodide-button-run").forEach((btn) => {
    btn.innerHTML = buttonText;
    btn.disabled = !enableCodeButton;
  });
}

// Function to update the status message in non-interactive cells
globalThis.qpyodideUpdateStatusMessage = function(message) {
  document.querySelectorAll(".qpyodide-status-text.qpyodide-cell-needs-evaluation").forEach((elem) => {
    elem.innerText = message;
  });
}

// Function to update the status message
globalThis.qpyodideUpdateStatusHeader = function(message) {

  if (!qpyodideShowStartupMessage) return;

  qpyodideStartupMessage.innerHTML = message;
}

// Status header update with customized spinner message
globalThis.qpyodideUpdateStatusHeaderSpinner = function(message) {

  qpyodideUpdateStatusHeader(`
    <i class="fa-solid fa-spinner fa-spin qpyodide-icon-status-spinner"></i>
    <span>${message}</span>
  `);
}


// Function that attaches the document status message
function qpyodideDisplayStartupMessage(showStartupMessage) {
  if (!showStartupMessage) {
    return;
  }

  // Get references to header elements
  const headerHTML = document.getElementById("title-block-header");
  const headerRevealJS = document.getElementById("title-slide");

  // Create the outermost div element for metadata
  const quartoTitleMeta = document.createElement("div");
  quartoTitleMeta.classList.add("quarto-title-meta");

  // Create the first inner div element
  const firstInnerDiv = document.createElement("div");
  firstInnerDiv.setAttribute("id", "qpyodide-status-message-area");

  // Create the second inner div element for "Pyodide Status" heading and contents
  const secondInnerDiv = document.createElement("div");
  secondInnerDiv.setAttribute("id", "qpyodide-status-message-title");
  secondInnerDiv.classList.add("quarto-title-meta-heading");
  secondInnerDiv.innerText = "Pyodide Status";

  // Create another inner div for contents
  const secondInnerDivContents = document.createElement("div");
  secondInnerDivContents.setAttribute("id", "qpyodide-status-message-body");
  secondInnerDivContents.classList.add("quarto-title-meta-contents");

  // Describe the Pyodide state
  qpyodideStartupMessage.innerText = "🟡 Loading...";
  qpyodideStartupMessage.setAttribute("id", "qpyodide-status-message-text");
  // Add `aria-live` to auto-announce the startup status to screen readers
  qpyodideStartupMessage.setAttribute("aria-live", "assertive");

  // Append the startup message to the contents
  secondInnerDivContents.appendChild(qpyodideStartupMessage);

  // Combine the inner divs and contents
  firstInnerDiv.appendChild(secondInnerDiv);
  firstInnerDiv.appendChild(secondInnerDivContents);
  quartoTitleMeta.appendChild(firstInnerDiv);

  // Determine where to insert the quartoTitleMeta element
  if (headerHTML || headerRevealJS) {
    // Append to the existing "title-block-header" element or "title-slide" div
    (headerHTML || headerRevealJS).appendChild(quartoTitleMeta);
  } else {
    // If neither headerHTML nor headerRevealJS is found, insert after "Pyodide-monaco-editor-init" script
    const monacoScript = document.getElementById("qpyodide-monaco-editor-init");
    const header = document.createElement("header");
    header.setAttribute("id", "title-block-header");
    header.appendChild(quartoTitleMeta);
    monacoScript.after(header);
  }
}

qpyodideDisplayStartupMessage(qpyodideShowStartupMessage);
</script>
<script type="module">
// Create a logging setup
globalThis.qpyodideMessageArray = []

// Add messages to array
globalThis.qpyodideAddToOutputArray = function(message, type) {
  qpyodideMessageArray.push({ message, type });
}

// Function to reset the output array
globalThis.qpyodideResetOutputArray = function() {
  qpyodideMessageArray = [];
}

globalThis.qpyodideRetrieveOutput = function() {
  return qpyodideMessageArray.map(entry => entry.message).join('\n');
}

// Start a timer
const initializePyodideTimerStart = performance.now();

// Encase with a dynamic import statement
globalThis.qpyodideInstance = await import(
  qpyodideCustomizedPyodideOptions.indexURL + "pyodide.mjs").then(
   async({ loadPyodide }) => {

    console.log("Start loading Pyodide");
    
    // Populate Pyodide options with defaults or new values based on `pyodide`` meta
    let mainPyodide = await loadPyodide(
      qpyodideCustomizedPyodideOptions
    );
    
    // Setup a namespace for global scoping
    // await loadedPyodide.runPythonAsync("globalScope = {}"); 
    
    // Update status to reflect the next stage of the procedure
    qpyodideUpdateStatusHeaderSpinner("Initializing Python Packages");

    // Load the `micropip` package to allow installation of packages.
    await mainPyodide.loadPackage("micropip");
    await mainPyodide.runPythonAsync(`import micropip`);

    // Load the `pyodide_http` package to shim uses of `requests` and `urllib3`.
    // This allows for `pd.read_csv(url)` to work flawlessly.
    // Details: https://github.com/coatless-quarto/pyodide/issues/9
    await mainPyodide.loadPackage("pyodide_http");
    await mainPyodide.runPythonAsync(`
    import pyodide_http
    pyodide_http.patch_all()  # Patch all libraries
    `);

    // Load the `matplotlib` package with necessary environment hook
    await mainPyodide.loadPackage("matplotlib");

    // Set the backend for matplotlib to be interactive.
    await mainPyodide.runPythonAsync(`
    import matplotlib
    matplotlib.use("module://matplotlib_pyodide.html5_canvas_backend")
    from matplotlib import pyplot as plt
    `);

    // Unlock interactive buttons
    qpyodideSetInteractiveButtonState(
      `<i class="fa-solid fa-play qpyodide-icon-run-code"></i> <span>Run Code</span>`, 
      true
    );

    // Set document status to viable
    qpyodideUpdateStatusHeader(
      "🟢 Ready!"
    );

    // Assign Pyodide into the global environment
    globalThis.mainPyodide = mainPyodide;

    console.log("Completed loading Pyodide");
    return mainPyodide;
  }
);

// Stop timer
const initializePyodideTimerEnd = performance.now();

// Create a function to retrieve the promise object.
globalThis._qpyodideGetInstance = function() {
    return qpyodideInstance;
}

</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../src/chap2.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Background</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning from Human Preferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/stair-lab/mlhp" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Machine-Learning-from-Human-Preferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Elicitation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Decisions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Aggregation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Alternatives</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/ack.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>license.qmd</p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-foundations" id="toc-sec-foundations" class="nav-link active" data-scroll-target="#sec-foundations"><span class="header-section-number">1.1</span> Random Preferences as a Model of Comparisons</a></li>
  <li><a href="#types-of-comparison-data" id="toc-types-of-comparison-data" class="nav-link" data-scroll-target="#types-of-comparison-data"><span class="header-section-number">1.2</span> Types of Comparison Data</a>
  <ul class="collapse">
  <li><a href="#full-preference-lists" id="toc-full-preference-lists" class="nav-link" data-scroll-target="#full-preference-lists"><span class="header-section-number">1.2.1</span> Full Preference Lists</a></li>
  <li><a href="#the-most-preferred-element-from-a-subset-binary-choices" id="toc-the-most-preferred-element-from-a-subset-binary-choices" class="nav-link" data-scroll-target="#the-most-preferred-element-from-a-subset-binary-choices"><span class="header-section-number">1.2.2</span> The Most-Preferred Element from a Subset: (Binary) Choices</a></li>
  <li><a href="#mind-the-context" id="toc-mind-the-context" class="nav-link" data-scroll-target="#mind-the-context"><span class="header-section-number">1.2.3</span> Mind the Context</a></li>
  </ul></li>
  <li><a href="#random-utility-models" id="toc-random-utility-models" class="nav-link" data-scroll-target="#random-utility-models"><span class="header-section-number">1.3</span> Random Utility Models</a>
  <ul class="collapse">
  <li><a href="#item-wise-model" id="toc-item-wise-model" class="nav-link" data-scroll-target="#item-wise-model"><span class="header-section-number">1.3.1</span> Item-wise Model</a></li>
  <li><a href="#pairwise-model" id="toc-pairwise-model" class="nav-link" data-scroll-target="#pairwise-model"><span class="header-section-number">1.3.2</span> Pairwise Model</a></li>
  </ul></li>
  <li><a href="#mean-utilities" id="toc-mean-utilities" class="nav-link" data-scroll-target="#mean-utilities"><span class="header-section-number">1.4</span> Mean Utilities</a></li>
  <li><a href="#independence-of-irrelevant-alternatives" id="toc-independence-of-irrelevant-alternatives" class="nav-link" data-scroll-target="#independence-of-irrelevant-alternatives"><span class="header-section-number">1.5</span> Independence of Irrelevant Alternatives</a></li>
  <li><a href="#iias-limitations" id="toc-iias-limitations" class="nav-link" data-scroll-target="#iias-limitations"><span class="header-section-number">1.6</span> IIA’s Limitations</a>
  <ul class="collapse">
  <li><a href="#iia-and-heterogeneity" id="toc-iia-and-heterogeneity" class="nav-link" data-scroll-target="#iia-and-heterogeneity"><span class="header-section-number">1.6.1</span> IIA and Heterogeneity</a></li>
  <li><a href="#similar-options" id="toc-similar-options" class="nav-link" data-scroll-target="#similar-options"><span class="header-section-number">1.6.2</span> Similar Options</a></li>
  </ul></li>
  <li><a href="#discussion-questions" id="toc-discussion-questions" class="nav-link" data-scroll-target="#discussion-questions"><span class="header-section-number">1.7</span> Discussion Questions</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">1.8</span> Exercises</a>
  <ul class="collapse">
  <li><a href="#properties-of-iia-models" id="toc-properties-of-iia-models" class="nav-link" data-scroll-target="#properties-of-iia-models"><span class="header-section-number">1.8.1</span> Properties of IIA Models ⭐</a></li>
  <li><a href="#discrete-choice-models" id="toc-discrete-choice-models" class="nav-link" data-scroll-target="#discrete-choice-models"><span class="header-section-number">1.8.2</span> Discrete Choice Models ⭐⭐</a></li>
  <li><a href="#mixtures-and-correlations" id="toc-mixtures-and-correlations" class="nav-link" data-scroll-target="#mixtures-and-correlations"><span class="header-section-number">1.8.3</span> Mixtures and correlations ⭐</a></li>
  <li><a href="#sufficient-statistics" id="toc-sufficient-statistics" class="nav-link" data-scroll-target="#sufficient-statistics"><span class="header-section-number">1.8.4</span> Sufficient Statistics ⭐⭐⭐</a></li>
  <li><a href="#non-random-utility-models" id="toc-non-random-utility-models" class="nav-link" data-scroll-target="#non-random-utility-models"><span class="header-section-number">1.8.5</span> Non-Random Utility Models ⭐⭐⭐</a></li>
  <li><a href="#posterior-inference-for-mixture-preferences" id="toc-posterior-inference-for-mixture-preferences" class="nav-link" data-scroll-target="#posterior-inference-for-mixture-preferences"><span class="header-section-number">1.8.6</span> Posterior Inference for Mixture Preferences ⭐⭐</a></li>
  </ul></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/stair-lab/mlhp/blob/main/src/chap2.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/stair-lab/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
<script src="https://cdn.jsdelivr.net/npm/monaco-editor@0.46.0/min/vs/loader.js"></script>
<script type="module" id="qpyodide-monaco-editor-init">

  // Configure the Monaco Editor's loader
  require.config({
    paths: {
      'vs': 'https://cdn.jsdelivr.net/npm/monaco-editor@0.46.0/min/vs'
    }
  });
</script>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Background</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<iframe src="https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/" style="width:45%; height:225px;">
</iframe>
<iframe src="https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/" style="width:45%; height:225px;">
</iframe>
<p><a href="https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/" class="btn btn-outline-primary" role="button">Fullscreen Part 1</a> <a href="https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/" class="btn btn-outline-primary" role="button">Fullscreen Part 2</a></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Intended Learning outcomes
</div>
</div>
<div class="callout-body-container callout-body">
<p>By the end of this chapter you will be able to:</p>
<ul>
<li><strong>Differentiate</strong> deterministic preferences from <em>stochastic (random)</em> preferences and justify why randomness is essential for modelling noisy human choice.</li>
<li><strong>Define</strong> and <strong>apply</strong> the <em>Independence of Irrelevant Alternatives (IIA)</em> axiom, explaining how it collapses the full preference distribution to an <span class="math inline">\(n\)</span>-parameter logit model.</li>
<li><strong>Derive</strong> choice probabilities for binary comparisons (Bradley–Terry), accept–reject decisions (logistic regression), and full or partial rankings (Plackett–Luce) from a random-utility model with i.i.d. Gumbel shocks.</li>
<li><strong>Identify</strong> and <strong>compare</strong> the main types of comparison data (full lists, choice-from-a-set, pairwise) and map each to the underlying random preference distribution.</li>
<li><strong>Simulate</strong> preference data using the Ackley test function, and <strong>visualize</strong> how utility landscapes translate into observed choices.</li>
<li><strong>Diagnose</strong> two key limitations of IIA—population heterogeneity (mixture models) and the <em>red-bus/blue-bus</em> cloning problem—and <strong>motivate</strong> richer models with correlated utility shocks.</li>
<li><strong>Incorporate context features</strong> <span class="math inline">\(x\)</span> into random-utility formulations to generalize learned preferences across environments, prompts, or tasks.</li>
<li><strong>Explain</strong> the identification issue in logit models and <strong>justify</strong> the convention of fixing one utility level (e.g., the outside option <span class="math inline">\(y_{0}\)</span>).</li>
</ul>
</div>
</div>
<p>This is a book about machine learning from human preferences. This first chapter is about generative models for human actions, in particular for <em>comparisons</em>. In classical supervised learning, comparisons implicitly arise for a trained model: If the logit of a particular output from a supervised learning model is higher for the label <span class="math inline">\(y\)</span> than <span class="math inline">\(y'\)</span> we would say that the model is <em>more likely</em> to produce <span class="math inline">\(y\)</span> than <span class="math inline">\(y'\)</span>. While this introduces some amount of comparison on model outputs, it does not help us if the data is given by <span class="math inline">\(y\)</span> being preferred to <span class="math inline">\(y'\)</span>, written <span class="math inline">\(y \succ y'\)</span>.</p>
<p>First, hence, we introduce stochastic preferences as a model of preferences. We then discuss the most important assumption made in stochastic choice, the Independence of Irrelevant Alternatives (IIA), and discuss its advantages and pitfalls. Chapters 1-5 will restrict to comparisons, including binary comparisons, accept-reject decisions, and ranking lists. Other related data types, such as Likert scales, will be considered in <span class="citation" data-cites="chapter-beyond">(<a href="#ref-chapter-beyond" role="doc-biblioref"><strong>chapter-beyond?</strong></a>)</span>.</p>
<section id="sec-foundations" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="sec-foundations"><span class="header-section-number">1.1</span> Random Preferences as a Model of Comparisons</h2>
<p>We start with a set of <strong>objects</strong> <span class="math inline">\(y \in Y\)</span>—be they products, robot trajectories, or language model responses. We will consider models to generate comparisons that are orders. For realism, but also for mathematical simplicity, we will assume in this book that the set <span class="math inline">\(Y\)</span> of objects is discrete and has <span class="math inline">\(n\)</span> objects.</p>
<p>Comparisons may be random and are generated by random draws of (total) orders. (Total) Orders have two properties.</p>
<ul>
<li>First, for two objects <span class="math inline">\(y, y'\)</span> either <span class="math inline">\(y \prec y'\)</span> and/or <span class="math inline">\(y \succ y'\)</span> must hold, an assumption called <em>totality</em>: Either <span class="math inline">\(y\)</span> is weakly preferred to <span class="math inline">\(y'\)</span> or <span class="math inline">\(y'\)</span> is weakly preferred to <span class="math inline">\(y\)</span>.</li>
<li>The second assumption is transitivity: if <span class="math inline">\(y \succ y'\)</span> and <span class="math inline">\(y' \succ y''\)</span>, then also <span class="math inline">\(y \succ y''\)</span>.</li>
</ul>
<p>In the following, we consider randomness as generated from a <em>decision-maker</em> who has an order, or preference relation, <span class="math inline">\(\prec\)</span> on a set of objects <span class="math inline">\(Y\)</span>. We refer to the random object <span class="math inline">\(\prec\)</span> as the oracle preference. Each preference <span class="math inline">\(\prec\)</span> has an associated probability mass <span class="math inline">\(\mathbb{P}[\mathord{\prec}]\)</span>, leading to an <span class="math inline">\((n!-1)\)</span>-dimensional vector encoding the full random preference set. (This might look like, and is already for small values of <span class="math inline">\(n\)</span> a large number. Reducing this representational complexity is a goal of this chapter.)</p>
<p>One might wonder why we need to have a random preference. Deterministic preferences are conceptually helpful constructs and are used broadly in the fields of Consumer theory (e.g., <span class="citation" data-cites="mas1995microeconomic">Mas-Colell et al. (<a href="#ref-mas1995microeconomic" role="doc-biblioref">1995</a>)</span>). However, they suffer when bringing them to data, as data is inherently noisy.</p>
<p>Noise plays three roles, which we will address in the chapters of this book.</p>
<ul>
<li>It first captures measurement noise. When observing choices from a person, this may be an imperfect preference relationship. We will consider such noise (homogeneity but uncertainty), for example, in <span class="quarto-unresolved-ref">?sec-learning</span> when discussing Gaussian processes.</li>
<li></li>
<li>A third role of noise arises from heterogeneity, where <span class="math inline">\(\prec\)</span> encodes the <em>population</em> preferences of several decision-makers. We consider an example like this in one of the exercises in this chapter.</li>
</ul>
<p>Even when allowing for randomness, assumptions we will impose in this section, be it transitivity or the Independence of Irrelevant Alternatives, are stark yet practical. In many situations they will fail, for good reasons. Whether these are human’s inability to express rankings, contextual challenges of domains, or community norms, we will discuss them in, humans cannot clearly rank alternatives, their choices reflect individualistic norms, or they might have self-control pictures. Many of these wrinkles on the approach to preferences presented here is contained in <span class="quarto-unresolved-ref">?sec-beyond</span>. Until then, we will make the fullest use of learning stochastic preferences.</p>
</section>
<section id="types-of-comparison-data" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="types-of-comparison-data"><span class="header-section-number">1.2</span> Types of Comparison Data</h2>
<p>There are different types of comparison data we may observe. We can relate them back to the population preferences <span class="math inline">\(\prec\)</span>.</p>
<section id="full-preference-lists" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="full-preference-lists"><span class="header-section-number">1.2.1</span> Full Preference Lists</h3>
<p>The conceptually simplest and practically most verbose preference sampling is to get the full preference ranking, i.e.&nbsp;<span class="math inline">\(L = (y_1, y_2, \dots, y_n)\)</span>, where <span class="math inline">\(y_1 \succ y_2 \succ \cdots \succ y_n\)</span>. In this case, we know not only that <span class="math inline">\(y_1\)</span> is preferred to <span class="math inline">\(y_2\)</span>, but also, by transitivity, that it is preferred to all other options. Similarly, we know that <span class="math inline">\(y_2\)</span> is preferred to all options but <span class="math inline">\(y_1\)</span>, <em>etc.</em> In many cases, we do not observe full preferences as the cognitive load for humans is too high.</p>
</section>
<section id="the-most-preferred-element-from-a-subset-binary-choices" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="the-most-preferred-element-from-a-subset-binary-choices"><span class="header-section-number">1.2.2</span> The Most-Preferred Element from a Subset: (Binary) Choices</h3>
<p>Another type of sample is <span class="math inline">\((y, Y')\)</span> where <span class="math inline">\(y\)</span> is the most preferred alternative from <span class="math inline">\(Y'\)</span> for a sampled preference. Formally, <span class="math inline">\(y \prec y'\)</span> for all <span class="math inline">\(y' \in Y' \setminus \{y\}\)</span>—<span class="math inline">\(y\)</span> is preferred to all elements of <span class="math inline">\(Y\)</span> but <span class="math inline">\(y'\)</span>.</p>
<p>Formally, the probability that we observe <span class="math inline">\((y, Y')\)</span> is <span class="math display">\[
\mathbb{P}[(y, Y')] = \sum_{\prec: y \prec y' \forall y' \in Y' \setminus \{y\}} \mathbb{P} [\mathord{\prec}].
\]</span> That is, the probability of observing <span class="math inline">\((y, Y')\)</span> is given by the sum of all preferred samples <span class="math inline">\(\prec\)</span> such that <span class="math inline">\(y\)</span> is preferred to all <span class="math inline">\(y'\)</span> in <span class="math inline">\(Y'\)</span> other than <span class="math inline">\(y\)</span>.</p>
<p>If the choice is binary, <span class="math inline">\(Y' = \{y, y'\}\)</span>, we also write <span class="math inline">\((y \succ y')\)</span> for a sample <span class="math inline">\((x, \{x,y\})\)</span>. We highlight that these objects are random, and depend on the sample of <span class="math inline">\(\prec\)</span>. Binary data is convenient and quick to elicit and has been prominently applied in language model finetuning and evaluation.</p>
<p>Sometimes, particularly when a decision-maker is offered an object or “nothing”, we will implicitly assume that there is an “outside option” <span class="math inline">\(y_0\)</span> in <span class="math inline">\(Y\)</span>, allowing us to interpret <span class="math inline">\((y, \{y, y_0\})\)</span> as “accepting” <span class="math inline">\(y\)</span>, and <span class="math inline">\((y_0, \{y, y_0\})\)</span> as rejecting it. Outside options can be thought of as fundamental limits to what a system designer can obtain. Consider a recommendation system. A user of that system might engage with content or not. In principle, instead of engaging, they will do something else. We do not model this in out set of objects <span class="math inline">\(Y\)</span> as a fundamental abstraction. <em>All models are wrong, but some are useful.</em></p>
</section>
<section id="mind-the-context" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="mind-the-context"><span class="header-section-number">1.2.3</span> Mind the Context</h3>
<p>Choices are often conditional, and data is given by <span class="math inline">\((x, L)\)</span> (for list-based data), <span class="math inline">\((x, y, Y')\)</span> (for general choice-based data), or <span class="math inline">\((x, y, y')\)</span> for binary data. <span class="math inline">\(x \in X\)</span> is some <em>context</em>: the environment of a purchase, the goal of a robot, or a user prompt for a large language model. It can also be a prompt to the decision-maker, e.g., to human raters on whether they should pick preferences based on helplessness or harmlessness <span class="citation" data-cites="ganguli2022redteaminglanguagemodels">Ganguli et al. (<a href="#ref-ganguli2022redteaminglanguagemodels" role="doc-biblioref">2022</a>)</span>. The inclusion of context in learning allows for the generalization of preferences, as we will see in subsequent chapters.</p>
</section>
</section>
<section id="random-utility-models" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="random-utility-models"><span class="header-section-number">1.3</span> Random Utility Models</h2>
<p>An equivalent way to represent random preferences is to identify a sample <span class="math inline">\(\prec\)</span> with a vector <span class="math inline">\(u_{\prec} = (u_{\mathord{\prec}} (y))_{y \in Y} \in \mathbb R^Y\)</span> where <span class="math inline">\(y \succ y'\)</span> if and only if <span class="math inline">\(u(y) &gt; u(y')\)</span>. (For the concerned reader: We assume that <span class="math inline">\(u(y) = u(y')\)</span> happens with zero probability; and for discrete <span class="math inline">\(Y\)</span> such a vector always exists.)</p>
<p>To get a sense for different random utility models, we consider a particular model that has the complexity of many models in modern machine learning: The Ackley function. In this model, each alternative is represented by a <span class="math inline">\(d\)</span>-dimensional vector <span class="math inline">\((x_1, \ldots, x_d) \in \mathbb{R}^d\)</span>, the Ackley function is given by <span class="math display">\[
\text{Ackley}(x_1, x_2, \dots, x_d) = -a e^{-b \sqrt{\frac{1}{d} \sum_{j=1}^d x_j^2}} - e^{\frac{1}{d} \sum_{j=1}^d \cos(c x_j)} + a + e.
\]</span> for some constants <span class="math inline">\(a, b, c \in \mathbb{R}\)</span>. By stacking a number <span class="math inline">\(k\)</span> of human preferences, we can compute for <span class="math inline">\(k\)</span> samples from a random model the function in a vectorized way.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Code
</div>
</div>
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-1"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
<div id="95c9b978" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="kw">def</span> ackley(X, a<span class="op">=</span><span class="dv">20</span>, b<span class="op">=</span><span class="fl">0.2</span>, c<span class="op">=</span><span class="dv">2</span><span class="op">*</span>np.pi):</span>
<span id="cb1-5"><a href="#cb1-5"></a>    <span class="co">"""</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co">    Compute the Ackley function.</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co">    Parameters:</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co">      X: A NumPy array of shape (n, d) where each row is a n-dimensional point.</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="co">      a, b, c: Parameters of the Ackley function.</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="co">    Returns:</span></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="co">      A NumPy array of shape (n,) of function values</span></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co">    """</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>    X <span class="op">=</span> np.atleast_2d(X)</span>
<span id="cb1-14"><a href="#cb1-14"></a>    d <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb1-15"><a href="#cb1-15"></a>    sum_sq <span class="op">=</span> np.<span class="bu">sum</span>(X <span class="op">**</span> <span class="dv">2</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-16"><a href="#cb1-16"></a>    term1 <span class="op">=</span> <span class="op">-</span>a <span class="op">*</span> np.exp(<span class="op">-</span>b <span class="op">*</span> np.sqrt(sum_sq <span class="op">/</span> d))</span>
<span id="cb1-17"><a href="#cb1-17"></a>    term2 <span class="op">=</span> <span class="op">-</span>np.exp(np.<span class="bu">sum</span>(np.cos(c <span class="op">*</span> X), axis<span class="op">=</span><span class="dv">1</span>) <span class="op">/</span> d)</span>
<span id="cb1-18"><a href="#cb1-18"></a>    <span class="cf">return</span> term1 <span class="op">+</span> term2 <span class="op">+</span> a <span class="op">+</span> np.e</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can think of the rows of <span class="math inline">\(X\)</span> as features of different alternatives. We can visualize the full landscape of the utility function for two alternatives (<span class="math inline">\(n=2\)</span>) and features of a single dimension (<span class="math inline">\(d=1\)</span>).</p>
<div class="callout callout-style-default callout-note callout-titled" title="Code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Code
</div>
</div>
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-2"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
<div id="73089e30" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> LinearSegmentedColormap</span>
<span id="cb2-3"><a href="#cb2-3"></a>ccmap <span class="op">=</span> LinearSegmentedColormap.from_list(<span class="st">"ackley"</span>, [<span class="st">"#f76a05"</span>, <span class="st">"#FFF2C9"</span>])</span>
<span id="cb2-4"><a href="#cb2-4"></a>plt.rcParams.update({</span>
<span id="cb2-5"><a href="#cb2-5"></a>    <span class="st">"font.size"</span>: <span class="dv">14</span>,</span>
<span id="cb2-6"><a href="#cb2-6"></a>    <span class="st">"axes.labelsize"</span>: <span class="dv">16</span>,</span>
<span id="cb2-7"><a href="#cb2-7"></a>    <span class="st">"xtick.labelsize"</span>: <span class="dv">14</span>,</span>
<span id="cb2-8"><a href="#cb2-8"></a>    <span class="st">"ytick.labelsize"</span>: <span class="dv">14</span>,</span>
<span id="cb2-9"><a href="#cb2-9"></a>    <span class="st">"legend.fontsize"</span>: <span class="dv">14</span>,</span>
<span id="cb2-10"><a href="#cb2-10"></a>    <span class="st">"axes.titlesize"</span>: <span class="dv">16</span>,</span>
<span id="cb2-11"><a href="#cb2-11"></a>})</span>
<span id="cb2-12"><a href="#cb2-12"></a>plt.rcParams[<span class="st">'text.usetex'</span>] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-13"><a href="#cb2-13"></a></span>
<span id="cb2-14"><a href="#cb2-14"></a><span class="kw">def</span> draw_surface():</span>
<span id="cb2-15"><a href="#cb2-15"></a>    inps <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb2-16"><a href="#cb2-16"></a>    X, Y <span class="op">=</span> np.meshgrid(inps, inps)</span>
<span id="cb2-17"><a href="#cb2-17"></a>    grid <span class="op">=</span> np.column_stack([X.ravel(), Y.ravel()])</span>
<span id="cb2-18"><a href="#cb2-18"></a>    Z <span class="op">=</span> ackley(grid).reshape(X.shape)</span>
<span id="cb2-19"><a href="#cb2-19"></a>    </span>
<span id="cb2-20"><a href="#cb2-20"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">5</span>))</span>
<span id="cb2-21"><a href="#cb2-21"></a>    contour <span class="op">=</span> plt.contourf(X, Y, Z, <span class="dv">50</span>, cmap<span class="op">=</span>ccmap)</span>
<span id="cb2-22"><a href="#cb2-22"></a>    plt.contour(X, Y, Z, levels<span class="op">=</span><span class="dv">15</span>, colors<span class="op">=</span><span class="st">'black'</span>, linewidths<span class="op">=</span><span class="fl">0.5</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb2-23"><a href="#cb2-23"></a>    plt.colorbar(contour, label<span class="op">=</span><span class="vs">r'$f(x)$'</span>, ticks<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">6</span>])</span>
<span id="cb2-24"><a href="#cb2-24"></a>    plt.xlim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-25"><a href="#cb2-25"></a>    plt.ylim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-26"><a href="#cb2-26"></a>    plt.xticks([<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>])</span>
<span id="cb2-27"><a href="#cb2-27"></a>    plt.yticks([<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>])</span>
<span id="cb2-28"><a href="#cb2-28"></a>    plt.xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb2-29"><a href="#cb2-29"></a>    plt.ylabel(<span class="vs">r'$x_2$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this model, we can sample choice data when assuming a random generating model of, e.g., <code>np.random.randn(n, d)*0.5 + np.ones((n, d))*0.5</code>.</p>
<section id="item-wise-model" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="item-wise-model"><span class="header-section-number">1.3.1</span> Item-wise Model</h3>
<p>One method for data collection is accept-reject sampling, where the decision-maker considers one item at a time and decides if they like it compared to an outside option. This is common in applications like recommendation systems, where accepting refers to a consumption signal.</p>
<p>We will use a simulation to familiarize ourselves with accept-reject sampling. On the surface below, blue and red points correspond to accept or reject points.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Code
</div>
</div>
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-3"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
<div id="4827b6af" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>d <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>n <span class="op">=</span> <span class="dv">800</span></span>
<span id="cb3-3"><a href="#cb3-3"></a>items <span class="op">=</span> np.random.randn(n, d)<span class="op">*</span><span class="fl">0.5</span> <span class="op">+</span> np.ones((n, d))<span class="op">*</span><span class="fl">0.5</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>utilities <span class="op">=</span> ackley(items)</span>
<span id="cb3-5"><a href="#cb3-5"></a>y <span class="op">=</span> (utilities <span class="op">&gt;</span> utilities.mean())</span>
<span id="cb3-6"><a href="#cb3-6"></a>draw_surface()</span>
<span id="cb3-7"><a href="#cb3-7"></a>plt.scatter(items[:, <span class="dv">0</span>], items[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">'coolwarm'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb3-8"><a href="#cb3-8"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chap2_files/figure-html/cell-4-output-1.png" width="519" height="445" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="pairwise-model" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="pairwise-model"><span class="header-section-number">1.3.2</span> Pairwise Model</h3>
<p>Pairwise comparisons, now used to fine-tune large language models can similarly be generated in this model.</p>
<iframe src="https://app.opinionx.co/6bef4ca1-82f5-4c1d-8c5a-2274509f22e2" style="width:100%; height:450px;">
</iframe>
<div class="callout callout-style-default callout-note callout-titled" title="Code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Code
</div>
</div>
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-4"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
<div id="ea3d3f04" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>n_pairs <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>pair_indices <span class="op">=</span> np.random.randint(<span class="dv">0</span>, n, size<span class="op">=</span>(n_pairs, <span class="dv">2</span>))</span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="co"># Exclude pairs where both indices are the same</span></span>
<span id="cb4-4"><a href="#cb4-4"></a>mask <span class="op">=</span> pair_indices[:, <span class="dv">0</span>] <span class="op">!=</span> pair_indices[:, <span class="dv">1</span>]</span>
<span id="cb4-5"><a href="#cb4-5"></a>pair_indices <span class="op">=</span> pair_indices[mask]</span>
<span id="cb4-6"><a href="#cb4-6"></a></span>
<span id="cb4-7"><a href="#cb4-7"></a>scores <span class="op">=</span> np.zeros(n, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb4-8"><a href="#cb4-8"></a>wins <span class="op">=</span> utilities[pair_indices[:, <span class="dv">0</span>]] <span class="op">&gt;</span> utilities[pair_indices[:, <span class="dv">1</span>]]</span>
<span id="cb4-9"><a href="#cb4-9"></a></span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="co"># For pairs where the first item wins:</span></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="co">#   - Increase score for the first item by 1</span></span>
<span id="cb4-12"><a href="#cb4-12"></a><span class="co">#   - Decrease score for the second item by 1</span></span>
<span id="cb4-13"><a href="#cb4-13"></a>np.add.at(scores, pair_indices[wins, <span class="dv">0</span>], <span class="dv">1</span>)</span>
<span id="cb4-14"><a href="#cb4-14"></a>np.add.at(scores, pair_indices[wins, <span class="dv">1</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-15"><a href="#cb4-15"></a></span>
<span id="cb4-16"><a href="#cb4-16"></a><span class="co"># For pairs where the second item wins or it's a tie:</span></span>
<span id="cb4-17"><a href="#cb4-17"></a><span class="co">#   - Decrease score for the first item by 1</span></span>
<span id="cb4-18"><a href="#cb4-18"></a><span class="co">#   - Increase score for the second item by 1</span></span>
<span id="cb4-19"><a href="#cb4-19"></a>np.add.at(scores, pair_indices[<span class="op">~</span>wins, <span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-20"><a href="#cb4-20"></a>np.add.at(scores, pair_indices[<span class="op">~</span>wins, <span class="dv">1</span>], <span class="dv">1</span>)</span>
<span id="cb4-21"><a href="#cb4-21"></a></span>
<span id="cb4-22"><a href="#cb4-22"></a><span class="co"># Determine preferred and non-preferred items based on scores</span></span>
<span id="cb4-23"><a href="#cb4-23"></a>preferred <span class="op">=</span> scores <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb4-24"><a href="#cb4-24"></a>non_preferred <span class="op">=</span> scores <span class="op">&lt;</span> <span class="dv">0</span></span>
<span id="cb4-25"><a href="#cb4-25"></a></span>
<span id="cb4-26"><a href="#cb4-26"></a>draw_surface()</span>
<span id="cb4-27"><a href="#cb4-27"></a>plt.scatter(items[preferred, <span class="dv">0</span>], items[preferred, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Preferred'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb4-28"><a href="#cb4-28"></a>plt.scatter(items[non_preferred, <span class="dv">0</span>], items[non_preferred, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'purple'</span>, label<span class="op">=</span><span class="st">'Non-preferred'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb4-29"><a href="#cb4-29"></a>plt.legend()</span>
<span id="cb4-30"><a href="#cb4-30"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chap2_files/figure-html/cell-5-output-1.png" width="519" height="445" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Similarly, we can sample data for <span class="math inline">\((y, Y')\)</span> for <span class="math inline">\(y \in Y' \subseteq Y\)</span>.</p>
</section>
</section>
<section id="mean-utilities" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="mean-utilities"><span class="header-section-number">1.4</span> Mean Utilities</h2>
<p>We can view a random utility model <span class="math inline">\(u_{\mathord{\prec}}\)</span> as a deterministic part and a random part: <span class="math display">\[
u_{\mathord{\prec}} (y) = u(y) + \varepsilon_{\mathord{\prec}y}.
\]</span> The vector <span class="math inline">\(u(y)\)</span> is deterministic, and a vector <span class="math inline">\((\varepsilon_{\mathord{\prec}y})_{y \in Y}\)</span> of noise is independent for different <span class="math inline">\(\prec\)</span>. We say that <span class="math inline">\(u(y)\)</span> is the mean utility and <span class="math inline">\(\varepsilon_{\mathord{\prec}y}\)</span> is the noise. There are different forms of noise possible. We will focus on a particular one (called Type-1 Extreme value), but others are also popular, for example Gaussian noise.</p>
<p>There are (at least) three different ways to view this noise:</p>
<ul>
<li>Either it is capturing the heterogeneity of different decision-makers—a view that is taken in the Economics field of Industrial Organization. Under this view, observing <span class="math inline">\((y, Y')\)</span> more frequently than <span class="math inline">\((y', Y')\)</span> is a sign of there being a higher number of decision-makers preferring <span class="math inline">\(y\)</span> over <span class="math inline">\(y'\)</span> than the other way around.</li>
<li>or as errors of a decision-maker’s optimization of utilities <span class="math inline">\(u(y)\)</span>. This view is endorsed in the literature on Bounded Rationality. Under this view, it cannot be directly concluded from frequent observation of <span class="math inline">\((y, Y')\)</span> compared to <span class="math inline">\((y', Y')\)</span> that <span class="math inline">\(y\)</span> is preferred to <span class="math inline">\(y\)</span>. It might have been chosen in error.</li>
<li>We can also view it as a belief of the designer about the preferences <span class="math inline">\((u(y))_{y \in Y}\)</span>. In this view, the posterior after observing data can be used to make claims about relative preferences.</li>
</ul>
<p>The interpretation will guide our decision-making predictions in Chapters 4 and 5.</p>
<p>We next introduce a main way to simplify learning utility functions: The axiom of Independence of Irrelevant Alternatives.</p>
</section>
<section id="independence-of-irrelevant-alternatives" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="independence-of-irrelevant-alternatives"><span class="header-section-number">1.5</span> Independence of Irrelevant Alternatives</h2>
<p>In later chapters, we will consider cases where we sample from the most preferred elements from all objects <span class="math inline">\((y,Y)\)</span>, which we call generation task. A simple assumption will allow us to recover the probabilities of <span class="math inline">\((y, Y)\)</span>, and in fact, the full distribution of <span class="math inline">\(\prec\)</span> from binary comparisons: The so-called Independence of Irrelevant Alternatives, introduced in <span class="citation" data-cites="luce1959individual">Luce et al. (<a href="#ref-luce1959individual" role="doc-biblioref">1959</a>)</span>. This assumption not only allows us to easily identify a preference model, it will also massively reduce what is needed to be estimated from data: Instead of the full <span class="math inline">\(n!-1\)</span>-dimensional object, it will be sufficient to learn <span class="math inline">\(n\)</span> values.</p>
<p>IIA assumes that the relative likelihood of choosing <span class="math inline">\(y\)</span> compared to z does not change whether a third alternative <span class="math inline">\(w\)</span> is in the choice set or not. Formally, for every <span class="math inline">\(Y' \subseteq Y\)</span>, <span class="math inline">\(y,z \in Y'\)</span>, and <span class="math inline">\(w \in Y \setminus Y'\)</span>, <span class="math display">\[
\frac{\mathbb{P}[(y, Y')]}{\mathbb{P}[(z, Y')]} = \frac{\mathbb{P}[(y, Y' \cup \{w\})]}{\mathbb{P}[(z, Y' \cup \{w\})]}.
\]</span> (In particular, it must be that <span class="math inline">\(\mathbb{P}[(z, Y')] \neq 0\)</span> and <span class="math inline">\(\mathbb{P}[(z, Y' \cup \{w\})] \neq 0\)</span>.) That is, the relative probability of choosing <span class="math inline">\(y\)</span> over <span class="math inline">\(y''\)</span> and <span class="math inline">\(y'\)</span> over <span class="math inline">\(y''\)</span> should be independent of whether <span class="math inline">\(z\)</span> is present in the choice set <span class="math inline">\(Y' \subseteq Y\)</span>. We will show that this single assumption is sufficient to make the choice model <span class="math inline">\(n\)</span>-dimensional, making learning feasible.</p>
<p>First, to our primary example: All random utility models with <em>independent and identically distributed</em> noise terms satisfy IIA. (We ask the reader to convince themselves that the Ackerman function does not satisfy IIA.)</p>
<div class="callout callout-style-default callout-tip callout-titled" title="theorem">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>A random utility model <span class="math inline">\(u_{\mathord{\prec}}(y)\)</span> satisfies IIA if and only if we can write it as <span class="math inline">\(u_{\mathord{\prec}}(y) = u(y) + \varepsilon_{\mathord{\prec}y}\)</span>, where <span class="math inline">\(u(y)\)</span> is deterministic and <span class="math inline">\(\varepsilon_{\mathord{\prec}y}\)</span> is sampled independently and identically from the Gumbel distribution. The Gumbel distribution has cumulative distribution function <span class="math inline">\(F(x) = e^{-e^{-x}}\)</span>.</p>
</div>
</div>
<p>This is quite strong, and an <strong>equivalence</strong>. If we are willing to assume IIA, it is sufficient to learn <span class="math inline">\(n\)</span> parameters to characterize the full distribution—an exponential decrease in parameters to learn. The Gumbel model may be unusual in particular for those with a stronger background in machine learning. A more familiar formulation arises for the probabilities of choice.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="theorem">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>Assume a random preference model satisfies IIA, hence <span class="math inline">\(u_{\mathord{\prec}} (y)= u(y) + \varepsilon_{\mathord{\prec}y}\)</span>. Then, the probabilities of lists are: <span class="math display">\[
\mathbb{P}[(y_1 \succ y_2 \succ \cdots \succ y_n)] = \frac{e^{u(y_1)}}{\sum_{i=1}^n e^{u(y_1)}} \cdot \frac{e^{u(y_2)}}{\sum_{i=2}^n e^{u(y_i)}}\cdot \frac{e^{u(y_3)}}{\sum_{i=3}^n e^{u(y_1)}} \cdots \frac{e^{u(y_{n-1})}}{ e^{u(y_{n-1})} +  e^{u(y_{n})}}.
\]</span> For choices from sets, <span class="math display">\[
\mathbb{P} [(y, Y')] = \frac{e^{u(y)}}{\sum_{y' \in Y'} e^{u(y')}} = \operatorname{softmax}_y ((u(y'))_{y' \in Y'}).
\]</span> In particular, for binary comparisons <span class="math display">\[
\mathbb{P} [(y_1 \succ y_2)] = \frac{e^{u(y_1)}}{e^{u(y_1)} + e^{u(y_1)}} = \frac{1}{1 + e^{u(y_1) - u(y_2)}} = \sigma (u(y_1) - u(y_2)).
\]</span> where <span class="math inline">\(\sigma = 1/(1 + e^x)\)</span> is the sigmoid function.</p>
</div>
</div>
<p>In particular, the choice probabilities <span class="math inline">\(\mathbb{P} [(y, Y)]\)</span> are equivalent to the multi-class logistic regression model (also called multinomial logit model), and generation from this model, that is, sampling <span class="math inline">\(y\)</span> with probability <span class="math inline">\(\mathbb{P}[(y, Y)]\)</span> is given by softmax-sampling <span class="math inline">\(\operatorname{softmax}((u(y))_{y \in Y})\)</span>.</p>
<p>This model has many names, depending on the feedback type we consider. For binary comparison data, it is the Bradley-Terry model <span class="citation" data-cites="bradley1952rank">Bradley and Terry (<a href="#ref-bradley1952rank" role="doc-biblioref">1952</a>)</span>. If data is in forms of list, it is called the Plackett-Luce model <span class="citation" data-cites="plackett1975analysis">Plackett (<a href="#ref-plackett1975analysis" role="doc-biblioref">1975</a>)</span>. For accept-reject sampling it is also called logistic regression. For choices from subsets <span class="math inline">\(Y\)</span>, is is called the (discrete choice) logit model. We will usually call the model the <strong>logit model</strong> and specify the feedback type.</p>
<p>The IIA assumption has many desirable properties, such as stochastic transitivity and relativity. The reader is asked to prove them in the exercises to this chapter. The learning of, and optimization based on, the mean utilities <span class="math inline">\((u(y))_{y \in Y}\)</span> is one of the central goal of this book. Supervised learning based on it will be covered in the next chapter.</p>
<p>One note on identification, that is, whether different utility functions <span class="math inline">\(u\)</span> generate the same random preference model <span class="math inline">\(\prec\)</span>. The models that are implied by <span class="math inline">\(u(y)\)</span> and by <span class="math inline">\(u(y) + c\)</span> for any constant <span class="math inline">\(c \in \mathbb{R}\)</span> are the same, which means that any learning of the function <span class="math inline">\(u\)</span>, which we will engage in in the next chapters, will need to fix one of the values <span class="math inline">\(u(y)\)</span>. If there is an outside option <span class="math inline">\(y_0\)</span>, then, it is typically chosen <span class="math inline">\(u(y_0) = 0\)</span> and all mean utilities are in comparison to the outside option.</p>
<p>IIA has limitations, which might require to allow for more flexible specifications of noise and heterogeneity.</p>
</section>
<section id="iias-limitations" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="iias-limitations"><span class="header-section-number">1.6</span> IIA’s Limitations</h2>
<p>IIA is surprisingly strong, but does not allow for choice probabilities that are, fundamentally, results of multiple decision-makers making choices together. A first restriction is given by</p>
<section id="iia-and-heterogeneity" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" class="anchored" data-anchor-id="iia-and-heterogeneity"><span class="header-section-number">1.6.1</span> IIA and Heterogeneity</h3>
<p>A crucial shortcoming of IIA is that if sub-populations satisfy IIA this does not mean that the full population satisfies IIA. Assume that our population consists of sub-populations <span class="math inline">\(i = 1, \dots, m\)</span> which have mass <span class="math inline">\(\alpha_1, \alpha_2, \dots, \alpha_m\)</span>, respectively, in the population, and that each of the groups has preferences satisfying IIA. Because of IIA, we can represent each sub-group’s stochastic preferences with an average utility <span class="math inline">\(u_i \colon Y \to \mathbb R\)</span>, <span class="math inline">\(i = 1, 2, \dots, n\)</span>. The distribution of the full population is then given by a mixture of the sub-population preferences. For example, for binary comparisons</p>
<p><span class="math display">\[
\mathbb{P} [y_1 \succ y_2 ] = \sum_{i=1}^n \alpha_i \mathbb{P} [y_1 \succ y_2  | \text{ group } i]   = \sum_{i = 1}^m \alpha_i \sigma (u_i (y_1) - u_i(y_2)).
\]</span></p>
<p>Sadly, such mixtures are far from IIA, as we see in the following coding example.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Code
</div>
</div>
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-5"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
<div id="df9138d6" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2"></a></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="co"># Define utilities for each group</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>group1_utilities <span class="op">=</span> {<span class="st">'A'</span>: <span class="fl">1.0</span>, <span class="st">'B'</span>: <span class="fl">2.0</span>, <span class="st">'C'</span>: <span class="fl">3.0</span>}</span>
<span id="cb5-5"><a href="#cb5-5"></a>group2_utilities <span class="op">=</span> {<span class="st">'A'</span>: <span class="fl">3.0</span>, <span class="st">'B'</span>: <span class="fl">2.0</span>, <span class="st">'C'</span>: <span class="fl">1.0</span>}</span>
<span id="cb5-6"><a href="#cb5-6"></a></span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="co"># Group weights</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>alpha1 <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb5-9"><a href="#cb5-9"></a>alpha2 <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb5-10"><a href="#cb5-10"></a></span>
<span id="cb5-11"><a href="#cb5-11"></a><span class="kw">def</span> softmax(utilities):</span>
<span id="cb5-12"><a href="#cb5-12"></a>    exp_vals <span class="op">=</span> np.exp(<span class="bu">list</span>(utilities.values()))</span>
<span id="cb5-13"><a href="#cb5-13"></a>    total <span class="op">=</span> np.<span class="bu">sum</span>(exp_vals)</span>
<span id="cb5-14"><a href="#cb5-14"></a>    <span class="cf">return</span> {k: np.exp(v) <span class="op">/</span> total <span class="cf">for</span> k, v <span class="kw">in</span> utilities.items()}</span>
<span id="cb5-15"><a href="#cb5-15"></a></span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="co"># Compute mixed logit probabilities over all 3 options</span></span>
<span id="cb5-17"><a href="#cb5-17"></a>p1_full <span class="op">=</span> softmax(group1_utilities)</span>
<span id="cb5-18"><a href="#cb5-18"></a>p2_full <span class="op">=</span> softmax(group2_utilities)</span>
<span id="cb5-19"><a href="#cb5-19"></a>p_mix_full <span class="op">=</span> {k: alpha1 <span class="op">*</span> p1_full[k] <span class="op">+</span> alpha2 <span class="op">*</span> p2_full[k] <span class="cf">for</span> k <span class="kw">in</span> group1_utilities}</span>
<span id="cb5-20"><a href="#cb5-20"></a></span>
<span id="cb5-21"><a href="#cb5-21"></a><span class="co"># Compute ratio A/B in full model</span></span>
<span id="cb5-22"><a href="#cb5-22"></a>ratio_full <span class="op">=</span> p_mix_full[<span class="st">'A'</span>] <span class="op">/</span> p_mix_full[<span class="st">'B'</span>]</span>
<span id="cb5-23"><a href="#cb5-23"></a></span>
<span id="cb5-24"><a href="#cb5-24"></a><span class="co"># Now remove option C</span></span>
<span id="cb5-25"><a href="#cb5-25"></a>reduced_utils1 <span class="op">=</span> {<span class="st">'A'</span>: group1_utilities[<span class="st">'A'</span>], <span class="st">'B'</span>: group1_utilities[<span class="st">'B'</span>]}</span>
<span id="cb5-26"><a href="#cb5-26"></a>reduced_utils2 <span class="op">=</span> {<span class="st">'A'</span>: group2_utilities[<span class="st">'A'</span>], <span class="st">'B'</span>: group2_utilities[<span class="st">'B'</span>]}</span>
<span id="cb5-27"><a href="#cb5-27"></a>p1_reduced <span class="op">=</span> softmax(reduced_utils1)</span>
<span id="cb5-28"><a href="#cb5-28"></a>p2_reduced <span class="op">=</span> softmax(reduced_utils2)</span>
<span id="cb5-29"><a href="#cb5-29"></a>p_mix_reduced <span class="op">=</span> {k: alpha1 <span class="op">*</span> p1_reduced[k] <span class="op">+</span> alpha2 <span class="op">*</span> p2_reduced[k] <span class="cf">for</span> k <span class="kw">in</span> reduced_utils1}</span>
<span id="cb5-30"><a href="#cb5-30"></a></span>
<span id="cb5-31"><a href="#cb5-31"></a><span class="co"># Compute ratio A/B in reduced model</span></span>
<span id="cb5-32"><a href="#cb5-32"></a>ratio_reduced <span class="op">=</span> p_mix_reduced[<span class="st">'A'</span>] <span class="op">/</span> p_mix_reduced[<span class="st">'B'</span>]</span>
<span id="cb5-33"><a href="#cb5-33"></a></span>
<span id="cb5-34"><a href="#cb5-34"></a><span class="co"># Show violation of IIA</span></span>
<span id="cb5-35"><a href="#cb5-35"></a><span class="bu">print</span>(<span class="ss">f"Ratio A/B with all options: </span><span class="sc">{</span>ratio_full<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb5-36"><a href="#cb5-36"></a><span class="bu">print</span>(<span class="ss">f"Ratio A/B after removing C: </span><span class="sc">{</span>ratio_reduced<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ratio A/B with all options: 1.5431
Ratio A/B after removing C: 1.0000</code></pre>
</div>
</div>
<p>An intuition for IIA’s failure comes from viewing it as random utility functions: A mixture of vectors that have independent entries is not Gaussian.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Code
</div>
</div>
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-6"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
<div id="9c8fc884" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb7-4"><a href="#cb7-4"></a></span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="co"># Define two Gaussian distributions</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>mu1, sigma1 <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>mu2, sigma2 <span class="op">=</span> <span class="dv">4</span>, <span class="dv">1</span></span>
<span id="cb7-8"><a href="#cb7-8"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">1000</span>)</span>
<span id="cb7-9"><a href="#cb7-9"></a></span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="co"># Evaluate the PDFs</span></span>
<span id="cb7-11"><a href="#cb7-11"></a>pdf1 <span class="op">=</span> norm.pdf(x, mu1, sigma1)</span>
<span id="cb7-12"><a href="#cb7-12"></a>pdf2 <span class="op">=</span> norm.pdf(x, mu2, sigma2)</span>
<span id="cb7-13"><a href="#cb7-13"></a></span>
<span id="cb7-14"><a href="#cb7-14"></a><span class="co"># Mixture: equal weight</span></span>
<span id="cb7-15"><a href="#cb7-15"></a>mixture_pdf <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> pdf1 <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> pdf2</span>
<span id="cb7-16"><a href="#cb7-16"></a></span>
<span id="cb7-17"><a href="#cb7-17"></a><span class="co"># Plot</span></span>
<span id="cb7-18"><a href="#cb7-18"></a>plt.plot(x, pdf1, label<span class="op">=</span><span class="st">'N(0, 1)'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb7-19"><a href="#cb7-19"></a>plt.plot(x, pdf2, label<span class="op">=</span><span class="st">'N(4, 1)'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb7-20"><a href="#cb7-20"></a>plt.plot(x, mixture_pdf, label<span class="op">=</span><span class="st">'Mixture 0.5*N(0,1) + 0.5*N(4,1)'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb7-21"><a href="#cb7-21"></a>plt.title(<span class="st">"Mixture of Two Gaussians is Not Gaussian"</span>)</span>
<span id="cb7-22"><a href="#cb7-22"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb7-23"><a href="#cb7-23"></a>plt.ylabel(<span class="st">"Density"</span>)</span>
<span id="cb7-24"><a href="#cb7-24"></a>plt.legend()</span>
<span id="cb7-25"><a href="#cb7-25"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb7-26"><a href="#cb7-26"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chap2_files/figure-html/cell-7-output-1.png" width="606" height="462" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Classic ways to solve this concern is to consider a model with explicit representation of heterogeneity, where <span class="math inline">\(y \prec y'\)</span> holds if <span class="math inline">\(\alpha \sim F\)</span> for some distribution <span class="math inline">\(F\)</span>, and <span class="math inline">\((u(y))_{y \in Y} \alpha\)</span> is a random utility model with independent error terms. For example, consider a logit random utility model <span class="math display">\[
u(y) = \beta^\top x + \varepsilon_y
\]</span> and assume <span class="math inline">\(\beta \sim N(\mu, \Sigma)\)</span> is a normally distributed vector, a model called the random coefficients logit model. Equivalently, we can view this as a model with correlated utility shocks.</p>
</section>
<section id="similar-options" class="level3" data-number="1.6.2">
<h3 data-number="1.6.2" class="anchored" data-anchor-id="similar-options"><span class="header-section-number">1.6.2</span> Similar Options</h3>
<p>A second limitation is only relevant if we move beyond binary choices, or observe preference lists. Let <span class="math inline">\(Y = \{y, y', z\}\)</span>, where <span class="math inline">\(y\)</span> and <span class="math inline">\(y'\)</span> are (almost) identical and different from <span class="math inline">\(z\)</span>. (In the classical example, <span class="math inline">\(y, y'\)</span> are red and blue buses, respectively, and <span class="math inline">\(z\)</span> is a train). Assume an IIA model given by average utility <span class="math inline">\(u \colon Y \to \mathbb{R}\)</span>. As <span class="math inline">\(y\)</span> and <span class="math inline">\(y'\)</span> are almost identical, assume <span class="math inline">\(u(y) = u(y')\)</span>. We have <span class="math inline">\(\mathbb{P}[z, \{y, z\}] = \mathbb{P}[z, \{y', z\}]\)</span>. How do these values compare to <span class="math inline">\(\mathbb{P}[z, \{y, y', z\}]\)</span>? It would be intuitive to think that <span class="math inline">\(z\)</span> is chosen with the same frequency, as there should not be more “demand” for object <span class="math inline">\(z\)</span> only because <span class="math inline">\(y\)</span> is cloned. This is not the case.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Code
</div>
</div>
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-7"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
<div id="f0d3d36a" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2"></a></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="co"># Deterministic utilities</span></span>
<span id="cb8-4"><a href="#cb8-4"></a>v_car <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb8-5"><a href="#cb8-5"></a>v_bus <span class="op">=</span> <span class="fl">2.0</span>  <span class="co"># Initially a single bus alternative</span></span>
<span id="cb8-6"><a href="#cb8-6"></a></span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="co"># Logit choice probabilities (before splitting bus)</span></span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="kw">def</span> softmax(utilities: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb8-9"><a href="#cb8-9"></a>    exp_util <span class="op">=</span> np.exp(utilities)</span>
<span id="cb8-10"><a href="#cb8-10"></a>    <span class="cf">return</span> exp_util <span class="op">/</span> np.<span class="bu">sum</span>(exp_util)</span>
<span id="cb8-11"><a href="#cb8-11"></a></span>
<span id="cb8-12"><a href="#cb8-12"></a><span class="co"># Before splitting: two alternatives</span></span>
<span id="cb8-13"><a href="#cb8-13"></a>utilities_before <span class="op">=</span> np.array([v_car, v_bus])</span>
<span id="cb8-14"><a href="#cb8-14"></a>probs_before <span class="op">=</span> softmax(utilities_before)</span>
<span id="cb8-15"><a href="#cb8-15"></a><span class="bu">print</span>(<span class="st">"Before splitting (Car, Bus):"</span>, probs_before)</span>
<span id="cb8-16"><a href="#cb8-16"></a></span>
<span id="cb8-17"><a href="#cb8-17"></a><span class="co"># After splitting: three alternatives</span></span>
<span id="cb8-18"><a href="#cb8-18"></a>v_red_bus <span class="op">=</span> v_bus</span>
<span id="cb8-19"><a href="#cb8-19"></a>v_blue_bus <span class="op">=</span> v_bus</span>
<span id="cb8-20"><a href="#cb8-20"></a>utilities_after <span class="op">=</span> np.array([v_car, v_red_bus, v_blue_bus])</span>
<span id="cb8-21"><a href="#cb8-21"></a>probs_after <span class="op">=</span> softmax(utilities_after)</span>
<span id="cb8-22"><a href="#cb8-22"></a><span class="bu">print</span>(<span class="st">"After splitting (Car, Red Bus, Blue Bus):"</span>, probs_after)</span>
<span id="cb8-23"><a href="#cb8-23"></a><span class="bu">print</span>(<span class="st">"After splitting, total bus share:"</span>, probs_after[<span class="dv">1</span>] <span class="op">+</span> probs_after[<span class="dv">2</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Before splitting (Car, Bus): [0.26894142 0.73105858]
After splitting (Car, Red Bus, Blue Bus): [0.1553624 0.4223188 0.4223188]
After splitting, total bus share: 0.8446375965030364</code></pre>
</div>
</div>
<p>The choice probability of <code>car</code> is reduced. Why is our intuition making us think that <span class="math inline">\(y\)</span> and <span class="math inline">\(y'\)</span> should split their choice probability? One option is because we assume some correlation: If you like <span class="math inline">\(y\)</span> over <span class="math inline">\(z\)</span> then you should also like <span class="math inline">\(y'\)</span> over <span class="math inline">\(z\)</span>, and vice versa. Hence, we would like the correlation between choice probabilities in random utility models. For example, if we allow in a logit random utility model the error terms in <span class="math inline">\(y, y'\)</span> to be perfectly correlated (and we break ties uniformly at random), then <span class="math display">\[
(\mathbb{P}[y, \{y, y', z\}], \mathbb{P}[y', \{y, y', z\}], \mathbb{P}[z, \{y, y', z\}]) = \left(\frac{\mathbb{P}[y, \{y, z\}]}{2}, \frac{\mathbb{P}[y', \{y', z\}]}{2}, \frac{\mathbb{P}[z, \{y, z\}]}{2}\right),
\]</span> confirming our intuition.</p>
<p>This is the end of our discussion of the Independence of Irrelevant Alternatives. Additional features can be found in <span class="citation" data-cites="train2009discrete ben1985discrete mcfadden1981econometric">(<a href="#ref-train2009discrete" role="doc-biblioref">Train 2009</a>; <a href="#ref-ben1985discrete" role="doc-biblioref">Ben-Akiva and Lerman 1985</a>; <a href="#ref-mcfadden1981econometric" role="doc-biblioref">McFadden 1981</a>)</span> and the original paper for logit analysis <span class="citation" data-cites="mcfadden1972conditional">McFadden (<a href="#ref-mcfadden1972conditional" role="doc-biblioref">1972</a>)</span>.</p>
<p>The next chapter is the first to study learning of average utility functions from preference data, and assumes that a dataset is given of (average) utility functions <span class="math inline">\(u \colon Y \to \mathbb R\)</span> for different types of sampling and for different notions of “inference”.</p>
<div id="tbl-notation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-notation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1.1: <strong>Table 1 — Notation used in Chapter “Background”.</strong>
</figcaption>
<div aria-describedby="tbl-notation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Notation</th>
<th>Meaning</th>
<th>Domain / Type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(Y\)</span></td>
<td>Finite set of objects/alternatives</td>
<td><span class="math inline">\(\{y_1,\dots ,y_n\}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(n\)</span></td>
<td>Number of objects</td>
<td><span class="math inline">\(\lvert Y\rvert\in\mathbb N\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(y,y',y''\)</span></td>
<td>Generic objects in <span class="math inline">\(Y\)</span></td>
<td>Elements of <span class="math inline">\(Y\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(y_0\)</span></td>
<td>Outside (“no-choice”) option</td>
<td>Element of <span class="math inline">\(Y\)</span> (reference)</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\prec\)</span> / <span class="math inline">\(\succ\)</span></td>
<td>Weak preference relation / its strict part</td>
<td>Binary relation on <span class="math inline">\(Y\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathord{\prec}\)</span></td>
<td>Random preference (draw of <span class="math inline">\(\prec\)</span>)</td>
<td>RV over total orders on <span class="math inline">\(Y\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(L=(y_1,\dots ,y_n)\)</span></td>
<td>Full ranking (preference list)</td>
<td>Permutation of <span class="math inline">\(Y\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\((y,Y')\)</span></td>
<td>Observation that <span class="math inline">\(y\)</span> is chosen from subset <span class="math inline">\(Y'\)</span></td>
<td><span class="math inline">\(y\in Y'\subseteq Y\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(x\in X\)</span></td>
<td>Exogenous context / features</td>
<td><span class="math inline">\(X\)</span> (arbitrary feature space)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(u(y)\)</span></td>
<td>Mean (deterministic) utility of <span class="math inline">\(y\)</span></td>
<td><span class="math inline">\(\mathbb R\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(u_{\mathord{\prec}}(y)\)</span></td>
<td>Random utility in draw <span class="math inline">\(\mathord{\prec}\)</span></td>
<td><span class="math inline">\(\mathbb R\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\varepsilon_{\mathord{\prec}y}\)</span></td>
<td>Stochastic utility shock</td>
<td><span class="math inline">\(\mathbb R\)</span> (i.i.d.)</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathbb P[\cdot]\)</span></td>
<td>Probability measure over preferences/choices</td>
<td><span class="math inline">\([0,1]\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\operatorname{softmax}_y\bigl((u(y'))_{y'\in Y'}\bigr)\)</span></td>
<td>Logit/Plackett-Luce choice probability of <span class="math inline">\(y\)</span> from <span class="math inline">\(Y'\)</span></td>
<td><span class="math inline">\([0,1]\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\sigma(z)\)</span></td>
<td>Sigmoid <span class="math inline">\(1/(1+e^{-z})\)</span></td>
<td><span class="math inline">\([0,1]\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(d\)</span></td>
<td>Dimensionality of feature vectors</td>
<td><span class="math inline">\(\mathbb N\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\boldsymbol{x}\in\mathbb R^{d}\)</span></td>
<td>Feature vector of an object</td>
<td><span class="math inline">\(\mathbb R^{d}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\text{Ackley}(\boldsymbol{x})\)</span></td>
<td>Ackley test-function value</td>
<td><span class="math inline">\(\mathbb R\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(a,b,c\)</span></td>
<td>Ackley parameters</td>
<td>Scalars</td>
</tr>
<tr class="even">
<td><span class="math inline">\(k\)</span></td>
<td>Number of preference samples</td>
<td><span class="math inline">\(\mathbb N\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha_i\)</span></td>
<td>Population weight of subgroup <span class="math inline">\(i\)</span></td>
<td><span class="math inline">\((0,1)\)</span> with <span class="math inline">\(\sum_i\alpha_i=1\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta\)</span></td>
<td>Random-coefficients vector in linear RUM</td>
<td><span class="math inline">\(\mathbb R^{d}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\Sigma\)</span></td>
<td>Covariance matrix of <span class="math inline">\(\beta\)</span></td>
<td><span class="math inline">\(\mathbb R^{d\times d}\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="discussion-questions" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="discussion-questions"><span class="header-section-number">1.7</span> Discussion Questions</h2>
<ul>
<li>How does modeling preferences as <strong>random</strong> (rather than deterministic) help us capture real-world choice behavior?<br>
</li>
<li>What is the <strong>Independence of Irrelevant Alternatives</strong> (IIA) axiom, and why does it simplify the estimation of choice models?<br>
</li>
<li>Why do i.i.d. Gumbel shocks in a random utility model lead to the Plackett–Luce (list) and softmax/logit (choice) formulas?<br>
</li>
<li>In what ways can <strong>binary comparisons</strong>, <strong>choice-from-a-set</strong>, and <strong>full rankings</strong> each be seen as observations of the same underlying stochastic preference distribution?<br>
</li>
<li>What are the practical advantages and drawbacks of eliciting <strong>full preference lists</strong> versus <strong>pairwise comparisons</strong> from human subjects?<br>
</li>
<li>How does introducing an “outside option” <span class="math inline">\(y_{0}\)</span> allow us to interpret <strong>accept–reject</strong> data within the same logit framework?<br>
</li>
<li>Why does mixing multiple IIA-satisfying sub-populations generally <strong>violate</strong> IIA at the aggregate level?<br>
</li>
<li>Explain the “<strong>red bus–blue bus</strong>” problem: why does splitting a single alternative into two identical ones distort logit choice probabilities?<br>
</li>
<li>How does context <span class="math inline">\(x\)</span> enter the random utility framework, and what role does it play in generalizing preferences to new situations?<br>
</li>
<li>What identification issues arise from the fact that adding a constant to all utilities <span class="math inline">\(u(y)\)</span> does not change observable choice probabilities?<br>
</li>
<li>In what scenarios would you consider <strong>relaxing</strong> IIA and what additional model complexity does that introduce?</li>
</ul>
</section>
<section id="exercises" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="exercises"><span class="header-section-number">1.8</span> Exercises</h2>
<p>We place ⭐, ⭐⭐, and ⭐⭐⭐ for exercises we deem relatively easy, medium, and hard, respectively.</p>
<section id="properties-of-iia-models" class="level3" data-number="1.8.1">
<h3 data-number="1.8.1" class="anchored" data-anchor-id="properties-of-iia-models"><span class="header-section-number">1.8.1</span> Properties of IIA Models ⭐</h3>
<p>Prove that if a preference model satisfies IIA, it will also satisfy <span class="math inline">\(\mathbb{P}[(y, Y')] \le \mathbb{P}[(y, Y'')]\)</span> for any <span class="math inline">\(y \in Y\)</span> and <span class="math inline">\(Y' \subseteq Y'' \subseteq Y\)</span> (called regularity) and for all <span class="math inline">\((x,y,z)\)</span>, if <span class="math inline">\(\mathbb{P}[(x, \{x,y\})] \ge 0.5\)</span> and <span class="math inline">\(\mathbb{P}[(y, \{y,z\})] \ge 0.5\)</span>, then necessarily <span class="math inline">\(\mathbb{P}[(x, \{x,z\})] \ge 0.5\)</span>.</p>
</section>
<section id="discrete-choice-models" class="level3" data-number="1.8.2">
<h3 data-number="1.8.2" class="anchored" data-anchor-id="discrete-choice-models"><span class="header-section-number">1.8.2</span> Discrete Choice Models ⭐⭐</h3>
<p>Consider a linear random utility model <span class="math inline">\(u(y)=\beta_i^\top x+\epsilon_i\)</span> for <span class="math inline">\(i=1, 2, \cdots, N\)</span>, where <span class="math inline">\(\varepsilon_y\)</span> is i.i.d. sampled from a Gumbel distribution. We would like to compute <span class="math inline">\(\mathbb{P}[(y, Y)]\)</span> and connect it to multi-class logistic regression.</p>
<ol type="a">
<li><p>First <span class="math inline">\(\mathbb{P}[u(y)&lt;t]\)</span> for any $ for <span class="math inline">\(j\neq i\)</span> in terms of <span class="math inline">\(F\)</span>. Use this probability to provide a formula for <span class="math inline">\(\mathbb{P}[(y, Y)]\)</span> over <span class="math inline">\(t\)</span> in terms of <span class="math inline">\(f\)</span> and <span class="math inline">\(F\)</span>.</p></li>
<li><p>Compute the integral derived in part (a) with the appropriate <span class="math inline">\(u\)</span>-substitution. You should arrive at the multi-class logistic regression model.</p></li>
</ol>
</section>
<section id="mixtures-and-correlations" class="level3" data-number="1.8.3">
<h3 data-number="1.8.3" class="anchored" data-anchor-id="mixtures-and-correlations"><span class="header-section-number">1.8.3</span> Mixtures and correlations ⭐</h3>
<p>Prove that the class of random preferences induced by the following two are identical: (a) mixtures of IIA random utility models (that is, those with i.i.d. noise) (b) random utility models with correlated noise.</p>
</section>
<section id="sufficient-statistics" class="level3" data-number="1.8.4">
<h3 data-number="1.8.4" class="anchored" data-anchor-id="sufficient-statistics"><span class="header-section-number">1.8.4</span> Sufficient Statistics ⭐⭐⭐</h3>
<ol type="a">
<li><p>Show that choice data completely specifies the preference model. That is, express <span class="math inline">\(\mathbb{P}[\mathord{\prec}]\)</span> for any <span class="math inline">\(\prec\)</span> in terms of <span class="math inline">\(\mathbb{P}[(y, Y')]\)</span>, <span class="math inline">\(y \in Y' \subseteq Y\)</span>.</p></li>
<li><p>Shows that this is not the case for binary comparisons. That is, give an example of two different preference models that induce the same probabilities <span class="math inline">\(\mathbb{P}[y, \{x, y\}]\)</span>.</p></li>
</ol>
</section>
<section id="non-random-utility-models" class="level3" data-number="1.8.5">
<h3 data-number="1.8.5" class="anchored" data-anchor-id="non-random-utility-models"><span class="header-section-number">1.8.5</span> Non-Random Utility Models ⭐⭐⭐</h3>
<p>Not all probability assignments for binary comparisons <span class="math inline">\(p_{y_1y_2} = \mathbb{P}[y_1 \prec y_1]\)</span> can be realized with a random preference model. Give an example of binary comparisons <span class="math inline">\((p_{y_1y_2}, p_{y_2y_3}, p_{y_3y_1})\)</span> that cannot be a result of a random preference model.</p>
</section>
<section id="posterior-inference-for-mixture-preferences" class="level3" data-number="1.8.6">
<h3 data-number="1.8.6" class="anchored" data-anchor-id="posterior-inference-for-mixture-preferences"><span class="header-section-number">1.8.6</span> Posterior Inference for Mixture Preferences ⭐⭐</h3>
<p>(This exercise previews some of the aspects for learning utility functions from the next chapter but is self-contained.) You are part of the ML team on the movie streaming site “Preferential”. You receive full preference orderings in the form <span class="math inline">\(y_1 \succ y_2 \succ \cdots \succ y_n\)</span>, where <span class="math inline">\(y_1\)</span> is the most, and <span class="math inline">\(y_n\)</span> the least preferred option. The preferences come from <span class="math inline">\(600\)</span> distinct users with <span class="math inline">\(50\)</span> examples per user. Each movie has a <span class="math inline">\(10\)</span>-dimensional feature vector <span class="math inline">\(m_y\)</span>, and each user has a <span class="math inline">\(10\)</span>-dimensional weight vector <span class="math inline">\(v_i\)</span>. The preferences for user <span class="math inline">\(i\)</span> follow the random utility model <span class="math inline">\(u(y) = v_i^\top m_y + \varepsilon_y\)</span>, where <span class="math inline">\(\varepsilon_y\)</span> is i.i.d. Gumbel distributed.</p>
<p>Sadly, you lost all user identifiers. Unashamedly, you assume a model where a proportion <span class="math inline">\(p\)</span> of the users have weights <span class="math inline">\(w_1\)</span>, and a proportion <span class="math inline">\(1-p\)</span> has weights <span class="math inline">\(w_2\)</span>. Each user belongs to one of two groups: users with weights <span class="math inline">\(w_1\)</span> are part of Group 1, and users with weights <span class="math inline">\(w_2\)</span> are part of Group 2.</p>
<ol type="a">
<li><p>For a datapoint <span class="math inline">\((y_1 \succ y_2)\)</span> with label and conditional on <span class="math inline">\(p\)</span>, <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span>, compute the likelihood <span class="math inline">\(P(y_1\succ y_2 | p, w_1, w_2)\)</span>.</p></li>
<li><p>Use the likelihood to simplify the posterior distribution of <span class="math inline">\(p, w_1, w_2\)</span> after updating on <span class="math inline">\((m_1, m_2)\)</span> leaving terms for the priors unchanged.</p></li>
<li><p>Assume priors <span class="math inline">\(p\sim B(1, 1)\)</span>, <span class="math inline">\(w_1\sim N (0, \mathbf{I})\)</span>, and <span class="math inline">\(w_2\sim N(0, \mathbf{I})\)</span> where <span class="math inline">\(B\)</span> represents the Beta distribution and <span class="math inline">\(\mathcal{N}\)</span> represents the normal distribution (all three sampled independently). You will notice that the posterior from part (b) has no simple closed form, requiring numerical methods. One such method, allowing to approximate sample from the posterior <span class="math inline">\(\pi\)</span>, is called Metropolis-Hastings. (The reason why one might want to sample from the posterior will be discussed in <span class="quarto-unresolved-ref">?sec-beyond</span>.) Broadly, the idea of Metropolis-Hastings and similar, so-called Markov Chain Monte Carlo methods is the following: Construct a Markov chain <span class="math inline">\(\{x_t\}_{t=1}^\infty\)</span> which has as “ergodic” distribution given by your desired distribution. By properties of Markov chains, for <span class="math inline">\(t \gg 0\)</span>, <span class="math inline">\(x_t\)</span> will be almost as good as sampled from the “ergodic” distribution. In Metropolis-Hastings, the distribution is a proposal <span class="math inline">\(P\)</span> for <span class="math inline">\(x_{t+1}\)</span> is made via sampling from a chosen probability kernel <span class="math inline">\(Q(\bullet | x_t)\)</span> (e.g., adding Gaussian noise). The acceptance probability of the proposal is given by</p></li>
</ol>
<p><span class="math display">\[
x_{t+1}=\begin{cases} \tilde Q(\bullet | x_t) &amp; \text{with probability } A, \\ x_t &amp; \text{with probability } 1 - A. \end{cases}
\]</span> where <span class="math display">\[
A= \min \left( 1, \frac{\pi(\bullet )Q(x_t | \bullet )}{\pi(x_t)Q( \bullet | x_t)} \right).
\]</span> We will extract samples from the Markov chain after a “burn-in period”, <span class="math inline">\((x_{T+1}, x_{T+2},\cdots, x_{N})\)</span>.</p>
<p>To build some intuition, suppose we have a biased coin that turns heads with probability <span class="math inline">\(p_{\text{heads}}\)</span>. We observe <span class="math inline">\(12\)</span> coin flips to have <span class="math inline">\(9\)</span> heads (H) and <span class="math inline">\(3\)</span> tails (T). If our prior for <span class="math inline">\(p_{\text{H}}\)</span> was <span class="math inline">\(B(1, 1)\)</span>, then, by properties of the Beta distribution, our posterior will be <span class="math inline">\(B(1 + 9, 1 + 3)=B(10, 4)\)</span>. The Bayesian update is given by</p>
<p><span class="math display">\[
p(p_{\text{H}}|9\text{H}, 3\text{T}) = \frac{p(9\text{H}, 3\text{T} | p_{\text{H}})B(1, 1)(p_{\text{H}})}{\int_0^1 P(9\text{H}, 3\text{T} | p_{\text{H}})B(1, 1)(p_{\text{H}}) \, \mathrm dp_{\text{H}}} =\frac{p(9\text{H}, 3\text{T} | p_{\text{H}})}{\int_0^1 p(9\text{H}, 3\text{T} | p_{\text{H}}) \, \mathrm dp_{\text{H}}}.
\]</span></p>
<p><strong>Find the acceptance probability</strong> <span class="math inline">\(A\)</span> in the setting of the biased coin assuming the proposal distribution <span class="math inline">\(Q(\cdot|x_t)=x_t+N(0,\sigma)\)</span> for given <span class="math inline">\(\sigma\)</span>. Notice that this choice of <span class="math inline">\(Q\)</span> is symmetric, i.e., <span class="math inline">\(Q(x_t|p)=Q(p|x_t)\)</span> for all <span class="math inline">\(p \in \mathbb R\)</span>. Note that it is unnecessary to compute the normalizing constant of the Bayesian update (i.e., the integral in the denominator). This simplification is one of the main practical advantages of Metropolis-Hastings.</p>
<ol start="4" type="a">
<li>Implement Metropolis-Hastings to sample from the posterior distribution of the biased coin in <code>multimodal_preferences/biased_coin.py</code>. Attach a histogram of your MCMC samples overlayed on top of the true posterior <span class="math inline">\(B(10, 4)\)</span> by running <code>python biased_coin.py</code>.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Code
</div>
</div>
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-8"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
<ol start="5" type="a">
<li>Implement Metropolis-Hastings in the movie setting inside&nbsp;<code>multimodal_preferences/movie_metropolis.py</code>. You should be able to achieve a <span class="math inline">\(90\%\)</span> success rate with most <code>fraction_accepted</code> values above <span class="math inline">\(0.1\)</span>. Success is measured by thresholded closeness of predicted parameters to true parameters. You may notice occasional failures that occur due to lack of convergence which we will account for in grading.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Code
</div>
</div>
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-9"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>


<!-- -->

</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ben1985discrete" class="csl-entry" role="listitem">
Ben-Akiva, Moshe E, and Steven R Lerman. 1985. <em>Discrete Choice Analysis: Theory and Application to Travel Demand</em>. Vol. 9. MIT press.
</div>
<div id="ref-bradley1952rank" class="csl-entry" role="listitem">
Bradley, Ralph Allan, and Milton E Terry. 1952. <span>“Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons.”</span> <em>Biometrika</em> 39 (3/4): 324–45.
</div>
<div id="ref-ganguli2022redteaminglanguagemodels" class="csl-entry" role="listitem">
Ganguli, Deep, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, et al. 2022. <span>“Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned.”</span> <a href="https://arxiv.org/abs/2209.07858">https://arxiv.org/abs/2209.07858</a>.
</div>
<div id="ref-luce1959individual" class="csl-entry" role="listitem">
Luce, R Duncan et al. 1959. <em>Individual Choice Behavior</em>. Vol. 4. Wiley New York.
</div>
<div id="ref-mas1995microeconomic" class="csl-entry" role="listitem">
Mas-Colell, Andreu, Michael Dennis Whinston, Jerry R Green, et al. 1995. <em>Microeconomic Theory</em>. Vol. 1. Oxford university press New York.
</div>
<div id="ref-mcfadden1972conditional" class="csl-entry" role="listitem">
McFadden, Daniel. 1972. <span>“Conditional Logit Analysis of Qualitative Choice Behavior.”</span>
</div>
<div id="ref-mcfadden1981econometric" class="csl-entry" role="listitem">
———. 1981. <span>“Econometric Models of Probabilistic Choice.”</span> <em>Structural Analysis of Discrete Data with Econometric Applications</em> 198272.
</div>
<div id="ref-plackett1975analysis" class="csl-entry" role="listitem">
Plackett, Robin L. 1975. <span>“The Analysis of Permutations.”</span> <em>Journal of the Royal Statistical Society Series C: Applied Statistics</em> 24 (2): 193–202.
</div>
<div id="ref-train2009discrete" class="csl-entry" role="listitem">
Train, Kenneth E. 2009. <em>Discrete Choice Methods with Simulation</em>. Cambridge university press.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../index.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Introduction</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../src/chap3.html" class="pagination-link" aria-label="Learning">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb10" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb10-1"><a href="#cb10-1"></a><span class="co">---</span></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="an">title:</span><span class="co"> Background</span></span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="an">format:</span><span class="co"> html</span></span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="an">filters:</span></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="co">  - pyodide</span></span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="an">execute:</span></span>
<span id="cb10-7"><a href="#cb10-7"></a><span class="co">  engine: pyodide</span></span>
<span id="cb10-8"><a href="#cb10-8"></a><span class="co">  pyodide:</span></span>
<span id="cb10-9"><a href="#cb10-9"></a><span class="co">    auto: true</span></span>
<span id="cb10-10"><a href="#cb10-10"></a></span>
<span id="cb10-11"><a href="#cb10-11"></a><span class="co">---</span></span>
<span id="cb10-12"><a href="#cb10-12"></a></span>
<span id="cb10-13"><a href="#cb10-13"></a>::: {.content-visible when-format="html"}</span>
<span id="cb10-14"><a href="#cb10-14"></a></span>
<span id="cb10-15"><a href="#cb10-15"></a>&lt;iframe</span>
<span id="cb10-16"><a href="#cb10-16"></a>  src="https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/"</span>
<span id="cb10-17"><a href="#cb10-17"></a>  style="width:45%; height:225px;"</span>
<span id="cb10-18"><a href="#cb10-18"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb10-19"><a href="#cb10-19"></a>&lt;iframe</span>
<span id="cb10-20"><a href="#cb10-20"></a>  src="https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/"</span>
<span id="cb10-21"><a href="#cb10-21"></a>  style="width:45%; height:225px;"</span>
<span id="cb10-22"><a href="#cb10-22"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb10-23"><a href="#cb10-23"></a><span class="co">[</span><span class="ot">Fullscreen Part 1</span><span class="co">](https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/)</span>{.btn .btn-outline-primary .btn role="button"}</span>
<span id="cb10-24"><a href="#cb10-24"></a><span class="co">[</span><span class="ot">Fullscreen Part 2</span><span class="co">](https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/)</span>{.btn .btn-outline-primary .btn role="button"}</span>
<span id="cb10-25"><a href="#cb10-25"></a></span>
<span id="cb10-26"><a href="#cb10-26"></a>:::</span>
<span id="cb10-27"><a href="#cb10-27"></a></span>
<span id="cb10-28"><a href="#cb10-28"></a></span>
<span id="cb10-29"><a href="#cb10-29"></a>::: {.callout-note}</span>
<span id="cb10-30"><a href="#cb10-30"></a><span class="fu">## Intended Learning outcomes</span></span>
<span id="cb10-31"><a href="#cb10-31"></a></span>
<span id="cb10-32"><a href="#cb10-32"></a>By the end of this chapter you will be able to:</span>
<span id="cb10-33"><a href="#cb10-33"></a></span>
<span id="cb10-34"><a href="#cb10-34"></a><span class="ss">- </span>**Differentiate** deterministic preferences from *stochastic (random)* preferences and justify why randomness is essential for modelling noisy human choice.</span>
<span id="cb10-35"><a href="#cb10-35"></a><span class="ss">- </span>**Define** and **apply** the *Independence of Irrelevant Alternatives (IIA)* axiom, explaining how it collapses the full preference distribution to an $n$-parameter logit model.</span>
<span id="cb10-36"><a href="#cb10-36"></a><span class="ss">- </span>**Derive** choice probabilities for binary comparisons (Bradley–Terry), accept–reject decisions (logistic regression), and full or partial rankings (Plackett–Luce) from a random-utility model with i.i.d. Gumbel shocks.</span>
<span id="cb10-37"><a href="#cb10-37"></a><span class="ss">- </span>**Identify** and **compare** the main types of comparison data (full lists, choice-from-a-set, pairwise) and map each to the underlying random preference distribution.</span>
<span id="cb10-38"><a href="#cb10-38"></a><span class="ss">- </span>**Simulate** preference data using the Ackley test function, and **visualize** how utility landscapes translate into observed choices.</span>
<span id="cb10-39"><a href="#cb10-39"></a><span class="ss">- </span>**Diagnose** two key limitations of IIA—population heterogeneity (mixture models) and the *red-bus/blue-bus* cloning problem—and **motivate** richer models with correlated utility shocks.</span>
<span id="cb10-40"><a href="#cb10-40"></a><span class="ss">- </span>**Incorporate context features** $x$ into random-utility formulations to generalize learned preferences across environments, prompts, or tasks.</span>
<span id="cb10-41"><a href="#cb10-41"></a><span class="ss">- </span>**Explain** the identification issue in logit models and **justify** the convention of fixing one utility level (e.g., the outside option $y_{0}$).</span>
<span id="cb10-42"><a href="#cb10-42"></a></span>
<span id="cb10-43"><a href="#cb10-43"></a>:::</span>
<span id="cb10-44"><a href="#cb10-44"></a></span>
<span id="cb10-45"><a href="#cb10-45"></a></span>
<span id="cb10-46"><a href="#cb10-46"></a></span>
<span id="cb10-47"><a href="#cb10-47"></a>This is a book about machine learning from human preferences. This first chapter is about generative models for human actions, in particular for *comparisons*. In classical supervised learning, comparisons implicitly arise for a trained model: If the logit of a particular output from a supervised learning model is higher for the label $y$ than $y'$ we would say that the model is *more likely* to produce $y$ than $y'$. While this introduces some amount of comparison on model outputs, it does not help us if the data is given by $y$ being preferred to $y'$, written $y \succ y'$.</span>
<span id="cb10-48"><a href="#cb10-48"></a></span>
<span id="cb10-49"><a href="#cb10-49"></a>First, hence, we introduce stochastic preferences as a model of preferences. We then discuss the most important assumption made in stochastic choice, the Independence of Irrelevant Alternatives (IIA), and discuss its advantages and pitfalls. Chapters 1-5 will restrict to comparisons, including binary comparisons, accept-reject decisions, and ranking lists. Other related data types, such as Likert scales, will be considered in @chapter-beyond.</span>
<span id="cb10-50"><a href="#cb10-50"></a></span>
<span id="cb10-51"><a href="#cb10-51"></a><span class="fu">## Random Preferences as a Model of Comparisons {#sec-foundations}</span></span>
<span id="cb10-52"><a href="#cb10-52"></a>We start with a set of **objects** $y \in Y$---be they products, robot trajectories, or language model responses. We will consider models to generate comparisons that are orders. For realism, but also for mathematical simplicity, we will assume in this book that the set $Y$ of objects is discrete and has $n$ objects. </span>
<span id="cb10-53"><a href="#cb10-53"></a></span>
<span id="cb10-54"><a href="#cb10-54"></a>Comparisons may be random and are generated by random draws of (total) orders. (Total) Orders have two properties. </span>
<span id="cb10-55"><a href="#cb10-55"></a></span>
<span id="cb10-56"><a href="#cb10-56"></a><span class="ss"> - </span>First, for two objects $y, y'$ either $y \prec y'$ and/or $y \succ y'$ must hold, an assumption called *totality*:\footnote{One often also allows for preferences to capture a notion of equivalence, called indifference, which is unlikely to happen in random choice models we will learn from data. We use the benefit of simplified notation and restrict to no indifference.} Either $y$ is weakly preferred to $y'$ or $y'$ is weakly preferred to $y$. </span>
<span id="cb10-57"><a href="#cb10-57"></a><span class="ss"> - </span>The second assumption is transitivity: if $y \succ y'$ and $y' \succ y''$, then also $y \succ y''$. </span>
<span id="cb10-58"><a href="#cb10-58"></a> </span>
<span id="cb10-59"><a href="#cb10-59"></a>In the following, we consider randomness as generated from a *decision-maker* who has an order, or preference relation, $\prec$ on a set of objects $Y$. We refer to the random object $\prec$ as the oracle preference. Each preference $\prec$ has an associated probability mass $\mathbb{P}<span class="co">[</span><span class="ot">\mathord{\prec}</span><span class="co">]</span>$, leading to an $(n!-1)$-dimensional vector encoding the full random preference set. (This might look like, and is already for small values of $n$ a large number. Reducing this representational complexity is a goal of this chapter.)</span>
<span id="cb10-60"><a href="#cb10-60"></a></span>
<span id="cb10-61"><a href="#cb10-61"></a>One might wonder why we need to have a random preference. Deterministic preferences are conceptually helpful constructs and are used broadly in the fields of Consumer theory (e.g., @mas1995microeconomic). However, they suffer when bringing them to data, as data is inherently noisy. </span>
<span id="cb10-62"><a href="#cb10-62"></a></span>
<span id="cb10-63"><a href="#cb10-63"></a>Noise plays three roles, which we will address in the chapters of this book. </span>
<span id="cb10-64"><a href="#cb10-64"></a></span>
<span id="cb10-65"><a href="#cb10-65"></a><span class="ss"> - </span>It first captures measurement noise. When observing choices from a person, this may be an imperfect preference relationship. We will consider such noise (homogeneity but uncertainty), for example, in @sec-learning when discussing Gaussian processes.</span>
<span id="cb10-66"><a href="#cb10-66"></a><span class="ss"> - </span></span>
<span id="cb10-67"><a href="#cb10-67"></a><span class="ss"> - </span>A third role of noise arises from heterogeneity, where $\prec$ encodes the *population* preferences of several decision-makers. We consider an example like this in one of the exercises in this chapter.</span>
<span id="cb10-68"><a href="#cb10-68"></a></span>
<span id="cb10-69"><a href="#cb10-69"></a>Even when allowing for randomness, assumptions we will impose in this section, be it transitivity or the Independence of Irrelevant Alternatives, are stark yet practical. In many situations they will fail, for good reasons. Whether these are human's inability to express rankings, contextual challenges of domains, or community norms, we will discuss them in, humans cannot clearly rank alternatives, their choices reflect individualistic norms, or they might have self-control pictures. Many of these wrinkles on the approach to preferences presented here is contained in @sec-beyond. Until then, we will make the fullest use of learning stochastic preferences.</span>
<span id="cb10-70"><a href="#cb10-70"></a></span>
<span id="cb10-71"><a href="#cb10-71"></a><span class="fu">## Types of Comparison Data</span></span>
<span id="cb10-72"><a href="#cb10-72"></a>There are different types of comparison data we may observe. We can relate them back to the population preferences $\prec$. </span>
<span id="cb10-73"><a href="#cb10-73"></a></span>
<span id="cb10-74"><a href="#cb10-74"></a><span class="fu">### Full Preference Lists</span></span>
<span id="cb10-75"><a href="#cb10-75"></a></span>
<span id="cb10-76"><a href="#cb10-76"></a>The conceptually simplest and practically most verbose preference sampling is to get the full preference ranking, i.e. $L = (y_1, y_2, \dots, y_n)$, where $y_1 \succ y_2 \succ \cdots \succ y_n$. In this case, we know not only that $y_1$ is preferred to $y_2$, but also, by transitivity, that it is preferred to all other options. Similarly, we know that $y_2$ is preferred to all options but $y_1$, *etc.* In many cases, we do not observe full preferences as the cognitive load for humans is too high.</span>
<span id="cb10-77"><a href="#cb10-77"></a></span>
<span id="cb10-78"><a href="#cb10-78"></a><span class="fu">### The Most-Preferred Element from a Subset: (Binary) Choices</span></span>
<span id="cb10-79"><a href="#cb10-79"></a>Another type of sample is $(y, Y')$ where $y$ is the most preferred alternative from $Y'$ for a sampled preference. Formally, $y \prec y'$ for all $y' \in Y' \setminus <span class="sc">\{</span>y<span class="sc">\}</span>$---$y$ is preferred to all elements of $Y$ but $y'$.  </span>
<span id="cb10-80"><a href="#cb10-80"></a></span>
<span id="cb10-81"><a href="#cb10-81"></a>Formally, the probability that we observe $(y, Y')$ is</span>
<span id="cb10-82"><a href="#cb10-82"></a>$$</span>
<span id="cb10-83"><a href="#cb10-83"></a>\mathbb{P}<span class="co">[</span><span class="ot">(y, Y')</span><span class="co">]</span> = \sum_{\prec: y \prec y' \forall y' \in Y' \setminus <span class="sc">\{</span>y<span class="sc">\}</span>} \mathbb{P} <span class="co">[</span><span class="ot">\mathord{\prec}</span><span class="co">]</span>.</span>
<span id="cb10-84"><a href="#cb10-84"></a>$$</span>
<span id="cb10-85"><a href="#cb10-85"></a>That is, the probability of observing $(y, Y')$ is given by the sum of all preferred samples $\prec$ such that $y$ is preferred to all $y'$ in $Y'$ other than $y$.</span>
<span id="cb10-86"><a href="#cb10-86"></a></span>
<span id="cb10-87"><a href="#cb10-87"></a>If the choice is binary, $Y' = <span class="sc">\{</span>y, y'<span class="sc">\}</span>$, we also write $(y \succ y')$ for a sample $(x, <span class="sc">\{</span>x,y<span class="sc">\}</span>)$. We highlight that these objects are random, and depend on the sample of $\prec$. Binary data is convenient and quick to elicit and has been prominently applied in language model finetuning and evaluation.</span>
<span id="cb10-88"><a href="#cb10-88"></a></span>
<span id="cb10-89"><a href="#cb10-89"></a>Sometimes, particularly when a decision-maker is offered an object or "nothing", we will implicitly assume that there is an "outside option" $y_0$ in $Y$, allowing us to interpret $(y, <span class="sc">\{</span>y, y_0<span class="sc">\}</span>)$ as "accepting" $y$, and $(y_0, <span class="sc">\{</span>y, y_0<span class="sc">\}</span>)$ as rejecting it. Outside options can be thought of as fundamental limits to what a system designer can obtain. Consider a recommendation system. A user of that system might engage with content or not. In principle, instead of engaging, they will do something else. We do not model this in out set of objects $Y$ as a fundamental abstraction. *All models are wrong, but some are useful.*</span>
<span id="cb10-90"><a href="#cb10-90"></a></span>
<span id="cb10-91"><a href="#cb10-91"></a><span class="fu">### Mind the Context</span></span>
<span id="cb10-92"><a href="#cb10-92"></a></span>
<span id="cb10-93"><a href="#cb10-93"></a>Choices are often conditional, and data is given by $(x, L)$ (for list-based data), $(x, y, Y')$ (for general choice-based data), or $(x, y, y')$ for binary data. $x \in X$ is some *context*: the environment of a purchase, the goal of a robot, or a user prompt for a large language model. It can also be a prompt to the decision-maker, e.g., to human raters on whether they should pick preferences based on helplessness or harmlessness @ganguli2022redteaminglanguagemodels. The inclusion of context in learning allows for the generalization of preferences, as we will see in subsequent chapters.</span>
<span id="cb10-94"><a href="#cb10-94"></a></span>
<span id="cb10-95"><a href="#cb10-95"></a><span class="fu">## Random Utility Models</span></span>
<span id="cb10-96"><a href="#cb10-96"></a>An equivalent way to represent random preferences is to identify a sample $\prec$ with a vector $u_{\prec} = (u_{\mathord{\prec}} (y))_{y \in Y} \in \mathbb R^Y$ where $y \succ y'$ if and only if $u(y) &gt; u(y')$. (For the concerned reader: We assume that $u(y) = u(y')$ happens with zero probability; and for discrete $Y$ such a vector always exists.)</span>
<span id="cb10-97"><a href="#cb10-97"></a></span>
<span id="cb10-98"><a href="#cb10-98"></a>To get a sense for different random utility models, we consider a particular model that has the complexity of many models in modern machine learning: The Ackley function. In this model, each alternative is represented by a $d$-dimensional vector $(x_1, \ldots, x_d) \in \mathbb{R}^d$, the Ackley function is given by</span>
<span id="cb10-99"><a href="#cb10-99"></a>$$</span>
<span id="cb10-100"><a href="#cb10-100"></a>\text{Ackley}(x_1, x_2, \dots, x_d) = -a e^{-b \sqrt{\frac{1}{d} \sum_{j=1}^d x_j^2}} - e^{\frac{1}{d} \sum_{j=1}^d \cos(c x_j)} + a + e.</span>
<span id="cb10-101"><a href="#cb10-101"></a>$$</span>
<span id="cb10-102"><a href="#cb10-102"></a>for some constants $a, b, c \in \mathbb{R}$. By stacking a number $k$ of human preferences, we can compute for $k$ samples from a random model the function in a vectorized way.</span>
<span id="cb10-103"><a href="#cb10-103"></a>    </span>
<span id="cb10-104"><a href="#cb10-104"></a>::: {.callout-note title="Code"}</span>
<span id="cb10-105"><a href="#cb10-105"></a></span>
<span id="cb10-106"><a href="#cb10-106"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb10-107"><a href="#cb10-107"></a><span class="in">import numpy as np</span></span>
<span id="cb10-108"><a href="#cb10-108"></a><span class="in">np.random.seed(0)</span></span>
<span id="cb10-109"><a href="#cb10-109"></a></span>
<span id="cb10-110"><a href="#cb10-110"></a><span class="in">def ackley(X, a=20, b=0.2, c=2*np.pi):</span></span>
<span id="cb10-111"><a href="#cb10-111"></a><span class="in">    """</span></span>
<span id="cb10-112"><a href="#cb10-112"></a><span class="in">    Compute the Ackley function.</span></span>
<span id="cb10-113"><a href="#cb10-113"></a><span class="in">    Parameters:</span></span>
<span id="cb10-114"><a href="#cb10-114"></a><span class="in">      X: A NumPy array of shape (n, d) where each row is a n-dimensional point.</span></span>
<span id="cb10-115"><a href="#cb10-115"></a><span class="in">      a, b, c: Parameters of the Ackley function.</span></span>
<span id="cb10-116"><a href="#cb10-116"></a><span class="in">    Returns:</span></span>
<span id="cb10-117"><a href="#cb10-117"></a><span class="in">      A NumPy array of shape (n,) of function values</span></span>
<span id="cb10-118"><a href="#cb10-118"></a><span class="in">    """</span></span>
<span id="cb10-119"><a href="#cb10-119"></a><span class="in">    X = np.atleast_2d(X)</span></span>
<span id="cb10-120"><a href="#cb10-120"></a><span class="in">    d = X.shape[1]</span></span>
<span id="cb10-121"><a href="#cb10-121"></a><span class="in">    sum_sq = np.sum(X ** 2, axis=1)</span></span>
<span id="cb10-122"><a href="#cb10-122"></a><span class="in">    term1 = -a * np.exp(-b * np.sqrt(sum_sq / d))</span></span>
<span id="cb10-123"><a href="#cb10-123"></a><span class="in">    term2 = -np.exp(np.sum(np.cos(c * X), axis=1) / d)</span></span>
<span id="cb10-124"><a href="#cb10-124"></a><span class="in">    return term1 + term2 + a + np.e</span></span>
<span id="cb10-125"><a href="#cb10-125"></a><span class="in">```</span></span>
<span id="cb10-126"><a href="#cb10-126"></a></span>
<span id="cb10-127"><a href="#cb10-127"></a>:::</span>
<span id="cb10-128"><a href="#cb10-128"></a></span>
<span id="cb10-131"><a href="#cb10-131"></a><span class="in">```{python}</span></span>
<span id="cb10-132"><a href="#cb10-132"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-133"><a href="#cb10-133"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb10-134"><a href="#cb10-134"></a></span>
<span id="cb10-135"><a href="#cb10-135"></a><span class="kw">def</span> ackley(X, a<span class="op">=</span><span class="dv">20</span>, b<span class="op">=</span><span class="fl">0.2</span>, c<span class="op">=</span><span class="dv">2</span><span class="op">*</span>np.pi):</span>
<span id="cb10-136"><a href="#cb10-136"></a>    <span class="co">"""</span></span>
<span id="cb10-137"><a href="#cb10-137"></a><span class="co">    Compute the Ackley function.</span></span>
<span id="cb10-138"><a href="#cb10-138"></a><span class="co">    Parameters:</span></span>
<span id="cb10-139"><a href="#cb10-139"></a><span class="co">      X: A NumPy array of shape (n, d) where each row is a n-dimensional point.</span></span>
<span id="cb10-140"><a href="#cb10-140"></a><span class="co">      a, b, c: Parameters of the Ackley function.</span></span>
<span id="cb10-141"><a href="#cb10-141"></a><span class="co">    Returns:</span></span>
<span id="cb10-142"><a href="#cb10-142"></a><span class="co">      A NumPy array of shape (n,) of function values</span></span>
<span id="cb10-143"><a href="#cb10-143"></a><span class="co">    """</span></span>
<span id="cb10-144"><a href="#cb10-144"></a>    X <span class="op">=</span> np.atleast_2d(X)</span>
<span id="cb10-145"><a href="#cb10-145"></a>    d <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb10-146"><a href="#cb10-146"></a>    sum_sq <span class="op">=</span> np.<span class="bu">sum</span>(X <span class="op">**</span> <span class="dv">2</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-147"><a href="#cb10-147"></a>    term1 <span class="op">=</span> <span class="op">-</span>a <span class="op">*</span> np.exp(<span class="op">-</span>b <span class="op">*</span> np.sqrt(sum_sq <span class="op">/</span> d))</span>
<span id="cb10-148"><a href="#cb10-148"></a>    term2 <span class="op">=</span> <span class="op">-</span>np.exp(np.<span class="bu">sum</span>(np.cos(c <span class="op">*</span> X), axis<span class="op">=</span><span class="dv">1</span>) <span class="op">/</span> d)</span>
<span id="cb10-149"><a href="#cb10-149"></a>    <span class="cf">return</span> term1 <span class="op">+</span> term2 <span class="op">+</span> a <span class="op">+</span> np.e</span>
<span id="cb10-150"><a href="#cb10-150"></a><span class="in">```</span></span>
<span id="cb10-151"><a href="#cb10-151"></a></span>
<span id="cb10-152"><a href="#cb10-152"></a>We can think of the rows of $X$ as features of different alternatives. We can visualize the full landscape of the utility function for two alternatives ($n=2$) and features of a single dimension ($d=1$).</span>
<span id="cb10-153"><a href="#cb10-153"></a></span>
<span id="cb10-154"><a href="#cb10-154"></a>::: {.callout-note title="Code"}</span>
<span id="cb10-155"><a href="#cb10-155"></a></span>
<span id="cb10-156"><a href="#cb10-156"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb10-157"><a href="#cb10-157"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb10-158"><a href="#cb10-158"></a><span class="in">from matplotlib.colors import LinearSegmentedColormap</span></span>
<span id="cb10-159"><a href="#cb10-159"></a><span class="in">ccmap = LinearSegmentedColormap.from_list("ackley", ["#f76a05", "#FFF2C9"])</span></span>
<span id="cb10-160"><a href="#cb10-160"></a><span class="in">plt.rcParams.update({</span></span>
<span id="cb10-161"><a href="#cb10-161"></a><span class="in">    "font.size": 14,</span></span>
<span id="cb10-162"><a href="#cb10-162"></a><span class="in">    "axes.labelsize": 16,</span></span>
<span id="cb10-163"><a href="#cb10-163"></a><span class="in">    "xtick.labelsize": 14,</span></span>
<span id="cb10-164"><a href="#cb10-164"></a><span class="in">    "ytick.labelsize": 14,</span></span>
<span id="cb10-165"><a href="#cb10-165"></a><span class="in">    "legend.fontsize": 14,</span></span>
<span id="cb10-166"><a href="#cb10-166"></a><span class="in">    "axes.titlesize": 16,</span></span>
<span id="cb10-167"><a href="#cb10-167"></a><span class="in">})</span></span>
<span id="cb10-168"><a href="#cb10-168"></a></span>
<span id="cb10-169"><a href="#cb10-169"></a><span class="in">def draw_surface():</span></span>
<span id="cb10-170"><a href="#cb10-170"></a><span class="in">    inps = np.linspace(-2, 2, 100)</span></span>
<span id="cb10-171"><a href="#cb10-171"></a><span class="in">    X, Y = np.meshgrid(inps, inps)</span></span>
<span id="cb10-172"><a href="#cb10-172"></a><span class="in">    grid = np.column_stack([X.ravel(), Y.ravel()])</span></span>
<span id="cb10-173"><a href="#cb10-173"></a><span class="in">    Z = ackley(grid).reshape(X.shape)</span></span>
<span id="cb10-174"><a href="#cb10-174"></a><span class="in">    </span></span>
<span id="cb10-175"><a href="#cb10-175"></a><span class="in">    plt.figure(figsize=(6, 5))</span></span>
<span id="cb10-176"><a href="#cb10-176"></a><span class="in">    contour = plt.contourf(X, Y, Z, 50, cmap=ccmap)</span></span>
<span id="cb10-177"><a href="#cb10-177"></a><span class="in">    plt.contour(X, Y, Z, levels=15, colors='black', linewidths=0.5, alpha=0.6)</span></span>
<span id="cb10-178"><a href="#cb10-178"></a><span class="in">    plt.colorbar(contour, label=r'$f(x)$', ticks=[0, 3, 6])</span></span>
<span id="cb10-179"><a href="#cb10-179"></a><span class="in">    plt.xlim(-2, 2)</span></span>
<span id="cb10-180"><a href="#cb10-180"></a><span class="in">    plt.ylim(-2, 2)</span></span>
<span id="cb10-181"><a href="#cb10-181"></a><span class="in">    plt.xticks([-2, 0, 2])</span></span>
<span id="cb10-182"><a href="#cb10-182"></a><span class="in">    plt.yticks([-2, 0, 2])</span></span>
<span id="cb10-183"><a href="#cb10-183"></a><span class="in">    plt.xlabel(r'$x_1$')</span></span>
<span id="cb10-184"><a href="#cb10-184"></a><span class="in">    plt.ylabel(r'$x_2$')</span></span>
<span id="cb10-185"><a href="#cb10-185"></a><span class="in">```</span></span>
<span id="cb10-186"><a href="#cb10-186"></a></span>
<span id="cb10-187"><a href="#cb10-187"></a>:::</span>
<span id="cb10-188"><a href="#cb10-188"></a></span>
<span id="cb10-191"><a href="#cb10-191"></a><span class="in">```{python}</span></span>
<span id="cb10-192"><a href="#cb10-192"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-193"><a href="#cb10-193"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> LinearSegmentedColormap</span>
<span id="cb10-194"><a href="#cb10-194"></a>ccmap <span class="op">=</span> LinearSegmentedColormap.from_list(<span class="st">"ackley"</span>, [<span class="st">"#f76a05"</span>, <span class="st">"#FFF2C9"</span>])</span>
<span id="cb10-195"><a href="#cb10-195"></a>plt.rcParams.update({</span>
<span id="cb10-196"><a href="#cb10-196"></a>    <span class="st">"font.size"</span>: <span class="dv">14</span>,</span>
<span id="cb10-197"><a href="#cb10-197"></a>    <span class="st">"axes.labelsize"</span>: <span class="dv">16</span>,</span>
<span id="cb10-198"><a href="#cb10-198"></a>    <span class="st">"xtick.labelsize"</span>: <span class="dv">14</span>,</span>
<span id="cb10-199"><a href="#cb10-199"></a>    <span class="st">"ytick.labelsize"</span>: <span class="dv">14</span>,</span>
<span id="cb10-200"><a href="#cb10-200"></a>    <span class="st">"legend.fontsize"</span>: <span class="dv">14</span>,</span>
<span id="cb10-201"><a href="#cb10-201"></a>    <span class="st">"axes.titlesize"</span>: <span class="dv">16</span>,</span>
<span id="cb10-202"><a href="#cb10-202"></a>})</span>
<span id="cb10-203"><a href="#cb10-203"></a>plt.rcParams[<span class="st">'text.usetex'</span>] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb10-204"><a href="#cb10-204"></a></span>
<span id="cb10-205"><a href="#cb10-205"></a><span class="kw">def</span> draw_surface():</span>
<span id="cb10-206"><a href="#cb10-206"></a>    inps <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb10-207"><a href="#cb10-207"></a>    X, Y <span class="op">=</span> np.meshgrid(inps, inps)</span>
<span id="cb10-208"><a href="#cb10-208"></a>    grid <span class="op">=</span> np.column_stack([X.ravel(), Y.ravel()])</span>
<span id="cb10-209"><a href="#cb10-209"></a>    Z <span class="op">=</span> ackley(grid).reshape(X.shape)</span>
<span id="cb10-210"><a href="#cb10-210"></a>    </span>
<span id="cb10-211"><a href="#cb10-211"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">5</span>))</span>
<span id="cb10-212"><a href="#cb10-212"></a>    contour <span class="op">=</span> plt.contourf(X, Y, Z, <span class="dv">50</span>, cmap<span class="op">=</span>ccmap)</span>
<span id="cb10-213"><a href="#cb10-213"></a>    plt.contour(X, Y, Z, levels<span class="op">=</span><span class="dv">15</span>, colors<span class="op">=</span><span class="st">'black'</span>, linewidths<span class="op">=</span><span class="fl">0.5</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb10-214"><a href="#cb10-214"></a>    plt.colorbar(contour, label<span class="op">=</span><span class="vs">r'$f(x)$'</span>, ticks<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">6</span>])</span>
<span id="cb10-215"><a href="#cb10-215"></a>    plt.xlim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb10-216"><a href="#cb10-216"></a>    plt.ylim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb10-217"><a href="#cb10-217"></a>    plt.xticks([<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>])</span>
<span id="cb10-218"><a href="#cb10-218"></a>    plt.yticks([<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>])</span>
<span id="cb10-219"><a href="#cb10-219"></a>    plt.xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb10-220"><a href="#cb10-220"></a>    plt.ylabel(<span class="vs">r'$x_2$'</span>)</span>
<span id="cb10-221"><a href="#cb10-221"></a><span class="in">```</span></span>
<span id="cb10-222"><a href="#cb10-222"></a></span>
<span id="cb10-223"><a href="#cb10-223"></a>In this model, we can sample choice data when assuming a random generating model of, e.g., <span class="in">`np.random.randn(n, d)*0.5 + np.ones((n, d))*0.5`</span>.</span>
<span id="cb10-224"><a href="#cb10-224"></a></span>
<span id="cb10-225"><a href="#cb10-225"></a><span class="fu">### Item-wise Model {#item-wise-model}</span></span>
<span id="cb10-226"><a href="#cb10-226"></a>One method for data collection is accept-reject sampling, where the decision-maker considers one item at a time and decides if they like it compared to an outside option. This is common in applications like recommendation systems, where accepting refers to a consumption signal.</span>
<span id="cb10-227"><a href="#cb10-227"></a></span>
<span id="cb10-228"><a href="#cb10-228"></a>We will use a simulation to familiarize ourselves with accept-reject sampling. On the surface below, blue and red points correspond to accept or reject points.</span>
<span id="cb10-229"><a href="#cb10-229"></a></span>
<span id="cb10-230"><a href="#cb10-230"></a>::: {.callout-note title="Code"}</span>
<span id="cb10-231"><a href="#cb10-231"></a></span>
<span id="cb10-232"><a href="#cb10-232"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb10-233"><a href="#cb10-233"></a><span class="in">d = 2</span></span>
<span id="cb10-234"><a href="#cb10-234"></a><span class="in">n = 800</span></span>
<span id="cb10-235"><a href="#cb10-235"></a><span class="in">items = np.random.randn(n, d)*0.5 + np.ones((n, d))*0.5</span></span>
<span id="cb10-236"><a href="#cb10-236"></a><span class="in">utilities = ackley(items)</span></span>
<span id="cb10-237"><a href="#cb10-237"></a><span class="in">y = (utilities &gt; utilities.mean())</span></span>
<span id="cb10-238"><a href="#cb10-238"></a><span class="in">draw_surface()</span></span>
<span id="cb10-239"><a href="#cb10-239"></a><span class="in">plt.scatter(items[:, 0], items[:, 1], c=y, cmap='coolwarm', alpha=0.5)</span></span>
<span id="cb10-240"><a href="#cb10-240"></a><span class="in">plt.show()</span></span>
<span id="cb10-241"><a href="#cb10-241"></a><span class="in">```</span></span>
<span id="cb10-242"><a href="#cb10-242"></a></span>
<span id="cb10-243"><a href="#cb10-243"></a>:::</span>
<span id="cb10-244"><a href="#cb10-244"></a></span>
<span id="cb10-247"><a href="#cb10-247"></a><span class="in">```{python}</span></span>
<span id="cb10-248"><a href="#cb10-248"></a>d <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb10-249"><a href="#cb10-249"></a>n <span class="op">=</span> <span class="dv">800</span></span>
<span id="cb10-250"><a href="#cb10-250"></a>items <span class="op">=</span> np.random.randn(n, d)<span class="op">*</span><span class="fl">0.5</span> <span class="op">+</span> np.ones((n, d))<span class="op">*</span><span class="fl">0.5</span></span>
<span id="cb10-251"><a href="#cb10-251"></a>utilities <span class="op">=</span> ackley(items)</span>
<span id="cb10-252"><a href="#cb10-252"></a>y <span class="op">=</span> (utilities <span class="op">&gt;</span> utilities.mean())</span>
<span id="cb10-253"><a href="#cb10-253"></a>draw_surface()</span>
<span id="cb10-254"><a href="#cb10-254"></a>plt.scatter(items[:, <span class="dv">0</span>], items[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">'coolwarm'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb10-255"><a href="#cb10-255"></a>plt.show()</span>
<span id="cb10-256"><a href="#cb10-256"></a><span class="in">```</span></span>
<span id="cb10-257"><a href="#cb10-257"></a></span>
<span id="cb10-258"><a href="#cb10-258"></a></span>
<span id="cb10-259"><a href="#cb10-259"></a><span class="fu">### Pairwise Model {#pairwise-model}</span></span>
<span id="cb10-260"><a href="#cb10-260"></a>Pairwise comparisons, now used to fine-tune large language models can similarly be generated in this model.</span>
<span id="cb10-261"><a href="#cb10-261"></a></span>
<span id="cb10-262"><a href="#cb10-262"></a>::: {.content-visible when-format="html"}</span>
<span id="cb10-263"><a href="#cb10-263"></a></span>
<span id="cb10-264"><a href="#cb10-264"></a>&lt;iframe</span>
<span id="cb10-265"><a href="#cb10-265"></a>  src="https://app.opinionx.co/6bef4ca1-82f5-4c1d-8c5a-2274509f22e2"</span>
<span id="cb10-266"><a href="#cb10-266"></a>  style="width:100%; height:450px;"</span>
<span id="cb10-267"><a href="#cb10-267"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb10-268"><a href="#cb10-268"></a></span>
<span id="cb10-269"><a href="#cb10-269"></a>:::</span>
<span id="cb10-270"><a href="#cb10-270"></a></span>
<span id="cb10-271"><a href="#cb10-271"></a>::: {.callout-note title="Code"}</span>
<span id="cb10-272"><a href="#cb10-272"></a></span>
<span id="cb10-273"><a href="#cb10-273"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb10-274"><a href="#cb10-274"></a><span class="in">n_pairs = 10000</span></span>
<span id="cb10-275"><a href="#cb10-275"></a><span class="in">pair_indices = np.random.randint(0, n, size=(n_pairs, 2))</span></span>
<span id="cb10-276"><a href="#cb10-276"></a><span class="in"># Exclude pairs where both indices are the same</span></span>
<span id="cb10-277"><a href="#cb10-277"></a><span class="in">mask = pair_indices[:, 0] != pair_indices[:, 1]</span></span>
<span id="cb10-278"><a href="#cb10-278"></a><span class="in">pair_indices = pair_indices[mask]</span></span>
<span id="cb10-279"><a href="#cb10-279"></a></span>
<span id="cb10-280"><a href="#cb10-280"></a><span class="in">scores = np.zeros(n, dtype=int)</span></span>
<span id="cb10-281"><a href="#cb10-281"></a><span class="in">wins = utilities[pair_indices[:, 0]] &gt; utilities[pair_indices[:, 1]]</span></span>
<span id="cb10-282"><a href="#cb10-282"></a></span>
<span id="cb10-283"><a href="#cb10-283"></a><span class="in"># For pairs where the first item wins:</span></span>
<span id="cb10-284"><a href="#cb10-284"></a><span class="in">#   - Increase score for the first item by 1</span></span>
<span id="cb10-285"><a href="#cb10-285"></a><span class="in">#   - Decrease score for the second item by 1</span></span>
<span id="cb10-286"><a href="#cb10-286"></a><span class="in">np.add.at(scores, pair_indices[wins, 0], 1)</span></span>
<span id="cb10-287"><a href="#cb10-287"></a><span class="in">np.add.at(scores, pair_indices[wins, 1], -1)</span></span>
<span id="cb10-288"><a href="#cb10-288"></a></span>
<span id="cb10-289"><a href="#cb10-289"></a><span class="in"># For pairs where the second item wins or it's a tie:</span></span>
<span id="cb10-290"><a href="#cb10-290"></a><span class="in">#   - Decrease score for the first item by 1</span></span>
<span id="cb10-291"><a href="#cb10-291"></a><span class="in">#   - Increase score for the second item by 1</span></span>
<span id="cb10-292"><a href="#cb10-292"></a><span class="in">np.add.at(scores, pair_indices[~wins, 0], -1)</span></span>
<span id="cb10-293"><a href="#cb10-293"></a><span class="in">np.add.at(scores, pair_indices[~wins, 1], 1)</span></span>
<span id="cb10-294"><a href="#cb10-294"></a></span>
<span id="cb10-295"><a href="#cb10-295"></a><span class="in"># Determine preferred and non-preferred items based on scores</span></span>
<span id="cb10-296"><a href="#cb10-296"></a><span class="in">preferred = scores &gt; 0</span></span>
<span id="cb10-297"><a href="#cb10-297"></a><span class="in">non_preferred = scores &lt; 0</span></span>
<span id="cb10-298"><a href="#cb10-298"></a></span>
<span id="cb10-299"><a href="#cb10-299"></a><span class="in">draw_surface()</span></span>
<span id="cb10-300"><a href="#cb10-300"></a><span class="in">plt.scatter(items[preferred, 0], items[preferred, 1], c='blue', label='Preferred', alpha=0.5)</span></span>
<span id="cb10-301"><a href="#cb10-301"></a><span class="in">plt.scatter(items[non_preferred, 0], items[non_preferred, 1], c='purple', label='Non-preferred', alpha=0.5)</span></span>
<span id="cb10-302"><a href="#cb10-302"></a><span class="in">plt.legend()</span></span>
<span id="cb10-303"><a href="#cb10-303"></a><span class="in">plt.show()</span></span>
<span id="cb10-304"><a href="#cb10-304"></a><span class="in">```</span></span>
<span id="cb10-305"><a href="#cb10-305"></a></span>
<span id="cb10-306"><a href="#cb10-306"></a>:::</span>
<span id="cb10-307"><a href="#cb10-307"></a></span>
<span id="cb10-310"><a href="#cb10-310"></a><span class="in">```{python}</span></span>
<span id="cb10-311"><a href="#cb10-311"></a>n_pairs <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb10-312"><a href="#cb10-312"></a>pair_indices <span class="op">=</span> np.random.randint(<span class="dv">0</span>, n, size<span class="op">=</span>(n_pairs, <span class="dv">2</span>))</span>
<span id="cb10-313"><a href="#cb10-313"></a><span class="co"># Exclude pairs where both indices are the same</span></span>
<span id="cb10-314"><a href="#cb10-314"></a>mask <span class="op">=</span> pair_indices[:, <span class="dv">0</span>] <span class="op">!=</span> pair_indices[:, <span class="dv">1</span>]</span>
<span id="cb10-315"><a href="#cb10-315"></a>pair_indices <span class="op">=</span> pair_indices[mask]</span>
<span id="cb10-316"><a href="#cb10-316"></a></span>
<span id="cb10-317"><a href="#cb10-317"></a>scores <span class="op">=</span> np.zeros(n, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb10-318"><a href="#cb10-318"></a>wins <span class="op">=</span> utilities[pair_indices[:, <span class="dv">0</span>]] <span class="op">&gt;</span> utilities[pair_indices[:, <span class="dv">1</span>]]</span>
<span id="cb10-319"><a href="#cb10-319"></a></span>
<span id="cb10-320"><a href="#cb10-320"></a><span class="co"># For pairs where the first item wins:</span></span>
<span id="cb10-321"><a href="#cb10-321"></a><span class="co">#   - Increase score for the first item by 1</span></span>
<span id="cb10-322"><a href="#cb10-322"></a><span class="co">#   - Decrease score for the second item by 1</span></span>
<span id="cb10-323"><a href="#cb10-323"></a>np.add.at(scores, pair_indices[wins, <span class="dv">0</span>], <span class="dv">1</span>)</span>
<span id="cb10-324"><a href="#cb10-324"></a>np.add.at(scores, pair_indices[wins, <span class="dv">1</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb10-325"><a href="#cb10-325"></a></span>
<span id="cb10-326"><a href="#cb10-326"></a><span class="co"># For pairs where the second item wins or it's a tie:</span></span>
<span id="cb10-327"><a href="#cb10-327"></a><span class="co">#   - Decrease score for the first item by 1</span></span>
<span id="cb10-328"><a href="#cb10-328"></a><span class="co">#   - Increase score for the second item by 1</span></span>
<span id="cb10-329"><a href="#cb10-329"></a>np.add.at(scores, pair_indices[<span class="op">~</span>wins, <span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb10-330"><a href="#cb10-330"></a>np.add.at(scores, pair_indices[<span class="op">~</span>wins, <span class="dv">1</span>], <span class="dv">1</span>)</span>
<span id="cb10-331"><a href="#cb10-331"></a></span>
<span id="cb10-332"><a href="#cb10-332"></a><span class="co"># Determine preferred and non-preferred items based on scores</span></span>
<span id="cb10-333"><a href="#cb10-333"></a>preferred <span class="op">=</span> scores <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb10-334"><a href="#cb10-334"></a>non_preferred <span class="op">=</span> scores <span class="op">&lt;</span> <span class="dv">0</span></span>
<span id="cb10-335"><a href="#cb10-335"></a></span>
<span id="cb10-336"><a href="#cb10-336"></a>draw_surface()</span>
<span id="cb10-337"><a href="#cb10-337"></a>plt.scatter(items[preferred, <span class="dv">0</span>], items[preferred, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Preferred'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb10-338"><a href="#cb10-338"></a>plt.scatter(items[non_preferred, <span class="dv">0</span>], items[non_preferred, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'purple'</span>, label<span class="op">=</span><span class="st">'Non-preferred'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb10-339"><a href="#cb10-339"></a>plt.legend()</span>
<span id="cb10-340"><a href="#cb10-340"></a>plt.show()</span>
<span id="cb10-341"><a href="#cb10-341"></a><span class="in">```</span></span>
<span id="cb10-342"><a href="#cb10-342"></a>Similarly, we can sample data for $(y, Y')$ for $y \in Y' \subseteq Y$. </span>
<span id="cb10-343"><a href="#cb10-343"></a></span>
<span id="cb10-344"><a href="#cb10-344"></a><span class="fu">## Mean Utilities </span></span>
<span id="cb10-345"><a href="#cb10-345"></a></span>
<span id="cb10-346"><a href="#cb10-346"></a>We can view a random utility model $u_{\mathord{\prec}}$ as a deterministic part and a random part:</span>
<span id="cb10-347"><a href="#cb10-347"></a>$$</span>
<span id="cb10-348"><a href="#cb10-348"></a>u_{\mathord{\prec}} (y) = u(y) + \varepsilon_{\mathord{\prec}y}.</span>
<span id="cb10-349"><a href="#cb10-349"></a>$$</span>
<span id="cb10-350"><a href="#cb10-350"></a>The vector $u(y)$ is deterministic, and a vector $(\varepsilon_{\mathord{\prec}y})_{y \in Y}$ of noise is independent for different $\prec$. We say that $u(y)$ is the mean utility and $\varepsilon_{\mathord{\prec}y}$ is the noise. There are different forms of noise possible. We will focus on a particular one (called Type-1 Extreme value), but others are also popular, for example Gaussian noise.</span>
<span id="cb10-351"><a href="#cb10-351"></a></span>
<span id="cb10-352"><a href="#cb10-352"></a>There are (at least) three different ways to view this noise: </span>
<span id="cb10-353"><a href="#cb10-353"></a></span>
<span id="cb10-354"><a href="#cb10-354"></a><span class="ss"> - </span>Either it is capturing the heterogeneity of different decision-makers---a view that is taken in the Economics field of Industrial Organization. Under this view, observing $(y, Y')$ more frequently than $(y', Y')$ is a sign of there being a higher number of decision-makers preferring $y$ over $y'$ than the other way around. </span>
<span id="cb10-355"><a href="#cb10-355"></a><span class="ss"> - </span>or as errors of a decision-maker's optimization of utilities $u(y)$. This view is endorsed in the literature on Bounded Rationality. Under this view, it cannot be directly concluded from frequent observation of $(y, Y')$ compared to $(y', Y')$ that $y$ is preferred to $y$. It might have been chosen in error.</span>
<span id="cb10-356"><a href="#cb10-356"></a><span class="ss"> - </span>We can also view it as a belief of the designer about the preferences $(u(y))_{y \in Y}$. In this view, the posterior after observing data can be used to make claims about relative preferences. </span>
<span id="cb10-357"><a href="#cb10-357"></a></span>
<span id="cb10-358"><a href="#cb10-358"></a>The interpretation will guide our decision-making predictions in Chapters 4 and 5.</span>
<span id="cb10-359"><a href="#cb10-359"></a></span>
<span id="cb10-360"><a href="#cb10-360"></a>We next introduce a main way to simplify learning utility functions: The axiom of Independence of Irrelevant Alternatives.</span>
<span id="cb10-361"><a href="#cb10-361"></a></span>
<span id="cb10-362"><a href="#cb10-362"></a><span class="fu">## Independence of Irrelevant Alternatives</span></span>
<span id="cb10-363"><a href="#cb10-363"></a>In later chapters, we will consider cases where we sample from the most preferred elements from all objects $(y,Y)$, which we call generation task. A simple assumption will allow us to recover the probabilities of $(y, Y)$, and in fact, the full distribution of $\prec$ from binary comparisons: The so-called Independence of Irrelevant Alternatives, introduced in @luce1959individual. This assumption not only allows us to easily identify a preference model, it will also massively reduce what is needed to be estimated from data: Instead of the full $n!-1$-dimensional object, it will be sufficient to learn $n$ values. </span>
<span id="cb10-364"><a href="#cb10-364"></a></span>
<span id="cb10-365"><a href="#cb10-365"></a>IIA assumes that the relative likelihood of choosing $y$ compared to z does not change whether a third alternative $w$ is in the choice set or not. Formally, for every $Y' \subseteq Y$, $y,z \in Y'$, and $w \in Y \setminus Y'$,</span>
<span id="cb10-366"><a href="#cb10-366"></a>$$</span>
<span id="cb10-367"><a href="#cb10-367"></a>\frac{\mathbb{P}<span class="co">[</span><span class="ot">(y, Y')</span><span class="co">]</span>}{\mathbb{P}<span class="co">[</span><span class="ot">(z, Y')</span><span class="co">]</span>} = \frac{\mathbb{P}<span class="co">[</span><span class="ot">(y, Y' \cup \{w\})</span><span class="co">]</span>}{\mathbb{P}<span class="co">[</span><span class="ot">(z, Y' \cup \{w\})</span><span class="co">]</span>}.</span>
<span id="cb10-368"><a href="#cb10-368"></a>$$</span>
<span id="cb10-369"><a href="#cb10-369"></a>(In particular, it must be that $\mathbb{P}<span class="co">[</span><span class="ot">(z, Y')</span><span class="co">]</span> \neq 0$ and $\mathbb{P}<span class="co">[</span><span class="ot">(z, Y' \cup \{w\})</span><span class="co">]</span> \neq 0$.) That is, the relative probability of choosing $y$ over $y''$ and $y'$ over $y''$ should be independent of whether $z$ is present in the choice set $Y' \subseteq Y$. We will show that this single assumption is sufficient to make the choice model $n$-dimensional, making learning feasible.</span>
<span id="cb10-370"><a href="#cb10-370"></a></span>
<span id="cb10-371"><a href="#cb10-371"></a>First, to our primary example: All random utility models with *independent and identically distributed* noise terms satisfy IIA. (We ask the reader to convince themselves that the Ackerman function does not satisfy IIA.)</span>
<span id="cb10-372"><a href="#cb10-372"></a></span>
<span id="cb10-373"><a href="#cb10-373"></a>::: {.callout-tip title="theorem"}</span>
<span id="cb10-374"><a href="#cb10-374"></a></span>
<span id="cb10-375"><a href="#cb10-375"></a>A random utility model $u_{\mathord{\prec}}(y)$ satisfies IIA if and only if we can write it as $u_{\mathord{\prec}}(y) = u(y) + \varepsilon_{\mathord{\prec}y}$, where $u(y)$ is deterministic and $\varepsilon_{\mathord{\prec}y}$ is sampled independently and identically from the Gumbel distribution. The Gumbel distribution has cumulative distribution function $F(x) = e^{-e^{-x}}$.</span>
<span id="cb10-376"><a href="#cb10-376"></a></span>
<span id="cb10-377"><a href="#cb10-377"></a>::: </span>
<span id="cb10-378"><a href="#cb10-378"></a></span>
<span id="cb10-379"><a href="#cb10-379"></a>This is quite strong, and an **equivalence**. If we are willing to assume IIA, it is sufficient to learn $n$ parameters to characterize the full distribution---an exponential decrease in parameters to learn. The Gumbel model may be unusual in particular for those with a stronger background in machine learning. A more familiar formulation arises for the probabilities of choice. </span>
<span id="cb10-380"><a href="#cb10-380"></a></span>
<span id="cb10-381"><a href="#cb10-381"></a>::: {.callout-tip title="theorem"}</span>
<span id="cb10-382"><a href="#cb10-382"></a></span>
<span id="cb10-383"><a href="#cb10-383"></a>Assume a random preference model satisfies IIA, hence $u_{\mathord{\prec}} (y)= u(y) + \varepsilon_{\mathord{\prec}y}$. Then, the probabilities of lists are:</span>
<span id="cb10-384"><a href="#cb10-384"></a>$$</span>
<span id="cb10-385"><a href="#cb10-385"></a>\mathbb{P}<span class="co">[</span><span class="ot">(y_1 \succ y_2 \succ \cdots \succ y_n)</span><span class="co">]</span> = \frac{e^{u(y_1)}}{\sum_{i=1}^n e^{u(y_1)}} \cdot \frac{e^{u(y_2)}}{\sum_{i=2}^n e^{u(y_i)}}\cdot \frac{e^{u(y_3)}}{\sum_{i=3}^n e^{u(y_1)}} \cdots \frac{e^{u(y_{n-1})}}{ e^{u(y_{n-1})} +  e^{u(y_{n})}}.</span>
<span id="cb10-386"><a href="#cb10-386"></a>$$</span>
<span id="cb10-387"><a href="#cb10-387"></a>For choices from sets, </span>
<span id="cb10-388"><a href="#cb10-388"></a>$$</span>
<span id="cb10-389"><a href="#cb10-389"></a>\mathbb{P} <span class="co">[</span><span class="ot">(y, Y')</span><span class="co">]</span> = \frac{e^{u(y)}}{\sum_{y' \in Y'} e^{u(y')}} = \operatorname{softmax}_y ((u(y'))_{y' \in Y'}).</span>
<span id="cb10-390"><a href="#cb10-390"></a>$$</span>
<span id="cb10-391"><a href="#cb10-391"></a>In particular, for binary comparisons</span>
<span id="cb10-392"><a href="#cb10-392"></a>$$</span>
<span id="cb10-393"><a href="#cb10-393"></a>\mathbb{P} <span class="co">[</span><span class="ot">(y_1 \succ y_2)</span><span class="co">]</span> = \frac{e^{u(y_1)}}{e^{u(y_1)} + e^{u(y_1)}} = \frac{1}{1 + e^{u(y_1) - u(y_2)}} = \sigma (u(y_1) - u(y_2)).</span>
<span id="cb10-394"><a href="#cb10-394"></a>$$</span>
<span id="cb10-395"><a href="#cb10-395"></a>where $\sigma = 1/(1 + e^x)$ is the sigmoid function.</span>
<span id="cb10-396"><a href="#cb10-396"></a></span>
<span id="cb10-397"><a href="#cb10-397"></a>:::</span>
<span id="cb10-398"><a href="#cb10-398"></a>In particular, the choice probabilities $\mathbb{P} <span class="co">[</span><span class="ot">(y, Y)</span><span class="co">]</span>$ are equivalent to the multi-class logistic regression model (also called multinomial logit model), and generation from this model, that is, sampling $y$ with probability $\mathbb{P}<span class="co">[</span><span class="ot">(y, Y)</span><span class="co">]</span>$ is given by softmax-sampling $\operatorname{softmax}((u(y))_{y \in Y})$. </span>
<span id="cb10-399"><a href="#cb10-399"></a></span>
<span id="cb10-400"><a href="#cb10-400"></a>This model has many names, depending on the feedback type we consider. For binary comparison data, it is the Bradley-Terry model @bradley1952rank. If data is in forms of list, it is called the Plackett-Luce model @plackett1975analysis. For accept-reject sampling it is also called logistic regression. For choices from subsets $Y$, is is called the (discrete choice) logit model. We will usually call the model the **logit model** and specify the feedback type.</span>
<span id="cb10-401"><a href="#cb10-401"></a></span>
<span id="cb10-402"><a href="#cb10-402"></a>The IIA assumption has many desirable properties, such as stochastic transitivity and relativity. The reader is asked to prove them in the exercises to this chapter. The learning of, and optimization based on, the mean utilities $(u(y))_{y \in Y}$ is one of the central goal of this book. Supervised learning based on it will be covered in the next chapter.</span>
<span id="cb10-403"><a href="#cb10-403"></a></span>
<span id="cb10-404"><a href="#cb10-404"></a>One note on identification, that is, whether different utility functions $u$ generate the same random preference model $\prec$. The models that are implied by $u(y)$ and by $u(y) + c$ for any constant $c \in \mathbb{R}$ are the same, which means that any learning of the function $u$, which we will engage in in the next chapters, will need to fix one of the values $u(y)$. If there is an outside option $y_0$, then, it is typically chosen $u(y_0) = 0$ and all mean utilities are in comparison to the outside option.</span>
<span id="cb10-405"><a href="#cb10-405"></a></span>
<span id="cb10-406"><a href="#cb10-406"></a>IIA has limitations, which might require to allow for more flexible specifications of noise and heterogeneity. </span>
<span id="cb10-407"><a href="#cb10-407"></a></span>
<span id="cb10-408"><a href="#cb10-408"></a><span class="fu">## IIA's Limitations</span></span>
<span id="cb10-409"><a href="#cb10-409"></a></span>
<span id="cb10-410"><a href="#cb10-410"></a>IIA is surprisingly strong, but does not allow for choice probabilities that are, fundamentally, results of multiple decision-makers making choices together. A first restriction is given by </span>
<span id="cb10-411"><a href="#cb10-411"></a></span>
<span id="cb10-412"><a href="#cb10-412"></a><span class="fu">### IIA and Heterogeneity</span></span>
<span id="cb10-413"><a href="#cb10-413"></a></span>
<span id="cb10-414"><a href="#cb10-414"></a>A crucial shortcoming of IIA is that if sub-populations satisfy IIA this does not mean that the full population satisfies IIA. Assume that our population consists of sub-populations $i = 1, \dots, m$ which have mass $\alpha_1, \alpha_2, \dots, \alpha_m$, respectively, in the population, and that each of the groups has preferences satisfying IIA. Because of IIA, we can represent each sub-group's stochastic preferences with an average utility $u_i \colon Y \to \mathbb R$, $i = 1, 2, \dots, n$. The distribution of the full population is then given by a mixture of the sub-population preferences. For example, for binary comparisons</span>
<span id="cb10-415"><a href="#cb10-415"></a></span>
<span id="cb10-416"><a href="#cb10-416"></a>$$</span>
<span id="cb10-417"><a href="#cb10-417"></a>\mathbb{P} <span class="co">[</span><span class="ot">y_1 \succ y_2 </span><span class="co">]</span> = \sum_{i=1}^n \alpha_i \mathbb{P} <span class="co">[</span><span class="ot">y_1 \succ y_2  | \text{ group } i</span><span class="co">]</span>   = \sum_{i = 1}^m \alpha_i \sigma (u_i (y_1) - u_i(y_2)).</span>
<span id="cb10-418"><a href="#cb10-418"></a>$$</span>
<span id="cb10-419"><a href="#cb10-419"></a></span>
<span id="cb10-420"><a href="#cb10-420"></a>Sadly, such mixtures are far from IIA, as we see in the following coding example.</span>
<span id="cb10-421"><a href="#cb10-421"></a></span>
<span id="cb10-422"><a href="#cb10-422"></a>::: {.callout-note title="Code"}</span>
<span id="cb10-423"><a href="#cb10-423"></a></span>
<span id="cb10-424"><a href="#cb10-424"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb10-425"><a href="#cb10-425"></a><span class="in">import numpy as np</span></span>
<span id="cb10-426"><a href="#cb10-426"></a></span>
<span id="cb10-427"><a href="#cb10-427"></a><span class="in"># Define utilities for each group</span></span>
<span id="cb10-428"><a href="#cb10-428"></a><span class="in">group1_utilities = {'A': 1.0, 'B': 2.0, 'C': 3.0}</span></span>
<span id="cb10-429"><a href="#cb10-429"></a><span class="in">group2_utilities = {'A': 3.0, 'B': 2.0, 'C': 1.0}</span></span>
<span id="cb10-430"><a href="#cb10-430"></a></span>
<span id="cb10-431"><a href="#cb10-431"></a><span class="in"># Group weights</span></span>
<span id="cb10-432"><a href="#cb10-432"></a><span class="in">alpha1 = 0.5</span></span>
<span id="cb10-433"><a href="#cb10-433"></a><span class="in">alpha2 = 0.5</span></span>
<span id="cb10-434"><a href="#cb10-434"></a></span>
<span id="cb10-435"><a href="#cb10-435"></a><span class="in">def softmax(utilities):</span></span>
<span id="cb10-436"><a href="#cb10-436"></a><span class="in">    exp_vals = np.exp(list(utilities.values()))</span></span>
<span id="cb10-437"><a href="#cb10-437"></a><span class="in">    total = np.sum(exp_vals)</span></span>
<span id="cb10-438"><a href="#cb10-438"></a><span class="in">    return {k: np.exp(v) / total for k, v in utilities.items()}</span></span>
<span id="cb10-439"><a href="#cb10-439"></a></span>
<span id="cb10-440"><a href="#cb10-440"></a><span class="in"># Compute mixed logit probabilities over all 3 options</span></span>
<span id="cb10-441"><a href="#cb10-441"></a><span class="in">p1_full = softmax(group1_utilities)</span></span>
<span id="cb10-442"><a href="#cb10-442"></a><span class="in">p2_full = softmax(group2_utilities)</span></span>
<span id="cb10-443"><a href="#cb10-443"></a><span class="in">p_mix_full = {k: alpha1 * p1_full[k] + alpha2 * p2_full[k] for k in group1_utilities}</span></span>
<span id="cb10-444"><a href="#cb10-444"></a></span>
<span id="cb10-445"><a href="#cb10-445"></a><span class="in"># Compute ratio A/B in full model</span></span>
<span id="cb10-446"><a href="#cb10-446"></a><span class="in">ratio_full = p_mix_full['A'] / p_mix_full['B']</span></span>
<span id="cb10-447"><a href="#cb10-447"></a></span>
<span id="cb10-448"><a href="#cb10-448"></a><span class="in"># Now remove option C</span></span>
<span id="cb10-449"><a href="#cb10-449"></a><span class="in">reduced_utils1 = {'A': group1_utilities['A'], 'B': group1_utilities['B']}</span></span>
<span id="cb10-450"><a href="#cb10-450"></a><span class="in">reduced_utils2 = {'A': group2_utilities['A'], 'B': group2_utilities['B']}</span></span>
<span id="cb10-451"><a href="#cb10-451"></a><span class="in">p1_reduced = softmax(reduced_utils1)</span></span>
<span id="cb10-452"><a href="#cb10-452"></a><span class="in">p2_reduced = softmax(reduced_utils2)</span></span>
<span id="cb10-453"><a href="#cb10-453"></a><span class="in">p_mix_reduced = {k: alpha1 * p1_reduced[k] + alpha2 * p2_reduced[k] for k in reduced_utils1}</span></span>
<span id="cb10-454"><a href="#cb10-454"></a></span>
<span id="cb10-455"><a href="#cb10-455"></a><span class="in"># Compute ratio A/B in reduced model</span></span>
<span id="cb10-456"><a href="#cb10-456"></a><span class="in">ratio_reduced = p_mix_reduced['A'] / p_mix_reduced['B']</span></span>
<span id="cb10-457"><a href="#cb10-457"></a></span>
<span id="cb10-458"><a href="#cb10-458"></a><span class="in"># Show violation of IIA</span></span>
<span id="cb10-459"><a href="#cb10-459"></a><span class="in">print(f"Ratio A/B with all options: {ratio_full:.4f}")</span></span>
<span id="cb10-460"><a href="#cb10-460"></a><span class="in">print(f"Ratio A/B after removing C: {ratio_reduced:.4f}")</span></span>
<span id="cb10-461"><a href="#cb10-461"></a><span class="in">```</span></span>
<span id="cb10-462"><a href="#cb10-462"></a></span>
<span id="cb10-463"><a href="#cb10-463"></a>:::</span>
<span id="cb10-464"><a href="#cb10-464"></a></span>
<span id="cb10-467"><a href="#cb10-467"></a><span class="in">```{python}</span></span>
<span id="cb10-468"><a href="#cb10-468"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-469"><a href="#cb10-469"></a></span>
<span id="cb10-470"><a href="#cb10-470"></a><span class="co"># Define utilities for each group</span></span>
<span id="cb10-471"><a href="#cb10-471"></a>group1_utilities <span class="op">=</span> {<span class="st">'A'</span>: <span class="fl">1.0</span>, <span class="st">'B'</span>: <span class="fl">2.0</span>, <span class="st">'C'</span>: <span class="fl">3.0</span>}</span>
<span id="cb10-472"><a href="#cb10-472"></a>group2_utilities <span class="op">=</span> {<span class="st">'A'</span>: <span class="fl">3.0</span>, <span class="st">'B'</span>: <span class="fl">2.0</span>, <span class="st">'C'</span>: <span class="fl">1.0</span>}</span>
<span id="cb10-473"><a href="#cb10-473"></a></span>
<span id="cb10-474"><a href="#cb10-474"></a><span class="co"># Group weights</span></span>
<span id="cb10-475"><a href="#cb10-475"></a>alpha1 <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb10-476"><a href="#cb10-476"></a>alpha2 <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb10-477"><a href="#cb10-477"></a></span>
<span id="cb10-478"><a href="#cb10-478"></a><span class="kw">def</span> softmax(utilities):</span>
<span id="cb10-479"><a href="#cb10-479"></a>    exp_vals <span class="op">=</span> np.exp(<span class="bu">list</span>(utilities.values()))</span>
<span id="cb10-480"><a href="#cb10-480"></a>    total <span class="op">=</span> np.<span class="bu">sum</span>(exp_vals)</span>
<span id="cb10-481"><a href="#cb10-481"></a>    <span class="cf">return</span> {k: np.exp(v) <span class="op">/</span> total <span class="cf">for</span> k, v <span class="kw">in</span> utilities.items()}</span>
<span id="cb10-482"><a href="#cb10-482"></a></span>
<span id="cb10-483"><a href="#cb10-483"></a><span class="co"># Compute mixed logit probabilities over all 3 options</span></span>
<span id="cb10-484"><a href="#cb10-484"></a>p1_full <span class="op">=</span> softmax(group1_utilities)</span>
<span id="cb10-485"><a href="#cb10-485"></a>p2_full <span class="op">=</span> softmax(group2_utilities)</span>
<span id="cb10-486"><a href="#cb10-486"></a>p_mix_full <span class="op">=</span> {k: alpha1 <span class="op">*</span> p1_full[k] <span class="op">+</span> alpha2 <span class="op">*</span> p2_full[k] <span class="cf">for</span> k <span class="kw">in</span> group1_utilities}</span>
<span id="cb10-487"><a href="#cb10-487"></a></span>
<span id="cb10-488"><a href="#cb10-488"></a><span class="co"># Compute ratio A/B in full model</span></span>
<span id="cb10-489"><a href="#cb10-489"></a>ratio_full <span class="op">=</span> p_mix_full[<span class="st">'A'</span>] <span class="op">/</span> p_mix_full[<span class="st">'B'</span>]</span>
<span id="cb10-490"><a href="#cb10-490"></a></span>
<span id="cb10-491"><a href="#cb10-491"></a><span class="co"># Now remove option C</span></span>
<span id="cb10-492"><a href="#cb10-492"></a>reduced_utils1 <span class="op">=</span> {<span class="st">'A'</span>: group1_utilities[<span class="st">'A'</span>], <span class="st">'B'</span>: group1_utilities[<span class="st">'B'</span>]}</span>
<span id="cb10-493"><a href="#cb10-493"></a>reduced_utils2 <span class="op">=</span> {<span class="st">'A'</span>: group2_utilities[<span class="st">'A'</span>], <span class="st">'B'</span>: group2_utilities[<span class="st">'B'</span>]}</span>
<span id="cb10-494"><a href="#cb10-494"></a>p1_reduced <span class="op">=</span> softmax(reduced_utils1)</span>
<span id="cb10-495"><a href="#cb10-495"></a>p2_reduced <span class="op">=</span> softmax(reduced_utils2)</span>
<span id="cb10-496"><a href="#cb10-496"></a>p_mix_reduced <span class="op">=</span> {k: alpha1 <span class="op">*</span> p1_reduced[k] <span class="op">+</span> alpha2 <span class="op">*</span> p2_reduced[k] <span class="cf">for</span> k <span class="kw">in</span> reduced_utils1}</span>
<span id="cb10-497"><a href="#cb10-497"></a></span>
<span id="cb10-498"><a href="#cb10-498"></a><span class="co"># Compute ratio A/B in reduced model</span></span>
<span id="cb10-499"><a href="#cb10-499"></a>ratio_reduced <span class="op">=</span> p_mix_reduced[<span class="st">'A'</span>] <span class="op">/</span> p_mix_reduced[<span class="st">'B'</span>]</span>
<span id="cb10-500"><a href="#cb10-500"></a></span>
<span id="cb10-501"><a href="#cb10-501"></a><span class="co"># Show violation of IIA</span></span>
<span id="cb10-502"><a href="#cb10-502"></a><span class="bu">print</span>(<span class="ss">f"Ratio A/B with all options: </span><span class="sc">{</span>ratio_full<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb10-503"><a href="#cb10-503"></a><span class="bu">print</span>(<span class="ss">f"Ratio A/B after removing C: </span><span class="sc">{</span>ratio_reduced<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb10-504"><a href="#cb10-504"></a><span class="in">```</span></span>
<span id="cb10-505"><a href="#cb10-505"></a></span>
<span id="cb10-506"><a href="#cb10-506"></a>An intuition for IIA's failure comes from viewing it as random utility functions: A mixture of vectors that have independent entries is not Gaussian.</span>
<span id="cb10-507"><a href="#cb10-507"></a></span>
<span id="cb10-508"><a href="#cb10-508"></a>::: {.callout-note title="Code"}</span>
<span id="cb10-509"><a href="#cb10-509"></a></span>
<span id="cb10-510"><a href="#cb10-510"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb10-511"><a href="#cb10-511"></a><span class="in">import numpy as np</span></span>
<span id="cb10-512"><a href="#cb10-512"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb10-513"><a href="#cb10-513"></a><span class="in">from scipy.stats import norm</span></span>
<span id="cb10-514"><a href="#cb10-514"></a></span>
<span id="cb10-515"><a href="#cb10-515"></a><span class="in"># Define two Gaussian distributions</span></span>
<span id="cb10-516"><a href="#cb10-516"></a><span class="in">mu1, sigma1 = 0, 1</span></span>
<span id="cb10-517"><a href="#cb10-517"></a><span class="in">mu2, sigma2 = 4, 1</span></span>
<span id="cb10-518"><a href="#cb10-518"></a><span class="in">x = np.linspace(-4, 8, 1000)</span></span>
<span id="cb10-519"><a href="#cb10-519"></a></span>
<span id="cb10-520"><a href="#cb10-520"></a><span class="in"># Evaluate the PDFs</span></span>
<span id="cb10-521"><a href="#cb10-521"></a><span class="in">pdf1 = norm.pdf(x, mu1, sigma1)</span></span>
<span id="cb10-522"><a href="#cb10-522"></a><span class="in">pdf2 = norm.pdf(x, mu2, sigma2)</span></span>
<span id="cb10-523"><a href="#cb10-523"></a></span>
<span id="cb10-524"><a href="#cb10-524"></a><span class="in"># Mixture: equal weight</span></span>
<span id="cb10-525"><a href="#cb10-525"></a><span class="in">mixture_pdf = 0.5 * pdf1 + 0.5 * pdf2</span></span>
<span id="cb10-526"><a href="#cb10-526"></a></span>
<span id="cb10-527"><a href="#cb10-527"></a><span class="in"># Plot</span></span>
<span id="cb10-528"><a href="#cb10-528"></a><span class="in">plt.plot(x, pdf1, label='N(0, 1)', linestyle='--')</span></span>
<span id="cb10-529"><a href="#cb10-529"></a><span class="in">plt.plot(x, pdf2, label='N(4, 1)', linestyle='--')</span></span>
<span id="cb10-530"><a href="#cb10-530"></a><span class="in">plt.plot(x, mixture_pdf, label='Mixture 0.5*N(0,1) + 0.5*N(4,1)', linewidth=2)</span></span>
<span id="cb10-531"><a href="#cb10-531"></a><span class="in">plt.title("Mixture of Two Gaussians is Not Gaussian")</span></span>
<span id="cb10-532"><a href="#cb10-532"></a><span class="in">plt.xlabel("x")</span></span>
<span id="cb10-533"><a href="#cb10-533"></a><span class="in">plt.ylabel("Density")</span></span>
<span id="cb10-534"><a href="#cb10-534"></a><span class="in">plt.legend()</span></span>
<span id="cb10-535"><a href="#cb10-535"></a><span class="in">plt.grid(True)</span></span>
<span id="cb10-536"><a href="#cb10-536"></a><span class="in">plt.show()</span></span>
<span id="cb10-537"><a href="#cb10-537"></a><span class="in">```</span></span>
<span id="cb10-538"><a href="#cb10-538"></a></span>
<span id="cb10-539"><a href="#cb10-539"></a>:::</span>
<span id="cb10-540"><a href="#cb10-540"></a></span>
<span id="cb10-543"><a href="#cb10-543"></a><span class="in">```{python}</span></span>
<span id="cb10-544"><a href="#cb10-544"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-545"><a href="#cb10-545"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-546"><a href="#cb10-546"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb10-547"><a href="#cb10-547"></a></span>
<span id="cb10-548"><a href="#cb10-548"></a><span class="co"># Define two Gaussian distributions</span></span>
<span id="cb10-549"><a href="#cb10-549"></a>mu1, sigma1 <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span></span>
<span id="cb10-550"><a href="#cb10-550"></a>mu2, sigma2 <span class="op">=</span> <span class="dv">4</span>, <span class="dv">1</span></span>
<span id="cb10-551"><a href="#cb10-551"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">1000</span>)</span>
<span id="cb10-552"><a href="#cb10-552"></a></span>
<span id="cb10-553"><a href="#cb10-553"></a><span class="co"># Evaluate the PDFs</span></span>
<span id="cb10-554"><a href="#cb10-554"></a>pdf1 <span class="op">=</span> norm.pdf(x, mu1, sigma1)</span>
<span id="cb10-555"><a href="#cb10-555"></a>pdf2 <span class="op">=</span> norm.pdf(x, mu2, sigma2)</span>
<span id="cb10-556"><a href="#cb10-556"></a></span>
<span id="cb10-557"><a href="#cb10-557"></a><span class="co"># Mixture: equal weight</span></span>
<span id="cb10-558"><a href="#cb10-558"></a>mixture_pdf <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> pdf1 <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> pdf2</span>
<span id="cb10-559"><a href="#cb10-559"></a></span>
<span id="cb10-560"><a href="#cb10-560"></a><span class="co"># Plot</span></span>
<span id="cb10-561"><a href="#cb10-561"></a>plt.plot(x, pdf1, label<span class="op">=</span><span class="st">'N(0, 1)'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb10-562"><a href="#cb10-562"></a>plt.plot(x, pdf2, label<span class="op">=</span><span class="st">'N(4, 1)'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb10-563"><a href="#cb10-563"></a>plt.plot(x, mixture_pdf, label<span class="op">=</span><span class="st">'Mixture 0.5*N(0,1) + 0.5*N(4,1)'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-564"><a href="#cb10-564"></a>plt.title(<span class="st">"Mixture of Two Gaussians is Not Gaussian"</span>)</span>
<span id="cb10-565"><a href="#cb10-565"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb10-566"><a href="#cb10-566"></a>plt.ylabel(<span class="st">"Density"</span>)</span>
<span id="cb10-567"><a href="#cb10-567"></a>plt.legend()</span>
<span id="cb10-568"><a href="#cb10-568"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb10-569"><a href="#cb10-569"></a>plt.show()</span>
<span id="cb10-570"><a href="#cb10-570"></a><span class="in">```</span></span>
<span id="cb10-571"><a href="#cb10-571"></a></span>
<span id="cb10-572"><a href="#cb10-572"></a>Classic ways to solve this concern is to consider a model with explicit representation of heterogeneity, where $y \prec y'$ holds if $\alpha \sim F$ for some distribution $F$, and $(u(y))_{y \in Y} \alpha$ is a random utility model with independent error terms. For example, consider a logit random utility model </span>
<span id="cb10-573"><a href="#cb10-573"></a>$$</span>
<span id="cb10-574"><a href="#cb10-574"></a>u(y) = \beta^\top x + \varepsilon_y</span>
<span id="cb10-575"><a href="#cb10-575"></a>$$</span>
<span id="cb10-576"><a href="#cb10-576"></a>and assume $\beta \sim N(\mu, \Sigma)$ is a normally distributed vector, a model called the random coefficients logit model. Equivalently, we can view this as a model with correlated utility shocks.</span>
<span id="cb10-577"><a href="#cb10-577"></a></span>
<span id="cb10-578"><a href="#cb10-578"></a><span class="fu">### Similar Options</span></span>
<span id="cb10-579"><a href="#cb10-579"></a></span>
<span id="cb10-580"><a href="#cb10-580"></a>A second limitation is only relevant if we move beyond binary choices, or observe preference lists. Let $Y = <span class="sc">\{</span>y, y', z<span class="sc">\}</span>$, where $y$ and $y'$ are (almost) identical and different from $z$. (In the classical example, $y, y'$ are red and blue buses, respectively, and $z$ is a train). Assume an IIA model given by average utility $u \colon Y \to \mathbb{R}$. As $y$ and $y'$ are almost identical, assume $u(y) = u(y')$. We have $\mathbb{P}<span class="co">[</span><span class="ot">z, \{y, z\}</span><span class="co">]</span> = \mathbb{P}<span class="co">[</span><span class="ot">z, \{y', z\}</span><span class="co">]</span>$. How do these values compare to $\mathbb{P}<span class="co">[</span><span class="ot">z, \{y, y', z\}</span><span class="co">]</span>$? It would be intuitive to think that $z$ is chosen with the same frequency, as there should not be more "demand" for object $z$ only because $y$ is cloned. This is not the case.</span>
<span id="cb10-581"><a href="#cb10-581"></a></span>
<span id="cb10-582"><a href="#cb10-582"></a>::: {.callout-note title="Code"}</span>
<span id="cb10-583"><a href="#cb10-583"></a></span>
<span id="cb10-584"><a href="#cb10-584"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb10-585"><a href="#cb10-585"></a><span class="in">import numpy as np</span></span>
<span id="cb10-586"><a href="#cb10-586"></a></span>
<span id="cb10-587"><a href="#cb10-587"></a><span class="in"># Deterministic utilities</span></span>
<span id="cb10-588"><a href="#cb10-588"></a><span class="in">v_car = 1.0</span></span>
<span id="cb10-589"><a href="#cb10-589"></a><span class="in">v_bus = 2.0  # Initially a single bus alternative</span></span>
<span id="cb10-590"><a href="#cb10-590"></a></span>
<span id="cb10-591"><a href="#cb10-591"></a><span class="in"># Logit choice probabilities (before splitting bus)</span></span>
<span id="cb10-592"><a href="#cb10-592"></a><span class="in">def softmax(utilities: np.ndarray) -&gt; np.ndarray:</span></span>
<span id="cb10-593"><a href="#cb10-593"></a><span class="in">    exp_util = np.exp(utilities)</span></span>
<span id="cb10-594"><a href="#cb10-594"></a><span class="in">    return exp_util / np.sum(exp_util)</span></span>
<span id="cb10-595"><a href="#cb10-595"></a></span>
<span id="cb10-596"><a href="#cb10-596"></a><span class="in"># Before splitting: two alternatives</span></span>
<span id="cb10-597"><a href="#cb10-597"></a><span class="in">utilities_before = np.array([v_car, v_bus])</span></span>
<span id="cb10-598"><a href="#cb10-598"></a><span class="in">probs_before = softmax(utilities_before)</span></span>
<span id="cb10-599"><a href="#cb10-599"></a><span class="in">print("Before splitting (Car, Bus):", probs_before)</span></span>
<span id="cb10-600"><a href="#cb10-600"></a></span>
<span id="cb10-601"><a href="#cb10-601"></a><span class="in"># After splitting: three alternatives</span></span>
<span id="cb10-602"><a href="#cb10-602"></a><span class="in">v_red_bus = v_bus</span></span>
<span id="cb10-603"><a href="#cb10-603"></a><span class="in">v_blue_bus = v_bus</span></span>
<span id="cb10-604"><a href="#cb10-604"></a><span class="in">utilities_after = np.array([v_car, v_red_bus, v_blue_bus])</span></span>
<span id="cb10-605"><a href="#cb10-605"></a><span class="in">probs_after = softmax(utilities_after)</span></span>
<span id="cb10-606"><a href="#cb10-606"></a><span class="in">print("After splitting (Car, Red Bus, Blue Bus):", probs_after)</span></span>
<span id="cb10-607"><a href="#cb10-607"></a><span class="in">print("After splitting, total bus share:", probs_after[1] + probs_after[2])</span></span>
<span id="cb10-608"><a href="#cb10-608"></a><span class="in">```</span></span>
<span id="cb10-609"><a href="#cb10-609"></a></span>
<span id="cb10-610"><a href="#cb10-610"></a>:::</span>
<span id="cb10-611"><a href="#cb10-611"></a></span>
<span id="cb10-614"><a href="#cb10-614"></a><span class="in">```{python}</span></span>
<span id="cb10-615"><a href="#cb10-615"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-616"><a href="#cb10-616"></a></span>
<span id="cb10-617"><a href="#cb10-617"></a><span class="co"># Deterministic utilities</span></span>
<span id="cb10-618"><a href="#cb10-618"></a>v_car <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb10-619"><a href="#cb10-619"></a>v_bus <span class="op">=</span> <span class="fl">2.0</span>  <span class="co"># Initially a single bus alternative</span></span>
<span id="cb10-620"><a href="#cb10-620"></a></span>
<span id="cb10-621"><a href="#cb10-621"></a><span class="co"># Logit choice probabilities (before splitting bus)</span></span>
<span id="cb10-622"><a href="#cb10-622"></a><span class="kw">def</span> softmax(utilities: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb10-623"><a href="#cb10-623"></a>    exp_util <span class="op">=</span> np.exp(utilities)</span>
<span id="cb10-624"><a href="#cb10-624"></a>    <span class="cf">return</span> exp_util <span class="op">/</span> np.<span class="bu">sum</span>(exp_util)</span>
<span id="cb10-625"><a href="#cb10-625"></a></span>
<span id="cb10-626"><a href="#cb10-626"></a><span class="co"># Before splitting: two alternatives</span></span>
<span id="cb10-627"><a href="#cb10-627"></a>utilities_before <span class="op">=</span> np.array([v_car, v_bus])</span>
<span id="cb10-628"><a href="#cb10-628"></a>probs_before <span class="op">=</span> softmax(utilities_before)</span>
<span id="cb10-629"><a href="#cb10-629"></a><span class="bu">print</span>(<span class="st">"Before splitting (Car, Bus):"</span>, probs_before)</span>
<span id="cb10-630"><a href="#cb10-630"></a></span>
<span id="cb10-631"><a href="#cb10-631"></a><span class="co"># After splitting: three alternatives</span></span>
<span id="cb10-632"><a href="#cb10-632"></a>v_red_bus <span class="op">=</span> v_bus</span>
<span id="cb10-633"><a href="#cb10-633"></a>v_blue_bus <span class="op">=</span> v_bus</span>
<span id="cb10-634"><a href="#cb10-634"></a>utilities_after <span class="op">=</span> np.array([v_car, v_red_bus, v_blue_bus])</span>
<span id="cb10-635"><a href="#cb10-635"></a>probs_after <span class="op">=</span> softmax(utilities_after)</span>
<span id="cb10-636"><a href="#cb10-636"></a><span class="bu">print</span>(<span class="st">"After splitting (Car, Red Bus, Blue Bus):"</span>, probs_after)</span>
<span id="cb10-637"><a href="#cb10-637"></a><span class="bu">print</span>(<span class="st">"After splitting, total bus share:"</span>, probs_after[<span class="dv">1</span>] <span class="op">+</span> probs_after[<span class="dv">2</span>])</span>
<span id="cb10-638"><a href="#cb10-638"></a><span class="in">```</span></span>
<span id="cb10-639"><a href="#cb10-639"></a></span>
<span id="cb10-640"><a href="#cb10-640"></a>The choice probability of <span class="in">`car`</span> is reduced. Why is our intuition making us think that $y$ and $y'$ should split their choice probability? One option is because we assume some correlation: If you like $y$ over $z$ then you should also like $y'$ over $z$, and vice versa. Hence, we would like the correlation between choice probabilities in random utility models. For example, if we allow in a logit random utility model the error terms in $y, y'$ to be perfectly correlated (and we break ties uniformly at random), then </span>
<span id="cb10-641"><a href="#cb10-641"></a>$$</span>
<span id="cb10-642"><a href="#cb10-642"></a>(\mathbb{P}<span class="co">[</span><span class="ot">y, \{y, y', z\}</span><span class="co">]</span>, \mathbb{P}<span class="co">[</span><span class="ot">y', \{y, y', z\}</span><span class="co">]</span>, \mathbb{P}<span class="co">[</span><span class="ot">z, \{y, y', z\}</span><span class="co">]</span>) = \left(\frac{\mathbb{P}<span class="co">[</span><span class="ot">y, \{y, z\}</span><span class="co">]</span>}{2}, \frac{\mathbb{P}<span class="co">[</span><span class="ot">y', \{y', z\}</span><span class="co">]</span>}{2}, \frac{\mathbb{P}<span class="co">[</span><span class="ot">z, \{y, z\}</span><span class="co">]</span>}{2}\right),</span>
<span id="cb10-643"><a href="#cb10-643"></a>$$</span>
<span id="cb10-644"><a href="#cb10-644"></a>confirming our intuition. </span>
<span id="cb10-645"><a href="#cb10-645"></a></span>
<span id="cb10-646"><a href="#cb10-646"></a>This is the end of our discussion of the Independence of Irrelevant Alternatives. Additional features can be found in <span class="co">[</span><span class="ot">@train2009discrete;@ben1985discrete;@mcfadden1981econometric</span><span class="co">]</span> and the original paper for logit analysis @mcfadden1972conditional.</span>
<span id="cb10-647"><a href="#cb10-647"></a></span>
<span id="cb10-648"><a href="#cb10-648"></a>The next chapter is the first to study learning of average utility functions from preference data, and assumes that a dataset is given of (average) utility functions $u \colon Y \to \mathbb R$ for different types of sampling and for different notions of "inference".</span>
<span id="cb10-649"><a href="#cb10-649"></a></span>
<span id="cb10-650"><a href="#cb10-650"></a>| Notation | Meaning | Domain / Type |</span>
<span id="cb10-651"><a href="#cb10-651"></a>|---|---|---|</span>
<span id="cb10-652"><a href="#cb10-652"></a>| $Y$ | Finite set of objects/alternatives | $<span class="sc">\{</span>y_1,\dots ,y_n<span class="sc">\}</span>$ |</span>
<span id="cb10-653"><a href="#cb10-653"></a>| $n$ | Number of objects | $\lvert Y\rvert\in\mathbb N$ |</span>
<span id="cb10-654"><a href="#cb10-654"></a>| $y,y',y''$ | Generic objects in $Y$ | Elements of $Y$ |</span>
<span id="cb10-655"><a href="#cb10-655"></a>| $y_0$ | Outside (“no-choice”) option | Element of $Y$ (reference) |</span>
<span id="cb10-656"><a href="#cb10-656"></a>| $\prec$ / $\succ$ | Weak preference relation / its strict part | Binary relation on $Y$ |</span>
<span id="cb10-657"><a href="#cb10-657"></a>| $\mathord{\prec}$ | Random preference (draw of $\prec$) | RV over total orders on $Y$ |</span>
<span id="cb10-658"><a href="#cb10-658"></a>| $L=(y_1,\dots ,y_n)$ | Full ranking (preference list) | Permutation of $Y$ |</span>
<span id="cb10-659"><a href="#cb10-659"></a>| $(y,Y')$ | Observation that $y$ is chosen from subset $Y'$ | $y\in Y'\subseteq Y$ |</span>
<span id="cb10-660"><a href="#cb10-660"></a>| $x\in X$ | Exogenous context / features | $X$ (arbitrary feature space) |</span>
<span id="cb10-661"><a href="#cb10-661"></a>| $u(y)$ | Mean (deterministic) utility of $y$ | $\mathbb R$ |</span>
<span id="cb10-662"><a href="#cb10-662"></a>| $u_{\mathord{\prec}}(y)$ | Random utility in draw $\mathord{\prec}$ | $\mathbb R$ |</span>
<span id="cb10-663"><a href="#cb10-663"></a>| $\varepsilon_{\mathord{\prec}y}$ | Stochastic utility shock | $\mathbb R$ (i.i.d.) |</span>
<span id="cb10-664"><a href="#cb10-664"></a>| $\mathbb P<span class="co">[</span><span class="ot">\cdot</span><span class="co">]</span>$ | Probability measure over preferences/choices | $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ |</span>
<span id="cb10-665"><a href="#cb10-665"></a>| $\operatorname{softmax}_y\bigl((u(y'))_{y'\in Y'}\bigr)$ | Logit/Plackett-Luce choice probability of $y$ from $Y'$ | $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ |</span>
<span id="cb10-666"><a href="#cb10-666"></a>| $\sigma(z)$ | Sigmoid $1/(1+e^{-z})$ | $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ |</span>
<span id="cb10-667"><a href="#cb10-667"></a>| $d$ | Dimensionality of feature vectors | $\mathbb N$ |</span>
<span id="cb10-668"><a href="#cb10-668"></a>| $\boldsymbol{x}\in\mathbb R^{d}$ | Feature vector of an object | $\mathbb R^{d}$ |</span>
<span id="cb10-669"><a href="#cb10-669"></a>| $\text{Ackley}(\boldsymbol{x})$ | Ackley test-function value | $\mathbb R$ |</span>
<span id="cb10-670"><a href="#cb10-670"></a>| $a,b,c$ | Ackley parameters | Scalars |</span>
<span id="cb10-671"><a href="#cb10-671"></a>| $k$ | Number of preference samples | $\mathbb N$ |</span>
<span id="cb10-672"><a href="#cb10-672"></a>| $\alpha_i$ | Population weight of subgroup $i$ | $(0,1)$ with $\sum_i\alpha_i=1$ |</span>
<span id="cb10-673"><a href="#cb10-673"></a>| $\beta$ | Random-coefficients vector in linear RUM | $\mathbb R^{d}$ |</span>
<span id="cb10-674"><a href="#cb10-674"></a>| $\Sigma$ | Covariance matrix of $\beta$ | $\mathbb R^{d\times d}$ |</span>
<span id="cb10-675"><a href="#cb10-675"></a></span>
<span id="cb10-676"><a href="#cb10-676"></a>: **Table 1 — Notation used in Chapter “Background”.** {#tbl-notation}</span>
<span id="cb10-677"><a href="#cb10-677"></a></span>
<span id="cb10-678"><a href="#cb10-678"></a><span class="fu">## Discussion Questions</span></span>
<span id="cb10-679"><a href="#cb10-679"></a></span>
<span id="cb10-680"><a href="#cb10-680"></a><span class="ss">- </span>How does modeling preferences as **random** (rather than deterministic) help us capture real-world choice behavior?  </span>
<span id="cb10-681"><a href="#cb10-681"></a><span class="ss">- </span>What is the **Independence of Irrelevant Alternatives** (IIA) axiom, and why does it simplify the estimation of choice models?  </span>
<span id="cb10-682"><a href="#cb10-682"></a><span class="ss">- </span>Why do i.i.d. Gumbel shocks in a random utility model lead to the Plackett–Luce (list) and softmax/logit (choice) formulas?  </span>
<span id="cb10-683"><a href="#cb10-683"></a><span class="ss">- </span>In what ways can **binary comparisons**, **choice-from-a-set**, and **full rankings** each be seen as observations of the same underlying stochastic preference distribution?  </span>
<span id="cb10-684"><a href="#cb10-684"></a><span class="ss">- </span>What are the practical advantages and drawbacks of eliciting **full preference lists** versus **pairwise comparisons** from human subjects?  </span>
<span id="cb10-685"><a href="#cb10-685"></a><span class="ss">- </span>How does introducing an “outside option” $y_{0}$ allow us to interpret **accept–reject** data within the same logit framework?  </span>
<span id="cb10-686"><a href="#cb10-686"></a><span class="ss">- </span>Why does mixing multiple IIA-satisfying sub-populations generally **violate** IIA at the aggregate level?  </span>
<span id="cb10-687"><a href="#cb10-687"></a><span class="ss">- </span>Explain the “**red bus–blue bus**” problem: why does splitting a single alternative into two identical ones distort logit choice probabilities?  </span>
<span id="cb10-688"><a href="#cb10-688"></a><span class="ss">- </span>How does context $x$ enter the random utility framework, and what role does it play in generalizing preferences to new situations?  </span>
<span id="cb10-689"><a href="#cb10-689"></a><span class="ss">- </span>What identification issues arise from the fact that adding a constant to all utilities $u(y)$ does not change observable choice probabilities?  </span>
<span id="cb10-690"><a href="#cb10-690"></a><span class="ss">- </span>In what scenarios would you consider **relaxing** IIA and what additional model complexity does that introduce?</span>
<span id="cb10-691"><a href="#cb10-691"></a></span>
<span id="cb10-692"><a href="#cb10-692"></a><span class="fu">## Exercises</span></span>
<span id="cb10-693"><a href="#cb10-693"></a>We place ⭐, ⭐⭐, and ⭐⭐⭐ for exercises we deem relatively easy, medium, and hard, respectively.</span>
<span id="cb10-694"><a href="#cb10-694"></a></span>
<span id="cb10-695"><a href="#cb10-695"></a><span class="fu">### Properties of IIA Models ⭐</span></span>
<span id="cb10-696"><a href="#cb10-696"></a></span>
<span id="cb10-697"><a href="#cb10-697"></a>Prove that if a preference model satisfies IIA, it will also satisfy $\mathbb{P}<span class="co">[</span><span class="ot">(y, Y')</span><span class="co">]</span> \le \mathbb{P}<span class="co">[</span><span class="ot">(y, Y'')</span><span class="co">]</span>$ for any $y \in Y$ and $Y' \subseteq Y'' \subseteq Y$ (called regularity) and for all $(x,y,z)$, if $\mathbb{P}<span class="co">[</span><span class="ot">(x, \{x,y\})</span><span class="co">]</span> \ge 0.5$ and $\mathbb{P}<span class="co">[</span><span class="ot">(y, \{y,z\})</span><span class="co">]</span> \ge 0.5$, then necessarily $\mathbb{P}<span class="co">[</span><span class="ot">(x, \{x,z\})</span><span class="co">]</span> \ge 0.5$.</span>
<span id="cb10-698"><a href="#cb10-698"></a></span>
<span id="cb10-699"><a href="#cb10-699"></a><span class="fu">### Discrete Choice Models ⭐⭐</span></span>
<span id="cb10-700"><a href="#cb10-700"></a></span>
<span id="cb10-701"><a href="#cb10-701"></a>Consider a linear random utility model $u(y)=\beta_i^\top x+\epsilon_i$ for $i=1, 2, \cdots, N$, where $\varepsilon_y$ is i.i.d. sampled from a Gumbel distribution. We would like to compute $\mathbb{P}<span class="co">[</span><span class="ot">(y, Y)</span><span class="co">]</span>$ and connect it to multi-class logistic regression.</span>
<span id="cb10-702"><a href="#cb10-702"></a></span>
<span id="cb10-703"><a href="#cb10-703"></a>(a) First $\mathbb{P}<span class="co">[</span><span class="ot">u(y)&lt;t</span><span class="co">]</span>$ for any $ for $j\neq i$ in terms of $F$. Use this probability to provide a formula for $\mathbb{P}<span class="co">[</span><span class="ot">(y, Y)</span><span class="co">]</span>$ over $t$ in terms of $f$ and $F$.</span>
<span id="cb10-704"><a href="#cb10-704"></a></span>
<span id="cb10-705"><a href="#cb10-705"></a>(b) Compute the integral derived in part (a) with the appropriate $u$-substitution. You should arrive at the multi-class logistic regression model.</span>
<span id="cb10-706"><a href="#cb10-706"></a></span>
<span id="cb10-707"><a href="#cb10-707"></a><span class="fu">### Mixtures and correlations ⭐</span></span>
<span id="cb10-708"><a href="#cb10-708"></a>Prove that the class of random preferences induced by the following two are identical: (a) mixtures of IIA random utility models (that is, those with i.i.d. noise) (b) random utility models with correlated noise.</span>
<span id="cb10-709"><a href="#cb10-709"></a></span>
<span id="cb10-710"><a href="#cb10-710"></a><span class="fu">### Sufficient Statistics ⭐⭐⭐</span></span>
<span id="cb10-711"><a href="#cb10-711"></a>(a) Show that choice data completely specifies the preference model. That is, express $\mathbb{P}<span class="co">[</span><span class="ot">\mathord{\prec}</span><span class="co">]</span>$ for any $\prec$ in terms of $\mathbb{P}<span class="co">[</span><span class="ot">(y, Y')</span><span class="co">]</span>$, $y \in Y' \subseteq Y$.</span>
<span id="cb10-712"><a href="#cb10-712"></a></span>
<span id="cb10-713"><a href="#cb10-713"></a>(b) Shows that this is not the case for binary comparisons. That is, give an example of two different preference models that induce the same probabilities $\mathbb{P}<span class="co">[</span><span class="ot">y, \{x, y\}</span><span class="co">]</span>$. </span>
<span id="cb10-714"><a href="#cb10-714"></a></span>
<span id="cb10-715"><a href="#cb10-715"></a><span class="fu">### Non-Random Utility Models ⭐⭐⭐</span></span>
<span id="cb10-716"><a href="#cb10-716"></a>Not all probability assignments for binary comparisons $p_{y_1y_2} = \mathbb{P}<span class="co">[</span><span class="ot">y_1 \prec y_1</span><span class="co">]</span>$ can be realized with a random preference model. Give an example of binary comparisons $(p_{y_1y_2}, p_{y_2y_3}, p_{y_3y_1})$ that cannot be a result of a random preference model.</span>
<span id="cb10-717"><a href="#cb10-717"></a></span>
<span id="cb10-718"><a href="#cb10-718"></a><span class="fu">### Posterior Inference for Mixture Preferences ⭐⭐</span></span>
<span id="cb10-719"><a href="#cb10-719"></a></span>
<span id="cb10-720"><a href="#cb10-720"></a>(This exercise previews some of the aspects for learning utility functions from the next chapter but is self-contained.) You are part of the ML team on the movie streaming site "Preferential". You receive full preference orderings in the form $y_1 \succ y_2 \succ \cdots \succ y_n$, where $y_1$ is the most, and $y_n$ the least preferred option. The preferences come from $600$ distinct users with $50$ examples per user. Each movie has a $10$-dimensional feature vector $m_y$, and each user has a $10$-dimensional weight vector $v_i$. The preferences for user $i$ follow the random utility model $u(y) = v_i^\top m_y + \varepsilon_y$, where $\varepsilon_y$ is i.i.d. Gumbel distributed. </span>
<span id="cb10-721"><a href="#cb10-721"></a></span>
<span id="cb10-722"><a href="#cb10-722"></a>Sadly, you lost all user identifiers. Unashamedly, you assume a model where a proportion $p$ of the users have weights $w_1$, and a proportion $1-p$ has weights $w_2$. Each user belongs to one of two groups: users with weights $w_1$ are part of Group 1, and users with weights $w_2$ are part of Group 2.</span>
<span id="cb10-723"><a href="#cb10-723"></a></span>
<span id="cb10-724"><a href="#cb10-724"></a>(a) For a datapoint $(y_1 \succ y_2)$ with label and conditional on $p$, $w_1$ and $w_2$, compute the likelihood $P(y_1\succ y_2 | p, w_1, w_2)$. </span>
<span id="cb10-725"><a href="#cb10-725"></a></span>
<span id="cb10-726"><a href="#cb10-726"></a>(b) Use the likelihood to simplify the posterior distribution of $p, w_1, w_2$ after updating on $(m_1, m_2)$ leaving terms for the priors unchanged.</span>
<span id="cb10-727"><a href="#cb10-727"></a></span>
<span id="cb10-728"><a href="#cb10-728"></a>(c) Assume priors $p\sim B(1, 1)$, $w_1\sim N (0, \mathbf{I})$, and $w_2\sim N(0, \mathbf{I})$ where $B$ represents the Beta distribution and $\mathcal{N}$ represents the normal distribution (all three sampled independently). You will notice that the posterior from part (b) has no simple closed form, requiring numerical methods. One such method, allowing to approximate sample from the posterior $\pi$, is called Metropolis-Hastings. (The reason why one might want to sample from the posterior will be discussed in @sec-beyond.) Broadly, the idea of Metropolis-Hastings and similar, so-called Markov Chain Monte Carlo methods is the following: Construct a Markov chain $<span class="sc">\{</span>x_t<span class="sc">\}</span>_{t=1}^\infty$ which has as "ergodic" distribution given by your desired distribution.\footnote{That is, $x_{t}$ is independent of $(x_1, x_2, \dots, x_{t-2})$ conditional on $x_{t-1}$.} By properties of Markov chains, for $t \gg 0$, $x_t$ will be almost as good as sampled from the "ergodic" distribution. In Metropolis-Hastings, the distribution is a proposal $P$ for $x_{t+1}$ is made via sampling from a chosen probability kernel $Q(\bullet | x_t)$ (e.g., adding Gaussian noise). The acceptance probability of the proposal is given by</span>
<span id="cb10-729"><a href="#cb10-729"></a></span>
<span id="cb10-730"><a href="#cb10-730"></a>$$</span>
<span id="cb10-731"><a href="#cb10-731"></a>x_{t+1}=\begin{cases} \tilde Q(\bullet | x_t) &amp; \text{with probability } A, <span class="sc">\\</span> x_t &amp; \text{with probability } 1 - A. \end{cases}</span>
<span id="cb10-732"><a href="#cb10-732"></a>$$</span>
<span id="cb10-733"><a href="#cb10-733"></a>where </span>
<span id="cb10-734"><a href="#cb10-734"></a>$$</span>
<span id="cb10-735"><a href="#cb10-735"></a>A= \min \left( 1, \frac{\pi(\bullet )Q(x_t | \bullet )}{\pi(x_t)Q( \bullet | x_t)} \right).</span>
<span id="cb10-736"><a href="#cb10-736"></a>$$</span>
<span id="cb10-737"><a href="#cb10-737"></a>We will extract samples from the Markov chain after a "burn-in period", $(x_{T+1}, x_{T+2},\cdots, x_{N})$. </span>
<span id="cb10-738"><a href="#cb10-738"></a></span>
<span id="cb10-739"><a href="#cb10-739"></a>To build some intuition, suppose we have a biased coin that turns heads with probability $p_{\text{heads}}$. We observe $12$ coin flips to have $9$ heads (H) and $3$ tails (T). If our prior for $p_{\text{H}}$ was $B(1, 1)$, then, by properties of the Beta distribution, our posterior will be $B(1 + 9, 1 + 3)=B(10, 4)$. The Bayesian update is given by</span>
<span id="cb10-740"><a href="#cb10-740"></a></span>
<span id="cb10-741"><a href="#cb10-741"></a>$$</span>
<span id="cb10-742"><a href="#cb10-742"></a>p(p_{\text{H}}|9\text{H}, 3\text{T}) = \frac{p(9\text{H}, 3\text{T} | p_{\text{H}})B(1, 1)(p_{\text{H}})}{\int_0^1 P(9\text{H}, 3\text{T} | p_{\text{H}})B(1, 1)(p_{\text{H}}) \, \mathrm dp_{\text{H}}} =\frac{p(9\text{H}, 3\text{T} | p_{\text{H}})}{\int_0^1 p(9\text{H}, 3\text{T} | p_{\text{H}}) \, \mathrm dp_{\text{H}}}.</span>
<span id="cb10-743"><a href="#cb10-743"></a>$$</span>
<span id="cb10-744"><a href="#cb10-744"></a></span>
<span id="cb10-745"><a href="#cb10-745"></a>**Find the acceptance probability** $A$ in the setting of the biased coin assuming the proposal distribution $Q(\cdot|x_t)=x_t+N(0,\sigma)$ for given $\sigma$. Notice that this choice of $Q$ is symmetric, i.e., $Q(x_t|p)=Q(p|x_t)$ for all $p \in \mathbb R$. Note that it is unnecessary to compute the normalizing constant of the Bayesian update (i.e., the integral in the denominator). This simplification is one of the main practical advantages of Metropolis-Hastings.</span>
<span id="cb10-746"><a href="#cb10-746"></a></span>
<span id="cb10-747"><a href="#cb10-747"></a>(d) Implement Metropolis-Hastings to sample from the posterior distribution of the biased coin in <span class="in">`multimodal_preferences/biased_coin.py`</span>. Attach a histogram of your MCMC samples overlayed on top of the true posterior $B(10, 4)$ by running <span class="in">`python biased_coin.py`</span>.</span>
<span id="cb10-748"><a href="#cb10-748"></a></span>
<span id="cb10-749"><a href="#cb10-749"></a>::: {.callout-note title="Code"}</span>
<span id="cb10-750"><a href="#cb10-750"></a></span>
<span id="cb10-751"><a href="#cb10-751"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb10-752"><a href="#cb10-752"></a><span class="in">import numpy as np</span></span>
<span id="cb10-753"><a href="#cb10-753"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb10-754"><a href="#cb10-754"></a><span class="in">from scipy.stats import beta</span></span>
<span id="cb10-755"><a href="#cb10-755"></a></span>
<span id="cb10-756"><a href="#cb10-756"></a><span class="in">def likelihood(p: float) -&gt; float:</span></span>
<span id="cb10-757"><a href="#cb10-757"></a><span class="in">    """</span></span>
<span id="cb10-758"><a href="#cb10-758"></a><span class="in">    Computes the likelihood of 9 heads and 3 tails, assuming p_heads is p.</span></span>
<span id="cb10-759"><a href="#cb10-759"></a></span>
<span id="cb10-760"><a href="#cb10-760"></a><span class="in">    Args:</span></span>
<span id="cb10-761"><a href="#cb10-761"></a><span class="in">    p (float): A value between 0 and 1 representing the probability of heads.</span></span>
<span id="cb10-762"><a href="#cb10-762"></a></span>
<span id="cb10-763"><a href="#cb10-763"></a><span class="in">    Returns:</span></span>
<span id="cb10-764"><a href="#cb10-764"></a><span class="in">    float: The likelihood value at p_heads=p. Return 0 if p is outside the range [0, 1].</span></span>
<span id="cb10-765"><a href="#cb10-765"></a><span class="in">    """</span></span>
<span id="cb10-766"><a href="#cb10-766"></a><span class="in">    # YOUR CODE HERE (~1-3 lines)</span></span>
<span id="cb10-767"><a href="#cb10-767"></a><span class="in">    pass</span></span>
<span id="cb10-768"><a href="#cb10-768"></a><span class="in">    # END OF YOUR CODE</span></span>
<span id="cb10-769"><a href="#cb10-769"></a></span>
<span id="cb10-770"><a href="#cb10-770"></a></span>
<span id="cb10-771"><a href="#cb10-771"></a><span class="in">def propose(x_current: float, sigma: float) -&gt; float:</span></span>
<span id="cb10-772"><a href="#cb10-772"></a><span class="in">    """</span></span>
<span id="cb10-773"><a href="#cb10-773"></a><span class="in">    Proposes a new sample from the proposal distribution Q.</span></span>
<span id="cb10-774"><a href="#cb10-774"></a><span class="in">    Here, Q is a normal distribution centered at x_current with standard deviation sigma.</span></span>
<span id="cb10-775"><a href="#cb10-775"></a></span>
<span id="cb10-776"><a href="#cb10-776"></a><span class="in">    Args:</span></span>
<span id="cb10-777"><a href="#cb10-777"></a><span class="in">    x_current (float): The current value in the Markov chain.</span></span>
<span id="cb10-778"><a href="#cb10-778"></a><span class="in">    sigma (float): Standard deviation of the normal proposal distribution.</span></span>
<span id="cb10-779"><a href="#cb10-779"></a></span>
<span id="cb10-780"><a href="#cb10-780"></a><span class="in">    Returns:</span></span>
<span id="cb10-781"><a href="#cb10-781"></a><span class="in">    float: The proposed new sample.</span></span>
<span id="cb10-782"><a href="#cb10-782"></a><span class="in">    """</span></span>
<span id="cb10-783"><a href="#cb10-783"></a><span class="in">    # YOUR CODE HERE (~1-3 lines)</span></span>
<span id="cb10-784"><a href="#cb10-784"></a><span class="in">    pass</span></span>
<span id="cb10-785"><a href="#cb10-785"></a><span class="in">    # END OF YOUR CODE</span></span>
<span id="cb10-786"><a href="#cb10-786"></a></span>
<span id="cb10-787"><a href="#cb10-787"></a></span>
<span id="cb10-788"><a href="#cb10-788"></a><span class="in">def acceptance_probability(x_current: float, x_proposed: float) -&gt; float:</span></span>
<span id="cb10-789"><a href="#cb10-789"></a><span class="in">    """</span></span>
<span id="cb10-790"><a href="#cb10-790"></a><span class="in">    Computes the acceptance probability A for the proposed sample.</span></span>
<span id="cb10-791"><a href="#cb10-791"></a><span class="in">    Since the proposal distribution is symmetric, Q cancels out.</span></span>
<span id="cb10-792"><a href="#cb10-792"></a></span>
<span id="cb10-793"><a href="#cb10-793"></a><span class="in">    Args:</span></span>
<span id="cb10-794"><a href="#cb10-794"></a><span class="in">    x_current (float): The current value in the Markov chain.</span></span>
<span id="cb10-795"><a href="#cb10-795"></a><span class="in">    x_proposed (float): The proposed new value.</span></span>
<span id="cb10-796"><a href="#cb10-796"></a></span>
<span id="cb10-797"><a href="#cb10-797"></a><span class="in">    Returns:</span></span>
<span id="cb10-798"><a href="#cb10-798"></a><span class="in">    float: The acceptance probability</span></span>
<span id="cb10-799"><a href="#cb10-799"></a><span class="in">    """</span></span>
<span id="cb10-800"><a href="#cb10-800"></a><span class="in">    # YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb10-801"><a href="#cb10-801"></a><span class="in">    pass</span></span>
<span id="cb10-802"><a href="#cb10-802"></a><span class="in">    # END OF YOUR CODE</span></span>
<span id="cb10-803"><a href="#cb10-803"></a></span>
<span id="cb10-804"><a href="#cb10-804"></a></span>
<span id="cb10-805"><a href="#cb10-805"></a><span class="in">def metropolis_hastings(N: int, T: int, x_init: float, sigma: float) -&gt; np.ndarray:</span></span>
<span id="cb10-806"><a href="#cb10-806"></a><span class="in">    """</span></span>
<span id="cb10-807"><a href="#cb10-807"></a><span class="in">    Runs the Metropolis-Hastings algorithm to sample from a posterior distribution.</span></span>
<span id="cb10-808"><a href="#cb10-808"></a></span>
<span id="cb10-809"><a href="#cb10-809"></a><span class="in">    Args:</span></span>
<span id="cb10-810"><a href="#cb10-810"></a><span class="in">    N (int): Total number of iterations.</span></span>
<span id="cb10-811"><a href="#cb10-811"></a><span class="in">    T (int): Burn-in period (number of initial samples to discard).</span></span>
<span id="cb10-812"><a href="#cb10-812"></a><span class="in">    x_init (float): Initial value of the chain.</span></span>
<span id="cb10-813"><a href="#cb10-813"></a><span class="in">    sigma (float): Standard deviation of the proposal distribution.</span></span>
<span id="cb10-814"><a href="#cb10-814"></a></span>
<span id="cb10-815"><a href="#cb10-815"></a><span class="in">    Returns:</span></span>
<span id="cb10-816"><a href="#cb10-816"></a><span class="in">    list: Samples collected after the burn-in period.</span></span>
<span id="cb10-817"><a href="#cb10-817"></a><span class="in">    """</span></span>
<span id="cb10-818"><a href="#cb10-818"></a><span class="in">    samples = []</span></span>
<span id="cb10-819"><a href="#cb10-819"></a><span class="in">    x_current = x_init</span></span>
<span id="cb10-820"><a href="#cb10-820"></a></span>
<span id="cb10-821"><a href="#cb10-821"></a><span class="in">    for t in range(N):</span></span>
<span id="cb10-822"><a href="#cb10-822"></a><span class="in">        # YOUR CODE HERE (~7-10 lines)</span></span>
<span id="cb10-823"><a href="#cb10-823"></a><span class="in">        # Use the propose and acceptance_probability functions to get x_{t+1} and store it in samples after the burn-in period T</span></span>
<span id="cb10-824"><a href="#cb10-824"></a><span class="in">        pass</span></span>
<span id="cb10-825"><a href="#cb10-825"></a><span class="in">        # END OF YOUR CODE</span></span>
<span id="cb10-826"><a href="#cb10-826"></a></span>
<span id="cb10-827"><a href="#cb10-827"></a><span class="in">    return samples</span></span>
<span id="cb10-828"><a href="#cb10-828"></a></span>
<span id="cb10-829"><a href="#cb10-829"></a></span>
<span id="cb10-830"><a href="#cb10-830"></a><span class="in">def plot_results(samples: np.ndarray) -&gt; None:</span></span>
<span id="cb10-831"><a href="#cb10-831"></a><span class="in">    """</span></span>
<span id="cb10-832"><a href="#cb10-832"></a><span class="in">    Plots the histogram of MCMC samples along with the true Beta(10, 4) PDF.</span></span>
<span id="cb10-833"><a href="#cb10-833"></a></span>
<span id="cb10-834"><a href="#cb10-834"></a><span class="in">    Args:</span></span>
<span id="cb10-835"><a href="#cb10-835"></a><span class="in">    samples (np.ndarray): Array of samples collected from the Metropolis-Hastings algorithm.</span></span>
<span id="cb10-836"><a href="#cb10-836"></a></span>
<span id="cb10-837"><a href="#cb10-837"></a><span class="in">    Returns:</span></span>
<span id="cb10-838"><a href="#cb10-838"></a><span class="in">    None</span></span>
<span id="cb10-839"><a href="#cb10-839"></a><span class="in">    """</span></span>
<span id="cb10-840"><a href="#cb10-840"></a><span class="in">    # Histogram of the samples from the Metropolis-Hastings algorithm</span></span>
<span id="cb10-841"><a href="#cb10-841"></a><span class="in">    plt.hist(samples, bins=50, density=True, alpha=0.5, label="MCMC Samples")</span></span>
<span id="cb10-842"><a href="#cb10-842"></a></span>
<span id="cb10-843"><a href="#cb10-843"></a><span class="in">    # True Beta(10, 4) distribution for comparison</span></span>
<span id="cb10-844"><a href="#cb10-844"></a><span class="in">    p = np.linspace(0, 1, 1000)</span></span>
<span id="cb10-845"><a href="#cb10-845"></a><span class="in">    beta_pdf = beta.pdf(p, 10, 4)</span></span>
<span id="cb10-846"><a href="#cb10-846"></a><span class="in">    plt.plot(p, beta_pdf, "r-", label="Beta(10, 4) PDF")</span></span>
<span id="cb10-847"><a href="#cb10-847"></a></span>
<span id="cb10-848"><a href="#cb10-848"></a><span class="in">    plt.xlabel("p_heads")</span></span>
<span id="cb10-849"><a href="#cb10-849"></a><span class="in">    plt.ylabel("Density")</span></span>
<span id="cb10-850"><a href="#cb10-850"></a><span class="in">    plt.title("Metropolis-Hastings Sampling of Biased Coin Posterior")</span></span>
<span id="cb10-851"><a href="#cb10-851"></a><span class="in">    plt.legend()</span></span>
<span id="cb10-852"><a href="#cb10-852"></a><span class="in">    plt.show()</span></span>
<span id="cb10-853"><a href="#cb10-853"></a></span>
<span id="cb10-854"><a href="#cb10-854"></a></span>
<span id="cb10-855"><a href="#cb10-855"></a><span class="in">if __name__ == "__main__":</span></span>
<span id="cb10-856"><a href="#cb10-856"></a><span class="in">    # MCMC Parameters (DO NOT CHANGE!)</span></span>
<span id="cb10-857"><a href="#cb10-857"></a><span class="in">    N = 50000  # Total number of iterations</span></span>
<span id="cb10-858"><a href="#cb10-858"></a><span class="in">    T = 10000  # Burn-in period to discard</span></span>
<span id="cb10-859"><a href="#cb10-859"></a><span class="in">    x_init = 0.5  # Initial guess for p_heads</span></span>
<span id="cb10-860"><a href="#cb10-860"></a><span class="in">    sigma = 0.1  # Standard deviation of the proposal distribution</span></span>
<span id="cb10-861"><a href="#cb10-861"></a></span>
<span id="cb10-862"><a href="#cb10-862"></a><span class="in">    # Run Metropolis-Hastings and plot the results</span></span>
<span id="cb10-863"><a href="#cb10-863"></a><span class="in">    samples = metropolis_hastings(N, T, x_init, sigma)</span></span>
<span id="cb10-864"><a href="#cb10-864"></a><span class="in">    plot_results(samples)</span></span>
<span id="cb10-865"><a href="#cb10-865"></a><span class="in">```</span></span>
<span id="cb10-866"><a href="#cb10-866"></a></span>
<span id="cb10-867"><a href="#cb10-867"></a>:::</span>
<span id="cb10-868"><a href="#cb10-868"></a></span>
<span id="cb10-869"><a href="#cb10-869"></a>(e) Implement Metropolis-Hastings in the movie setting inside\ <span class="in">`multimodal_preferences/movie_metropolis.py`</span>. You should be able to achieve a $90\%$ success rate with most <span class="in">`fraction_accepted`</span> values above $0.1$. Success is measured by thresholded closeness of predicted parameters to true parameters. You may notice occasional failures that occur due to lack of convergence which we will account for in grading.</span>
<span id="cb10-870"><a href="#cb10-870"></a></span>
<span id="cb10-871"><a href="#cb10-871"></a>::: {.callout-note title="Code"}</span>
<span id="cb10-872"><a href="#cb10-872"></a></span>
<span id="cb10-873"><a href="#cb10-873"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb10-874"><a href="#cb10-874"></a><span class="in">import torch</span></span>
<span id="cb10-875"><a href="#cb10-875"></a><span class="in">import torch.distributions as dist</span></span>
<span id="cb10-876"><a href="#cb10-876"></a><span class="in">import math</span></span>
<span id="cb10-877"><a href="#cb10-877"></a><span class="in">from tqdm import tqdm</span></span>
<span id="cb10-878"><a href="#cb10-878"></a><span class="in">from typing import Tuple</span></span>
<span id="cb10-879"><a href="#cb10-879"></a></span>
<span id="cb10-880"><a href="#cb10-880"></a><span class="in">def make_data(</span></span>
<span id="cb10-881"><a href="#cb10-881"></a><span class="in">    true_p: torch.Tensor, true_weights_1: torch.Tensor, true_weights_2: torch.Tensor, num_movies: int, feature_dim: int</span></span>
<span id="cb10-882"><a href="#cb10-882"></a><span class="in">) -&gt; Tuple[torch.Tensor, torch.Tensor]:</span></span>
<span id="cb10-883"><a href="#cb10-883"></a><span class="in">    """</span></span>
<span id="cb10-884"><a href="#cb10-884"></a><span class="in">    Generates a synthetic movie dataset according to the CardinalStreams model.</span></span>
<span id="cb10-885"><a href="#cb10-885"></a></span>
<span id="cb10-886"><a href="#cb10-886"></a><span class="in">    Args:</span></span>
<span id="cb10-887"><a href="#cb10-887"></a><span class="in">        true_p (torch.Tensor): Probability of coming from Group 1.</span></span>
<span id="cb10-888"><a href="#cb10-888"></a><span class="in">        true_weights_1 (torch.Tensor): Weights for Group 1.</span></span>
<span id="cb10-889"><a href="#cb10-889"></a><span class="in">        true_weights_2 (torch.Tensor): Weights for Group 2.</span></span>
<span id="cb10-890"><a href="#cb10-890"></a></span>
<span id="cb10-891"><a href="#cb10-891"></a><span class="in">    Returns:</span></span>
<span id="cb10-892"><a href="#cb10-892"></a><span class="in">        Tuple[torch.Tensor, torch.Tensor]: A tuple containing the dataset and labels.</span></span>
<span id="cb10-893"><a href="#cb10-893"></a><span class="in">    """</span></span>
<span id="cb10-894"><a href="#cb10-894"></a><span class="in">    # Create movie features</span></span>
<span id="cb10-895"><a href="#cb10-895"></a><span class="in">    first_movie_features = torch.randn((num_movies, feature_dim))</span></span>
<span id="cb10-896"><a href="#cb10-896"></a><span class="in">    second_movie_features = torch.randn((num_movies, feature_dim))</span></span>
<span id="cb10-897"><a href="#cb10-897"></a></span>
<span id="cb10-898"><a href="#cb10-898"></a><span class="in">    # Only care about difference of features for Bradley-Terry</span></span>
<span id="cb10-899"><a href="#cb10-899"></a><span class="in">    dataset = first_movie_features - second_movie_features</span></span>
<span id="cb10-900"><a href="#cb10-900"></a></span>
<span id="cb10-901"><a href="#cb10-901"></a><span class="in">    # Get probabilities that first movie is preferred assuming Group 1 or Group 2</span></span>
<span id="cb10-902"><a href="#cb10-902"></a><span class="in">    weight_1_probs = torch.sigmoid(dataset @ true_weights_1)</span></span>
<span id="cb10-903"><a href="#cb10-903"></a><span class="in">    weight_2_probs = torch.sigmoid(dataset @ true_weights_2)</span></span>
<span id="cb10-904"><a href="#cb10-904"></a></span>
<span id="cb10-905"><a href="#cb10-905"></a><span class="in">    # Probability that first movie is preferred overall can be viewed as sum of conditioning on Group 1 and Group 2</span></span>
<span id="cb10-906"><a href="#cb10-906"></a><span class="in">    first_movie_preferred_probs = (</span></span>
<span id="cb10-907"><a href="#cb10-907"></a><span class="in">        true_p * weight_1_probs + (1 - true_p) * weight_2_probs</span></span>
<span id="cb10-908"><a href="#cb10-908"></a><span class="in">    )</span></span>
<span id="cb10-909"><a href="#cb10-909"></a><span class="in">    labels = dist.Bernoulli(first_movie_preferred_probs).sample()</span></span>
<span id="cb10-910"><a href="#cb10-910"></a><span class="in">    return dataset, labels</span></span>
<span id="cb10-911"><a href="#cb10-911"></a></span>
<span id="cb10-912"><a href="#cb10-912"></a></span>
<span id="cb10-913"><a href="#cb10-913"></a><span class="in">def compute_likelihoods(</span></span>
<span id="cb10-914"><a href="#cb10-914"></a><span class="in">    dataset: torch.Tensor,</span></span>
<span id="cb10-915"><a href="#cb10-915"></a><span class="in">    labels: torch.Tensor,</span></span>
<span id="cb10-916"><a href="#cb10-916"></a><span class="in">    p: torch.Tensor,</span></span>
<span id="cb10-917"><a href="#cb10-917"></a><span class="in">    w_1: torch.Tensor,</span></span>
<span id="cb10-918"><a href="#cb10-918"></a><span class="in">    w_2: torch.Tensor,</span></span>
<span id="cb10-919"><a href="#cb10-919"></a><span class="in">) -&gt; torch.Tensor:</span></span>
<span id="cb10-920"><a href="#cb10-920"></a><span class="in">    """</span></span>
<span id="cb10-921"><a href="#cb10-921"></a><span class="in">    Computes the likelihood of each datapoint. Use your calculation from part (a) to help.</span></span>
<span id="cb10-922"><a href="#cb10-922"></a></span>
<span id="cb10-923"><a href="#cb10-923"></a><span class="in">    Args:</span></span>
<span id="cb10-924"><a href="#cb10-924"></a><span class="in">        dataset (torch.Tensor): The dataset of differences between movie features.</span></span>
<span id="cb10-925"><a href="#cb10-925"></a><span class="in">        labels (torch.Tensor): The labels where 1 indicates the first movie is preferred, and 0 indicates preference of the second movie.</span></span>
<span id="cb10-926"><a href="#cb10-926"></a><span class="in">        p (torch.Tensor): The probability of coming from Group 1.</span></span>
<span id="cb10-927"><a href="#cb10-927"></a><span class="in">        w_1 (torch.Tensor): Weights for Group 1.</span></span>
<span id="cb10-928"><a href="#cb10-928"></a><span class="in">        w_2 (torch.Tensor): Weights for Group 2.</span></span>
<span id="cb10-929"><a href="#cb10-929"></a></span>
<span id="cb10-930"><a href="#cb10-930"></a><span class="in">    Returns:</span></span>
<span id="cb10-931"><a href="#cb10-931"></a><span class="in">        torch.Tensor: The likelihoods for each datapoint. Should have shape (dataset.shape[0], )</span></span>
<span id="cb10-932"><a href="#cb10-932"></a><span class="in">    """</span></span>
<span id="cb10-933"><a href="#cb10-933"></a><span class="in">    # YOUR CODE HERE (~6-8 lines)</span></span>
<span id="cb10-934"><a href="#cb10-934"></a><span class="in">    pass</span></span>
<span id="cb10-935"><a href="#cb10-935"></a><span class="in">    # END OF YOUR CODE</span></span>
<span id="cb10-936"><a href="#cb10-936"></a></span>
<span id="cb10-937"><a href="#cb10-937"></a><span class="in">def compute_prior_density(</span></span>
<span id="cb10-938"><a href="#cb10-938"></a><span class="in">    p: torch.Tensor, w_1: torch.Tensor, w_2: torch.Tensor</span></span>
<span id="cb10-939"><a href="#cb10-939"></a><span class="in">) -&gt; torch.Tensor:</span></span>
<span id="cb10-940"><a href="#cb10-940"></a><span class="in">    """</span></span>
<span id="cb10-941"><a href="#cb10-941"></a><span class="in">    Computes the prior density of the parameters.</span></span>
<span id="cb10-942"><a href="#cb10-942"></a></span>
<span id="cb10-943"><a href="#cb10-943"></a><span class="in">    Args:</span></span>
<span id="cb10-944"><a href="#cb10-944"></a><span class="in">        p (torch.Tensor): The probability of preferring model 1.</span></span>
<span id="cb10-945"><a href="#cb10-945"></a><span class="in">        w_1 (torch.Tensor): Weights for model 1.</span></span>
<span id="cb10-946"><a href="#cb10-946"></a><span class="in">        w_2 (torch.Tensor): Weights for model 2.</span></span>
<span id="cb10-947"><a href="#cb10-947"></a></span>
<span id="cb10-948"><a href="#cb10-948"></a><span class="in">    Returns:</span></span>
<span id="cb10-949"><a href="#cb10-949"></a><span class="in">        torch.Tensor: The prior densities of p, w_1, and w_2.</span></span>
<span id="cb10-950"><a href="#cb10-950"></a><span class="in">    """</span></span>
<span id="cb10-951"><a href="#cb10-951"></a><span class="in">    # Adjusts p to stay in the range [0.3, 0.7] to prevent multiple equilibria issues at p=0 and p=1</span></span>
<span id="cb10-952"><a href="#cb10-952"></a><span class="in">    p_prob = torch.tensor([2.5]) if 0.3 &lt;= p &lt;= 0.7 else torch.tensor([0.0])</span></span>
<span id="cb10-953"><a href="#cb10-953"></a></span>
<span id="cb10-954"><a href="#cb10-954"></a><span class="in">    def normal_pdf(x: torch.Tensor) -&gt; torch.Tensor:</span></span>
<span id="cb10-955"><a href="#cb10-955"></a><span class="in">        """Computes the PDF of the standard normal distribution at x."""</span></span>
<span id="cb10-956"><a href="#cb10-956"></a><span class="in">        return (1.0 / torch.sqrt(torch.tensor(2 * math.pi))) * torch.exp(-0.5 * x**2)</span></span>
<span id="cb10-957"><a href="#cb10-957"></a></span>
<span id="cb10-958"><a href="#cb10-958"></a><span class="in">    weights_1_prob = normal_pdf(w_1)</span></span>
<span id="cb10-959"><a href="#cb10-959"></a><span class="in">    weights_2_prob = normal_pdf(w_2)</span></span>
<span id="cb10-960"><a href="#cb10-960"></a></span>
<span id="cb10-961"><a href="#cb10-961"></a><span class="in">    # Concatenate the densities</span></span>
<span id="cb10-962"><a href="#cb10-962"></a><span class="in">    concatenated_prob = torch.cat([p_prob, weights_1_prob, weights_2_prob])</span></span>
<span id="cb10-963"><a href="#cb10-963"></a><span class="in">    return concatenated_prob</span></span>
<span id="cb10-964"><a href="#cb10-964"></a></span>
<span id="cb10-965"><a href="#cb10-965"></a></span>
<span id="cb10-966"><a href="#cb10-966"></a><span class="in">def metropolis_hastings(</span></span>
<span id="cb10-967"><a href="#cb10-967"></a><span class="in">    dataset: torch.Tensor,</span></span>
<span id="cb10-968"><a href="#cb10-968"></a><span class="in">    labels: torch.Tensor,</span></span>
<span id="cb10-969"><a href="#cb10-969"></a><span class="in">    sigma: float = 0.01,</span></span>
<span id="cb10-970"><a href="#cb10-970"></a><span class="in">    num_iters: int = 30000,</span></span>
<span id="cb10-971"><a href="#cb10-971"></a><span class="in">    burn_in: int = 20000,</span></span>
<span id="cb10-972"><a href="#cb10-972"></a><span class="in">) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]:</span></span>
<span id="cb10-973"><a href="#cb10-973"></a><span class="in">    """</span></span>
<span id="cb10-974"><a href="#cb10-974"></a><span class="in">    Performs the Metropolis-Hastings algorithm to sample from the posterior distribution.</span></span>
<span id="cb10-975"><a href="#cb10-975"></a><span class="in">    DO NOT CHANGE THE DEFAULT VALUES!</span></span>
<span id="cb10-976"><a href="#cb10-976"></a></span>
<span id="cb10-977"><a href="#cb10-977"></a><span class="in">    Args:</span></span>
<span id="cb10-978"><a href="#cb10-978"></a><span class="in">        dataset (torch.Tensor): The dataset of differences between movie features.</span></span>
<span id="cb10-979"><a href="#cb10-979"></a><span class="in">        labels (torch.Tensor): The labels indicating which movie is preferred.</span></span>
<span id="cb10-980"><a href="#cb10-980"></a><span class="in">        sigma (float, optional): Standard deviation for proposal distribution.</span></span>
<span id="cb10-981"><a href="#cb10-981"></a><span class="in">            Defaults to 0.01.</span></span>
<span id="cb10-982"><a href="#cb10-982"></a><span class="in">        num_iters (int, optional): Total number of iterations. Defaults to 30000.</span></span>
<span id="cb10-983"><a href="#cb10-983"></a><span class="in">        burn_in (int, optional): Number of iterations to discard as burn-in.</span></span>
<span id="cb10-984"><a href="#cb10-984"></a><span class="in">            Defaults to 20000.</span></span>
<span id="cb10-985"><a href="#cb10-985"></a></span>
<span id="cb10-986"><a href="#cb10-986"></a><span class="in">    Returns:</span></span>
<span id="cb10-987"><a href="#cb10-987"></a><span class="in">        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]: Samples of p,</span></span>
<span id="cb10-988"><a href="#cb10-988"></a><span class="in">        w_1, w_2, and the fraction of accepted proposals.</span></span>
<span id="cb10-989"><a href="#cb10-989"></a><span class="in">    """</span></span>
<span id="cb10-990"><a href="#cb10-990"></a><span class="in">    feature_dim = dataset.shape[1]</span></span>
<span id="cb10-991"><a href="#cb10-991"></a></span>
<span id="cb10-992"><a href="#cb10-992"></a><span class="in">    # Initialize random starting parameters by sampling priors</span></span>
<span id="cb10-993"><a href="#cb10-993"></a><span class="in">    curr_p = 0.3 + 0.4 * torch.rand(1)</span></span>
<span id="cb10-994"><a href="#cb10-994"></a><span class="in">    curr_w_1 = torch.randn(feature_dim)</span></span>
<span id="cb10-995"><a href="#cb10-995"></a><span class="in">    curr_w_2 = torch.randn(feature_dim)</span></span>
<span id="cb10-996"><a href="#cb10-996"></a></span>
<span id="cb10-997"><a href="#cb10-997"></a><span class="in">    # Keep track of samples and total number of accepted proposals</span></span>
<span id="cb10-998"><a href="#cb10-998"></a><span class="in">    p_samples = []</span></span>
<span id="cb10-999"><a href="#cb10-999"></a><span class="in">    w_1_samples = []</span></span>
<span id="cb10-1000"><a href="#cb10-1000"></a><span class="in">    w_2_samples = []</span></span>
<span id="cb10-1001"><a href="#cb10-1001"></a><span class="in">    accept_count = 0 </span></span>
<span id="cb10-1002"><a href="#cb10-1002"></a></span>
<span id="cb10-1003"><a href="#cb10-1003"></a><span class="in">    for T in tqdm(range(num_iters)):</span></span>
<span id="cb10-1004"><a href="#cb10-1004"></a><span class="in">        # YOUR CODE HERE (~3 lines)</span></span>
<span id="cb10-1005"><a href="#cb10-1005"></a><span class="in">        pass # Sample proposals for p, w_1, w_2</span></span>
<span id="cb10-1006"><a href="#cb10-1006"></a><span class="in">        # END OF YOUR CODE</span></span>
<span id="cb10-1007"><a href="#cb10-1007"></a></span>
<span id="cb10-1008"><a href="#cb10-1008"></a><span class="in">        # YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb10-1009"><a href="#cb10-1009"></a><span class="in">        pass # Compute likehoods and prior densities on both the proposed and current samples</span></span>
<span id="cb10-1010"><a href="#cb10-1010"></a><span class="in">        # END OF YOUR CODE</span></span>
<span id="cb10-1011"><a href="#cb10-1011"></a></span>
<span id="cb10-1012"><a href="#cb10-1012"></a><span class="in">        # YOUR CODE HERE (~2-4 lines)</span></span>
<span id="cb10-1013"><a href="#cb10-1013"></a><span class="in">        pass # Obtain the ratios of the likelihoods and prior densities between the proposed and current samples </span></span>
<span id="cb10-1014"><a href="#cb10-1014"></a><span class="in">        # END OF YOUR CODE </span></span>
<span id="cb10-1015"><a href="#cb10-1015"></a></span>
<span id="cb10-1016"><a href="#cb10-1016"></a><span class="in">        # YOUR CODE HERE (~1-2 lines)</span></span>
<span id="cb10-1017"><a href="#cb10-1017"></a><span class="in">        pass # Multiply all ratios (both likelihoods and prior densities) and use this to calculate the acceptance probability of the proposal</span></span>
<span id="cb10-1018"><a href="#cb10-1018"></a><span class="in">        # END OF YOUR CODE</span></span>
<span id="cb10-1019"><a href="#cb10-1019"></a></span>
<span id="cb10-1020"><a href="#cb10-1020"></a><span class="in">        # YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb10-1021"><a href="#cb10-1021"></a><span class="in">        pass # Sample randomness to determine whether the proposal should be accepted to update curr_p, curr_w_1, curr_w_2, and accept_count</span></span>
<span id="cb10-1022"><a href="#cb10-1022"></a><span class="in">        # END OF YOUR CODE </span></span>
<span id="cb10-1023"><a href="#cb10-1023"></a></span>
<span id="cb10-1024"><a href="#cb10-1024"></a><span class="in">        # YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb10-1025"><a href="#cb10-1025"></a><span class="in">        pass # Update p_samples, w_1_samples, w_2_samples if we have passed the burn in period T</span></span>
<span id="cb10-1026"><a href="#cb10-1026"></a><span class="in">        # END OF YOUR CODE </span></span>
<span id="cb10-1027"><a href="#cb10-1027"></a></span>
<span id="cb10-1028"><a href="#cb10-1028"></a><span class="in">    fraction_accepted = accept_count / num_iters</span></span>
<span id="cb10-1029"><a href="#cb10-1029"></a><span class="in">    print(f"Fraction of accepted proposals: {fraction_accepted}")</span></span>
<span id="cb10-1030"><a href="#cb10-1030"></a><span class="in">    return (</span></span>
<span id="cb10-1031"><a href="#cb10-1031"></a><span class="in">        torch.stack(p_samples),</span></span>
<span id="cb10-1032"><a href="#cb10-1032"></a><span class="in">        torch.stack(w_1_samples),</span></span>
<span id="cb10-1033"><a href="#cb10-1033"></a><span class="in">        torch.stack(w_2_samples),</span></span>
<span id="cb10-1034"><a href="#cb10-1034"></a><span class="in">        fraction_accepted,</span></span>
<span id="cb10-1035"><a href="#cb10-1035"></a><span class="in">    )</span></span>
<span id="cb10-1036"><a href="#cb10-1036"></a></span>
<span id="cb10-1037"><a href="#cb10-1037"></a></span>
<span id="cb10-1038"><a href="#cb10-1038"></a><span class="in">def evaluate_metropolis(num_sims: int, num_movies: int, feature_dim: int) -&gt; None:</span></span>
<span id="cb10-1039"><a href="#cb10-1039"></a><span class="in">    """</span></span>
<span id="cb10-1040"><a href="#cb10-1040"></a><span class="in">    Runs the Metropolis-Hastings algorithm N times and compare estimated parameters</span></span>
<span id="cb10-1041"><a href="#cb10-1041"></a><span class="in">    with true parameters to obtain success rate. You should attain a success rate of around 90%. </span></span>
<span id="cb10-1042"><a href="#cb10-1042"></a></span>
<span id="cb10-1043"><a href="#cb10-1043"></a><span class="in">    Note that there are two successful equilibria to converge to. They are true_weights_1 and true_weights_2 with probabilities</span></span>
<span id="cb10-1044"><a href="#cb10-1044"></a><span class="in">    p and 1-p in addition to true_weights_2 and true_weights_1 with probabilities 1-p and p. This is why even though it may appear your</span></span>
<span id="cb10-1045"><a href="#cb10-1045"></a><span class="in">    predicted parameters don't match the true parameters, they are in fact equivalent. </span></span>
<span id="cb10-1046"><a href="#cb10-1046"></a></span>
<span id="cb10-1047"><a href="#cb10-1047"></a><span class="in">    Args:</span></span>
<span id="cb10-1048"><a href="#cb10-1048"></a><span class="in">        num_sims (int): Number of simulations to run.</span></span>
<span id="cb10-1049"><a href="#cb10-1049"></a></span>
<span id="cb10-1050"><a href="#cb10-1050"></a><span class="in">    Returns:</span></span>
<span id="cb10-1051"><a href="#cb10-1051"></a><span class="in">        None</span></span>
<span id="cb10-1052"><a href="#cb10-1052"></a><span class="in">    """</span></span>
<span id="cb10-1053"><a href="#cb10-1053"></a><span class="in">    </span></span>
<span id="cb10-1054"><a href="#cb10-1054"></a><span class="in">    success_count = 0</span></span>
<span id="cb10-1055"><a href="#cb10-1055"></a><span class="in">    for _ in range(num_sims):</span></span>
<span id="cb10-1056"><a href="#cb10-1056"></a><span class="in">        # Sample random ground truth parameters</span></span>
<span id="cb10-1057"><a href="#cb10-1057"></a><span class="in">        true_p = 0.3 + 0.4 * torch.rand(1)</span></span>
<span id="cb10-1058"><a href="#cb10-1058"></a><span class="in">        true_weights_1 = torch.randn(feature_dim)</span></span>
<span id="cb10-1059"><a href="#cb10-1059"></a><span class="in">        true_weights_2 = torch.randn(feature_dim)</span></span>
<span id="cb10-1060"><a href="#cb10-1060"></a></span>
<span id="cb10-1061"><a href="#cb10-1061"></a><span class="in">        print("\n---- MCMC Simulation ----")</span></span>
<span id="cb10-1062"><a href="#cb10-1062"></a><span class="in">        print("True parameters:", true_p, true_weights_1, true_weights_2)</span></span>
<span id="cb10-1063"><a href="#cb10-1063"></a></span>
<span id="cb10-1064"><a href="#cb10-1064"></a><span class="in">        dataset, labels = make_data(true_p, true_weights_1, true_weights_2, num_movies, feature_dim)</span></span>
<span id="cb10-1065"><a href="#cb10-1065"></a><span class="in">        p_samples, w_1_samples, w_2_samples, _ = metropolis_hastings(dataset, labels)</span></span>
<span id="cb10-1066"><a href="#cb10-1066"></a></span>
<span id="cb10-1067"><a href="#cb10-1067"></a><span class="in">        p_pred = p_samples.mean(dim=0)</span></span>
<span id="cb10-1068"><a href="#cb10-1068"></a><span class="in">        w_1_pred = w_1_samples.mean(dim=0)</span></span>
<span id="cb10-1069"><a href="#cb10-1069"></a><span class="in">        w_2_pred = w_2_samples.mean(dim=0)</span></span>
<span id="cb10-1070"><a href="#cb10-1070"></a></span>
<span id="cb10-1071"><a href="#cb10-1071"></a><span class="in">        print("Predicted parameters:", p_pred, w_1_pred, w_2_pred)</span></span>
<span id="cb10-1072"><a href="#cb10-1072"></a></span>
<span id="cb10-1073"><a href="#cb10-1073"></a><span class="in">        # Do casework on two equilibria cases to check for success</span></span>
<span id="cb10-1074"><a href="#cb10-1074"></a><span class="in">        p_diff_case_1 = torch.abs(p_pred - true_p)</span></span>
<span id="cb10-1075"><a href="#cb10-1075"></a><span class="in">        p_diff_case_2 = torch.abs(p_pred - (1 - true_p))</span></span>
<span id="cb10-1076"><a href="#cb10-1076"></a></span>
<span id="cb10-1077"><a href="#cb10-1077"></a><span class="in">        w_1_diff_case_1 = torch.max(torch.abs(w_1_pred - true_weights_1))</span></span>
<span id="cb10-1078"><a href="#cb10-1078"></a><span class="in">        w_1_diff_case_2 = torch.max(torch.abs(w_1_pred - true_weights_2))</span></span>
<span id="cb10-1079"><a href="#cb10-1079"></a></span>
<span id="cb10-1080"><a href="#cb10-1080"></a><span class="in">        w_2_diff_case_1 = torch.max(torch.abs(w_2_pred - true_weights_2))</span></span>
<span id="cb10-1081"><a href="#cb10-1081"></a><span class="in">        w_2_diff_case_2 = torch.max(torch.abs(w_2_pred - true_weights_1))</span></span>
<span id="cb10-1082"><a href="#cb10-1082"></a></span>
<span id="cb10-1083"><a href="#cb10-1083"></a><span class="in">        pass_case_1 = (</span></span>
<span id="cb10-1084"><a href="#cb10-1084"></a><span class="in">            p_diff_case_1 &lt; 0.1 and w_1_diff_case_1 &lt; 0.5 and w_2_diff_case_1 &lt; 0.5</span></span>
<span id="cb10-1085"><a href="#cb10-1085"></a><span class="in">        )</span></span>
<span id="cb10-1086"><a href="#cb10-1086"></a><span class="in">        pass_case_2 = (</span></span>
<span id="cb10-1087"><a href="#cb10-1087"></a><span class="in">            p_diff_case_2 &lt; 0.1 and w_1_diff_case_2 &lt; 0.5 and w_2_diff_case_2 &lt; 0.5</span></span>
<span id="cb10-1088"><a href="#cb10-1088"></a><span class="in">        )</span></span>
<span id="cb10-1089"><a href="#cb10-1089"></a><span class="in">        passes = pass_case_1 or pass_case_2</span></span>
<span id="cb10-1090"><a href="#cb10-1090"></a></span>
<span id="cb10-1091"><a href="#cb10-1091"></a><span class="in">        print(f'Result: {"Success" if passes else "FAILED"}')</span></span>
<span id="cb10-1092"><a href="#cb10-1092"></a><span class="in">        if passes:</span></span>
<span id="cb10-1093"><a href="#cb10-1093"></a><span class="in">            success_count += 1</span></span>
<span id="cb10-1094"><a href="#cb10-1094"></a><span class="in">    print(f'Success rate: {success_count / num_sims}')</span></span>
<span id="cb10-1095"><a href="#cb10-1095"></a></span>
<span id="cb10-1096"><a href="#cb10-1096"></a></span>
<span id="cb10-1097"><a href="#cb10-1097"></a><span class="in">if __name__ == "__main__":</span></span>
<span id="cb10-1098"><a href="#cb10-1098"></a><span class="in">    evaluate_metropolis(num_sims=10, num_movies=30000, feature_dim=10)</span></span>
<span id="cb10-1099"><a href="#cb10-1099"></a><span class="in">```</span></span>
<span id="cb10-1100"><a href="#cb10-1100"></a></span>
<span id="cb10-1101"><a href="#cb10-1101"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
    <footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/stair-lab/mlhp/blob/main/src/chap2.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/stair-lab/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer><script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  
<script type="module">
/**
 * Factory function to create different types of cells based on options.
 * @param {Object} cellData - JSON object containing code, id, and options.
 * @returns {BaseCell} Instance of the appropriate cell class.
 */
globalThis.qpyodideCreateCell = function(cellData) {
    switch (cellData.options.context) {
        case 'interactive':
            return new InteractiveCell(cellData);
        case 'output':
            return new OutputCell(cellData);
        case 'setup':
            return new SetupCell(cellData);
        default:
            return new InteractiveCell(cellData);
            // throw new Error('Invalid cell type specified in options.');
    }
}  

/**
 * CellContainer class for managing a collection of cells.
 * @class
 */
class CellContainer {
    /**
     * Constructor for CellContainer.
     * Initializes an empty array to store cells.
     * @constructor
     */
    constructor() {
        this.cells = [];
    }

    /**
     * Add a cell to the container.
     * @param {BaseCell} cell - Instance of a cell (BaseCell or its subclasses).
     */
    addCell(cell) {
        this.cells.push(cell);
    }

    /**
     * Execute all cells in the container.
     */
    async executeAllCells() {
        for (const cell of this.cells) {
            await cell.executeCode();
        }
    }

    /**
     * Execute all cells in the container.
     */
    async autoRunExecuteAllCells() {
        for (const cell of this.cells) {
            await cell.autoRunExecuteCode();
        }
    }
}
  

/**
 * BaseCell class for handling code execution using Pyodide.
 * @class
 */
class BaseCell {
    /**
     * Constructor for BaseCell.
     * @constructor
     * @param {Object} cellData - JSON object containing code, id, and options.
     */
    constructor(cellData) {
        this.code = cellData.code;
        this.id = cellData.id;
        this.options = cellData.options;
        this.insertionLocation = document.getElementById(`qpyodide-insertion-location-${this.id}`);
        this.executionLock = false;
    }

    cellOptions() {
        // Subclass this? 
        console.log(this.options);
        return this.options;
    }

    /**
     * Execute the Python code using Pyodide.
     * @returns {*} Result of the code execution.
     */
    async executeCode() {
        // Execute code using Pyodide
        const result = getPyodide().runPython(this.code);
        return result;
    }
};

/**
 * InteractiveCell class for creating editable code editor with Monaco Editor.
 * @class
 * @extends BaseCell
 */
class InteractiveCell extends BaseCell {

    /**
     * Constructor for InteractiveCell.
     * @constructor
     * @param {Object} cellData - JSON object containing code, id, and options.
     */
    constructor(cellData) {
        super(cellData);
        this.editor = null;
        this.setupElement();
        this.setupMonacoEditor();
    }

    /**
     * Set up the interactive cell elements
     */
    setupElement() {

        // Create main div element
        var mainDiv = document.createElement('div');
        mainDiv.id = `qpyodide-interactive-area-${this.id}`;
        mainDiv.className = `qpyodide-interactive-area`;
        if (this.options.classes) {
            mainDiv.className += " " + this.options.classes
        }

        // Add a unique cell identifier that users can customize
        if (this.options.label) {
            mainDiv.setAttribute('data-id', this.options.label);
        }

        // Create toolbar div
        var toolbarDiv = document.createElement('div');
        toolbarDiv.className = 'qpyodide-editor-toolbar';
        toolbarDiv.id = `qpyodide-editor-toolbar-${this.id}`;

        // Create a div to hold the left buttons
        var leftButtonsDiv = document.createElement('div');
        leftButtonsDiv.className = 'qpyodide-editor-toolbar-left-buttons';

        // Create a div to hold the right buttons
        var rightButtonsDiv = document.createElement('div');
        rightButtonsDiv.className = 'qpyodide-editor-toolbar-right-buttons';

        // Create Run Code button
        var runCodeButton = document.createElement('button');
        runCodeButton.className = 'btn btn-default qpyodide-button qpyodide-button-run';
        runCodeButton.disabled = true;
        runCodeButton.type = 'button';
        runCodeButton.id = `qpyodide-button-run-${this.id}`;
        runCodeButton.textContent = '🟡 Loading Pyodide...';
        runCodeButton.title = `Run code (Shift + Enter)`;

        // Append buttons to the leftButtonsDiv
        leftButtonsDiv.appendChild(runCodeButton);

        // Create Reset button
        var resetButton = document.createElement('button');
        resetButton.className = 'btn btn-light btn-xs qpyodide-button qpyodide-button-reset';
        resetButton.type = 'button';
        resetButton.id = `qpyodide-button-reset-${this.id}`;
        resetButton.title = 'Start over';
        resetButton.innerHTML = '<i class="fa-solid fa-arrows-rotate"></i>';

        // Create Copy button
        var copyButton = document.createElement('button');
        copyButton.className = 'btn btn-light btn-xs qpyodide-button qpyodide-button-copy';
        copyButton.type = 'button';
        copyButton.id = `qpyodide-button-copy-${this.id}`;
        copyButton.title = 'Copy code';
        copyButton.innerHTML = '<i class="fa-regular fa-copy"></i>';

        // Append buttons to the rightButtonsDiv
        rightButtonsDiv.appendChild(resetButton);
        rightButtonsDiv.appendChild(copyButton);

        // Create console area div
        var consoleAreaDiv = document.createElement('div');
        consoleAreaDiv.id = `qpyodide-console-area-${this.id}`;
        consoleAreaDiv.className = 'qpyodide-console-area';

        // Create editor div
        var editorDiv = document.createElement('div');
        editorDiv.id = `qpyodide-editor-${this.id}`;
        editorDiv.className = 'qpyodide-editor';

        // Create output code area div
        var outputCodeAreaDiv = document.createElement('div');
        outputCodeAreaDiv.id = `qpyodide-output-code-area-${this.id}`;
        outputCodeAreaDiv.className = 'qpyodide-output-code-area';
        outputCodeAreaDiv.setAttribute('aria-live', 'assertive');

        // Create pre element inside output code area
        var preElement = document.createElement('pre');
        preElement.style.visibility = 'hidden';
        outputCodeAreaDiv.appendChild(preElement);

        // Create output graph area div
        var outputGraphAreaDiv = document.createElement('div');
        outputGraphAreaDiv.id = `qpyodide-output-graph-area-${this.id}`;
        outputGraphAreaDiv.className = 'qpyodide-output-graph-area';

        // Append buttons to the toolbar
        toolbarDiv.appendChild(leftButtonsDiv);
        toolbarDiv.appendChild(rightButtonsDiv);

        // Append all elements to the main div
        mainDiv.appendChild(toolbarDiv);
        consoleAreaDiv.appendChild(editorDiv);
        consoleAreaDiv.appendChild(outputCodeAreaDiv);
        mainDiv.appendChild(consoleAreaDiv);
        mainDiv.appendChild(outputGraphAreaDiv);

        // Insert the dynamically generated object at the document location.
        this.insertionLocation.appendChild(mainDiv);
    }

    /**
     * Set up Monaco Editor for code editing.
     */
    setupMonacoEditor() {

        // Retrieve the previously created document elements
        this.runButton = document.getElementById(`qpyodide-button-run-${this.id}`);
        this.resetButton = document.getElementById(`qpyodide-button-reset-${this.id}`);
        this.copyButton = document.getElementById(`qpyodide-button-copy-${this.id}`);
        this.editorDiv = document.getElementById(`qpyodide-editor-${this.id}`);
        this.outputCodeDiv = document.getElementById(`qpyodide-output-code-area-${this.id}`);
        this.outputGraphDiv = document.getElementById(`qpyodide-output-graph-area-${this.id}`);
        
        // Store reference to the object
        var thiz = this;

        // Load the Monaco Editor and create an instance
        require(['vs/editor/editor.main'], function () {
            thiz.editor = monaco.editor.create(
                thiz.editorDiv, {
                    value: thiz.code,
                    language: 'python',
                    theme: 'vs-light',
                    automaticLayout: true,           // Works wonderfully with RevealJS
                    scrollBeyondLastLine: false,
                    minimap: {
                        enabled: false
                    },
                    fontSize: '17.5pt',              // Bootstrap is 1 rem
                    renderLineHighlight: "none",     // Disable current line highlighting
                    hideCursorInOverviewRuler: true,  // Remove cursor indictor in right hand side scroll bar
                    readOnly: thiz.options['read-only'] ?? false
                }
            );
        
            // Store the official counter ID to be used in keyboard shortcuts
            thiz.editor.__qpyodideCounter = thiz.id;
        
            // Store the official div container ID
            thiz.editor.__qpyodideEditorId = `qpyodide-editor-${thiz.id}`;
        
            // Store the initial code value and options
            thiz.editor.__qpyodideinitialCode = thiz.code;
            thiz.editor.__qpyodideOptions = thiz.options;
        
            // Set at the model level the preferred end of line (EOL) character to LF.
            // This prevent `\r\n` from being given to the Pyodide engine if the user is on Windows.
            // See details in: https://github.com/coatless/quarto-Pyodide/issues/94
            // Associated error text: 
            // Error: <text>:1:7 unexpected input
        
            // Retrieve the underlying model
            const model = thiz.editor.getModel();
            // Set EOL for the model
            model.setEOL(monaco.editor.EndOfLineSequence.LF);
        
            // Dynamically modify the height of the editor window if new lines are added.
            let ignoreEvent = false;
            const updateHeight = () => {
            const contentHeight = thiz.editor.getContentHeight();
            // We're avoiding a width change
            //editorDiv.style.width = `${width}px`;
            thiz.editorDiv.style.height = `${contentHeight}px`;
                try {
                    ignoreEvent = true;
            
                    // The key to resizing is this call
                    thiz.editor.layout();
                } finally {
                    ignoreEvent = false;
                }
            };
        
            // Helper function to check if selected text is empty
            function isEmptyCodeText(selectedCodeText) {
                return (selectedCodeText === null || selectedCodeText === undefined || selectedCodeText === "");
            }
        
            // Registry of keyboard shortcuts that should be re-added to each editor window
            // when focus changes.
            const addPyodideKeyboardShortCutCommands = () => {
            // Add a keydown event listener for Shift+Enter to run all code in cell
            thiz.editor.addCommand(monaco.KeyMod.Shift | monaco.KeyCode.Enter, () => {
                // Retrieve all text inside the editor
                thiz.runCode(thiz.editor.getValue());
            });
        
            // Add a keydown event listener for CMD/Ctrl+Enter to run selected code
            thiz.editor.addCommand(monaco.KeyMod.CtrlCmd | monaco.KeyCode.Enter, () => {
                    // Get the selected text from the editor
                    const selectedText = thiz.editor.getModel().getValueInRange(thiz.editor.getSelection());
                    // Check if no code is selected
                    if (isEmptyCodeText(selectedText)) {
                        // Obtain the current cursor position
                        let currentPosition = thiz.editor.getPosition();
                        // Retrieve the current line content
                        let currentLine = thiz.editor.getModel().getLineContent(currentPosition.lineNumber);
                
                        // Propose a new position to move the cursor to
                        let newPosition = new monaco.Position(currentPosition.lineNumber + 1, 1);
                
                        // Check if the new position is beyond the last line of the editor
                        if (newPosition.lineNumber > thiz.editor.getModel().getLineCount()) {
                            // Add a new line at the end of the editor
                            thiz.editor.executeEdits("addNewLine", [{
                            range: new monaco.Range(newPosition.lineNumber, 1, newPosition.lineNumber, 1),
                            text: "\n", 
                            forceMoveMarkers: true,
                            }]);
                        }
                        
                        // Run the entire line of code.
                        thiz.runCode(currentLine);
                
                        // Move cursor to new position
                        thiz.editor.setPosition(newPosition);
                    } else {
                        // Code to run when Ctrl+Enter is pressed with selected code
                        thiz.runCode(selectedText);
                    }
                });
            }
        
            // Register an on focus event handler for when a code cell is selected to update
            // what keyboard shortcut commands should work.
            // This is a workaround to fix a regression that happened with multiple
            // editor windows since Monaco 0.32.0 
            // https://github.com/microsoft/monaco-editor/issues/2947
            thiz.editor.onDidFocusEditorText(addPyodideKeyboardShortCutCommands);
        
            // Register an on change event for when new code is added to the editor window
            thiz.editor.onDidContentSizeChange(updateHeight);
        
            // Manually re-update height to account for the content we inserted into the call
            updateHeight();
                
        });

        
        // Add a click event listener to the run button
        thiz.runButton.onclick = function () {
            thiz.runCode(
                thiz.editor.getValue()
            );
        };
        
        // Add a click event listener to the reset button
        thiz.copyButton.onclick = function () {
            // Retrieve current code data
            const data = thiz.editor.getValue();
            
            // Write code data onto the clipboard.
            navigator.clipboard.writeText(data || "");
        };
        
        // Add a click event listener to the copy button
        thiz.resetButton.onclick = function () {
            thiz.editor.setValue(thiz.editor.__qpyodideinitialCode);
        };
    }

    disableInteractiveCells() {
        // Enable locking of execution for the cell
        this.executionLock = true;

        // Disallowing execution of other code cells
        document.querySelectorAll(".qpyodide-button-run").forEach((btn) => {
            btn.disabled = true;
        });
    }

    enableInteractiveCells() {
        // Remove locking of execution for the cell
        this.executionLock = false;

        // All execution of other code cells
        document.querySelectorAll(".qpyodide-button-run").forEach((btn) => {
            btn.disabled = false;
        });
    }

    /**
     * Execute the Python code inside the editor.
     */
    async runCode(code) {
        
        // Check if we have an execution lock
        if (this.executeLock) return; 
        
        this.disableInteractiveCells();

        // Force wait procedure
        await mainPyodide;

        // Clear the output stock
        qpyodideResetOutputArray();

        // Generate a new canvas element, avoid attaching until the end
        let graphFigure = document.createElement("figure");
        document.pyodideMplTarget = graphFigure;

        console.log("Running code!");
        // Obtain results from the base class
        try {
            // Always check to see if the user adds new packages
            await mainPyodide.loadPackagesFromImports(code);

            // Process result
            const output = await mainPyodide.runPythonAsync(code);

            // Add output
            qpyodideAddToOutputArray(output, "stdout");
        } catch (err) {
            // Add error message
            qpyodideAddToOutputArray(err, "stderr");
            // TODO: There has to be a way to remove the Pyodide portion of the errors... 
        }

        const result = qpyodideRetrieveOutput();

        // Nullify the output area of content
        this.outputCodeDiv.innerHTML = "";
        this.outputGraphDiv.innerHTML = "";        

        // Design an output object for messages
        const pre = document.createElement("pre");
        if (/\S/.test(result)) {
            // Display results as HTML elements to retain output styling
            const div = document.createElement("div");
            div.innerHTML = result;
            pre.appendChild(div);
        } else {
            // If nothing is present, hide the element.
            pre.style.visibility = "hidden";
        }

        // Add output under interactive div
        this.outputCodeDiv.appendChild(pre);

        // Place the graphics onto the page
        if (graphFigure) {

            if (this.options['fig-cap']) {
                // Create figcaption element
                const figcaptionElement = document.createElement('figcaption');
                figcaptionElement.innerText = this.options['fig-cap'];
                // Append figcaption to figure
                graphFigure.appendChild(figcaptionElement);    
            }

            this.outputGraphDiv.appendChild(graphFigure);
        }

        // Re-enable execution
        this.enableInteractiveCells();
    }
};

/**
 * OutputCell class for customizing and displaying output.
 * @class
 * @extends BaseCell
 */
class OutputCell extends BaseCell {
    /**
     * Constructor for OutputCell.
     * @constructor
     * @param {Object} cellData - JSON object containing code, id, and options.
     */
    constructor(cellData) {
      super(cellData);
    }
  
    /**
     * Display customized output on the page.
     * @param {*} output - Result to be displayed.
     */
    displayOutput(output) {
        const results = this.executeCode();
        return results;
    }
  }

/**
 * SetupCell class for suppressed output.
 * @class
 * @extends BaseCell
 */
class SetupCell extends BaseCell {
    /**
     * Constructor for SetupCell.
     * @constructor
     * @param {Object} cellData - JSON object containing code, id, and options.
     */
    constructor(cellData) {
        super(cellData);
    }

    /**
     * Execute the Python code without displaying the results.
     */
    runSetupCode() {
        // Execute code without displaying output
        this.executeCode();
    }
};
</script>
<script type="module">
// Handle cell initialization initialization
qpyodideCellDetails.map(
    (entry) => {
      // Handle the creation of the element
      qpyodideCreateCell(entry);
    }
  );
</script>




</body></html>