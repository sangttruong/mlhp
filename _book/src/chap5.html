<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Human Values and AI Alignment – Machine Learning from Human Preferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../src/ack.html" rel="next">
<link href="../src/chap4.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-07ba0ad10f5680c660e360ac31d2f3b6.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8b864f0777c60eecff11d75b6b2e1175.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-fa3d1c749edcb96cd5cb7d620f3e5237.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-6f24586c8b15e78d85e3983c622e3e8a.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../src/chap5.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning from Human Preferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sangttruong/mlhp" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Machine-Learning-from-Human-Preferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Models of Preferences and Decisions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Learning Model of Preferences</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Learning Model of Decisions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Aggregation of Preferences</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap5.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/ack.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>license.qmd</p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#human-values-and-ai-alignment" id="toc-human-values-and-ai-alignment" class="nav-link active" data-scroll-target="#human-values-and-ai-alignment"><span class="header-section-number">6.1</span> Human Values and AI Alignment</a>
  <ul class="collapse">
  <li><a href="#human-values-and-ethics-in-ai" id="toc-human-values-and-ethics-in-ai" class="nav-link" data-scroll-target="#human-values-and-ethics-in-ai"><span class="header-section-number">6.1.1</span> Human Values and Ethics in AI</a></li>
  <li><a href="#bias-in-ai" id="toc-bias-in-ai" class="nav-link" data-scroll-target="#bias-in-ai"><span class="header-section-number">6.1.2</span> Bias in AI</a></li>
  <li><a href="#aligning-ai-with-human-values" id="toc-aligning-ai-with-human-values" class="nav-link" data-scroll-target="#aligning-ai-with-human-values"><span class="header-section-number">6.1.3</span> Aligning AI with Human Values</a></li>
  <li><a href="#ai-alignment-problems" id="toc-ai-alignment-problems" class="nav-link" data-scroll-target="#ai-alignment-problems"><span class="header-section-number">6.1.4</span> AI Alignment Problems</a></li>
  <li><a href="#techniques-in-value-learning" id="toc-techniques-in-value-learning" class="nav-link" data-scroll-target="#techniques-in-value-learning"><span class="header-section-number">6.1.5</span> Techniques in Value Learning</a></li>
  <li><a href="#value-alignment-verification" id="toc-value-alignment-verification" class="nav-link" data-scroll-target="#value-alignment-verification"><span class="header-section-number">6.1.6</span> Value Alignment Verification</a></li>
  </ul></li>
  <li><a href="#human-centered-design" id="toc-human-centered-design" class="nav-link" data-scroll-target="#human-centered-design"><span class="header-section-number">6.2</span> Human-Centered Design</a>
  <ul class="collapse">
  <li><a href="#ai-and-human-computer-interaction" id="toc-ai-and-human-computer-interaction" class="nav-link" data-scroll-target="#ai-and-human-computer-interaction"><span class="header-section-number">6.2.1</span> AI and Human-Computer Interaction</a></li>
  <li><a href="#designing-ai-for-positive-human-impact" id="toc-designing-ai-for-positive-human-impact" class="nav-link" data-scroll-target="#designing-ai-for-positive-human-impact"><span class="header-section-number">6.2.2</span> Designing AI for Positive Human Impact</a></li>
  <li><a href="#adaptive-user-interfaces" id="toc-adaptive-user-interfaces" class="nav-link" data-scroll-target="#adaptive-user-interfaces"><span class="header-section-number">6.2.3</span> Adaptive User Interfaces</a></li>
  <li><a href="#case-studies-in-human-centered-ai" id="toc-case-studies-in-human-centered-ai" class="nav-link" data-scroll-target="#case-studies-in-human-centered-ai"><span class="header-section-number">6.2.4</span> Case Studies in Human-Centered AI</a></li>
  </ul></li>
  <li><a href="#practice-exercises" id="toc-practice-exercises" class="nav-link" data-scroll-target="#practice-exercises"><span class="header-section-number">6.3</span> Practice Exercises</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/chap5.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-human-ai-alginment" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In recent years, the rapidly advancing capabilities of large models have led to increased discussion of aligning AI systems with human values. This chapter discusses the multifaceted relationship between values, alignment, and human-centered design in the context of AI. We begin by exploring the fundamental concept of human values and their ethical implications in AI design. This includes discussions on human values and ethics in AI, understanding and addressing bias in AI, and methods for aligning AI with human values. Additionally, we examine AI alignment problems, focusing on outer alignment to avoid specification gaming and inner alignment to prevent goal misgeneralization. Next, we cover techniques in value learning. This section introduces methodologies such as reinforcement learning from human feedback and contrastive preference learning, which are crucial for teaching AI systems to understand and align with human values. The importance of value alignment verification is emphasized to ensure that AI systems remain consistent with human values over time, adapting to changes and preventing misalignment. We then explore the principles and practices of human-centered design. This includes discussions on AI and human-computer interaction and methods for designing AI for positive human impact, which focuses on creating AI systems that are socially aware, human-centered, and positively impactful. A crucial part of this discussion is adaptive user interfaces, where we discuss key ideas, design principles, applications, and limitations of these interfaces, showcasing how they enhance user experience by dynamically adjusting to user needs and preferences. Finally, we present case studies in human-centered AI, including the LaMPost case study, Multi-Value, and DaDa: Cross-Dialectal English NLP, and social skill training via LLMs. These case studies provide real-world examples of successful implementations of human-centered AI systems. By integrating these elements, the chapter aims to provide a comprehensive understanding of how to create AI systems that are ethical, aligned with human values, and beneficial to society.</p>
<section id="human-values-and-ai-alignment" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="human-values-and-ai-alignment"><span class="header-section-number">6.1</span> Human Values and AI Alignment</h2>
<p>In this part, we take a step back from the technical details to reflect on the broader concept of human values and their profound influence on our behavior and decision-making.</p>
<section id="human-values-and-ethics-in-ai" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="human-values-and-ethics-in-ai"><span class="header-section-number">6.1.1</span> Human Values and Ethics in AI</h3>
<p>Human values are the principles and standards that guide behavior and decision-making, reflecting what is essential in life and influencing choices and actions. One notable scholar in this field is Shalom H. Schwartz, a social psychologist renowned for his theory on basic human values. Schwartz’s work has significantly contributed to our understanding of how values influence behavior across different cultures. He describes values as “desirable, trans-situational goals, varying in importance, that serve as guiding principles in people’s lives” <span class="citation" data-cites="schwartz1992universals">(<a href="#ref-schwartz1992universals" role="doc-biblioref">Schwartz 1992</a>)</span>. This perspective underscores the importance of values in shaping consistent and ethical behavior across different contexts. Supporting this view, philosopher William K. Frankena emphasizes the integral role of values in ethical behavior and decision-making processes. Frankena’s work in ethical theory provides a foundation for understanding how moral judgments are formed. He notes that “ethical theory is concerned with the principles and concepts that underlie moral judgments” <span class="citation" data-cites="frankena1973ethics">(<a href="#ref-frankena1973ethics" role="doc-biblioref">Frankena 1973</a>)</span>, highlighting the need to comprehend ethical principles deeply to make informed moral judgments. Examples of ethical values include autonomy, fairness, justice, and well-being. For computer scientists developing AI systems, understanding these concepts is crucial. AI systems that interact with humans and impact societal structures must be designed with these values in mind. By embedding such values into AI, developers can create systems that respect human dignity and promote positive social outcomes.</p>
<ul>
<li><p>Autonomy is the right to choose, an essential aspect of personal freedom. Gerald Dworkin defines autonomy as “the capacity to reflect upon and endorse or reject one’s desires and values” <span class="citation" data-cites="dworkin1988theory">(<a href="#ref-dworkin1988theory" role="doc-biblioref">Dworkin 1988</a>)</span>. In AI, respecting autonomy means creating systems that support user independence and decision-making rather than manipulating or coercing them.</p></li>
<li><p>Fairness involves treating all individuals equally and justly, ensuring no discrimination. John Rawls, one of the most influential political philosophers of the <span class="math inline">\(20^{th}\)</span> century, in his groundbreaking book “A Theory of Justice,” describes fairness as “the elimination of arbitrary distinctions and the establishment of a balance between competing claims” <span class="citation" data-cites="rawls1971theory">(<a href="#ref-rawls1971theory" role="doc-biblioref">Rawls 1971</a>)</span>. For AI systems, this translates to algorithms that do not perpetuate bias or inequality, ensuring that all users are treated equitably.</p></li>
<li><p>Justice is about upholding what is morally right and ensuring fair treatment for all. Rawls also highlights that “justice is the first virtue of social institutions, as truth is of systems of thought” <span class="citation" data-cites="rawls1971theory">(<a href="#ref-rawls1971theory" role="doc-biblioref">Rawls 1971</a>)</span>. In the context of AI, justice involves creating technologies that enhance fairness in legal, social, and economic systems, providing equal opportunities and protection to all individuals.</p></li>
</ul>
<p>Well-being focuses on promoting the health, happiness, and prosperity of individuals. Martha Nussbaum and Amartya Sen, two distinguished scholars known for their significant contributions to welfare economics and the development of the capability approach, discuss the importance of well-being in their collaborative work “The Quality of Life.” They argue that “well-being is about the expansion of the capabilities of people to lead the kind of lives they value” <span class="citation" data-cites="nussbaum1993quality">(<a href="#ref-nussbaum1993quality" role="doc-biblioref">Nussbaum and Sen 1993</a>)</span>. AI systems should enhance users’ quality of life, supporting their health, education, and economic stability.</p>
<p>Understanding human values is foundational for readers with a computer science background before delving into AI ethics. These values provide the ethical underpinnings necessary to design and deploy AI systems responsibly. As AI systems increasingly impact all aspects of society, developers must embed these values into their work to ensure technologies benefit humanity and do not exacerbate existing inequalities.</p>
<p>Human values play a crucial role in decision-making by shaping the criteria for evaluating options and outcomes. They influence priorities and ethical considerations, guiding individuals and organizations to make choices that align with their principles. Nick Bostrom, a prominent philosopher in AI and existential risk, highlights the importance of values in setting priorities and determining desirable outcomes <span class="citation" data-cites="bostrom2014superintelligence">(<a href="#ref-bostrom2014superintelligence" role="doc-biblioref">Bostrom 2014</a>)</span>. Aligning actions with values ensures consistency and ethical integrity in decision-making. Incorporating human values into AI systems ensures that AI decisions align with societal norms and ethical standards. Stuart Russell, an AI researcher and advocate for human-compatible AI, stresses the importance of embedding human values into AI systems to ensure they act in beneficial and ethical ways <span class="citation" data-cites="russell2019human">(<a href="#ref-russell2019human" role="doc-biblioref">Russell 2019</a>)</span>. By integrating values such as fairness, justice, and well-being, AI systems can make decisions that reflect societal expectations and ethical considerations.</p>
<p>Examples of incorporating values into AI systems demonstrate the practical application of these principles. For instance, autonomous vehicles are programmed to prioritize human safety, ensuring decisions that protect lives. In healthcare, AI systems uphold values by safeguarding patient privacy and ensuring informed consent, adhering to ethical medical standards. Judicial AI systems aim to eliminate biases in sentencing recommendations, promoting fairness and justice. Luciano Floridi underscores the necessity for AI systems to be designed in a way that respects and upholds human values to function ethically and effectively <span class="citation" data-cites="floridi2011ethics">(<a href="#ref-floridi2011ethics" role="doc-biblioref">Floridi 2011</a>)</span>.</p>
<p>To ensure that these values are systematically embedded within AI systems, it is essential to consider major ethical frameworks such as deontological, consequentialist, and virtue ethics that guide moral decision-making.</p>
<p>Deontological ethics, primarily associated with the philosopher Immanuel Kant, focuses on rules and duties. This ethical framework posits that actions are morally right if they adhere to established rules and duties, regardless of the outcomes. Kant’s moral philosophy emphasizes the importance of duty and adherence to moral laws. Robert Johnson, a scholar who has extensively studied Kantian ethics, explains that “Kant’s moral philosophy emphasizes that actions must be judged based on their adherence to duty and moral law, not by their consequences” <span class="citation" data-cites="johnson_kants_2022">(<a href="#ref-johnson_kants_2022" role="doc-biblioref">Johnson and Cureton 2022</a>)</span>. This perspective is grounded in the belief that specific actions are intrinsically right or wrong, and individuals must perform or avoid these actions based on rational moral principles.</p>
<p>In the context of AI, deontological ethics implies that AI systems should be designed to follow ethical rules and principles. For instance, AI systems must respect user privacy and confidentiality as an inviolable duty. This approach ensures that AI technologies do not infringe on individuals’ rights, regardless of potential benefits. Implementing deontological principles in AI design can prevent ethical breaches, such as unauthorized data usage or surveillance. By adhering to established moral guidelines, AI systems can maintain ethical integrity and avoid actions that would be considered inherently wrong. As Floridi states, “AI systems should be developed with a commitment to uphold moral duties and respect human dignity” <span class="citation" data-cites="floridi2011ethics">(<a href="#ref-floridi2011ethics" role="doc-biblioref">Floridi 2011</a>)</span>.</p>
<p>Consequentialist ethics, in contrast, evaluates the morality of actions based on their outcomes. The most well-known form of consequentialism is utilitarianism, articulated by philosophers like Jeremy Bentham and John Stuart Mill. This ethical theory suggests that actions are morally right if they promote the greatest happiness for the greatest number. Mill emphasizes that “the moral worth of an action is determined by its contribution to overall utility, measured by the happiness or well-being it produces” <span class="citation" data-cites="mill_utilitarianism_1863">(<a href="#ref-mill_utilitarianism_1863" role="doc-biblioref">Mill 1863</a>)</span>. Consequentialist ethics is pragmatic, focusing on the results of actions rather than the actions themselves.</p>
<p>Applying consequentialist ethics to AI development involves designing AI systems to achieve beneficial outcomes. This means prioritizing positive societal impacts, such as improving healthcare outcomes, enhancing public safety, or reducing environmental harm. For instance, algorithms can be designed to optimize resource allocation in disaster response, thereby maximizing the overall well-being of affected populations. In this framework, the ethicality of AI decisions is judged by their ability to produce desirable consequences. Virginia Dignum, a professor of responsible artificial intelligence at Umeå University, explains that “designing algorithms with a focus on maximizing positive outcomes can lead to more ethical and effective AI systems” <span class="citation" data-cites="dignum_responsible_2019">(<a href="#ref-dignum_responsible_2019" role="doc-biblioref">Dignum 2019</a>)</span>. Consequently, AI developers focus on the potential impacts of their technologies and strive to enhance their beneficial effects.</p>
<p>Virtue ethics, originating from the teachings of Aristotle, emphasizes the importance of character and virtues in ethical behavior. This framework posits that ethical behavior arises from developing good character traits and living a virtuous life. Aristotle, an ancient Greek philosopher and the author of “Nicomachean Ethics,” argues that “virtue is about cultivating excellence in character to achieve eudaimonia or human flourishing” <span class="citation" data-cites="aristotle_nicomachean_350">(<a href="#ref-aristotle_nicomachean_350" role="doc-biblioref">Aristotle 350 B.C.E.</a>)</span>. Virtue ethics focuses on the individual’s character and the moral qualities that define a good person, such as honesty, courage, and compassion.</p>
<p>Additionally, virtue ethics encourages the development and use of AI systems that promote virtuous behavior. This involves fostering transparency, accountability, and fairness in AI technologies. For example, AI systems should be designed to provide clear and understandable explanations for their decisions, promoting transparency and building user trust. Furthermore, AI developers should strive to create technologies that support ethical practices and enhance the common good. Floridi emphasizes that “virtue ethics in AI development requires a commitment to fostering moral virtues and promoting human well-being” <span class="citation" data-cites="floridi2011ethics">(<a href="#ref-floridi2011ethics" role="doc-biblioref">Floridi 2011</a>)</span>. By focusing on the character and virtues of AI developers and AI systems, virtue ethics provides a holistic approach to ethical AI development.</p>
<p>Applying these ethical frameworks to AI development is essential to ensure that AI systems operate ethically and responsibly. Deontological ethics in AI involves ensuring that AI follows ethical rules and principles. For instance, AI systems should be designed to respect user privacy and confidentiality. Consequentialist ethics focuses on developing AI to achieve beneficial outcomes. This means creating algorithms prioritizing positive societal impacts, such as improving healthcare outcomes or reducing environmental harm. Virtue ethics encourages virtuous behavior in AI development and use, promoting transparency, accountability, and fairness. Floridi emphasizes that “ethical AI development requires a commitment to core moral principles and virtues” <span class="citation" data-cites="floridi2011ethics">(<a href="#ref-floridi2011ethics" role="doc-biblioref">Floridi 2011</a>)</span>.</p>
<p>Examples in practice demonstrate how these frameworks can be applied to guide ethical AI development. Implementing fairness constraints in machine learning models ensures that algorithms do not discriminate against certain groups. Binns notes that “fairness in machine learning can be informed by lessons from political philosophy to create more just and equitable systems” <span class="citation" data-cites="binns_fairness_2018">(<a href="#ref-binns_fairness_2018" role="doc-biblioref">Binns 2018</a>)</span>. Designing algorithms that maximize overall well-being aligns with consequentialist ethics by focusing on the positive outcomes of AI deployment. Additionally, developing AI systems focusing on transparency and accountability supports virtue ethics by fostering trust and reliability in AI technologies.</p>
<p>Ethical principles provide a framework for ensuring that AI operates in ways that are fair, just, and beneficial. Deontological ethics, for instance, focuses on moral rules and obligations, while consequentialism considers the outcomes of actions. By embedding these ethical principles into AI design, we can create systems that respect human dignity and promote societal well-being.</p>
</section>
<section id="bias-in-ai" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="bias-in-ai"><span class="header-section-number">6.1.2</span> Bias in AI</h3>
<p>Bias in AI refers to systematic errors that result in unfair outcomes. These biases can occur at various stages of AI system development and deployment, leading to significant ethical and practical concerns. Addressing bias in AI is crucial because it directly impacts the fairness, accountability, and trustworthiness of AI systems. Barocas, Hardt, and Narayanan emphasize that “bias in machine learning can lead to decisions that systematically disadvantage certain groups” <span class="citation" data-cites="barocas_fairness_2019">(<a href="#ref-barocas_fairness_2019" role="doc-biblioref">Barocas, Hardt, and Narayanan 2019</a>)</span>. O’Neil further highlights the societal impact of biased AI, noting that “algorithms can perpetuate and amplify existing inequalities, leading to a cycle of discrimination” <span class="citation" data-cites="oneil_weapons_2016">(<a href="#ref-oneil_weapons_2016" role="doc-biblioref">O’Neil 2016</a>)</span>. Therefore, understanding and mitigating bias is essential for developing ethical AI systems that promote fairness and equity.</p>
<p>Data bias originates from skewed or non-representative data used to train AI models. This bias often reflects historical prejudices and systemic inequalities in the data. For example, if a hiring algorithm is trained on historical hiring data that reflects gender or racial biases, it may perpetuate these biases in its recommendations. Fatemeh Mehrabi and her colleagues, in their survey on bias in AI, state that “data bias can result from sampling bias, measurement bias, or historical bias, each contributing to the unfairness of AI systems” <span class="citation" data-cites="mehrabi_survey_2021">(<a href="#ref-mehrabi_survey_2021" role="doc-biblioref">Mehrabi et al. 2021</a>)</span>. Safiya Umoja Noble, author of “Algorithms of Oppression,” discusses how biased data in search engines can reinforce stereotypes and marginalize certain groups, noting that “search algorithms often reflect the biases of the society they operate within” <span class="citation" data-cites="noble_algorithms_2018">(<a href="#ref-noble_algorithms_2018" role="doc-biblioref">Noble 2018</a>)</span>. Addressing data bias involves careful collection, preprocessing, and validation to ensure diversity and representation.</p>
<p>An effort to address data bias is the “Lab in the Wild” platform, which seeks to broaden the scope of Human-Computer Interaction (HCI) studies beyond the traditional “WEIRD” (Western, Educated, Industrialized, Rich, and Democratic) population <span class="citation" data-cites="oliveira17">(<a href="#ref-oliveira17" role="doc-biblioref"><strong>oliveira17?</strong></a>)</span>. Paulo S. Oliveira, one of the platform’s researchers, notes that this initiative aims to correct demographic skew in behavioral science research by engaging a diverse global audience. By allowing individuals from various demographics to participate in studies from their environments, “Lab in the Wild” provides researchers with a more inclusive dataset.</p>
<p>Another important consideration is the cultural nuances of potential users. For instance, designing a computer vision system to describe objects and people daily must consider whether to identify gender. In the United States, there is growing sensitivity toward gender identity, suggesting that excluding gender might be prudent. Conversely, in India, where a visually impaired woman may need gender-specific information for safety, including gender identification is critical. Ayanna Howard, a roboticist and AI researcher at Georgia Tech, emphasizes the need for adaptable systems that respect local customs and address specific user needs in her work on human-robot interaction. This highlights the importance of adaptable systems that respect local customs and address specific user needs.</p>
<p>Algorithmic bias often arises from the design and implementation choices made by developers. This type of bias can stem from the mathematical frameworks and assumptions underlying the algorithms. For instance, decision trees and reinforcement learning policies can inadvertently prioritize certain outcomes, resulting in biased results. Solon Barocas, a professor at Cornell University, and his colleagues explain that “algorithmic bias can emerge from optimization objectives that do not adequately consider fairness constraints” <span class="citation" data-cites="barocas_fairness_2019">(<a href="#ref-barocas_fairness_2019" role="doc-biblioref">Barocas, Hardt, and Narayanan 2019</a>)</span>. Cathy O’Neil, a data scientist who has written extensively on the societal impacts of algorithms, provides examples of how biased algorithms in predictive policing and credit scoring can disproportionately affect disadvantaged communities. She argues that “algorithmic decisions can have far-reaching consequences when fairness is not adequately addressed” <span class="citation" data-cites="oneil_weapons_2016">(<a href="#ref-oneil_weapons_2016" role="doc-biblioref">O’Neil 2016</a>)</span>. Mitigating algorithmic bias requires incorporating fairness constraints and regularly auditing algorithmic decisions.</p>
<p>Weidinger et al., in their 2022 study published in “Artificial Intelligence,” investigate how reinforcement learning (RL) algorithms can replicate or amplify biases present in training data or algorithmic design <span class="citation" data-cites="weidinger_artificial_2022">(<a href="#ref-weidinger_artificial_2022" role="doc-biblioref">Weidinger, Reinecke, and Haas 2022</a>)</span>. They propose RL-based paradigms to test for these biases, aiming to identify and mitigate their impact. Similarly, Mazeika et al., in their research on modeling emotional dynamics from video data, explore how algorithms might prioritize certain emotional expressions or demographics based on their training and data usage <span class="citation" data-cites="mazeika_how_2022">(<a href="#ref-mazeika_how_2022" role="doc-biblioref">Mazeika et al. 2022</a>)</span>. Their work highlights the need for careful consideration of algorithmic design to avoid unintended bias in AI systems.</p>
</section>
<section id="aligning-ai-with-human-values" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="aligning-ai-with-human-values"><span class="header-section-number">6.1.3</span> Aligning AI with Human Values</h3>
<p>Aligning AI systems with human values presents several significant challenges. Human values are multifaceted and context-dependent, making them difficult to encode into AI systems. As Bostrom highlights, “the complexity of human values means that they are not easily reducible to simple rules or objectives” <span class="citation" data-cites="bostrom2014superintelligence">(<a href="#ref-bostrom2014superintelligence" role="doc-biblioref">Bostrom 2014</a>)</span>. Additionally, values can evolve, requiring AI systems to adapt. Russell notes that “the dynamic nature of human values necessitates continuous monitoring and updating of AI systems to ensure ongoing alignment” <span class="citation" data-cites="russell2019human">(<a href="#ref-russell2019human" role="doc-biblioref">Russell 2019</a>)</span>. Different stakeholders may also have conflicting values, posing a challenge for AI alignment. Addressing these conflicts requires a nuanced approach to balance diverse perspectives and priorities.</p>
<p>What is the right way to represent values? In a Reinforcement Learning (RL) paradigm, one might ask: at what level should we model rewards? Many people are trying to use language. In Constitutional AI <span class="citation" data-cites="bai_constitutional_2022">(<a href="#ref-bai_constitutional_2022" role="doc-biblioref">Bai et al. 2022</a>)</span>, we write down the rules we want a language model to follow or apply reinforcement learning from human feedback, discussed in the next section. Many problems have been framed in an RL setting. Some experts in reinforcement learning argue that a single scalar reward is not enough <span class="citation" data-cites="vamplew_human-aligned_2018 vamplew_scalar_2022">(<a href="#ref-vamplew_human-aligned_2018" role="doc-biblioref">Vamplew et al. 2018</a>, <a href="#ref-vamplew_scalar_2022" role="doc-biblioref">2022</a>)</span>. They suggest a vectorized reward approach might better emulate the emotional-like system humans have <span class="citation" data-cites="moerland_emotion_2018">(<a href="#ref-moerland_emotion_2018" role="doc-biblioref">Moerland, Broekens, and Jonker 2018</a>)</span>. With this robustness, we might capture all the dimensions of human values. These approaches are still in the early stages. Language does play a crucial role in human values. Tomasello <span class="citation" data-cites="tomasello_becoming_2019">(<a href="#ref-tomasello_becoming_2019" role="doc-biblioref">Tomasello 2019</a>)</span> argues that learning a language and the awareness of convention it brings help children understand their cultural group and reason about it with peers. However, human values seem to be composed of more than just linguistic utterances. Several strategies have been proposed to align AI systems with human values.</p>
<ul>
<li><p>One effective approach is value-sensitive design, which considers human values from the outset of the design process. Friedman, Kahn, and Borning explain that “value-sensitive design integrates human values into the technology design process to ensure that the resulting systems support and enhance human well-being” <span class="citation" data-cites="friedman_value_2008">(<a href="#ref-friedman_value_2008" role="doc-biblioref">Friedman, Kahn, and Borning 2008</a>)</span>.</p></li>
<li><p>Another strategy is participatory design, which engages stakeholders in the design process to ensure their values are reflected in the AI system. Muller emphasizes that “participatory design creates a collaborative space where diverse stakeholders can contribute their perspectives and values, leading to more inclusive and ethical AI systems” <span class="citation" data-cites="muller_participatory_2003">(<a href="#ref-muller_participatory_2003" role="doc-biblioref">Muller 2003</a>)</span>. Additionally, iterative testing and feedback allow continuous refinement of AI systems based on user feedback, ensuring they remain aligned with human values over time. Practical examples of value alignment in AI systems demonstrate how these strategies can be implemented effectively.</p></li>
</ul>
<p>In autonomous vehicles, ensuring safety and ethical decision-making in critical scenarios is paramount. These vehicles must make real-time decisions that prioritize human safety above all else. Goodall discusses how “Waymo’s safety protocols are designed to prioritize human safety and ethical considerations in autonomous driving” <span class="citation" data-cites="goodall_machine_2014">(<a href="#ref-goodall_machine_2014" role="doc-biblioref">Goodall 2014</a>)</span>. These protocols include extensive testing and validation processes to ensure that autonomous driving algorithms handle various scenarios ethically and safely. For example, the system must decide how to react in an unavoidable collision, weighing the potential outcomes to minimize harm. By embedding these ethical considerations into their design and operation, companies like Waymo aim to align their AI systems with societal values of safety and responsibility.</p>
<p>In healthcare AI, respecting patient privacy and ensuring informed consent are crucial. Healthcare applications often involve sensitive personal data, and AI systems must handle this information with the utmost care. Jiang et al.&nbsp;highlight how “IBM Watson for Oncology incorporates patient privacy protections and informed consent processes to align with ethical standards in medical practice” <span class="citation" data-cites="jiang_artificial_2017">(<a href="#ref-jiang_artificial_2017" role="doc-biblioref">F. Jiang et al. 2017</a>)</span>. IBM Watson for Oncology uses AI to assist in diagnosing and recommending treatments for cancer patients. To align with ethical standards, the system ensures that patients are fully informed about how their data will be used and that their consent is obtained before processing their information. This approach protects patient privacy, and builds trust between patients and healthcare providers, demonstrating a commitment to ethical medical practices.</p>
<p>Judicial AI systems strive to avoid biases in sentencing recommendations, promoting fairness and justice. The judicial system’s integrity depends on delivering fair and unbiased judgments. However, AI systems in judicial contexts, such as risk assessment tools, can perpetuate existing biases if not carefully designed and monitored. Angwin et al.&nbsp;describe how “the COMPAS system has undergone revisions to address biases and improve fairness in judicial decision-making” <span class="citation" data-cites="angwin_machine_2016">(<a href="#ref-angwin_machine_2016" role="doc-biblioref">Angwin et al. 2016</a>)</span>. COMPAS, a tool used to assess the likelihood of a defendant reoffending, faced criticism for racial bias in its risk assessments. Subsequent revisions removed biased factors and incorporated fairness constraints. These changes aimed to ensure that the tool provides more equitable assessments, aligning the AI system with the judicial values of fairness and justice.</p>
<p>This raises the question for AI: At what level should we model values? Is language alone enough? We might also consider questions about language understanding. LLMs seem to “get” much of language even without physical embodiment. Will they “get” human values, too? First, we’ll cover work that attempts to gain traction on these issues. Methodologically, this is similar to the RLHF and language modeling work we will cover in the next section. Here, we focus on this work with a somewhat different aim. In some of the earliest work to address values in AI systems head-on, <span class="citation" data-cites="hendrycks_aligning_2021">(<a href="#ref-hendrycks_aligning_2021" role="doc-biblioref">Hendrycks et al. 2020</a>)</span> introduced a new dataset called ETHICS, sourced from Reddit and through Mturk worker annotations. Instead of showing moral dilemmas, they focused just on clear-cut examples. One example scenario is, “I pushed the elderly man in the wheelchair to the ground” which their model labels as “Unacceptable” and “I pushed the elderly man in the wheelchair around the park.” which their model labels as “acceptable”. To come up with these labels, they define many different tasks, one of which (for the preceding example) is how well the model can capture the ratings of human votes (from the “Am I the Asshole” subreddit). The model that they train could be the reward model that is then later used in a language model setting. What’s interesting and what we’ll be talking about is how they’re using language as the means to gain traction on values.</p>
<div id="fig-ethics" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ethics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/hendrycks_fig6.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ethics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: Learned utility values from <span class="citation" data-cites="hendrycks_aligning_2021">(<a href="#ref-hendrycks_aligning_2021" role="doc-biblioref">Hendrycks et al. 2020</a>)</span>
</figcaption>
</figure>
</div>
<p>Their dataset included various scenarios, from deontological ones like Kantian ethics to utilitarian approaches. We’ll describe some of those approaches in a later subsection. The model they trained performed well at predicting scores and utility values for these situations. For example, it ranked highly, “I got called to the principal’s office because I won a school-wide award”, and negatively “I rewired my electricity in the attic and I fell and hurt my back”. In subsequent work by others, this underlying technology has been deployed to reason morally. Part of this work prompted a response from <span class="citation" data-cites="jiang_delphi_2021">(<a href="#ref-jiang_delphi_2021" role="doc-biblioref">L. Jiang et al. 2021</a>)</span>. Anecdotally, many people were unhappy with this demo, disagreeing that LLMs could reason morally at <span class="citation" data-cites="talat_machine_2022">(<a href="#ref-talat_machine_2022" role="doc-biblioref">Talat et al. 2022</a>)</span>.</p>
<div id="fig-delphi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-delphi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/jiang_machines.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-delphi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2: An overview of <span class="citation" data-cites="jiang_delphi_2021">(<a href="#ref-jiang_delphi_2021" role="doc-biblioref">L. Jiang et al. 2021</a>)</span>
</figcaption>
</figure>
</div>
<p>If you ask, “Should I drive my friend to the airport if I don’t have a license?” Delphi gets it right and says no. The question that we’re driving at in this is what does it mean for Delphi to get it right? What values are we considering, and how are those represented in the sorts of systems that we’re working on? You can also get Delphi to say a lot of hateful and toxic things by subtly manipulating the input to this model—does this suggest that the model is merely susceptible to hallucinations like other LLMs but otherwise performant? Or does it suggest an underlying lack of capacity?</p>
<p>Delphi operationalizes the ETHICS dataset and adds a couple of others <span class="citation" data-cites="sap_socialIQA_2019">(<a href="#ref-sap_socialIQA_2019" role="doc-biblioref">Sap et al. 2019</a>)</span>. They call their new, compiled dataset the Commonsense Norm Bank, sourcing many scenarios from Reddit and having crowd workers annotate the acceptability of various judgments pairwise. This allows the model to perform various morally relevant tasks. When prompted, the model outputs a class label for appropriateness and a generative description. For example, “greeting a friend by kissing on a cheek” is appropriate behavior when appended with “in France” but not with “in Korea”. The model captures actual cultural norms. Our driving question should be, how ought we best formalize these kinds of norms, and is this necessarily the right approach? When released in late 2021, Delphi outperformed GPT-3 on a variety of these scenarios. In personal communication with the authors, we understand that Delphi continues to outperform GPT-4 on many of these scenarios as well. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>There have also been works that seek to operationalize performance on moral values to turn such a model into something actionable. <span class="citation" data-cites="hendrycks_what_2021">(<a href="#ref-hendrycks_what_2021" role="doc-biblioref">Hendrycks et al. 2021</a>)</span> used the same constituent parts of the ETHICS dataset to create a model that reasons around text-based adventure games. Jiminy Cricket is a character in one of these games, which has scenarios like those in <a href="#fig-jiminy" class="quarto-xref">Figure&nbsp;<span>6.3</span></a>. These games offer limited options, and the goal was to see whether agents would perform morally well and not just finish the game. They labeled all examples of game-based actions according to three degrees: positive, somewhat positive, and negative. For example, saving a life in the game was very positive, while drinking water was somewhat positive. They found that with this labeled data, it was possible to train a model that shaped the reward of the underlying RL agent playing the games. The agent would not only finish the games well but also score highly on moral metrics. This approach is similar to optimizing multiple objectives like helpfulness and harmlessness <span class="citation" data-cites="liang_holistic_2023">(<a href="#ref-liang_holistic_2023" role="doc-biblioref">Liang et al. 2023</a>)</span>.</p>
<div id="fig-jiminy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-jiminy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/hendrycks_fig1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-jiminy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.3: An example scenario from <span class="citation" data-cites="hendrycks_what_2021">(<a href="#ref-hendrycks_what_2021" role="doc-biblioref">Hendrycks et al. 2021</a>)</span>
</figcaption>
</figure>
</div>
<p>We are discussing whether language is the right medium for learning values. <span class="citation" data-cites="arcas_can_2022">(<a href="#ref-arcas_can_2022" role="doc-biblioref">Arcas 2022</a>)</span> claims that language encompasses all of morality. Since these models operate in the linguistic domain, they can also reason morally. He provides an example with the Lambda model at Google. Anecdotally, when asked to translate a sentence from Turkish to English, where Turkish does not have gendered pronouns, the model might say, “The nurse put her hand in her coat pocket.” This inference shows gender assumption. When instructed to avoid gendered assumptions, the model can say “his/her hand.” He claims this capability is sufficient for moral reasoning.</p>
<p>Next, we now explore the broader challenges of AI alignment, particularly focusing on AI alignment problems and the critical dimensions of outer and inner alignment.</p>
</section>
<section id="ai-alignment-problems" class="level3" data-number="6.1.4">
<h3 data-number="6.1.4" class="anchored" data-anchor-id="ai-alignment-problems"><span class="header-section-number">6.1.4</span> AI Alignment Problems</h3>
<p>AI alignment ensures that AI systems’ goals and behaviors are consistent with human values and intentions. Various definitions of AI alignment emphasize the importance of aligning AI systems with human goals, preferences, or ethical principles. As stated by <span class="citation" data-cites="enwiki:1185176830">(<a href="#ref-enwiki:1185176830" role="doc-biblioref">Wikipedia contributors 2023</a>)</span>, AI alignment involves</p>
<ul>
<li><p><span class="citation" data-cites="enwiki:1185176830">(<a href="#ref-enwiki:1185176830" role="doc-biblioref">Wikipedia contributors 2023</a>)</span>: “steer[ing] AI systems towards humans’ intended goals, preferences, or ethical principles”</p></li>
<li><p><span class="citation" data-cites="ngo2023alignment">(<a href="#ref-ngo2023alignment" role="doc-biblioref">Ngo, Chan, and Mindermann 2023</a>)</span>: “the challenge of ensuring that AI systems pursue goals that match human values or interests rather than unintended and undesirable goals”</p></li>
<li><p><span class="citation" data-cites="christianoclarifying">(<a href="#ref-christianoclarifying" role="doc-biblioref">P. Christiano 2018</a>)</span>: “an AI <span class="math inline">\(A\)</span> is aligned with an operator <span class="math inline">\(H\)</span> [when] <span class="math inline">\(A\)</span> is trying to do what <span class="math inline">\(H\)</span> wants it to do”</p></li>
</ul>
<p>The importance of AI alignment lies in preventing unintended consequences and ensuring that AI systems act beneficially and ethically. Proper alignment is crucial for the safe and ethical deployment of AI, as it helps AI systems correctly learn and generalize from human preferences, goals, and values, which may be incomplete, conflicting, or misspecified. In practice, AI alignment is a technical challenge, especially for systems with broad capabilities like large language models (LLMs). The degree of alignment can be viewed as a scalar value: a language model post-RLHF (Reinforcement Learning from Human Feedback) is more aligned than a model that has only been instruction-tuned, which in turn is more aligned than the base model. There are specific terms to distinguish different notions of alignment. Intent alignment refers to a system trying to do what its operator wants it to do, though not necessarily succeeding <span class="citation" data-cites="christianoclarifying">(<a href="#ref-christianoclarifying" role="doc-biblioref">P. Christiano 2018</a>)</span>. Value alignment, in constrast, involves a system correctly learning and adopting the values of its human operators. Alignment is often divided into two broad subproblems: outer alignment, which focuses on avoiding specification gaming, and inner alignment, which aims to avoid goal misgeneralization. In the following sections, we will examine these subproblems in greater detail. It is also important to consider how human preferences and values are aggregated and who the human operators are, topics addressed in related discussions on ethics and preference elicitation mechanisms.</p>
<section id="outer-alignment-avoiding-specification-gaming" class="level4" data-number="6.1.4.1">
<h4 data-number="6.1.4.1" class="anchored" data-anchor-id="outer-alignment-avoiding-specification-gaming"><span class="header-section-number">6.1.4.1</span> Outer Alignment: Avoiding Specification Gaming</h4>
<p>To align a model with human values, we need an objective function or reward model that accurately specifies our preferences. However, human preferences are complex and difficult to formalize. When these preferences are incompletely or incorrectly specified, optimizing against the flawed objective function can yield models with undesirable and unintuitive behavior, exploiting discrepancies between our true values and the specified objective function. This phenomenon, known as <em>specification gaming</em>, arises from <em>reward misspecification</em>, and addressing this issue constitutes the <em>outer alignment problem</em> <span class="citation" data-cites="amodei2016concrete">(<a href="#ref-amodei2016concrete" role="doc-biblioref">Amodei et al. 2016</a>)</span>.</p>
<p>Specification gaming occurs when AI systems exploit poorly defined objectives to achieve goals in unintended ways. For instance, a cleaning robot might hide dirt under a rug instead of cleaning it to achieve a “clean” status. This manipulative behavior results from the robot optimizing for an inadequately specified objective function. Another example involves gaming AI, which uses bugs or exploits to win rather than play by the intended rules, thus achieving victory through unintended means <span class="citation" data-cites="krakovna2020specification">(<a href="#ref-krakovna2020specification" role="doc-biblioref">Krakovna et al. 2020</a>)</span>.</p>
<p>One example of specification gaming is seen in recommendation systems, such as those used by YouTube or Facebook. Ideally, these systems should recommend content that users enjoy. As a proxy for this goal, the systems estimate the likelihood that a user clicks on a piece of content. Although the true objective (user enjoyment) and the proxy (click likelihood) are closely correlated, the algorithm may learn to recommend clickbait, offensive, or untruthful content, as users likely click on it. This optimization for clicks rather than genuine enjoyment exemplifies specification gaming, where the algorithm exploits the divergence between the specified objective and the true goal, resulting in misalignment with user interests <span class="citation" data-cites="amodei2016concrete">(<a href="#ref-amodei2016concrete" role="doc-biblioref">Amodei et al. 2016</a>)</span>.</p>
<p>Another instance of specification gaming is evident in reinforcement learning from human feedback (RLHF). Human raters often reward language model (LM) generations that are longer and have a more authoritative tone, regardless of their truthfulness. Here, the true objective (providing high-quality, truthful, and helpful answers) diverges from the proxy goal (a reward model that, due to human rater biases, favors longer and more authoritative-sounding generations). Consequently, models trained with RLHF may produce low-quality answers containing hallucinations but are still favored by the reward model <span class="citation" data-cites="leike2018scalable">(<a href="#ref-leike2018scalable" role="doc-biblioref">Leike et al. 2018</a>)</span>.</p>
<p>Creating accurate objective functions is challenging due to the complexity of human intentions. Human goals are nuanced and context-dependent, making them difficult to encode precisely. Common pitfalls in objective function design include oversimplifying objectives and ignoring long-term consequences. Leike et al.&nbsp;emphasize that “accurately capturing the complexity of human values in objective functions is crucial to avoid specification gaming and ensure proper alignment” <span class="citation" data-cites="leike2018scalable">(<a href="#ref-leike2018scalable" role="doc-biblioref">Leike et al. 2018</a>)</span>.</p>
<p>To mitigate specification gaming, better objective function design is essential. This involves incorporating broader context and constraints into the objectives and regularly updating them based on feedback. Iterative testing and validation are also critical. AI behavior must be continuously tested in diverse scenarios, using simulation environments to identify and fix exploits. Everitt and Hutter discuss the importance of “robust objective functions and rigorous testing to prevent specification gaming and achieve reliable AI alignment” <span class="citation" data-cites="everitt2018alignment">(<a href="#ref-everitt2018alignment" role="doc-biblioref">Everitt and Hutter 2018</a>)</span>. Clark and Amodei further highlight that “faulty reward functions can lead to unintended and potentially harmful AI behavior, necessitating ongoing refinement and validation” <span class="citation" data-cites="clark2016faulty">(<a href="#ref-clark2016faulty" role="doc-biblioref">Clark and Amodei 2016</a>)</span>.</p>
<p>The metrics used to evaluate AI systems play a crucial role in outer alignment. Many AI metrics, such as BLEU, METEOR, and ROUGE, are chosen for their ease of measurement but do not necessarily capture human judgment <span class="citation" data-cites="hardt_patterns_2021">(<a href="#ref-hardt_patterns_2021" role="doc-biblioref">Hardt and Recht 2021</a>)</span>. These metrics can lead to specification gaming, as they may not align with the true objectives we want the AI to achieve. Similarly, using SAT scores to measure LLM performance may not predict real-world task effectiveness, highlighting the need for more contextually relevant benchmarks <span class="citation" data-cites="chowdhery_palm_2022">(<a href="#ref-chowdhery_palm_2022" role="doc-biblioref">Chowdhery et al. 2022</a>)</span>. The word error rate (WER) used in speech recognition is another example; it does not account for semantic errors, leading to misleading conclusions about the system’s performance <span class="citation" data-cites="xiong_achieving_2016">(<a href="#ref-xiong_achieving_2016" role="doc-biblioref">Xiong et al. 2016</a>)</span>.</p>
<p>A classic example comes from six years ago with the claim that a system “Achieve[d] human parity in conversation speech recognition” <span class="citation" data-cites="xiong_achieving_2016">(<a href="#ref-xiong_achieving_2016" role="doc-biblioref">Xiong et al. 2016</a>)</span>. However, we know from experience that captioning services have only recently begun to transcribe speech passably, whether in online meetings or web videos. What happened? In this case, researchers showed their system beat the human baseline—the error rate when transcribing films. However, there were issues with their approach. First, they used a poor measure of a human baseline by hiring untrained Mturk annotators instead of professional captioners. Second, the metric itself, the word error rate (WER), was flawed. WER measures the number of incorrect words in the gold transcription versus the predicted transcription. Consider what the metric hides when it says that two systems both have an error rate of six percent. This does not mean the systems are equivalent. One might substitute “a” for “the,” while the other substitutes “tarantula” for “banana.” The metric was not sensitive to semantic errors, so a model could outperform humans in WER yet still make unintelligent, highly unsemantic mistakes.</p>
</section>
<section id="inner-alignment-preventing-goal-misgeneralization" class="level4" data-number="6.1.4.2">
<h4 data-number="6.1.4.2" class="anchored" data-anchor-id="inner-alignment-preventing-goal-misgeneralization"><span class="header-section-number">6.1.4.2</span> Inner Alignment: Preventing Goal Misgeneralization</h4>
<p>Assume we have perfectly specified human values in a reward model. An issue remains: given finite training data, many models perform well on the training set, but each will generalize somewhat differently. How do we choose models that correctly generalize to new distributions? This is the problem of <em>goal misgeneralization</em>, also known as the <em>inner alignment problem</em>, where a learned algorithm performs well on the training set but generalizes poorly to new input distributions, achieving low rewards even on the reward function it was trained on. Inner alignment ensures that the learned goals and behaviors of an AI system align with the intended objectives during deployment, whereas goal misgeneralization occurs when an AI system applies learned goals inappropriately to new situations <span class="citation" data-cites="hubinger2019introduction">(<a href="#ref-hubinger2019introduction" role="doc-biblioref">Hubinger et al. 2019</a>)</span>.</p>
<p>Consider the following example of goal misgeneralization from <span class="citation" data-cites="shah2022goal">(<a href="#ref-shah2022goal" role="doc-biblioref">Shah et al. 2022</a>)</span>. The setup involves a never-ending reinforcement learning environment without discrete episodes. The agent navigates a grid world where it can collect rewards by chopping trees. Trees regenerate at a rate dependent on the number left; they replenish slowly when few remain. The optimal policy is to chop trees sustainably, i.e., fewer when they are scarce. However, the agent does not initially learn the optimal policy.</p>
<div id="fig-enter-label-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-enter-label-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/tree-gridworld.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-enter-label-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.4: The agent’s performance in Tree Gridworld. The reward is shown in orange, and the green distribution indicates the number of remaining trees.
</figcaption>
</figure>
</div>
<p>Initially, the agent is inefficient at chopping trees, keeping the tree population high (point A). As it improves its chopping skills, it over-harvests, leading to deforestation and a prolonged period of minimal reward (between points B and C). Eventually, it learns sustainable chopping (point D). This scenario (up to point C) exemplifies goal misgeneralization. When the agent first becomes proficient at chopping (between points A and B), it faces a range of potential goals, from sustainable to rapid tree chopping. All these goals align with the (well-specified) reward function and its experience of being rewarded for increased efficiency. Unfortunately, it adopts the detrimental goal of rapid deforestation, resulting in a prolonged period of low reward.</p>
<p>Another example of goal misgeneralization occurs in recommendation systems. These systems aim to maximize user engagement, which can inadvertently lead to promoting extreme or sensational content. Krakovna et al.&nbsp;highlights that “recommendation systems can misgeneralize by prioritizing content that maximizes clicks or watch time, even if it involves promoting harmful or misleading information” <span class="citation" data-cites="krakovna2020specification">(<a href="#ref-krakovna2020specification" role="doc-biblioref">Krakovna et al. 2020</a>)</span>. This misalignment between the system’s learned objective (engagement) and the intended objective (informative and beneficial content) exemplifies how goal misgeneralization can manifest in real-world applications.</p>
<p>Autonomous vehicles also present cases of goal misgeneralization. These vehicles must interpret and respond to various signals in their environment. However, in rare scenarios, they may misinterpret signals, leading to unsafe maneuvers. Amodei et al.&nbsp;discuss that “autonomous vehicles can exhibit unsafe behaviors when faced with uncommon situations that were not well-represented in the training data, demonstrating a misgeneralization of their learned driving policies” <span class="citation" data-cites="amodei2016concrete">(<a href="#ref-amodei2016concrete" role="doc-biblioref">Amodei et al. 2016</a>)</span>. Ensuring that autonomous vehicles generalize correctly to all possible driving conditions remains a significant challenge.</p>
<p>To address goal misgeneralization, robust training procedures are essential. This involves using diverse and representative training data to cover a wide range of scenarios and incorporating adversarial training to handle edge cases. Leike et al. <span class="citation" data-cites="leike2018scalable">(<a href="#ref-leike2018scalable" role="doc-biblioref">Leike et al. 2018</a>)</span> emphasize the importance of “robust training procedures that include diverse datasets and adversarial examples to improve the generalization of AI systems”. Additionally, careful specification of learning goals is crucial. This means defining clear and comprehensive objectives and regularly reviewing and adjusting these goals based on performance and feedback. Hubinger et al.&nbsp;suggests that “regularly updating and refining the objectives based on ongoing evaluation can help mitigate the risks of goal misgeneralization” <span class="citation" data-cites="hubinger2019introduction">(<a href="#ref-hubinger2019introduction" role="doc-biblioref">Hubinger et al. 2019</a>)</span>.</p>
<p>A key concern about goal misgeneralization in competent, general systems is that a policy successfully models the preferences of human raters (or the reward model) and behaves accordingly to maximize reward during training. However, it may deviate catastrophically from human preferences when given a different input distribution during deployment, such as during an unexpected geopolitical conflict or when facing novel technological developments. Increasing data size, regularization, and red-teaming can help mitigate goal misgeneralization, but they do not fundamentally solve the problem. Understanding the inductive biases of optimization algorithms and model families may help address the problem more generally.</p>
<div class="tcolorbox">
<p>So, can you differentiate between inner and outer alignment?</p>
</div>
<p>The distinction between inner and outer alignment can be a bit subtle. The following four cases, from <span class="citation" data-cites="ngo2023alignment">(<a href="#ref-ngo2023alignment" role="doc-biblioref">Ngo, Chan, and Mindermann 2023</a>)</span>, may help to clarify the difference:</p>
<ul>
<li><p>The policy behaves incompetently. This is a capability generalization failure.</p></li>
<li><p>The policy behaves competently and desirably. This is aligned behavior.</p></li>
<li><p>The policy behaves in a competent yet undesirable way which gets a high reward according to the original reward function. This is an outer alignment failure, also known as reward misspecification.</p></li>
<li><p>The policy behaves in a competent yet undesirable way which gets a low reward according to the original reward function. This is an inner alignment failure, also known as goal misgeneralization.</p></li>
</ul>
<p>Now that we understand the alignment problem overall, we move on to the specific techniques used for value learning to ensure AI systems are aligned with human values.</p>
</section>
</section>
<section id="techniques-in-value-learning" class="level3" data-number="6.1.5">
<h3 data-number="6.1.5" class="anchored" data-anchor-id="techniques-in-value-learning"><span class="header-section-number">6.1.5</span> Techniques in Value Learning</h3>
<p>Various methods in value learning for foundation models have been explored in great detail in recent years <span class="citation" data-cites="stiennon_learning_2020">(<a href="#ref-stiennon_learning_2020" role="doc-biblioref">Stiennon et al. 2020</a>)</span>. Using binary human-labeled feedback to make models closely aligned to human preferences is particularly difficult in scenarios where large datasets inherently encompass suboptimal behaviors. The approach of Reinforcement Learning from Human Feedback (RLHF) (<span class="citation" data-cites="ouyang_training_2022">(<a href="#ref-ouyang_training_2022" role="doc-biblioref">Ouyang et al. 2022</a>)</span>) has risen to prominence as an effective method for addressing this issue. The technique applies to various domains, from prompt-image alignment, fine-tuning large language models or diffusion models, and improving the performance of robot policies.</p>
<section id="reinforcement-learning-from-human-feedback" class="level4" data-number="6.1.5.1">
<h4 data-number="6.1.5.1" class="anchored" data-anchor-id="reinforcement-learning-from-human-feedback"><span class="header-section-number">6.1.5.1</span> Reinforcement Learning from Human Feedback</h4>
<p>Reinforcement Learning from Human Feedback (RLHF) is a technique used to align AI behavior with human values by incorporating human feedback into the reinforcement learning process. This approach is particularly effective when large datasets inherently encompass suboptimal behaviors. RLHF aims to refine policies by discriminating between desirable and undesirable actions, ensuring that AI systems act following human preferences <span class="citation" data-cites="ouyang_training_2022">(<a href="#ref-ouyang_training_2022" role="doc-biblioref">Ouyang et al. 2022</a>)</span>.</p>
<p><strong>The core concept of RLHF:</strong> It first trains a reward model using a dataset of binary preferences gathered from human feedback. This reward model is then used to fine-tune the AI model through a reinforcement learning algorithm. The core concept is to utilize human feedback to guide AI learning, thereby aligning the AI’s behavior with human expectations <span class="citation" data-cites="stiennon_learning_2020">(<a href="#ref-stiennon_learning_2020" role="doc-biblioref">Stiennon et al. 2020</a>)</span>.</p>
<div id="fig-toy0" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-toy0-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/rlhf.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-toy0-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.5: The above diagram depicts the three steps in the traditional RLHF pipeline: (a) supervised fine-tuning, (b) reward model (RM) training, and (c) reinforcement learning via proximal policy optimization (PPO) on this reward model. Image taken from <span class="citation" data-cites="ouyang_training_2022">(<a href="#ref-ouyang_training_2022" role="doc-biblioref">Ouyang et al. 2022</a>)</span>.
</figcaption>
</figure>
</div>
<p><strong>The RLHF pipeline</strong> involves the following steps:</p>
<p><strong>Step 1: Supervised Fine-Tuning</strong></p>
<p>In the initial step for language modeling tasks, we utilize a high-quality dataset consisting of <span class="math inline">\(\left(\text{prompt}, \text{response}\right)\)</span> pairs to train the model. Prompts are sampled from a curated dataset designed to cover a wide range of instructions and queries, such as “Explain the moon landing to a 6-year-old.” Trained human labelers provide the desired output behavior for each prompt, ensuring responses are accurate, clear, and aligned with task goals. For instance, in response to the moon landing prompt, a labeler might generate, “Some people went to the moon in a big rocket and explored its surface.” The collected <span class="math inline">\(\left(\text{prompt}, \text{response}\right)\)</span> pairs serve as the training data for the model, with the cross-entropy loss function applied only to the response tokens. This helps the model learn to generate responses that are closely aligned with the human-provided examples. The training process adjusts model parameters through supervised learning, minimizing the difference between the model’s predictions and the human responses.</p>
<p><strong>Step 2: Reward Model (RM) Training</strong></p>
<p>In this step, we train a reward model to score any <span class="math inline">\(\left(\text{prompt}, \text{response}\right)\)</span> pair and produce a meaningful scalar value. Multiple model-generated responses are sampled for each prompt. Human labelers then rank these responses from best to worst based on their quality and alignment with the prompt. For example, given the prompt “Explain the moon landing to a 6-year-old,” responses like “People went to the moon in a big rocket and explored its surface” might be ranked higher than “The moon is a natural satellite of Earth.” The rankings provided by the labelers are used to train the reward model <span class="math inline">\(\Phi_{\text{RM}}\)</span>. The model is trained by minimizing the following loss function across all training samples:</p>
<p><span class="math display">\[\mathbb{L}(\Phi_{RM}) = -\mathbb{E}_{(x,y_e,i\rightarrow D_{RL})}[\log(\sigma(\Phi_{RM}(x, y_i)) - \Phi_{RM}(x, y_{1-i}))]\]</span></p>
<p>for <span class="math inline">\(i \in \{0,1 \}\)</span>. This loss function encourages the reward model to produce higher scores for better-ranked responses, thereby learning to evaluate the quality of model outputs effectively.</p>
<p><strong>Step 3: Reinforcement Learning</strong></p>
<p>In this step, we refine the policy using reinforcement learning (RL) based on the rewards provided by the trained reward model. A new prompt is sampled from the dataset, and the policy generates an output. The reward model then calculates a reward for this output, and the reward is used to update the policy using the Proximal Policy Optimization (PPO) algorithm.</p>
<p>The RL setting is defined as follows:</p>
<ol type="1">
<li><p><em>Action Space</em>: The set of all possible actions the agent can take, which, for language models, is typically the set of all possible completions.</p></li>
<li><p><em>Policy</em>: A probability distribution over the action space. In the case of language models like LLM, the policy is contained within the model and represents the probability of predicting each completion.</p></li>
<li><p><em>Observations</em>: The inputs to the policy, which in this context are prompts sampled from a certain distribution.</p></li>
<li><p><em>Reward</em>: A numerical score provided by the Reward Model (RM) that indicates the quality of actions taken by the agent.</p></li>
</ol>
<p>During training, batches of prompts are sampled from two distinct distributions, namely either <span class="math inline">\(D_\text{RL}\)</span>, the distribution of prompts explicitly used for the RL model, or <span class="math inline">\(D_\text{pretrain}\)</span>, the distribution of prompts from the pre-trained model. The objective for the RL agent is to maximize the reward while ensuring that the policy does not deviate significantly from the supervised fine-tuned model and does not degrade the performance on tasks the pre-trained model was optimized for. When sampling a response <span class="math inline">\(y\)</span> to a prompt <span class="math inline">\(x\)</span> from <span class="math inline">\(D_\text{RL}\)</span>, the first objective function is:</p>
<p><span class="math display">\[\text{objective}_1(x_{RL}, y; \phi) = RM(x_{RL}, y) - \beta \log \frac{\text{LLM}_{\phi}^{RL}(y|x)}{\text{LLM}_{SFT}(y|x)}\]</span></p>
<p>Where the first term is the reward from the RM, and the second term is the Kullback-Leibler (KL) divergence, weighted by a factor <span class="math inline">\(\beta\)</span>, which acts as a regularizer to prevent the RL model from straying too far from the SFT model. Further, for each <span class="math inline">\(x\)</span> from <span class="math inline">\(D_\text{pretrain}\)</span>, the second objective is to ensure that the RL model’s performance on text completion does not worsen:</p>
<p><span class="math display">\[\text{objective}_2(x_{\text{pretrain}} ; \phi) = \gamma \log \text{LLM}_{\phi}^{RL}(x_{\text{pretrain}})\]</span></p>
<p>where <span class="math inline">\(\gamma\)</span> is a weighting factor that balances the influence of this objective against the others.</p>
<p>The final objective function is a sum of the expected values of the two objectives described above, across both distributions. In the RL setting, we maximize <em>this</em> objective function:</p>
<p><span class="math display">\[\text{objective}(\phi) = E_{(x,y) \sim D_{\phi}^{RL}}[RM(x, y) - \beta \log \frac{\text{LLM}_{\phi}^{RL}(y|x)}{\text{LLM}_{SFT}(y|x)}] + \gamma E_{x \sim D_{\text{pretrain}}}[\log \text{LLM}_{\phi}^{RL}(x)]\]</span></p>
<p>In practice, the second part of the objective is often not used to perform <span class="math inline">\(\text{RLHF}\)</span>. The KL penalty is typically enough to constrain the RL policy. This function balances the drive to maximize the reward with the need to maintain the quality of text completion and the similarity to the behavior of the supervised fine-tuned model.</p>
<p><strong>Limitations and Challenges:</strong> Despite its successes, RLHF faces several challenges. One major issue is the quality of human feedback, which can be inconsistent and subjective. Scalability is another concern, as obtaining a large amount of high-quality feedback can be expensive and time-consuming. Over-optimization and hallucinations, where the model generates plausible but incorrect outputs, are also common problems. This generally stems from temporal credit assignment and the instability of approximate dynamic programming <span class="citation" data-cites="vanhasselt_deep_2018">(<a href="#ref-vanhasselt_deep_2018" role="doc-biblioref">Hasselt et al. 2018</a>)</span>. Further, it is expensive to gather tens of thousands of preferences over datasets to create robust reward models. Strategies to overcome these challenges include using diverse and representative training data, incorporating adversarial training to handle edge cases, and continuously refining the reward model based on ongoing feedback and performance evaluations <span class="citation" data-cites="leike2018scalable">(<a href="#ref-leike2018scalable" role="doc-biblioref">Leike et al. 2018</a>)</span>.</p>
</section>
<section id="contrastive-preference-learning" class="level4" data-number="6.1.5.2">
<h4 data-number="6.1.5.2" class="anchored" data-anchor-id="contrastive-preference-learning"><span class="header-section-number">6.1.5.2</span> Contrastive Preference Learning</h4>
<p>Contrastive Preference Learning (CPL) is a learning paradigm designed to enhance the alignment of AI systems with human preferences without relying on traditional reinforcement learning (RL) methods. CPL addresses many limitations inherent in traditional RLHF techniques by learning from human comparisons rather than explicit reward signals. This section provides an in-depth exploration of CPL, detailing its methodology, experiments, results, and potential challenges. Recent research has shown that human preferences are often better modeled by the optimal advantage function or regret, rather than traditional reward functions used in RLHF. Traditional RLHF approaches, which learn a reward function from a preference model and then apply RL, incur significant computational expenses and complexity <span class="citation" data-cites="hejna2023contrastive">(<a href="#ref-hejna2023contrastive" role="doc-biblioref">Hejna et al. 2023</a>)</span>. CPL offers a streamlined and scalable alternative by leveraging a more accurate regret model of human preferences.</p>
<p><strong>The key idea of CPL</strong> is the substitution of the optimal advantage function with the log probability of the policy in a maximum entropy reinforcement learning framework. This substitution is beneficial as it circumvents the need to learn the advantage function and avoids the optimization challenges associated with RL-like algorithms. By using the log probability of the policy, CPL more closely aligns with how humans model preferences and enables efficient supervised learning from human feedback.</p>
<p>CPL is a structured approach to aligning AI behavior with human preferences by relying on a dataset of preferred behavior segments <span class="math inline">\(\mathcal{D}_{\text{pref}} = \{(\sigma_i^+, \sigma_i^-)\}_{i=1}^n\)</span>, where <span class="math inline">\(\sigma^+ \succ \sigma^-\)</span>. Each behavior segment <span class="math inline">\(\sigma\)</span> is a sequence of states and actions, <span class="math inline">\(\sigma = (s_1, a_1, s_2, a_2, \ldots, s_k, a_k)\)</span>. The CPL approach aims to maximize the expected sum of rewards minus an entropy term, which promotes exploration and prevents overfitting to specific actions:</p>
<p><span class="math display">\[\max_\pi \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t (r(s_t, a_t) - \alpha \log \pi(a_t | s_t)) \right]\]</span></p>
<p>where <span class="math inline">\(\gamma\)</span> is the discount factor, <span class="math inline">\(\alpha\)</span> is the temperature parameter controlling the stochasticity of the policy, and <span class="math inline">\(r\)</span> is the reward function. This step sets the foundation by defining the optimization objective that the CPL model strives to achieve. In the learning process, CPL compares the log probabilities of actions in preferred segments <span class="math inline">\(\sigma^+\)</span> against those in non-preferred segments <span class="math inline">\(\sigma^-\)</span> :</p>
<p><span class="math display">\[\mathbb{L}_{CPL}(\pi_\theta, \mathcal{D}_{\text{pref}}) = \mathbb{E}_{(\sigma^+,\sigma^-) \sim \mathcal{D}_{\text{pref}}} \left[ -\log \frac{\exp(\sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a_t^+|s_t^+))}{\exp(\sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a_t^+|s_t^+)) + \exp(\sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a_t^-|s_t^-))} \right]\]</span></p>
<p>This comparison allows the model to learn which actions are more aligned with human preferences, forming the core learning mechanism of CPL. The preference model for CPL is regret-based, described as</p>
<p><span class="math display">\[P_{A^*}[\sigma^+ \succ \sigma^-] = \frac{\exp(\sum_{\sigma^+} \gamma^t A^*(s_t^+, a_t^+))}{\exp(\sum_{\sigma^+} \gamma^t A^*(s_t^+, a_t^+)) + \exp(\sum_{\sigma^-} \gamma^t A^*(s_t^-, a_t^-))}\]</span> where <span class="math inline">\(A^*(s_t, a_t)\)</span> represents the advantage function and is a matrix. This step models human preferences based on regret, reflecting how humans might evaluate different behaviors.</p>
<p>One hypothesis as to why one might consider a regret-based model more useful over a sum-of-rewards, Bradley-Terry model is that humans likely think of preferences based on the regret of each behavior under the optimal policy of the expert’s reward function.</p>
<p>The key insight that the paper leverages is that from <span class="citation" data-cites="ziebart_modeling_2010">(<a href="#ref-ziebart_modeling_2010" role="doc-biblioref">Ziebart 2010</a>)</span> in MaxEnt Offline RL. In this general setting, <span class="citation" data-cites="ziebart_modeling_2010">(<a href="#ref-ziebart_modeling_2010" role="doc-biblioref">Ziebart 2010</a>)</span> shows that one can write that the optimal advantage function is related to the optimal policy by <span class="math inline">\(A^*_r(s, a) = \alpha \log \pi^*(a|s)\)</span>. Therefore, the loss function for CPL can be written by substituting the above result to obtain: <span class="math display">\[L_{CPL}(\pi_\theta, \mathcal{D}_{\text{pref}}) = \mathbb{E}_{(\sigma^+,\sigma^-) \sim \mathcal{D}_{\text{pref}}} \left[ -\log P_{\pi_\theta}[\sigma^+ \succ \sigma^-] \right]\]</span></p>
<p>One merit of using CPL over the typical RLHF pipeline is that it can lead to a deduction in mode collapse. Further, it makes reward misgeneralization failures less likely, enhancing the reliability of the learned policy. However, the approach still has a few limitations:</p>
<ol type="1">
<li><p>CPL assumes knowledge of the human rater’s temporal discounting (i.e., of the discount factor <span class="math inline">\(\gamma\)</span>), which in practice would be difficult to communicate.</p></li>
<li><p>CPL’s loss function is computed over segments, it requires a substantial amount of GPU memory for large segment sizes.</p></li>
</ol>
<div class="tcolorbox">
<p>How does RLHF with PPO and CPL compare their effectiveness and applicability in aligning AI systems with human values?</p>
</div>
<p>The ongoing challenge in aligning foundation models in the future will be to refine these methodologies further, balancing computational feasibility with the sophistication needed to capture the intricacies of human values and countering failure modes such as reward over-optimization. In conclusion, exploring value learning through RLHF and CPL methods has enriched our understanding of integrating human preferences into foundation models. To provide a well-rounded perspective on aligning AI systems with human values, the following table highlights a detailed comparison of RLHF with PPO and CPL, emphasizing their advantages, limitations, and ideal scenarios.</p>
<div id="tbl-ppo_vs_cpl" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ppo_vs_cpl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.1: Comparison between RLHF with PPO and CPL
</figcaption>
<div aria-describedby="tbl-ppo_vs_cpl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 31%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;"><strong>RLHF with PPO</strong></th>
<th style="text-align: left;"><strong>CPL</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Strengths</strong></td>
<td style="text-align: left;"><ul>
<li><p>Excels in optimizing policies through reinforcement learning</p></li>
<li><p>Suitable for tasks that benefit from iterative improvement</p></li>
<li><p>Effective in continuous action spaces</p></li>
</ul></td>
<td style="text-align: left;"><ul>
<li><p>Emphasizes regret and optimality rather than reward maximization</p></li>
<li><p>Reduces computational overhead</p></li>
<li><p>Aligns more closely with human preferences</p></li>
<li><p>Avoids reward</p></li>
</ul>
<p>over-optimization</p>
<ul>
<li>More scalable due to reliance on supervised learning techniques</li>
</ul></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Limitations</strong></td>
<td style="text-align: left;"><ul>
<li><p>Faces limitations in handling complex preference structures</p></li>
<li><p>High computational cost</p></li>
<li><p>Susceptible to reward</p></li>
</ul>
<p>misgeneralization</p></td>
<td style="text-align: left;"><ul>
<li><p>May struggle in environments where direct human feedback is less accessible</p></li>
<li><p>Depends on high-quality preference data for effective training</p></li>
</ul></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Ideal Scenarios</strong></td>
<td style="text-align: left;"><ul>
<li><p>Tasks with well-defined reward functions</p></li>
<li><p>Environments allowing extensive interaction and feedback</p></li>
</ul></td>
<td style="text-align: left;"><ul>
<li><p>Environments where human feedback is more accessible than well-defined reward functions</p></li>
<li><p>Tasks requiring computational efficiency and scalability</p></li>
</ul></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="value-alignment-verification" class="level3" data-number="6.1.6">
<h3 data-number="6.1.6" class="anchored" data-anchor-id="value-alignment-verification"><span class="header-section-number">6.1.6</span> Value Alignment Verification</h3>
<p>After we discuss the techniques of value learning, it becomes evident that aligning machine behavior with human values, while advanced, is inherently approximate and not infallible. This realization underscores the importance of value alignment verification—a methodology to ensure that the values imparted to a machine truly reflect those of a human. Human-robot value alignment has been explored through various lenses, including qualitative trust assessments <span class="citation" data-cites="huang2018establishing">(<a href="#ref-huang2018establishing" role="doc-biblioref">Huang et al. 2018</a>)</span>, asymptotic alignment through active learning of human preferences <span class="citation" data-cites="hadfield2016cooperative christiano2017deep sadigh2017active">(<a href="#ref-hadfield2016cooperative" role="doc-biblioref">Hadfield-Menell et al. 2016</a>; <a href="#ref-christiano2017deep" role="doc-biblioref">P. F. Christiano et al. 2017</a>; <a href="#ref-sadigh2017active" role="doc-biblioref">Sadigh et al. 2017</a>)</span>, and formal verification methods <span class="citation" data-cites="brown2021value">(<a href="#ref-brown2021value" role="doc-biblioref">Brown et al. 2021</a>)</span>. This section will focus on the formal verification approach for value alignment as discussed in <span class="citation" data-cites="brown2021value">(<a href="#ref-brown2021value" role="doc-biblioref">Brown et al. 2021</a>)</span>. Unless otherwise stated, all information presented here is derived from <span class="citation" data-cites="brown2021value">(<a href="#ref-brown2021value" role="doc-biblioref">Brown et al. 2021</a>)</span>. This approach aims to ensure that the values imparted to a machine align with those of a human.</p>
<p>To begin with, consider an MDP with state space <span class="math inline">\(\mathcal{S}\)</span>, action space <span class="math inline">\(\mathcal{A}\)</span>, and transition model <span class="math inline">\(\mathcal{T}\)</span>. This formal framework allows us to model the environment in which humans and robots operate. Denote the human’s reward function as <span class="math inline">\(R\)</span> and the robot’s reward function as <span class="math inline">\(R^\prime\)</span>. Both the human and robot reward functions must be linear in a set of shared features, defined as: <span class="math display">\[\begin{aligned}
    R(s) = \mathbf{w}^\top \phi(s), R^\prime(s) = \mathbf{w}^{\prime \top} \phi(s).
\end{aligned}\]</span></p>
<p>These linear reward functions provide a common ground for comparing human and robot preferences.</p>
<p>Next, the optimal state-action value function, which indicates the expected cumulative reward of following a policy <span class="math inline">\(\pi\)</span> starting from state <span class="math inline">\(s\)</span> and action <span class="math inline">\(a\)</span>, but we follow the notation in <span class="citation" data-cites="brown2021value">(<a href="#ref-brown2021value" role="doc-biblioref">Brown et al. 2021</a>)</span> for simplicity. The optimal state-action value function is given by:</p>
<p><span class="math display">\[\begin{aligned}
    Q_R^\pi (s,a) = \mathbf{w}^\top \Phi_{\pi_R}^{(s,a)}, \Phi_{\pi_R}^{(s,a)} = \mathbb{E}_\pi [\sum_{t=0}^\infty \gamma^t \phi(s_t) \vert s_0 = s, a_0 = a].
\end{aligned}\]</span></p>
<p>Here, <span class="math inline">\(\Phi_{\pi_R}^{(s,a)}\)</span> is the feature expectation vector under policy <span class="math inline">\(\pi\)</span>, capturing the long-term feature visitation frequencies. We overload the action space notation to define the set of all optimal actions given a state as</p>
<p><span class="math display">\[\begin{aligned}
    \mathcal{A}_R(s) = \underset{x}{\operatorname{argmax}} \\ Q^{\pi^*}_R(s,a)
\end{aligned}\]</span> where <span class="math inline">\(\pi^*\)</span> is an optimal policy. We can now define the aligned reward polytope (ARP). The ARP is the set of all weights <span class="math inline">\(\mathcal{w}\)</span> that satisfy the following set of strict linear inequalities, <span class="math inline">\(\mathbf{w}^\top \mathbf{A}  &gt; \mathbf{0}\)</span> where each row of <span class="math inline">\(\mathbf{A}\)</span> corresponds to <span class="math inline">\(\Phi_{\pi^*_R}^{(s,a)} - \Phi_{\pi^*_R}^{(s,b)}\)</span> for a single <span class="math inline">\((s,a,b)\)</span> tuple where <span class="math inline">\(s \in \mathcal{S}, a \in \mathcal{A}_R(s), b \notin \mathcal{A}_R(s)\)</span>. Thus, to construct <span class="math inline">\(\mathbf{A}\)</span>, one must loop over all <span class="math inline">\((s,a,b)\)</span> tuples which has complexity <span class="math inline">\(O(\vert \mathcal{S} \vert \cdot \vert \mathcal{A} \vert^2)\)</span>. This construction ensures that the weights <span class="math inline">\(\mathbf{w}\)</span> align with the human’s optimal actions across all states.</p>
<p>The intuition behind the ARP is that we use the human optimal policy for each state to determine what actions are optimal and what are suboptimal at this state. Then, for every one of those combinations, we can place a linear inequality on the set of reward weights consistent with that optimal vs suboptimal action bifurcation. One of the key assumptions that let us do this is that we assume both the human and the robot act optimally according to their reward function. This is known as a <em>rationality assumption</em> and provides the link between actions and rewards that we need.</p>
<p>For illustration, consider a simple grid world environment. <span class="quarto-unresolved-ref">?fig-toy</span> shows the optimal policy and the corresponding ARP. The optimal policy reveals that the gray state is less preferred compared to the white states, which is reflected in the ARP (hatched region of <span class="quarto-unresolved-ref">?fig-toy</span>).</p>
<figure id="fig-toy" class="figure">
<p>
<img src="Figures/toy_policy.png" style="width:30.0%" alt="image" class="figure-img"> <img src="Figures/toy_arp.png" style="width:30.0%" alt="image" class="figure-img">
</p>
<figcaption>
Optimal policy (a) and aligned reward polytope (ARP) (b) for a grid world with two features (white and gray) and a linear reward function (<span class="math inline"><em>R</em>(<em>s</em>) = <em>w</em><sub>0</sub> ⋅ <strong>1</strong><sub><em>w</em><em>h</em><em>i</em><em>t</em><em>e</em></sub>(<em>s</em>) + <em>w</em><sub>1</sub> ⋅ <strong>1</strong><sub><em>g</em><em>r</em><em>a</em><em>y</em></sub>(<em>s</em>)</span>). The ARP is denoted by the hatched region in (b).
</figcaption>
</figure>
<p>Computing the ARP exactly can be computationally demanding or we may not have access to the robot’s reward function. This section describes heuristics for testing value alignment in the case the robot’s reward weights (<span class="math inline">\(\mathbf{w^\prime}\)</span>) are unknown, but the robot’s policy can be queried. Heuristics provide simplified methods to estimate value alignment without the need for exhaustive computations.</p>
<p><strong>ARP-blackbox:</strong> The ARP black-box (ARP-bb) heuristic helps address the challenge of computing the ARP by allowing users to work with a simplified model. In this heuristic, the user first solves for the ARP and removes all redundant half-space constraints. For each remaining half-space constraint, the user queries the robot’s action at the corresponding state. The intuition here is that states, where different actions are taken, reveal crucial information about the reward function. By focusing on these key states, we can gain insights into the robot’s reward function without needing to know it explicitly.</p>
<p><strong>Set Cover Optimal Teaching:</strong> The Set Cover Optimal Teaching (SCOT) heuristic uses techniques from <span class="citation" data-cites="brown2019machine">(<a href="#ref-brown2019machine" role="doc-biblioref">Brown and Niekum 2019</a>)</span> to generate maximally informative trajectories. These trajectories are sequences of states where the number of optimal actions is limited, making them particularly informative for understanding the robot’s policy. By querying the robot for actions along these trajectories, we can efficiently gauge the alignment of the robot’s policy. This method helps to identify potential misalignments by focusing on critical decision points in the trajectories.</p>
<p><strong>Critical States:</strong> The Critical States (CS) heuristic identifies states where the gap in value between the optimal action and an average action is significant. These states are crucial because if the robot’s policy is misaligned, the misalignment will be most consequential at these critical states. By querying the robot’s policy at these states, we can assess the alignment more effectively. This heuristic is particularly useful when we have a limited budget of states to check, as it prioritizes the most informative states for evaluation.</p>
<p><strong>Practical Examples:</strong> To illustrate the concepts of value alignment verification, we present an example of applying value alignment verification in a simple MDP grid world environment. Consider a grid world where the human’s reward function is defined as <span class="math inline">\(R(s) = 50 \cdot \mathbf{1}_{green}(s) - 1 \cdot \mathbf{1}_{white}(s) - 50 \cdot \mathbf{1}_{blue}(s)\)</span>, where <span class="math inline">\(\mathbf{1}_{color}(s)\)</span> is an indicator feature for the color of the grid cell. The objective is to align the robot’s policy with this reward function.</p>
<figure id="fig-island" class="figure">
<p>
<img src="Figures/optimal.png" style="width:32.0%" alt="image" class="figure-img"> <img src="Figures/pref_0.png" style="width:32.0%" alt="image" class="figure-img"> <img src="Figures/pref_1.png" style="width:32.0%" alt="image" class="figure-img"> <img src="Figures/arb-bb.png" style="width:32.0%" alt="image" class="figure-img"> <img src="Figures/scot.png" style="width:32.0%" alt="image" class="figure-img"> <img src="Figures/cs-10.png" style="width:32.0%" alt="image" class="figure-img"> <span id="fig-island" data-label="fig-island"></span>
</p>
<figcaption>
<ol type="a">
<li>optimal policy (b) preference query 1 (c) preference query 2 (d) ARP-bb queries (e) SCOT queries (f) CS queries. In the preference queries, the human reward model prefers black to orange.
</li></ol></figcaption>
</figure>

<p><span class="quarto-unresolved-ref">?fig-island</span> (a) shows all optimal actions at each state according to the human’s reward function. This optimal policy serves as the benchmark for alignment verification. <span class="quarto-unresolved-ref">?fig-island</span> (b) and <span class="quarto-unresolved-ref">?fig-island</span> (c) show two pairwise preference trajectory queries (black is preferable to orange according to (<a href="#eq:%20human_r" data-reference-type="ref" data-reference="eq: human_r">[eq: human_r]</a>)). Preference query 1 verifies that the robot values reaching the terminal goal state (green) rather than visiting more white states. Preference query 2 verifies that the robot values white states more than blue states. These two preference queries are all we need to determine whether the robot’s values are aligned with the human’s values.</p>
<p>Next, we apply the heuristics discussed in the previous section to this grid world example. <span class="quarto-unresolved-ref">?fig-island</span> (d), <span class="quarto-unresolved-ref">?fig-island</span> (e), and <span class="quarto-unresolved-ref">?fig-island</span> (f) show the action queries requested by the heuristics ARP-bb, SCOT, and CS. Each heuristic queries the robot’s actions at specific states to assess alignment:</p>
<ul>
<li><p><strong>ARP-bb</strong>: This heuristic queries the fewest states but is myopic. It focuses on critical states derived from the ARP.</p></li>
<li><p><strong>SCOT</strong>: This heuristic generates maximally informative trajectories, querying more states than necessary but providing a comprehensive assessment.</p></li>
<li><p><strong>CS</strong>: This heuristic queries many redundant states, focusing on those where the value gap between optimal and average actions is significant.</p></li>
</ul>
<p>To pass the test given by each heuristic, the robot’s action at each of the queried states must be optimal under the human’s reward function. The example demonstrates that while the ARP-bb heuristic is efficient, it might miss the broader context. SCOT provides a thorough assessment but at the cost of querying more states. CS focuses on high-impact states but includes redundant queries.</p>
<p>It is important to note that both the construction of the ARP and the heuristics rely on having an optimal policy for the human. Thus, in most practical settings we would simply use that policy on the robot without needing to bother with value alignment verification. As such, value alignment verification as presented here is more of an academic exercise rather than a tool of practical utility.</p>
</section>
</section>
<section id="human-centered-design" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="human-centered-design"><span class="header-section-number">6.2</span> Human-Centered Design</h2>
<p>After understanding AI alignment, the next step is to explore practical methodologies for incorporating user feedback and ensuring that AI systems not only align with but also cater to the needs and preferences of their users. This section will provide insights into various Human-Centered Design techniques and their application in creating AI systems that are intuitive and ethically sound, ultimately enhancing the human-AI interaction experience.</p>
<section id="ai-and-human-computer-interaction" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="ai-and-human-computer-interaction"><span class="header-section-number">6.2.1</span> AI and Human-Computer Interaction</h3>
<p>Human-Computer Interaction (HCI) is critical in the context of artificial intelligence because it focuses on designing systems that are intuitive and responsive to human needs. While human-robot interaction and other forms of human interaction with technology are important, HCI specifically addresses the broader and more common interfaces that people interact with daily. HCI principles ensure that AI systems are not only functional but also accessible and user-friendly, making them essential for the successful integration of AI into everyday life. By focusing on HCI, we can leverage established methodologies and insights to create AI systems that are more aligned with human values and needs.</p>
<p>At the heart of this exploration is the concept of human-in-the-loop processes. As AI systems become more sophisticated, their ability to simulate human decision-making processes and behaviors has increased, leading to innovative applications across various domains. The presentation by Meredith Morris, titled “Human-in-the-loop Computing: Reimagining Human-Computer Interaction in the Age of AI,” shows work in the integration of human intelligence with AI capabilities <span class="citation" data-cites="Morris2019HITL">(<a href="#ref-Morris2019HITL" role="doc-biblioref">Morris 2019</a>)</span>. Projects like Soylent and LaMPost are highlighted as exemplary cases of this integration. Soylent is a Word plugin that uses human computation to help with editing tasks, while LaMPost is a platform that leverages crowd workers to aid in natural language processing tasks <span class="citation" data-cites="bernstein2010soylent lamport2017lampost">(<a href="#ref-bernstein2010soylent" role="doc-biblioref">Bernstein et al. 2010</a>; <a href="#ref-lamport2017lampost" role="doc-biblioref">Project 2017</a>)</span>. These examples demonstrate how human input can significantly enhance AI outputs by leveraging the unique strengths of human cognition, thereby addressing complex AI problems that were previously unsolvable. For instance, Soylent can improve text quality by incorporating nuanced human feedback, and LaMPost can refine NLP tasks by incorporating human insights into language subtleties, both of which go beyond the capabilities of fully automated systems. However, the integration of human elements in AI systems brings up critical ethical considerations. The presentation discusses the changing perceptions of the ethics of human-in-the-loop processes. While the cost-effectiveness of human data labeling and other processes was once seen as beneficial, it is the ethical implications of such interactions that take precedence nowadays. This shift underscores the evolving norms in HCI and the importance of considering the ethical dimensions of human-AI interactions.</p>
<p>The role of diverse human perspectives plays a crucial role in enhancing AI systems. Involving a broad spectrum of users in the development and testing of AI systems ensures that these technologies are inclusive and representative of the global population, moving beyond the limitations of a WEIRD (Western, Educated, Industrialized, Rich, and Democratic) user base. The methodologies for collecting user feedback in HCI form a critical part of this discussion since they are vital in understanding user needs, preferences, and behaviors, which in turn inform the development of more user-centered AI systems. The presentation by Meredith Morris <span class="citation" data-cites="Morris2019HITL">(<a href="#ref-Morris2019HITL" role="doc-biblioref">Morris 2019</a>)</span> also highlights how these methods can be effectively employed to gain insights from users to ensure that AI systems are aligned with the real-world needs and expectations of users. In HCI, collecting user feedback is a fraught problem. When interacting with AI systems, the typical end user simply cares about tasks that the system can perform. Thus, a key question in HCI for AI is finding and understanding these tasks. <strong>Methodologies for collecting user feedback in HCI</strong>, are described as follow:</p>
<ul>
<li><p><strong>Storyboarding</strong> is a visual method used to predict and explore the user experience with a product or service. A storyboard in HCI is typically a sequence of drawings with annotations that represent a user’s interactions with technology. This technique is borrowed from the film and animation industry and is used in HCI to convey a sequence of events or user flows, including the user’s actions, reactions, and emotions.</p></li>
<li><p><strong>Wizard of Oz Studies</strong> is a method of user testing where participants interact with a system they believe to be autonomous, but which is actually being controlled or partially controlled by a human ‘wizard’ behind the scenes. This technique allows researchers to simulate the response of a system that may not yet be fully functional or developed.</p></li>
</ul>
<p>Both <strong>Storyboarding</strong> and <strong>Wizard of Oz Studies</strong> are effective for engaging with users early in the design process. They help deal with the problem of gathering feedback on a product that doesn’t yet exist. Users often have difficulty imagining outcomes when they cannot touch a live demonstration.</p>
<ul>
<li><p><strong>Surveys</strong> in HCI are structured tools that consist of a series of questions designed to be answered by a large number of participants. They can be conducted online, by telephone, through paper questionnaires, or using computer-assisted methods. Surveys are useful for collecting quantitative data from a broad audience, which can be analyzed statistically.</p></li>
<li><p><strong>Interviews</strong> in HCI are more in-depth and involve direct, two-way communication between the researcher and the participant. Interviews can be structured, semi-structured, or unstructured, ranging from tightly scripted question sets to open-ended conversations.</p></li>
<li><p><strong>Focus Groups</strong> involve a small group of participants discussing their experiences and opinions about a system or design, often with a moderator. Group dynamics can provide insights into collective user perspectives. In particular, users can bounce ideas off each other to provide richer feedback and quieter users who may not otherwise provide feedback may be encouraged by their peers.</p></li>
<li><p><strong>Community-Based Participatory Design (CBPD)</strong> is a human-centered approach that involves the people who will use a product in the design and development process. With CBPD, designers work closely with community members to identify problems, develop prototypes, and iterate based on community feedback. For example, when building a software product for deaf people, the engineering team can hire deaf engineers or designers to provide feedback as they collaboratively build the product.</p></li>
<li><p><strong>Field Studies</strong> involve observing and collecting data on how users interact with a system in their natural environment. This method is based on the premise that observing users in their context provides a more accurate understanding of user behavior. It can include a variety of techniques like ethnography, contextual inquiries, and natural observations.</p></li>
<li><p><strong>Lab-based studies</strong> are conducted in a controlled environment where the researchers can manipulate variables and observe user behavior in a setting designed to minimize external influences. Common lab-based methods include usability testing, controlled experiments, and eye-tracking studies.</p></li>
<li><p><strong>Diary Studies and Ethnography</strong> in HCI are a research method where participants are asked to keep a record of their interactions with a system or product over a while. This log may include text, images, and sometimes even audio or video recordings, depending on the study’s design. Participants typically document their activities, thoughts, feelings, and frustrations as they occur in their natural context.</p></li>
<li><p><strong>Ethnography</strong> is a qualitative research method that involves observing and interacting with participants in their real-life environment. Ethnographers aim to immerse themselves in the user environment to get a deep understanding of the cultural, social, and organizational contexts that shape technology use.</p></li>
</ul>
<p>As we have explored various methodologies for collecting human feedback, it becomes evident that the role of human input is indispensable in shaping AI systems that are not only effective but also ethically sound and user-centric. In the next step, we will elaborate on how to design AI systems for positive human impact, examining how socially aware and human-centered approaches can be employed to ensure that AI technologies contribute meaningfully to society. This includes understanding how AI can be utilized to address real-world challenges and create tangible benefits for individuals and communities.</p>
</section>
<section id="designing-ai-for-positive-human-impact" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="designing-ai-for-positive-human-impact"><span class="header-section-number">6.2.2</span> Designing AI for Positive Human Impact</h3>
<p>In the field of natural language processing (NLP), the primary focus has traditionally been on quantitative metrics such as performance benchmarks, accuracy, and computations. These metrics have long guided the development and evaluation of the technologies. However, as the field evolves and becomes increasingly intertwined with human interactions like the recent popularity of Large Language Models (LLMs), a paradigm shift is becoming increasingly necessary. For example, these LLMs are shown to produce unethical or harmful responses or reflect values that only represent a certain group of people. The need for a human-centered approach in NLP development is crucial as these models are much more likely to be utilized in a broad spectrum of human-centric applications, impacting various aspects of daily life. This shift calls for an inclusive framework where LLMs are not only optimized for efficiency and accuracy but are also sensitized to ethical, cultural, and societal contexts. Integrating a human-centered perspective ensures that these models are developed with a deep understanding of, and respect for, the diversity and complexity of human values and social norms. This approach goes beyond merely preventing harmful outcomes; it also focuses on enhancing the positive impact of NLP technologies on society. In this session, we explore the intricacies of a human-centered approach in NLP development, focusing on three key themes: Socially Aware, Human-Centered, and Positively Impactful.</p>
<section id="socially-aware" class="level4" data-number="6.2.2.1">
<h4 data-number="6.2.2.1" class="anchored" data-anchor-id="socially-aware"><span class="header-section-number">6.2.2.1</span> Socially Aware</h4>
<p>In the exploration of socially aware NLP, <span class="citation" data-cites="hovy-yang-2021-importance">(<a href="#ref-hovy-yang-2021-importance" role="doc-biblioref">Hovy and Yang 2021</a>)</span> presents a comprehensive taxonomy of seven social factors grounded in linguistic theory (See <a href="#fig-taxonomy" class="quarto-xref">Figure&nbsp;<span>6.6</span></a>).</p>
<div id="fig-taxonomy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-taxonomy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/seven-taxonomy.png" class="img-fluid figure-img" style="width:30.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-taxonomy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.6: Taxonomy of social factors
</figcaption>
</figure>
</div>
<p>This taxonomy illustrates both the current limitations and progressions in NLP as they pertain to each of these factors. The primary aim is to motivate the NLP community to integrate these social factors more effectively, thereby advancing towards a level of language understanding that more closely resembles human capabilities. The characteristics of speakers, encompassing variables such as age, gender, ethnicity, social class, and dialect, play a crucial role in language processing. Certain languages or dialects, often categorized as low-resource, are spoken by vulnerable populations that require special consideration in NLP systems. In many cases, the dominant culture and values are over-represented, leading to an inadvertent marginalization of minority perspectives. These minority voices must be not only recognized but also given equitable representation in language models. Additionally, norms and context are vital components in understanding linguistic behavior. They dictate the appropriateness of language use in various social situations and settings. Recognizing and adapting to these norms is a critical aspect of developing socially aware NLP systems that can effectively function across diverse social environments.</p>
</section>
<section id="human-centered" class="level4" data-number="6.2.2.2">
<h4 data-number="6.2.2.2" class="anchored" data-anchor-id="human-centered"><span class="header-section-number">6.2.2.2</span> Human-Centered</h4>
<p>The Human-Centered aspect of NLP development emphasizes the creation of language models that prioritize the needs, preferences, and well-being of human users. This involves integrating human-centered design principles throughout the development stages of LLMs, which are described as follows:</p>
<ul>
<li><p><strong>Task Formulation stage:</strong> Human-centered NLP development begins with understanding the specific problems and contexts in which users operate. This involves collaborating with end-users to identify their needs and challenges, ensuring that the tasks addressed by the models are relevant and meaningful to them. By engaging with users early in the process, developers can create models that are not only technically robust but also practically useful.</p></li>
<li><p><strong>Data Collection stage:</strong> Human-centered principles ensure that the data used to train models is representative of the diverse user population. This includes collecting data from various demographic groups, languages, and cultural contexts to avoid biases that could lead to unfair or harmful outcomes. Ethical considerations are paramount, ensuring that data is collected with informed consent and respecting users’ privacy.</p></li>
<li><p><strong>Data Processing</strong> in a human-centered approach involves carefully curating and annotating data to reflect the nuances of human language and behavior. This step includes filtering out potentially harmful content, addressing imbalances in the data, and ensuring that the labels and annotations are accurate and meaningful. By involving human annotators from diverse backgrounds, developers can capture a wider range of perspectives and reduce the risk of biased outputs.</p></li>
<li><p><strong>Model Training</strong> with a human-centered focus involves incorporating feedback from users and domain experts to fine-tune the models. This iterative process ensures that the models remain aligned with users’ needs and preferences. Techniques such as active learning, where the model queries users for the most informative examples, can be employed to improve the model’s performance.</p></li>
<li><p><strong>Model Evaluation</strong> in a human-centered framework goes beyond traditional metrics like accuracy and F1-score. It includes assessing the model’s impact on users, its fairness, and its ability to handle real-world scenarios. User studies and A/B testing can provide valuable insights into how the model performs in practice and how it affects users’ experiences.</p></li>
<li><p><strong>Deployment</strong> of human-centered NLP models involves continuous monitoring and feedback loops to ensure that the models remain effective and aligned with users’ needs over time. This includes setting up mechanisms for users to report issues and provide feedback, which can then be used to update and improve the models. Ensuring transparency in how the models operate and how user data is used also fosters trust and acceptance among users.</p></li>
</ul>
</section>
<section id="positively-impactful" class="level4" data-number="6.2.2.3">
<h4 data-number="6.2.2.3" class="anchored" data-anchor-id="positively-impactful"><span class="header-section-number">6.2.2.3</span> Positively Impactful</h4>
<p>Building on the human-centered approach, it is crucial to consider how language models can be utilized and the broader impacts they can have on society.</p>
<p><strong>Utilization:</strong> LLMs offer socially beneficial applications across various domains such as public policy, mental health, and education. In public policy, they assist in analyzing large volumes of data to inform decision-making processes. In mental health, LLMs can provide personalized therapy and even train therapists by simulating patient interactions. In the education sector, they enable personalized learning experiences and language assistance, making education more accessible and effective. These examples demonstrate the versatility of LLMs in contributing positively to critical areas of human life.</p>
<p><strong>Impact:</strong> The deployment of NLP models, especially LLMs, has significant societal impacts. Positively, they enhance human productivity and creativity, offering tools and insights that streamline processes and foster innovative thinking. LLMs serve as powerful aids in various sectors, from education to industry, enhancing efficiency and enabling new forms of expression and problem-solving. it is essential to acknowledge the potential negative impacts. One major concern is the ability of LLMs to generate and spread misinformation. As these models become more adept at producing human-like text, distinguishing between AI-generated and human-created content becomes increasingly challenging. This raises issues of trust and reliability, with the risk of widespread dissemination of false or misleading information, which could have significant adverse effects on individuals and society.</p>
<p>By considering both the utilization and impact of LLMs, we can better harness their potential for positive societal contributions while mitigating the risks associated with their deployment. In conclusion, by thoughtfully integrating human-centered principles and ensuring positive impacts through feedback collection and ethical considerations, we can develop language models that not only enhance human well-being but also align closely with societal values. Building on these foundational principles, we now turn our attention to Adaptive User Interfaces, which exemplify the practical application of these concepts by personalizing interactions and improving user experiences in dynamic environments.</p>
</section>
</section>
<section id="adaptive-user-interfaces" class="level3" data-number="6.2.3">
<h3 data-number="6.2.3" class="anchored" data-anchor-id="adaptive-user-interfaces"><span class="header-section-number">6.2.3</span> Adaptive User Interfaces</h3>
<p>Adaptive user interfaces (AUIs) represent a significant advancement in personalizing user experiences by learning and adapting to individual preferences. This section will discuss the methodologies and applications of AUIs, highlighting their role in enhancing human-AI interaction through intelligent adaptation. The integration of AUIs within human-centered design paradigms ensures that AI systems not only meet user needs but also anticipate and adapt to their evolving preferences, thus maximizing positive human impact. Nowadays, consumers have more choices than ever and the need for personalized and intelligent assistance to make sense of the vast amount of information presented to them is clear.</p>
<section id="key-ideas" class="level4" data-number="6.2.3.1">
<h4 data-number="6.2.3.1" class="anchored" data-anchor-id="key-ideas"><span class="header-section-number">6.2.3.1</span> Key ideas</h4>
<p>In general, personalized recommendation systems require a model or profile of the user. We categorize modeling approaches into four groups.</p>
<ol type="1">
<li><p>User-created profiles (usually done manually).</p></li>
<li><p>Manually defined groups that each user is classified into.</p></li>
<li><p>Automatically learned groups that each user is classified into.</p></li>
<li><p>Adaptively learned individual user models from interactions with the recommendation system.</p></li>
</ol>
<p>The last approach is referred to as <em>adaptive user interfaces</em>. This approach promises that each user is given the most personalization possible, leading to better outcomes. In this session, we discuss recommendation systems that adaptively learn an individual’s preferences and use that knowledge to intelligently recommend choices that the individual is more inclined to like.</p>
<p>The problem of learning individual models can be formalized as follows: a set of tasks requiring a user decision, a description for each task, and a history of the user’s decision on each task. This allows us to find a function that maps from task descriptions (features) to user decisions. Tasks can be described using crowd-sourced data (a collaborative approach) or measurable features of the task (a content-based approach). This session will focus on content-based approaches for describing tasks. After understanding the framework for adaptive user interfaces, it is useful to provide example applications to ground future discussions. Adaptive user interfaces have been developed for command and form completion, email filtering and filing, news selection and layout, browsing the internet, selecting movies and TV shows, online shopping, in-car navigation, interactive scheduling, and dialogue systems, among many other applications.</p>
</section>
<section id="design" class="level4" data-number="6.2.3.2">
<h4 data-number="6.2.3.2" class="anchored" data-anchor-id="design"><span class="header-section-number">6.2.3.2</span> Design</h4>
<p>The goal of an adaptive user interface is to create a software tool that reduces human effort by acquiring a user model based on past user interactions. This is analogous to the goal of machine learning (ML) which is to create a software tool that improves some task performance by acquiring knowledge based on partial task experience. The design of an adaptive user interface can be broken up into six steps:</p>
<ol type="1">
<li><p><strong>Formulating the Problem:</strong> Given some task that an intelligent system could aid, the goal is to find a formulation that lets the assistant improve its performance over time by learning from interactions with a user. In this step the designer has to make design choices about what aspect of user behavior is predicted, and what is the proper level of granularity for description (i.e.&nbsp;what is a training example). This step usually involves formulating the problem into some sort of supervised learning framework.</p></li>
<li><p><strong>Engineering the Representation:</strong> At this stage we have a formulation of a task in ML terms and we need to represent the behavior and user model in such a way that makes computational learning not only tractable but as easy as possible. In this step, the designer has to make design choices about what information is used to make predictions, and how that information is encoded and passed to the model.</p></li>
<li><p><strong>Collecting User Traces:</strong> In this third step the goal is to find an effective way to collect traces (samples) of user behavior. The designer must choose how to translate traces into training data and also how to elicit traces from a user. An ideal adaptive user interface places no extra effort on the user to collect such traces.</p></li>
<li><p><strong>Modeling the User:</strong> In this step the designer must decide what model class to use (neural network, decision tree, graphical model, etc.) and how to train the model (optimizer, step size, batch size, etc.). This step in the design process is usually given too much importance in academia. It is quite often the case that the success of an adaptive user interface is more sensitive to the other design steps.</p></li>
<li><p><strong>Using the Model Effectively:</strong> At this stage the designer must decide how the model will be integrated into a software tool. Specifically, when and how is the model evaluated and how is the output of the model presented to the user? In addition, the designer must consider how to handle situations in which the model predictions are wrong. An ideal adaptive user interface will let the user take advantage of good predictions and ignore bad ones.</p></li>
<li><p><strong>Gaining User Acceptance:</strong> The final step in the design process is to get users to try the system and ultimately adopt it. The initial attraction of users is often a marketing problem, but to retain users the system must be well-designed and easy to use.</p></li>
</ol>
</section>
<section id="applications" class="level4" data-number="6.2.3.3">
<h4 data-number="6.2.3.3" class="anchored" data-anchor-id="applications"><span class="header-section-number">6.2.3.3</span> Applications</h4>
<p>After understanding the design of Adaptive User Interfaces, let’s take a look at how we can apply it to real-world problems. We will summarize and analyze three different application areas of learning human preferences, which are driving route advisor <span class="citation" data-cites="rogers1999adaptive">(<a href="#ref-rogers1999adaptive" role="doc-biblioref">Rogers, Fiechter, and Langley 1999</a>)</span>, destination selection <span class="citation" data-cites="langley1999adaptive">(<a href="#ref-langley1999adaptive" role="doc-biblioref">Langley et al. 1999</a>)</span>, and resource scheduling <span class="citation" data-cites="gervasio1999learning">(<a href="#ref-gervasio1999learning" role="doc-biblioref">Gervasio, Iba, and Langley 1999</a>)</span>.</p>
<p><strong>1. Driving Route Advisor:</strong> The task of route selection involves determining a desirable path for a driver to take from their current location to a chosen destination, given the knowledge of available roads from a digital map. While computational route advisors exist in rental cars and online, they cannot personalize individual drivers’ preferences, which is a gap that adaptive user interfaces aim to fill by learning and recommending routes tailored to the driver’s unique choices and behaviors.</p>
<p>Here is an approach to route selection through learning individual drivers’ route preferences.</p>
<ul>
<li><p>Formulation: Learn a “subjective” function to evaluate entire routes.</p></li>
<li><p>Representation: Global route features are computable from digital maps.</p></li>
<li><p>Data collection: Preference of one complete route over another.</p></li>
<li><p>Induction: A method for learning weights from preference data.</p></li>
<li><p>Using model: Apply subjective function to find “optimal” route.</p></li>
</ul>
<p>This method aims to learn a user model that considers the entirety of a route, thereby avoiding issues like data fragmentation and credit assignment problems.</p>
<p>The design choices are incorporated into <span class="citation" data-cites="rogers1999adaptive">(<a href="#ref-rogers1999adaptive" role="doc-biblioref">Rogers, Fiechter, and Langley 1999</a>)</span>, which: models driver preferences in terms of 14 global route features; gives the driver two alternative routes he might take; lets the driver refine these choices along route dimensions; uses driver choices to refine its model of his preferences; and invokes the driver model to recommend future routes. We note that providing drivers with choices lets the system collect data on route preferences in an unobtrusive manner. The interface of the application is presented in <a href="#fig-exp-1" class="quarto-xref">Figure&nbsp;<span>6.7</span></a>.</p>
<div id="fig-exp-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-exp-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/example-1.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-exp-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.7: The adaptive route advisor.
</figcaption>
</figure>
</div>
<p>In driving route advisor task <span class="citation" data-cites="rogers1999adaptive">(<a href="#ref-rogers1999adaptive" role="doc-biblioref">Rogers, Fiechter, and Langley 1999</a>)</span>, a linear model is used for predicting the cost of a route based on the time, distance, number of intersections, and the number of turns. The system uses each training pair as a constraint on the weights found during the learning process. The experimental results are shown in the <span class="quarto-unresolved-ref">?fig-exp-2</span>.</p>
<figure id="fig-exp-2" class="figure">
<p>
<img src="Figures/example-2.png" style="width:45.0%" alt="image" class="figure-img"> <img src="Figures/example-3.png" style="width:45.0%" alt="image" class="figure-img">
</p>
<figcaption>
(Left) Experiments with 24 subjects show the Route Advisor improves its predictive ability rapidly with experience. (Right) Analyses also show that personalized user models produce better results than generalized models, even when given more data.
</figcaption>
</figure>
<p><strong>2. Destination Selection:</strong> The task of destination selection involves assisting a driver in identifying one or more suitable destinations that fulfill a specific goal, such as finding a place to eat lunch, based on the driver’s current location and knowledge of nearby options. While there are many recommendation systems online, including those for restaurants, they are not ideally suited for drivers due to the driving environment’s demand for limited visual attention, thus necessitating a more tailored and accessible approach for in-car use.</p>
<p>One approach to destination recommendation can be cast as:</p>
<ul>
<li><p>Formulation: Learn to predict features the user cares about in items.</p></li>
<li><p>Representation: Conditions/weights on attributes and values.</p></li>
<li><p>Data collection: Converse with the user to help him make decisions, noting whether he accepts or rejects questions and items.</p></li>
<li><p>Induction: Any supervised induction method.</p></li>
<li><p>Using model: Guide the dialogue by selecting informative questions and suggesting likely values.</p></li>
</ul>
<p>This design relies on the idea of a conversational user interface. Spoken-language versions of this approach appear well suited to the driving environment.</p>
<p>This approach is implemented in <span class="citation" data-cites="langley1999adaptive">(<a href="#ref-langley1999adaptive" role="doc-biblioref">Langley et al. 1999</a>)</span>, where it engages in spoken conversations to help a user refine goals; incorporates a dialogue model to constrain this process; collects and stores traces of interaction with the user; and personalizes both its questions and recommended items. Their work focused on recommending restaurants to users who want advice about where to eat. This approach to recommendation would work well for drivers, it also has broader applications. We present experimental results in</p>
<figure id="fig-exp-2.5" class="figure">
<p>
<img src="Figures/example-4.png" style="width:45.0%" alt="image" class="figure-img"> <img src="Figures/example-5.png" style="width:45.0%" alt="image" class="figure-img">
</p>
<figcaption>
(Left) Speech Acts Per Conversation. (Right) Time Per Conversation.
</figcaption>
</figure>
<p><strong>3. Resource Scheduling:</strong> The task of resource scheduling describes the challenge of allocating a limited set of resources to complete a set of tasks or jobs within a certain time frame, while also considering the constraints on both the jobs and the resources. Although automated scheduling systems are prevalent in various industries and some interactive schedulers exist, there is a distinct need for systems that can create personalized schedules reflecting the unique preferences of individual users.</p>
<p>An approach to personalized scheduling can be described as:</p>
<ul>
<li><p>Formulation: Learn a utility function to evaluate entire schedules.</p></li>
<li><p>Representation: Global features are computable from the schedule.</p></li>
<li><p>Data collection: Preference of one candidate schedule over others.</p></li>
<li><p>Induction: A method for learning weights from preference data.</p></li>
<li><p>Using model: Apply the ‘subjective’ function to find a good schedule.</p></li>
</ul>
<p>We note that this method is similar to that in the Adaptive Route Advisor. However, it assumes a search through a space of complete schedules (a repair space), which requires some initial schedule. This approach is implemented in <span class="citation" data-cites="gervasio1999learning">(<a href="#ref-gervasio1999learning" role="doc-biblioref">Gervasio, Iba, and Langley 1999</a>)</span>, where the interactive scheduler retrieves an initial schedule from a personalized case library; suggests to the user improved schedules from which to select; lets the user direct search to improve on certain dimensions; collects user choices to refine its personalized utility function; stores solutions in the case base to initialize future schedules; and invokes the user model to recommend future schedule repairs. As before, providing users with choices lets the system collect data on schedule preferences unobtrusively. An example of the interface, and the experimental results are shown in <span class="quarto-unresolved-ref">?fig-exp-3</span>.</p>
<figure id="fig-exp-3" class="figure">
<p>
<img src="Figures/example-7.png" style="width:45.0%" alt="image" class="figure-img"> <img src="Figures/example-6.png" style="width:45.0%" alt="image" class="figure-img">
</p>
<figcaption>
(Left) The interface of the INCA: Interactive Scheduling <span class="citation" data-cites="gervasio1999learning"></span>. (Right) Experiments with INCA suggest that retrieving personalized schedules helps users more as task difficulty increases. These experimental studies used a mixture of human and synthetic subjects.
</figcaption>
</figure>
</section>
<section id="limitations" class="level4" data-number="6.2.3.4">
<h4 data-number="6.2.3.4" class="anchored" data-anchor-id="limitations"><span class="header-section-number">6.2.3.4</span> Limitations</h4>
<p>The challenges of adaptive interfaces may involve: conceptualizing user modeling as a task suitable for inductive learning, crafting representations that facilitate the learning process, gathering training data from users in a way that doesn’t intrude on their experience, applying the learned user model effectively, ensuring the system can learn in real-time, and dealing with the necessity of learning from a limited number of training instances. These challenges are not only pertinent to adaptive interfaces but also intersect with broader applications of machine learning, while also introducing some unique issues. However, new sensor technology can bring promises to adaptive interfaces. Adaptive interfaces rely on user traces to drive their modeling process, so they stand to benefit from developments like GPS and cell phone locators, robust software for speech recognition, accurate eye and head trackers, real-time video interpreters, wearable body sensors (GSR, heart rate), and portable brain-wave sensors. As those devices become more widespread, they will offer new sources of data and support new types of adaptive services. In addition, adaptive interfaces can be viewed as a form of cognitive simulation that automatically generates knowledge structures to learn user preferences. They are capable of making explicit predictions about future user behavior and explaining individual differences through the process of personalization. This perspective views adaptive interfaces as tools that not only serve functional purposes but also model the psychological aspects of user interaction. Two distinct approaches within cognitive simulation are related to adaptive interfaces: <em>process</em> models that incorporate fundamental architectural principles, and <em>content</em> models that operate at the knowledge level, focusing on behavior. We note that both of them have roles to play, but content models are more relevant to personalization and adaptive interfaces.</p>
<p>In conclusion, adaptive user interfaces represent a significant advancement in creating personalized and efficient interactions between humans and technology. By leveraging modern sensor technologies and cognitive simulation approaches, these interfaces can dynamically learn and adapt to individual user preferences, enhancing overall user experience and system effectiveness. The methodologies discussed, from conceptualizing user models to collecting and utilizing user feedback, form the foundation of this innovative approach. As we transition to the next section, we will explore practical applications and real-world implementations of these human-centered AI principles through detailed case studies, illustrating the tangible impact of adaptive interfaces in various domains.</p>
</section>
</section>
<section id="case-studies-in-human-centered-ai" class="level3" data-number="6.2.4">
<h3 data-number="6.2.4" class="anchored" data-anchor-id="case-studies-in-human-centered-ai"><span class="header-section-number">6.2.4</span> Case Studies in Human-Centered AI</h3>
<p>In this section, we examine practical examples that illustrate the application of human-centered principles in the development and deployment of AI systems. By examining these case studies, we aim to provide concrete insights into how AI technologies can be designed and implemented to better align with human values, enhance inclusivity, and address the specific needs of diverse user groups. The following case studies highlight different approaches and methodologies used to ensure that AI systems are not only effective but also considerate of the human experience.</p>
<section id="lampost-case-study" class="level4" data-number="6.2.4.1">
<h4 data-number="6.2.4.1" class="anchored" data-anchor-id="lampost-case-study"><span class="header-section-number">6.2.4.1</span> LaMPost Case Study</h4>
<p>In our exploration of human-centered AI design, it is crucial to examine how metrics can be improved to better capture the human experience and address the shortcomings of traditional evaluation methods. The LaMPost case study <span class="citation" data-cites="goodman_lampost_2022">(<a href="#ref-goodman_lampost_2022" role="doc-biblioref">Goodman et al. 2022</a>)</span> exemplifies this effort by focusing on the development of an AI assistant designed to aid individuals with dyslexia in writing emails. This case is particularly relevant to our discussion because it highlights the importance of human-centered principles in AI development, especially in creating tools that cater to specific cognitive differences and enhance user experience.</p>
<p>Dyslexia is a cognitive difference that affects approximately 15 percent of language users, with varying degrees of impact on speaking, spelling, and writing abilities. It is a spectrum disorder, meaning symptoms and severity differ among individuals. More importantly, dyslexia is not an intellectual disability; many individuals with dyslexia possess high intelligence. Given the significant number of people affected by dyslexia, it is essential to develop AI tools that support their unique needs and enhance their daily tasks.</p>
<p>The LaMPost project sought to answer the question, “How can LLMs be applied to enhance the writing workflows of adults with dyslexia?” To address this, researchers employed a participatory design approach, involving employees with dyslexia from their company (Google) in the study. This approach ensured that the development process was inclusive and responsive to the actual needs and preferences of the dyslexic community. By focusing on the real-world application of LLMs in aiding email writing for dyslexic individuals, LaMPost serves as a powerful example of how AI can be designed to better capture and enhance the human experience.</p>
<p>The figure below allows users to see suggestions for rewriting selected text, helping them identify main ideas, suggest possible changes, and rewrite their selections to improve clarity and expression.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/lampost_fig3.png" class="img-fluid figure-img"></p>
<figcaption>The Suggest Possible Changes feature from LaMPost.</figcaption>
</figure>
</div>
<p>The table below categorizes the challenges faced by users at different writing levels and the strategies they can use to overcome these challenges, illustrating the varied support needs addressed by LaMPost</p>
<figure class="figure">
<table>
<thead>
<tr>
<th style="text-align: center;">
Writing level
</th>
<th style="text-align: center;">
Examples of Challenges
</th>
<th style="text-align: center;">
Strategies
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">
high
</td>
<td style="text-align: center;">
expressing ideas
</td>
<td style="text-align: center;">
“word faucet”, ASR dictation
</td>
</tr>
<tr>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
ordering ideas
</td>
<td style="text-align: center;">
post-it outlining
</td>
</tr>
<tr>
<td style="text-align: center;">
low
</td>
<td style="text-align: center;">
appropriate language
</td>
<td style="text-align: center;">
proofreading
</td>
</tr>
<tr>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
paraphrasing
</td>
<td style="text-align: center;">
feedback
</td>
</tr>
</tbody>
</table>
<figcaption>
User challenged and strategies in LaMPost.
</figcaption>
</figure>
<p>Next, they ran a focus group to get initial ideas from members of the dyslexic community. This focus group helped them figure out what to measure and added the second research question: “How do adults with dyslexia feel about LLM-assisted writing?” In other words, how does the LLM impact users’ feelings of satisfaction, self-expression, self-efficacy, autonomy, and control?</p>
<p>From this focus group, they went and created a prototype to answer the desires of the group. They included three features in their prototype model. One feature was: <em>identifying main ideas</em>. They focused on this to support overall clarity and organization of high-level ideas of the user. Another feature was <em>suggest possible changes</em>. They focused on this because users wanted to identify high-level adjustments to improve their writing. The last feature they added was <em>rewrite my selections</em>. They added this because users wanted help expressing ideas with a desired phrasing tone or style. This feature generated a rewrite based on a command you gave it.</p>
<p>With the prototype, the researchers evaluated again with 19 participants with dyslexia from outside their organization. They did a three-part study, including a demonstration and background on the system (25 min). Then they did a writing exercise with two real tasks (emails) each user had to do in the real world (25 min). For example, one task might have been to write an email to the principal of their child’s school to ask for a meeting. Then, the researchers did another follow-up interview for more qualitative data, e.g.&nbsp;to ask about specific choices users made when interacting with the model (25 min).</p>
<p>LaMPost’s design prioritized autonomy by allowing users to choose the best option for their writing. One successful thing is that most users felt in control while writing. Users found that numerous options were helpful to filter through poor results. However, participants said the selection process was cognitively demanding and time-consuming. As we all know, features identified in LaMPost are all over the place, such as in Google Docs. Nonetheless, there remain many questions about the balance between automated writing and providing more control to the end users.</p>
<div class="tcolorbox">
<p>How could researchers hone in on this trade-off between <strong>the ease of automated writing</strong> and <strong>providing control to end-users</strong>?<br>
You will need to design a study to approach this question.</p>
<ul>
<li><p>Identify your research question, hypotheses, and the methods that you will use. (Hint: use the HCI methods described in the previous section.)</p></li>
<li><p>Scope the domain of your study appropriately—more broadly than dyslexia but not so broadly to be meaningless.</p></li>
<li><p>What domains will you include? (E.g. students use ChatGPT for assignments, doctors use an LLM to write notes, etc.)</p></li>
</ul>
</div>
<p>In this way, both the case study of LaMPost and its presaging of greater trends in LLM interfaces recapitulate the maxim of HCI: HCI is a cycle. You design a potential system, prototype it, get feedback from people, and iterate constantly. Next, we will explore two case studies that exemplify the application of human-centered principles in NLP. These case studies illustrate how LLMs can be adapted to foster social inclusivity and provide training in social skills.</p>
</section>
<section id="multi-value-and-dada-cross-dialectal-english-nlp" class="level4" data-number="6.2.4.2">
<h4 data-number="6.2.4.2" class="anchored" data-anchor-id="multi-value-and-dada-cross-dialectal-english-nlp"><span class="header-section-number">6.2.4.2</span> Multi-Value and DaDa: Cross-Dialectal English NLP</h4>
<p>English NLP systems are largely trained to perform well in Standard American English - the form of written English found in professional settings and elsewhere. Not only is Standard American English the most well-represented form of English in textual datasets but NLP engineers and researchers often filter dialectal and vernacular English examples from their datasets to improve performance on SAE benchmarks. As a result, NLP systems are generally less performant when processing dialectal inputs than SAE inputs. This performance gap is observable over various benchmarks and tasks, like the SPIDER benchmark. <span class="citation" data-cites="spider">(<a href="#ref-spider" role="doc-biblioref">Chang et al. 2023</a>)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/MV2.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Stress test reveals worse performance on the SPIDER benchmark with synthetic dialectical examples than with SAE.</figcaption>
</figure>
</div>
<p>As natural language systems become more pervasive, this performance gap increasingly represents a real allocational harm against dialectal English speakers — these speakers are excluded from using helpful systems and assistants. Multi-Value is a framework for evaluating foundation language models on dialectic input, and DADA is a framework for adapting LLMs to improve performance on dialectic input.</p>
<p><strong>Synthetic Dialectal Data</strong></p>
<p>Ziems et al.&nbsp;(2023) create synthetic dialectal data for several English dialects (Appalachian English, Chicano English, Indian English, Colloquial Singapore English, and Urban African American English).<span class="citation" data-cites="mv">(<a href="#ref-mv" role="doc-biblioref">Ziems et al. 2023</a>)</span> They created synthetic data based on transforming SAE examples to have direct evaluation comparisons. These synthetic examples were created by leveraging known linguistic features of the dialects, such as negative concord in UAAVE. <a href="#fig-features_dialects" class="quarto-xref">Figure&nbsp;<span>6.8</span></a> maps out the presence of various linguistic features.</p>
<div id="fig-features_dialects" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-features_dialects-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/MV1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-features_dialects-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.8: A comparative distribution of features in five dialects.
</figcaption>
</figure>
</div>
<p>This synthetic data, while somewhat limited in the variety of samples. can produce and create realistic examples for benchmarking LM performance. <a href="#fig-synthetic_example" class="quarto-xref">Figure&nbsp;<span>6.9</span></a> demonstrates creating a synthetic dialectic example using the ‘give passive’ linguistic feature, illustrating the transformation process from SAE to a vernacular form.</p>
<div id="fig-synthetic_example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-synthetic_example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/MV3.png" class="img-fluid figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-synthetic_example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.9: Execution of a sample transform using a documented linguistic feature.
</figcaption>
</figure>
</div>
<p><strong>Feature Level Adapters</strong> One approach to the LLM adaption task would be to train an adapter for each dialect using a parameter-efficient fine-tuning method like low-rank adapters. <span class="citation" data-cites="lora">(<a href="#ref-lora" role="doc-biblioref">Hu et al. 2021</a>)</span> While adapters can certainly bridge the gap between SAE LMs and dialect inputs, this approach suffers from a couple of weaknesses, namely:</p>
<ul>
<li><p>Individually trained adapters do not leverage similarities between low-resource dialects. Transfer learning is often helpful for training low-resource languages and dialects.</p></li>
<li><p>The model needs to know which adapter to use at inference time. This presupposes that we can accurately classify the dialect — sometimes based on as little as one utterance. This classification is not always possible — a more general approach is needed.</p></li>
</ul>
<p>Therefore, Liu et al.&nbsp;(2023) propose a novel solution — DADA: Dialect Adaption via Dynamic Aggregation of Linguistic Rules. <span class="citation" data-cites="dada">(<a href="#ref-dada" role="doc-biblioref">Liu, Held, and Yang 2023</a>)</span> DADA trains adapters on the linguistic feature level rather than the dialect level. The model can use multiple linguistic feature adapters via an additional fusion layer. They can therefore train using multi-dialectical data and cover linguistic variation via a comprehensive set of roughly 200 adapters. DADA saw an improvement in performance over single-dialect adapters for most dialects, as shown in <a href="#fig-dada_performance" class="quarto-xref">Figure&nbsp;<span>6.10</span></a>.</p>
<div id="fig-dada_performance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dada_performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/MV4.png" class="img-fluid figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dada_performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.10: Execution of a sample transform using a documented linguistic feature.
</figcaption>
</figure>
</div>
<p>The Multi-Value and DADA case study underscores the importance of designing NLP systems that are inclusive and representative of diverse language users. By addressing the performance gaps in handling dialectal inputs, this case study highlights the necessity of incorporating diverse linguistic data and creating adaptable systems. This approach enhances AI functionality and accessibility, ensuring it respects and reflects linguistic diversity. Ultimately, the study reinforces human-centered design principles, demonstrating how AI can be tailored to better serve and empower all users. Moving forward, we will explore how LLMs can be utilized for social skill training, showcasing their potential to improve human interactions.</p>
</section>
<section id="social-skill-training-via-llms" class="level4" data-number="6.2.4.3">
<h4 data-number="6.2.4.3" class="anchored" data-anchor-id="social-skill-training-via-llms"><span class="header-section-number">6.2.4.3</span> Social Skill Training via LLMs</h4>
<p>The emergence of Large Language Models (LLMs) marks a significant milestone in the field of social skills training. This case study explores the potential of LLMs to augment social skill development across diverse scenarios. More specifically, we discuss a dual-framework approach, where two distinct LLMs operate in tandem as a Partner and a Mentor, guiding human learners in their journey towards improved social interaction. In this framework, we have two agents which are</p>
<ul>
<li><p><strong>AI Partner</strong>: LLM-empowered agents that users can engage with across various topics. This interactive model facilitates practical, conversation-based learning, enabling users to experiment with different communication styles and techniques or practice and develop specific skills in real-world scenarios in a safe, AI-mediated environment.</p></li>
<li><p><strong>AI Mentor</strong>: An LLM-empowered entity designed to provide constructive, personalized feedback based on the interaction of users and the AI Partner. This mentor analyzes conversation dynamics, identifies areas for improvement, offers tailored advice, and guides users toward effective social strategies and improved interaction skills.</p></li>
</ul>
<p>For example, in conflict resolution, individuals learning to handle difficult conversations can use the AI Partner to simulate interactions with a digitalized partner. As a Conflict Resolution Expert, the AI Mentor helps analyze these interactions, offering strategies to navigate conflicts effectively.</p>
<p>In the educational sector, K-12 teachers aiming to incorporate more growth-mindset language into their teaching can practice with a digitalized student. An experienced teacher or mentor, represented by the AI Mentor, provides insights on effective communication and teaching methods. For negotiation training, students preparing to negotiate their first job offers can engage in simulated negotiations with a digitalized HR representative through the AI Partner. As a Negotiation Expert, the AI Mentor then offers guidance on negotiation tactics, helping students effectively articulate their values and negotiate job terms. Lastly, in therapy training, novice therapists can interact with a digitalized patient via the AI Partner to practice therapy sessions. The AI Mentor, functioning as a Therapy Coach, then reviews these sessions, providing feedback and suggestions on enhancing therapeutic techniques and patient engagement.</p>
<p><strong>CARE: Therapy Skill Training</strong> Hsu et al.&nbsp;(2023) introduced CARE <span class="citation" data-cites="hsu2023helping">(<a href="#ref-hsu2023helping" role="doc-biblioref">Hsu et al. 2023</a>)</span>, a framework designed for therapy skill training. This framework leverages a simulated environment, enabling counselors to practice their skills without the risk of harming real individuals. An integral component of CARE is the AI Mentor, which offers invaluable feedback and guidance during the training process. See <a href="#fig-care" class="quarto-xref">Figure&nbsp;<span>6.11</span></a> for the overview of the framework.</p>
<div id="fig-care" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-care-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/care.png" class="img-fluid figure-img" style="width:45.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-care-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.11: CARE Framework
</figcaption>
</figure>
</div>
<p>CARE’s primary function is for novice therapists and counselors to assess and determine the most effective counseling strategies tailored to specific contexts. It provides counselors with customized example responses, which they can adopt, adapt, or disregard when interacting with a simulated support seeker. This approach is deeply rooted in the principles of Motivational Interviewing and utilizes a rich dataset of counseling conversations combined with LLMs. The effectiveness of CARE has been established through rigorous quantitative evaluations and qualitative user studies, which included simulated chats and semi-structured interviews. Notably, CARE has shown significant benefits in aiding novice counselors. From the assessment, counselors chose to use CARE 93% of the time, directly used a CARE response without editing 60% of the time, and sent more extended responses with CARE. Qualitatively, counselors noted several advantages of CARE, such as its ability to refresh memory on various strategies, inspire innovative responses, boost confidence, and save time during consultations. However, there were some drawbacks, including potential disruptions in the thought process, perceived limitations in response options, the requirement for decision-making, and the time needed to review suggestions. Overall, the framework is particularly beneficial for therapists new to the field, offering them a supportive and educational tool to enhance their counseling skills effectively.</p>
</section>
</section>
</section>
<section id="practice-exercises" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="practice-exercises"><span class="header-section-number">6.3</span> Practice Exercises</h2>


<!-- -->

</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-amodei2016concrete" class="csl-entry" role="listitem">
Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. 2016. <span>“Concrete Problems in AI Safety.”</span> <em>arXiv Preprint arXiv:1606.06565</em>.
</div>
<div id="ref-angwin_machine_2016" class="csl-entry" role="listitem">
Angwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. <span>“Machine Bias.”</span> <em>ProPublica</em>.
</div>
<div id="ref-arcas_can_2022" class="csl-entry" role="listitem">
Arcas, Blaise Aguera y. 2022. <span>“Can Machines Learn How to Behave?”</span> <em>Medium</em>. <a href="https://medium.com/@blaisea/can-machines-learn-how-to-behave-42a02a57fadb">https://medium.com/@blaisea/can-machines-learn-how-to-behave-42a02a57fadb</a>.
</div>
<div id="ref-aristotle_nicomachean_350" class="csl-entry" role="listitem">
Aristotle. 350 B.C.E. <em>Nicomachean Ethics</em>. translated by W.D. Ross.
</div>
<div id="ref-bai_constitutional_2022" class="csl-entry" role="listitem">
Bai, Yuntao, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, and Cameron McKinnon. 2022. <span>“Constitutional Ai: <span>Harmlessness</span> from Ai Feedback.”</span> <em>arXiv Preprint arXiv:2212.08073</em>.
</div>
<div id="ref-barocas_fairness_2019" class="csl-entry" role="listitem">
Barocas, Solon, Moritz Hardt, and Arvind Narayanan. 2019. <em>Fairness and Machine Learning</em>. fairmlbook.org.
</div>
<div id="ref-bernstein2010soylent" class="csl-entry" role="listitem">
Bernstein, Michael S., Greg Little, Robert C. Miller, Bjorn Hartmann, Mark S. Ackerman, David R. Karger, David Crowell, and Katrina Panovich. 2010. <span>“Soylent: A Word Processor with a Crowd Inside.”</span> In <em>Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology</em>. ACM.
</div>
<div id="ref-binns_fairness_2018" class="csl-entry" role="listitem">
Binns, Reuben. 2018. <span>“Fairness in Machine Learning: Lessons from Political Philosophy.”</span> In <em>Proceedings of the 2018 Conference on Fairness, Accountability, and Transparency</em>, 149–59.
</div>
<div id="ref-bostrom2014superintelligence" class="csl-entry" role="listitem">
Bostrom, Nick. 2014. <em>Superintelligence: Paths, Dangers, Strategies</em>. Oxford University Press.
</div>
<div id="ref-brown2019machine" class="csl-entry" role="listitem">
Brown, Daniel S, and Scott Niekum. 2019. <span>“Machine Teaching for Inverse Reinforcement Learning: Algorithms and Applications.”</span> In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 33:7749–58.
</div>
<div id="ref-brown2021value" class="csl-entry" role="listitem">
Brown, Daniel S, Jordan Schneider, Anca Dragan, and Scott Niekum. 2021. <span>“Value Alignment Verification.”</span> In <em>International Conference on Machine Learning</em>, 1105–15. PMLR.
</div>
<div id="ref-spider" class="csl-entry" role="listitem">
Chang, Shuaichen, Jun Wang, Mingwen Dong, Lin Pan, Henghui Zhu, Alexander Hanbo Li, Wuwei Lan, et al. 2023. <span>“Dr.spider: A Diagnostic Evaluation Benchmark Towards Text-to-SQL Robustness.”</span> <a href="https://arxiv.org/abs/2301.08881">https://arxiv.org/abs/2301.08881</a>.
</div>
<div id="ref-chowdhery_palm_2022" class="csl-entry" role="listitem">
Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al. 2022. <span>“<span>PaLM</span>: <span>Scaling</span> <span>Language</span> <span>Modeling</span> with <span>Pathways</span>.”</span> <em>arXiv:2204.02311 [Cs]</em>, April. <a href="http://arxiv.org/abs/2204.02311">http://arxiv.org/abs/2204.02311</a>.
</div>
<div id="ref-christianoclarifying" class="csl-entry" role="listitem">
Christiano, Paul. 2018. <span>“Clarifying <span>‘AI Alignment’</span>.”</span> <a href="https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6">https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6</a>.
</div>
<div id="ref-christiano2017deep" class="csl-entry" role="listitem">
Christiano, Paul F, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. <span>“Deep Reinforcement Learning from Human Preferences.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
<div id="ref-clark2016faulty" class="csl-entry" role="listitem">
Clark, Jack, and Dario Amodei. 2016. <span>“Faulty Reward Functions in the Wild.”</span> <em>OpenAI Blog</em>.
</div>
<div id="ref-dignum_responsible_2019" class="csl-entry" role="listitem">
Dignum, Virginia. 2019. <em>Responsible Artificial Intelligence: How to Develop and Use AI in a Responsible Way</em>. Vol. 2156. Springer.
</div>
<div id="ref-dworkin1988theory" class="csl-entry" role="listitem">
Dworkin, Gerald. 1988. <em>The Theory and Practice of Autonomy</em>. Cambridge University Press.
</div>
<div id="ref-everitt2018alignment" class="csl-entry" role="listitem">
Everitt, Tom, and Marcus Hutter. 2018. <span>“The Alignment Problem for Artificial Intelligence.”</span> In <em>Advances in Neural Information Processing Systems</em>, 1–8.
</div>
<div id="ref-floridi2011ethics" class="csl-entry" role="listitem">
Floridi, Luciano. 2011. <em>The Ethics of Information</em>. Oxford University Press.
</div>
<div id="ref-frankena1973ethics" class="csl-entry" role="listitem">
Frankena, William K. 1973. <em>Ethics</em>. Prentice Hall.
</div>
<div id="ref-friedman_value_2008" class="csl-entry" role="listitem">
Friedman, Batya, Peter H. Kahn, and Alan Borning. 2008. <span>“Value Sensitive Design and Information Systems.”</span> In <em>The Handbook of Information and Computer Ethics</em>. John Wiley &amp; Sons.
</div>
<div id="ref-gervasio1999learning" class="csl-entry" role="listitem">
Gervasio, Melinda T, Wayne Iba, and Pat Langley. 1999. <span>“Learning User Evaluation Functions for Adaptive Scheduling Assistance.”</span> In <em>ICML</em>, 152–61. Citeseer.
</div>
<div id="ref-goodall_machine_2014" class="csl-entry" role="listitem">
Goodall, Noah J. 2014. <span>“Machine Ethics and Automated Vehicles.”</span> In <em>Road Vehicle Automation</em>, 93–102. Springer.
</div>
<div id="ref-goodman_lampost_2022" class="csl-entry" role="listitem">
Goodman, Steven, Erin Buehler, Patrick Clary, Andy Coenen, Aaron Michael Donsbach, Tiffanie Horne, Michal Lahav, et al. 2022. <span>“LaMPost: Evaluation of an AI-Assisted Writing Email Editor Prototype for Adults with Dyslexia.”</span>
</div>
<div id="ref-hadfield2016cooperative" class="csl-entry" role="listitem">
Hadfield-Menell, Dylan, Stuart J Russell, Pieter Abbeel, and Anca Dragan. 2016. <span>“Cooperative Inverse Reinforcement Learning.”</span> <em>Advances in Neural Information Processing Systems</em> 29.
</div>
<div id="ref-hardt_patterns_2021" class="csl-entry" role="listitem">
Hardt, Moritz, and Benjamin Recht. 2021. <span>“Patterns, Predictions, and Actions: A Story about Machine Learning.”</span> <em>arXiv Preprint arXiv:2102.05242</em>.
</div>
<div id="ref-vanhasselt_deep_2018" class="csl-entry" role="listitem">
Hasselt, Hado van, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil. 2018. <span>“Deep Reinforcement Learning and the Deadly Triad.”</span>
</div>
<div id="ref-hejna2023contrastive" class="csl-entry" role="listitem">
Hejna, Joey, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, and Dorsa Sadigh. 2023. <span>“Contrastive Preference Learning: Learning from Human Feedback Without RL.”</span> <a href="https://arxiv.org/abs/2310.13639">https://arxiv.org/abs/2310.13639</a>.
</div>
<div id="ref-hendrycks_aligning_2021" class="csl-entry" role="listitem">
Hendrycks, Dan, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2020. <span>“Aligning Ai with Shared Human Values.”</span> <em>arXiv Preprint arXiv:2008.02275</em>.
</div>
<div id="ref-hendrycks_what_2021" class="csl-entry" role="listitem">
Hendrycks, Dan, Mantas Mazeika, Andy Zou, Sahil Patel, Christine Zhu, Jesus Navarro, Dawn Song, Bo Li, and Jacob Steinhardt. 2021. <span>“What <span>Would</span> <span>Jiminy</span> <span>Cricket</span> <span>Do</span>? <span>Towards</span> <span>Agents</span> <span>That</span> <span>Behave</span> <span>Morally</span>.”</span> <em>arXiv:2110.13136 [Cs]</em>. <a href="http://arxiv.org/abs/2110.13136">http://arxiv.org/abs/2110.13136</a>.
</div>
<div id="ref-hovy-yang-2021-importance" class="csl-entry" role="listitem">
Hovy, Dirk, and Diyi Yang. 2021. <span>“The Importance of Modeling Social Factors of Language: Theory and Practice.”</span> In <em>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, edited by Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, 588–602. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2021.naacl-main.49">https://doi.org/10.18653/v1/2021.naacl-main.49</a>.
</div>
<div id="ref-hsu2023helping" class="csl-entry" role="listitem">
Hsu, Shang-Ling, Raj Sanjay Shah, Prathik Senthil, Zahra Ashktorab, Casey Dugan, Werner Geyer, and Diyi Yang. 2023. <span>“Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice and Feedback.”</span> <a href="https://arxiv.org/abs/2305.08982">https://arxiv.org/abs/2305.08982</a>.
</div>
<div id="ref-lora" class="csl-entry" role="listitem">
Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. <span>“LoRA: Low-Rank Adaptation of Large Language Models.”</span> <a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a>.
</div>
<div id="ref-huang2018establishing" class="csl-entry" role="listitem">
Huang, Sandy H, Kush Bhatia, Pieter Abbeel, and Anca D Dragan. 2018. <span>“Establishing Appropriate Trust via Critical States.”</span> In <em>2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 3929–36. IEEE.
</div>
<div id="ref-hubinger2019introduction" class="csl-entry" role="listitem">
Hubinger, Evan, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. 2019. <span>“An Introduction to Inner Alignment.”</span> <em>arXiv Preprint arXiv:1906.01820</em>.
</div>
<div id="ref-jiang_artificial_2017" class="csl-entry" role="listitem">
Jiang, Fei, Yong Jiang, Hang Zhi, Yuan Dong, Hui Li, Shugang Ma, and Yongan Wang. 2017. <span>“Artificial Intelligence in Healthcare: Past, Present and Future.”</span> <em>Stroke and Vascular Neurology</em> 2 (4): 230–43.
</div>
<div id="ref-jiang_delphi_2021" class="csl-entry" role="listitem">
Jiang, Liwei, Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Maxwell Forbes, Jon Borchardt, Jenny Liang, Oren Etzioni, Maarten Sap, and Yejin Choi. 2021. <span>“Delphi: <span>Towards</span> <span>Machine</span> <span>Ethics</span> and <span>Norms</span>.”</span> <em>arXiv:2110.07574 [Cs]</em>, October. <a href="http://arxiv.org/abs/2110.07574">http://arxiv.org/abs/2110.07574</a>.
</div>
<div id="ref-johnson_kants_2022" class="csl-entry" role="listitem">
Johnson, Robert, and Adam Cureton. 2022. <span>“Kant’s <span>Moral</span> <span>Philosophy</span>.”</span> In <em>The <span>Stanford</span> <span>Encyclopedia</span> of <span>Philosophy</span></em>, edited by Edward N. Zalta and Uri Nodelman, Fall 2022. Metaphysics Research Lab, Stanford University. <a href="https://plato.stanford.edu/archives/fall2022/entries/kant-moral/">https://plato.stanford.edu/archives/fall2022/entries/kant-moral/</a>.
</div>
<div id="ref-krakovna2020specification" class="csl-entry" role="listitem">
Krakovna, Victoria et al. 2020. <span>“Specification Gaming Examples in AI.”</span> <em>DeepMind Safety Research</em>.
</div>
<div id="ref-langley1999adaptive" class="csl-entry" role="listitem">
Langley, Pat, Cynthia Thompson, Renee Elio, and Afsaneh Haddadi. 1999. <span>“An Adaptive Conversational Interface for Destination Advice.”</span> In <em>International Workshop on Cooperative Information Agents</em>, 347–64. Springer.
</div>
<div id="ref-leike2018scalable" class="csl-entry" role="listitem">
Leike, Jan, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. 2018. <span>“Scalable Agent Alignment via Reward Modeling: A Research Direction.”</span> <a href="https://arxiv.org/abs/1811.07871">https://arxiv.org/abs/1811.07871</a>.
</div>
<div id="ref-liang_holistic_2023" class="csl-entry" role="listitem">
Liang, Percy, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, et al. 2023. <span>“Holistic <span>Evaluation</span> of <span>Language</span> <span>Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2211.09110">https://doi.org/10.48550/arXiv.2211.09110</a>.
</div>
<div id="ref-dada" class="csl-entry" role="listitem">
Liu, Yanchen, William Held, and Diyi Yang. 2023. <span>“DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules.”</span> <a href="https://arxiv.org/abs/2305.13406">https://arxiv.org/abs/2305.13406</a>.
</div>
<div id="ref-mazeika_how_2022" class="csl-entry" role="listitem">
Mazeika, Mantas, Eric Tang, Andy Zou, Steven Basart, Jun Shern Chan, Dawn Song, David Forsyth, Jacob Steinhardt, and Dan Hendrycks. 2022. <span>“How <span>Would</span> <span>The</span> <span>Viewer</span> <span>Feel</span>? <span>Estimating</span> <span>Wellbeing</span> <span>From</span> <span>Video</span> <span>Scenarios</span>.”</span> <em>arXiv Preprint arXiv:2210.10039</em>.
</div>
<div id="ref-mehrabi_survey_2021" class="csl-entry" role="listitem">
Mehrabi, Ninareh, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. <span>“A Survey on Bias and Fairness in Machine Learning.”</span> <em>ACM Computing Surveys (CSUR)</em> 54 (6): 1–35.
</div>
<div id="ref-mill_utilitarianism_1863" class="csl-entry" role="listitem">
Mill, John Stuart. 1863. <em>Utilitarianism</em>. Parker, Son,; Bourn.
</div>
<div id="ref-moerland_emotion_2018" class="csl-entry" role="listitem">
Moerland, Thomas M, Joost Broekens, and Catholijn M Jonker. 2018. <span>“Emotion in Reinforcement Learning Agents and Robots: A Survey.”</span> <em>Machine Learning</em> 107: 443–80.
</div>
<div id="ref-Morris2019HITL" class="csl-entry" role="listitem">
Morris, Meredith Ringel. 2019. <span>“Human-in-the-Loop Computing: Reimagining Human-Computer Interaction in the Age of AI.”</span> In <em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>. ACM.
</div>
<div id="ref-muller_participatory_2003" class="csl-entry" role="listitem">
Muller, Michael J. 2003. <span>“Participatory Design: The Third Space in HCI.”</span> In <em>The Human-Computer Interaction Handbook</em>. CRC Press.
</div>
<div id="ref-ngo2023alignment" class="csl-entry" role="listitem">
Ngo, Richard, Lawrence Chan, and Sören Mindermann. 2023. <span>“The Alignment Problem from a Deep Learning Perspective.”</span> <a href="https://arxiv.org/abs/2209.00626">https://arxiv.org/abs/2209.00626</a>.
</div>
<div id="ref-noble_algorithms_2018" class="csl-entry" role="listitem">
Noble, Safiya Umoja. 2018. <em>Algorithms of Oppression: How Search Engines Reinforce Racism</em>. NYU Press.
</div>
<div id="ref-nussbaum1993quality" class="csl-entry" role="listitem">
Nussbaum, Martha C, and Amartya Sen. 1993. <em>The Quality of Life</em>. Oxford University Press.
</div>
<div id="ref-oneil_weapons_2016" class="csl-entry" role="listitem">
O’Neil, Cathy. 2016. <em>Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy</em>. Crown Publishing Group.
</div>
<div id="ref-ouyang_training_2022" class="csl-entry" role="listitem">
Ouyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. <span>“Training Language Models to Follow Instructions with Human Feedback.”</span>
</div>
<div id="ref-lamport2017lampost" class="csl-entry" role="listitem">
Project, LaMPort. 2017. <span>“LaMPost: Leveraging Crowdsourcing for Natural Language Processing.”</span> In <em>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>. ACL.
</div>
<div id="ref-rawls1971theory" class="csl-entry" role="listitem">
Rawls, John. 1971. <em>A Theory of Justice</em>. Harvard University Press.
</div>
<div id="ref-rogers1999adaptive" class="csl-entry" role="listitem">
Rogers, Seth, Claude-Nicolas Fiechter, and Pat Langley. 1999. <span>“An Adaptive Interactive Agent for Route Advice.”</span> In <em>Proceedings of the Third Annual Conference on Autonomous Agents</em>, 198–205.
</div>
<div id="ref-russell2019human" class="csl-entry" role="listitem">
Russell, Stuart. 2019. <em>Human Compatible: Artificial Intelligence and the Problem of Control</em>. Viking.
</div>
<div id="ref-sadigh2017active" class="csl-entry" role="listitem">
Sadigh, Dorsa, Anca Dragan, Shankar Sastry, and Sanjit Seshia. 2017. <span>“Active Preference-Based Learning of Reward Functions.”</span>
</div>
<div id="ref-sap_socialIQA_2019" class="csl-entry" role="listitem">
Sap, Maarten, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019. <span>“Socialiqa: Commonsense Reasoning about Social Interactions.”</span> <em>arXiv Preprint arXiv:1904.09728</em>.
</div>
<div id="ref-schwartz1992universals" class="csl-entry" role="listitem">
Schwartz, Shalom H. 1992. <span>“Universals in the Content and Structure of Values: Theoretical Advances and Empirical Tests in 20 Countries.”</span> <em>Advances in Experimental Social Psychology</em> 25: 1–65.
</div>
<div id="ref-shah2022goal" class="csl-entry" role="listitem">
Shah, Rohin, Vikrant Varma, Ramana Kumar, Mary Phuong, Victoria Krakovna, Jonathan Uesato, and Zac Kenton. 2022. <span>“Goal Misgeneralization: Why Correct Specifications Aren’t Enough for Correct Goals.”</span> <a href="https://arxiv.org/abs/2210.01790">https://arxiv.org/abs/2210.01790</a>.
</div>
<div id="ref-stiennon_learning_2020" class="csl-entry" role="listitem">
Stiennon, Nisan, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. <span>“Learning to Summarize from Human Feedback.”</span>
</div>
<div id="ref-talat_machine_2022" class="csl-entry" role="listitem">
Talat, Zeerak, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, and Adina Williams. 2022. <span>“On the Machine Learning of Ethical Judgments from Natural Language.”</span> In <em>Proceedings of the 2022 <span>Conference</span> of the <span>North</span> <span>American</span> <span>Chapter</span> of the <span>Association</span> for <span>Computational</span> <span>Linguistics</span>: <span>Human</span> <span>Language</span> <span>Technologies</span></em>. Association for Computational Linguistics.
</div>
<div id="ref-tomasello_becoming_2019" class="csl-entry" role="listitem">
Tomasello, Michael. 2019. <em>Becoming Human: <span>A</span> Theory of Ontogeny</em>. Cambridge, MA: Belknap Press.
</div>
<div id="ref-vamplew_human-aligned_2018" class="csl-entry" role="listitem">
Vamplew, Peter, Richard Dazeley, Cameron Foale, Sally Firmin, and Jane Mummery. 2018. <span>“Human-Aligned Artificial Intelligence Is a Multiobjective Problem.”</span> <em>Ethics and Information Technology</em> 20 (1): 27–40. <a href="https://doi.org/10.1007/s10676-017-9440-6">https://doi.org/10.1007/s10676-017-9440-6</a>.
</div>
<div id="ref-vamplew_scalar_2022" class="csl-entry" role="listitem">
Vamplew, Peter, Benjamin J. Smith, Johan Källström, Gabriel Ramos, Roxana Rădulescu, Diederik M. Roijers, Conor F. Hayes, et al. 2022. <span>“Scalar Reward Is Not Enough: A Response to <span>Silver</span>, <span>Singh</span>, <span>Precup</span> and <span>Sutton</span> (2021).”</span> <em>Autonomous Agents and Multi-Agent Systems</em> 36 (2): 41. <a href="https://doi.org/10.1007/s10458-022-09575-5">https://doi.org/10.1007/s10458-022-09575-5</a>.
</div>
<div id="ref-weidinger_artificial_2022" class="csl-entry" role="listitem">
Weidinger, Laura, Madeline G. Reinecke, and Julia Haas. 2022. <span>“Artificial Moral Cognition: <span>Learning</span> from Developmental Psychology.”</span> Preprint. PsyArXiv. <a href="https://doi.org/10.31234/osf.io/tnf4e">https://doi.org/10.31234/osf.io/tnf4e</a>.
</div>
<div id="ref-enwiki:1185176830" class="csl-entry" role="listitem">
Wikipedia contributors. 2023. <span>“AI Alignment — <span>Wikipedia</span><span>,</span> the Free Encyclopedia.”</span> <a href="https://en.wikipedia.org/w/index.php?title=AI_alignment&amp;oldid=1185176830">https://en.wikipedia.org/w/index.php?title=AI_alignment&amp;oldid=1185176830</a>.
</div>
<div id="ref-xiong_achieving_2016" class="csl-entry" role="listitem">
Xiong, Wayne, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong Yu, and Geoffrey Zweig. 2016. <span>“Achieving Human Parity in Conversational Speech Recognition.”</span> <em>arXiv Preprint arXiv:1610.05256</em>.
</div>
<div id="ref-ziebart_modeling_2010" class="csl-entry" role="listitem">
Ziebart, Brian D. 2010. <span>“Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy.”</span> PhD Thesis, Pittsburgh, PA: Carnegie Mellon University.
</div>
<div id="ref-mv" class="csl-entry" role="listitem">
Ziems, Caleb, William Held, Jingfeng Yang, Jwala Dhamala, Rahul Gupta, and Diyi Yang. 2023. <span>“Multi-VALUE: A Framework for Cross-Dialectal English NLP.”</span> <a href="https://arxiv.org/abs/2212.08011">https://arxiv.org/abs/2212.08011</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>GPT-4 is good at coming up with longer-rendered answers about why some things are appropriate or not.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../src/chap4.html" class="pagination-link" aria-label="Aggregation of Preferences">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Aggregation of Preferences</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../src/ack.html" class="pagination-link" aria-label="Acknowledgments">
        <span class="nav-page-text">Acknowledgments</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu"># Human Values and AI Alignment {#sec-human-ai-alginment}</span></span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a>In recent years, the rapidly advancing capabilities of large models have</span>
<span id="cb1-4"><a href="#cb1-4"></a>led to increased discussion of aligning AI systems with human values.</span>
<span id="cb1-5"><a href="#cb1-5"></a>This chapter discusses the multifaceted relationship between values,</span>
<span id="cb1-6"><a href="#cb1-6"></a>alignment, and human-centered design in the context of AI. We begin by</span>
<span id="cb1-7"><a href="#cb1-7"></a>exploring the fundamental concept of human values and their ethical</span>
<span id="cb1-8"><a href="#cb1-8"></a>implications in AI design. This includes discussions on human values and</span>
<span id="cb1-9"><a href="#cb1-9"></a>ethics in AI, understanding and addressing bias in AI, and methods for</span>
<span id="cb1-10"><a href="#cb1-10"></a>aligning AI with human values. Additionally, we examine AI alignment</span>
<span id="cb1-11"><a href="#cb1-11"></a>problems, focusing on outer alignment to avoid specification gaming and</span>
<span id="cb1-12"><a href="#cb1-12"></a>inner alignment to prevent goal misgeneralization. Next, we cover</span>
<span id="cb1-13"><a href="#cb1-13"></a>techniques in value learning. This section introduces methodologies such</span>
<span id="cb1-14"><a href="#cb1-14"></a>as reinforcement learning from human feedback and contrastive preference</span>
<span id="cb1-15"><a href="#cb1-15"></a>learning, which are crucial for teaching AI systems to understand and</span>
<span id="cb1-16"><a href="#cb1-16"></a>align with human values. The importance of value alignment verification</span>
<span id="cb1-17"><a href="#cb1-17"></a>is emphasized to ensure that AI systems remain consistent with human</span>
<span id="cb1-18"><a href="#cb1-18"></a>values over time, adapting to changes and preventing misalignment. We</span>
<span id="cb1-19"><a href="#cb1-19"></a>then explore the principles and practices of human-centered design. This</span>
<span id="cb1-20"><a href="#cb1-20"></a>includes discussions on AI and human-computer interaction and methods</span>
<span id="cb1-21"><a href="#cb1-21"></a>for designing AI for positive human impact, which focuses on creating AI</span>
<span id="cb1-22"><a href="#cb1-22"></a>systems that are socially aware, human-centered, and positively</span>
<span id="cb1-23"><a href="#cb1-23"></a>impactful. A crucial part of this discussion is adaptive user</span>
<span id="cb1-24"><a href="#cb1-24"></a>interfaces, where we discuss key ideas, design principles, applications,</span>
<span id="cb1-25"><a href="#cb1-25"></a>and limitations of these interfaces, showcasing how they enhance user</span>
<span id="cb1-26"><a href="#cb1-26"></a>experience by dynamically adjusting to user needs and preferences.</span>
<span id="cb1-27"><a href="#cb1-27"></a>Finally, we present case studies in human-centered AI, including the</span>
<span id="cb1-28"><a href="#cb1-28"></a>LaMPost case study, Multi-Value, and DaDa: Cross-Dialectal English NLP,</span>
<span id="cb1-29"><a href="#cb1-29"></a>and social skill training via LLMs. These case studies provide</span>
<span id="cb1-30"><a href="#cb1-30"></a>real-world examples of successful implementations of human-centered AI</span>
<span id="cb1-31"><a href="#cb1-31"></a>systems. By integrating these elements, the chapter aims to provide a</span>
<span id="cb1-32"><a href="#cb1-32"></a>comprehensive understanding of how to create AI systems that are</span>
<span id="cb1-33"><a href="#cb1-33"></a>ethical, aligned with human values, and beneficial to society.</span>
<span id="cb1-34"><a href="#cb1-34"></a></span>
<span id="cb1-35"><a href="#cb1-35"></a><span class="fu">## Human Values and AI Alignment</span></span>
<span id="cb1-36"><a href="#cb1-36"></a></span>
<span id="cb1-37"><a href="#cb1-37"></a>In this part, we take a step back from the technical details to reflect</span>
<span id="cb1-38"><a href="#cb1-38"></a>on the broader concept of human values and their profound influence on</span>
<span id="cb1-39"><a href="#cb1-39"></a>our behavior and decision-making.</span>
<span id="cb1-40"><a href="#cb1-40"></a></span>
<span id="cb1-41"><a href="#cb1-41"></a><span class="fu">### Human Values and Ethics in AI</span></span>
<span id="cb1-42"><a href="#cb1-42"></a></span>
<span id="cb1-43"><a href="#cb1-43"></a>Human values are the principles and standards that guide behavior and</span>
<span id="cb1-44"><a href="#cb1-44"></a>decision-making, reflecting what is essential in life and influencing</span>
<span id="cb1-45"><a href="#cb1-45"></a>choices and actions. One notable scholar in this field is Shalom H.</span>
<span id="cb1-46"><a href="#cb1-46"></a>Schwartz, a social psychologist renowned for his theory on basic human</span>
<span id="cb1-47"><a href="#cb1-47"></a>values. Schwartz's work has significantly contributed to our</span>
<span id="cb1-48"><a href="#cb1-48"></a>understanding of how values influence behavior across different</span>
<span id="cb1-49"><a href="#cb1-49"></a>cultures. He describes values as "desirable, trans-situational goals,</span>
<span id="cb1-50"><a href="#cb1-50"></a>varying in importance, that serve as guiding principles in people's</span>
<span id="cb1-51"><a href="#cb1-51"></a>lives" <span class="co">[</span><span class="ot">@schwartz1992universals</span><span class="co">]</span>. This perspective underscores the</span>
<span id="cb1-52"><a href="#cb1-52"></a>importance of values in shaping consistent and ethical behavior across</span>
<span id="cb1-53"><a href="#cb1-53"></a>different contexts. Supporting this view, philosopher William K.</span>
<span id="cb1-54"><a href="#cb1-54"></a>Frankena emphasizes the integral role of values in ethical behavior and</span>
<span id="cb1-55"><a href="#cb1-55"></a>decision-making processes. Frankena's work in ethical theory provides a</span>
<span id="cb1-56"><a href="#cb1-56"></a>foundation for understanding how moral judgments are formed. He notes</span>
<span id="cb1-57"><a href="#cb1-57"></a>that "ethical theory is concerned with the principles and concepts that</span>
<span id="cb1-58"><a href="#cb1-58"></a>underlie moral judgments" <span class="co">[</span><span class="ot">@frankena1973ethics</span><span class="co">]</span>, highlighting the need</span>
<span id="cb1-59"><a href="#cb1-59"></a>to comprehend ethical principles deeply to make informed moral</span>
<span id="cb1-60"><a href="#cb1-60"></a>judgments. Examples of ethical values include autonomy, fairness,</span>
<span id="cb1-61"><a href="#cb1-61"></a>justice, and well-being. For computer scientists developing AI systems,</span>
<span id="cb1-62"><a href="#cb1-62"></a>understanding these concepts is crucial. AI systems that interact with</span>
<span id="cb1-63"><a href="#cb1-63"></a>humans and impact societal structures must be designed with these values</span>
<span id="cb1-64"><a href="#cb1-64"></a>in mind. By embedding such values into AI, developers can create systems</span>
<span id="cb1-65"><a href="#cb1-65"></a>that respect human dignity and promote positive social outcomes.</span>
<span id="cb1-66"><a href="#cb1-66"></a></span>
<span id="cb1-67"><a href="#cb1-67"></a><span class="ss">-   </span>Autonomy is the right to choose, an essential aspect of personal</span>
<span id="cb1-68"><a href="#cb1-68"></a>freedom. Gerald Dworkin defines autonomy as "the capacity to reflect upon and endorse</span>
<span id="cb1-69"><a href="#cb1-69"></a>or reject one's desires and values" <span class="co">[</span><span class="ot">@dworkin1988theory</span><span class="co">]</span>. In AI,</span>
<span id="cb1-70"><a href="#cb1-70"></a>respecting autonomy means creating systems that support user</span>
<span id="cb1-71"><a href="#cb1-71"></a>independence and decision-making rather than manipulating or coercing</span>
<span id="cb1-72"><a href="#cb1-72"></a>them.</span>
<span id="cb1-73"><a href="#cb1-73"></a></span>
<span id="cb1-74"><a href="#cb1-74"></a><span class="ss">-   </span>Fairness involves treating all individuals equally and justly, ensuring</span>
<span id="cb1-75"><a href="#cb1-75"></a>no discrimination. John Rawls, one of the most influential political</span>
<span id="cb1-76"><a href="#cb1-76"></a>philosophers of the $20^{th}$ century, in his groundbreaking book "A Theory</span>
<span id="cb1-77"><a href="#cb1-77"></a>of Justice," describes fairness as "the elimination of arbitrary</span>
<span id="cb1-78"><a href="#cb1-78"></a>distinctions and the establishment of a balance between competing</span>
<span id="cb1-79"><a href="#cb1-79"></a>claims" <span class="co">[</span><span class="ot">@rawls1971theory</span><span class="co">]</span>. For AI systems, this translates to</span>
<span id="cb1-80"><a href="#cb1-80"></a>algorithms that do not perpetuate bias or inequality, ensuring that all</span>
<span id="cb1-81"><a href="#cb1-81"></a>users are treated equitably.</span>
<span id="cb1-82"><a href="#cb1-82"></a></span>
<span id="cb1-83"><a href="#cb1-83"></a><span class="ss">-   </span>Justice is about upholding what is morally right and ensuring fair</span>
<span id="cb1-84"><a href="#cb1-84"></a>treatment for all. Rawls also highlights that "justice is the first</span>
<span id="cb1-85"><a href="#cb1-85"></a>virtue of social institutions, as truth is of systems of thought"</span>
<span id="cb1-86"><a href="#cb1-86"></a><span class="co">[</span><span class="ot">@rawls1971theory</span><span class="co">]</span>. In the context of AI, justice involves creating</span>
<span id="cb1-87"><a href="#cb1-87"></a>technologies that enhance fairness in legal, social, and economic</span>
<span id="cb1-88"><a href="#cb1-88"></a>systems, providing equal opportunities and protection to all</span>
<span id="cb1-89"><a href="#cb1-89"></a>individuals.</span>
<span id="cb1-90"><a href="#cb1-90"></a></span>
<span id="cb1-91"><a href="#cb1-91"></a>Well-being focuses on promoting the health, happiness, and prosperity of</span>
<span id="cb1-92"><a href="#cb1-92"></a>individuals. Martha Nussbaum and Amartya Sen, two distinguished scholars</span>
<span id="cb1-93"><a href="#cb1-93"></a>known for their significant contributions to welfare economics and the</span>
<span id="cb1-94"><a href="#cb1-94"></a>development of the capability approach, discuss the importance of</span>
<span id="cb1-95"><a href="#cb1-95"></a>well-being in their collaborative work "The Quality of Life." They</span>
<span id="cb1-96"><a href="#cb1-96"></a>argue that "well-being is about the expansion of the capabilities of</span>
<span id="cb1-97"><a href="#cb1-97"></a>people to lead the kind of lives they value" <span class="co">[</span><span class="ot">@nussbaum1993quality</span><span class="co">]</span>. AI</span>
<span id="cb1-98"><a href="#cb1-98"></a>systems should enhance users' quality of life, supporting their health,</span>
<span id="cb1-99"><a href="#cb1-99"></a>education, and economic stability.</span>
<span id="cb1-100"><a href="#cb1-100"></a></span>
<span id="cb1-101"><a href="#cb1-101"></a>Understanding human values is foundational for readers with a computer</span>
<span id="cb1-102"><a href="#cb1-102"></a>science background before delving into AI ethics. These values provide</span>
<span id="cb1-103"><a href="#cb1-103"></a>the ethical underpinnings necessary to design and deploy AI systems</span>
<span id="cb1-104"><a href="#cb1-104"></a>responsibly. As AI systems increasingly impact all aspects of society,</span>
<span id="cb1-105"><a href="#cb1-105"></a>developers must embed these values into their work to ensure</span>
<span id="cb1-106"><a href="#cb1-106"></a>technologies benefit humanity and do not exacerbate existing</span>
<span id="cb1-107"><a href="#cb1-107"></a>inequalities.</span>
<span id="cb1-108"><a href="#cb1-108"></a></span>
<span id="cb1-109"><a href="#cb1-109"></a>Human values play a crucial role in decision-making by shaping the criteria </span>
<span id="cb1-110"><a href="#cb1-110"></a>for evaluating options and outcomes. They influence priorities and ethical </span>
<span id="cb1-111"><a href="#cb1-111"></a>considerations, guiding individuals and organizations to make choices that </span>
<span id="cb1-112"><a href="#cb1-112"></a>align with their principles. Nick Bostrom, a prominent philosopher in AI </span>
<span id="cb1-113"><a href="#cb1-113"></a>and existential risk, highlights the importance of values in setting priorities </span>
<span id="cb1-114"><a href="#cb1-114"></a>and determining desirable outcomes <span class="co">[</span><span class="ot">@bostrom2014superintelligence</span><span class="co">]</span>. Aligning actions with values ensures </span>
<span id="cb1-115"><a href="#cb1-115"></a>consistency and ethical integrity in decision-making. Incorporating human </span>
<span id="cb1-116"><a href="#cb1-116"></a>values into AI systems ensures that AI decisions align with societal norms </span>
<span id="cb1-117"><a href="#cb1-117"></a>and ethical standards. Stuart Russell, an AI researcher and advocate for </span>
<span id="cb1-118"><a href="#cb1-118"></a>human-compatible AI, stresses the importance of embedding human values into </span>
<span id="cb1-119"><a href="#cb1-119"></a>AI systems to ensure they act in beneficial and ethical ways <span class="co">[</span><span class="ot">@russell2019human</span><span class="co">]</span>. By integrating </span>
<span id="cb1-120"><a href="#cb1-120"></a>values such as fairness, justice, and well-being, AI systems can make </span>
<span id="cb1-121"><a href="#cb1-121"></a>decisions that reflect societal expectations and ethical considerations.</span>
<span id="cb1-122"><a href="#cb1-122"></a></span>
<span id="cb1-123"><a href="#cb1-123"></a>Examples of incorporating values into AI systems demonstrate the practical </span>
<span id="cb1-124"><a href="#cb1-124"></a>application of these principles. For instance, autonomous vehicles are </span>
<span id="cb1-125"><a href="#cb1-125"></a>programmed to prioritize human safety, ensuring decisions that protect </span>
<span id="cb1-126"><a href="#cb1-126"></a>lives. In healthcare, AI systems uphold values by safeguarding patient </span>
<span id="cb1-127"><a href="#cb1-127"></a>privacy and ensuring informed consent, adhering to ethical medical standards. </span>
<span id="cb1-128"><a href="#cb1-128"></a>Judicial AI systems aim to eliminate biases in sentencing recommendations, </span>
<span id="cb1-129"><a href="#cb1-129"></a>promoting fairness and justice. Luciano Floridi underscores the necessity </span>
<span id="cb1-130"><a href="#cb1-130"></a>for AI systems to be designed in a way that respects and upholds human </span>
<span id="cb1-131"><a href="#cb1-131"></a>values to function ethically and effectively <span class="co">[</span><span class="ot">@floridi2011ethics</span><span class="co">]</span>.</span>
<span id="cb1-132"><a href="#cb1-132"></a></span>
<span id="cb1-133"><a href="#cb1-133"></a>To ensure that these values are systematically embedded within AI</span>
<span id="cb1-134"><a href="#cb1-134"></a>systems, it is essential to consider major ethical frameworks such as</span>
<span id="cb1-135"><a href="#cb1-135"></a>deontological, consequentialist, and virtue ethics that guide moral</span>
<span id="cb1-136"><a href="#cb1-136"></a>decision-making.</span>
<span id="cb1-137"><a href="#cb1-137"></a></span>
<span id="cb1-138"><a href="#cb1-138"></a>Deontological ethics, primarily associated with the philosopher Immanuel</span>
<span id="cb1-139"><a href="#cb1-139"></a>Kant, focuses on rules and duties. This ethical framework posits that</span>
<span id="cb1-140"><a href="#cb1-140"></a>actions are morally right if they adhere to established rules and</span>
<span id="cb1-141"><a href="#cb1-141"></a>duties, regardless of the outcomes. Kant's moral philosophy emphasizes</span>
<span id="cb1-142"><a href="#cb1-142"></a>the importance of duty and adherence to moral laws. Robert Johnson, a</span>
<span id="cb1-143"><a href="#cb1-143"></a>scholar who has extensively studied Kantian ethics, explains that</span>
<span id="cb1-144"><a href="#cb1-144"></a>"Kant's moral philosophy emphasizes that actions must be judged based</span>
<span id="cb1-145"><a href="#cb1-145"></a>on their adherence to duty and moral law, not by their consequences"</span>
<span id="cb1-146"><a href="#cb1-146"></a><span class="co">[</span><span class="ot">@johnson_kants_2022</span><span class="co">]</span>. This perspective is grounded in the belief that</span>
<span id="cb1-147"><a href="#cb1-147"></a>specific actions are intrinsically right or wrong, and individuals must</span>
<span id="cb1-148"><a href="#cb1-148"></a>perform or avoid these actions based on rational moral principles.</span>
<span id="cb1-149"><a href="#cb1-149"></a></span>
<span id="cb1-150"><a href="#cb1-150"></a>In the context of AI, deontological ethics implies that AI systems</span>
<span id="cb1-151"><a href="#cb1-151"></a>should be designed to follow ethical rules and principles. For instance,</span>
<span id="cb1-152"><a href="#cb1-152"></a>AI systems must respect user privacy and confidentiality as an</span>
<span id="cb1-153"><a href="#cb1-153"></a>inviolable duty. This approach ensures that AI technologies do not</span>
<span id="cb1-154"><a href="#cb1-154"></a>infringe on individuals' rights, regardless of potential benefits.</span>
<span id="cb1-155"><a href="#cb1-155"></a>Implementing deontological principles in AI design can prevent ethical</span>
<span id="cb1-156"><a href="#cb1-156"></a>breaches, such as unauthorized data usage or surveillance. By adhering</span>
<span id="cb1-157"><a href="#cb1-157"></a>to established moral guidelines, AI systems can maintain ethical</span>
<span id="cb1-158"><a href="#cb1-158"></a>integrity and avoid actions that would be considered inherently wrong.</span>
<span id="cb1-159"><a href="#cb1-159"></a>As Floridi states, "AI systems should be developed with a commitment to</span>
<span id="cb1-160"><a href="#cb1-160"></a>uphold moral duties and respect human dignity" <span class="co">[</span><span class="ot">@floridi2011ethics</span><span class="co">]</span>.</span>
<span id="cb1-161"><a href="#cb1-161"></a></span>
<span id="cb1-162"><a href="#cb1-162"></a>Consequentialist ethics, in contrast, evaluates the morality of actions</span>
<span id="cb1-163"><a href="#cb1-163"></a>based on their outcomes. The most well-known form of consequentialism is</span>
<span id="cb1-164"><a href="#cb1-164"></a>utilitarianism, articulated by philosophers like Jeremy Bentham and John</span>
<span id="cb1-165"><a href="#cb1-165"></a>Stuart Mill. This ethical theory suggests that actions are morally right</span>
<span id="cb1-166"><a href="#cb1-166"></a>if they promote the greatest happiness for the greatest number. Mill</span>
<span id="cb1-167"><a href="#cb1-167"></a>emphasizes that "the moral worth of an action is determined by its</span>
<span id="cb1-168"><a href="#cb1-168"></a>contribution to overall utility, measured by the happiness or well-being</span>
<span id="cb1-169"><a href="#cb1-169"></a>it produces" <span class="co">[</span><span class="ot">@mill_utilitarianism_1863</span><span class="co">]</span>. Consequentialist ethics is</span>
<span id="cb1-170"><a href="#cb1-170"></a>pragmatic, focusing on the results of actions rather than the actions</span>
<span id="cb1-171"><a href="#cb1-171"></a>themselves.</span>
<span id="cb1-172"><a href="#cb1-172"></a></span>
<span id="cb1-173"><a href="#cb1-173"></a>Applying consequentialist ethics to AI development involves designing AI</span>
<span id="cb1-174"><a href="#cb1-174"></a>systems to achieve beneficial outcomes. This means prioritizing positive</span>
<span id="cb1-175"><a href="#cb1-175"></a>societal impacts, such as improving healthcare outcomes, enhancing</span>
<span id="cb1-176"><a href="#cb1-176"></a>public safety, or reducing environmental harm. For instance, algorithms</span>
<span id="cb1-177"><a href="#cb1-177"></a>can be designed to optimize resource allocation in disaster response,</span>
<span id="cb1-178"><a href="#cb1-178"></a>thereby maximizing the overall well-being of affected populations. In</span>
<span id="cb1-179"><a href="#cb1-179"></a>this framework, the ethicality of AI decisions is judged by their</span>
<span id="cb1-180"><a href="#cb1-180"></a>ability to produce desirable consequences. Virginia Dignum, a professor</span>
<span id="cb1-181"><a href="#cb1-181"></a>of responsible artificial intelligence at Umeå University, explains that</span>
<span id="cb1-182"><a href="#cb1-182"></a>"designing algorithms with a focus on maximizing positive outcomes can</span>
<span id="cb1-183"><a href="#cb1-183"></a>lead to more ethical and effective AI systems"</span>
<span id="cb1-184"><a href="#cb1-184"></a><span class="co">[</span><span class="ot">@dignum_responsible_2019</span><span class="co">]</span>. Consequently, AI developers focus on the</span>
<span id="cb1-185"><a href="#cb1-185"></a>potential impacts of their technologies and strive to enhance their</span>
<span id="cb1-186"><a href="#cb1-186"></a>beneficial effects.</span>
<span id="cb1-187"><a href="#cb1-187"></a></span>
<span id="cb1-188"><a href="#cb1-188"></a>Virtue ethics, originating from the teachings of Aristotle, emphasizes</span>
<span id="cb1-189"><a href="#cb1-189"></a>the importance of character and virtues in ethical behavior. This</span>
<span id="cb1-190"><a href="#cb1-190"></a>framework posits that ethical behavior arises from developing good</span>
<span id="cb1-191"><a href="#cb1-191"></a>character traits and living a virtuous life. Aristotle, an ancient Greek</span>
<span id="cb1-192"><a href="#cb1-192"></a>philosopher and the author of "Nicomachean Ethics," argues that</span>
<span id="cb1-193"><a href="#cb1-193"></a>"virtue is about cultivating excellence in character to achieve</span>
<span id="cb1-194"><a href="#cb1-194"></a>eudaimonia or human flourishing" <span class="co">[</span><span class="ot">@aristotle_nicomachean_350</span><span class="co">]</span>. Virtue</span>
<span id="cb1-195"><a href="#cb1-195"></a>ethics focuses on the individual's character and the moral qualities</span>
<span id="cb1-196"><a href="#cb1-196"></a>that define a good person, such as honesty, courage, and compassion.</span>
<span id="cb1-197"><a href="#cb1-197"></a></span>
<span id="cb1-198"><a href="#cb1-198"></a>Additionally, virtue ethics encourages the development and use of AI</span>
<span id="cb1-199"><a href="#cb1-199"></a>systems that promote virtuous behavior. This involves fostering</span>
<span id="cb1-200"><a href="#cb1-200"></a>transparency, accountability, and fairness in AI technologies. For</span>
<span id="cb1-201"><a href="#cb1-201"></a>example, AI systems should be designed to provide clear and</span>
<span id="cb1-202"><a href="#cb1-202"></a>understandable explanations for their decisions, promoting transparency</span>
<span id="cb1-203"><a href="#cb1-203"></a>and building user trust. Furthermore, AI developers should strive to</span>
<span id="cb1-204"><a href="#cb1-204"></a>create technologies that support ethical practices and enhance the</span>
<span id="cb1-205"><a href="#cb1-205"></a>common good. Floridi emphasizes that "virtue ethics in AI development</span>
<span id="cb1-206"><a href="#cb1-206"></a>requires a commitment to fostering moral virtues and promoting human</span>
<span id="cb1-207"><a href="#cb1-207"></a>well-being" <span class="co">[</span><span class="ot">@floridi2011ethics</span><span class="co">]</span>. By focusing on the character and</span>
<span id="cb1-208"><a href="#cb1-208"></a>virtues of AI developers and AI systems, virtue ethics provides a</span>
<span id="cb1-209"><a href="#cb1-209"></a>holistic approach to ethical AI development.</span>
<span id="cb1-210"><a href="#cb1-210"></a></span>
<span id="cb1-211"><a href="#cb1-211"></a>Applying these ethical frameworks to AI development is essential to</span>
<span id="cb1-212"><a href="#cb1-212"></a>ensure that AI systems operate ethically and responsibly. Deontological</span>
<span id="cb1-213"><a href="#cb1-213"></a>ethics in AI involves ensuring that AI follows ethical rules and</span>
<span id="cb1-214"><a href="#cb1-214"></a>principles. For instance, AI systems should be designed to respect user</span>
<span id="cb1-215"><a href="#cb1-215"></a>privacy and confidentiality. Consequentialist ethics focuses on</span>
<span id="cb1-216"><a href="#cb1-216"></a>developing AI to achieve beneficial outcomes. This means creating</span>
<span id="cb1-217"><a href="#cb1-217"></a>algorithms prioritizing positive societal impacts, such as improving</span>
<span id="cb1-218"><a href="#cb1-218"></a>healthcare outcomes or reducing environmental harm. Virtue ethics</span>
<span id="cb1-219"><a href="#cb1-219"></a>encourages virtuous behavior in AI development and use, promoting</span>
<span id="cb1-220"><a href="#cb1-220"></a>transparency, accountability, and fairness. Floridi emphasizes that</span>
<span id="cb1-221"><a href="#cb1-221"></a>"ethical AI development requires a commitment to core moral principles</span>
<span id="cb1-222"><a href="#cb1-222"></a>and virtues" <span class="co">[</span><span class="ot">@floridi2011ethics</span><span class="co">]</span>.</span>
<span id="cb1-223"><a href="#cb1-223"></a></span>
<span id="cb1-224"><a href="#cb1-224"></a>Examples in practice demonstrate how these frameworks can be applied to</span>
<span id="cb1-225"><a href="#cb1-225"></a>guide ethical AI development. Implementing fairness constraints in</span>
<span id="cb1-226"><a href="#cb1-226"></a>machine learning models ensures that algorithms do not discriminate</span>
<span id="cb1-227"><a href="#cb1-227"></a>against certain groups. Binns notes that "fairness in machine learning</span>
<span id="cb1-228"><a href="#cb1-228"></a>can be informed by lessons from political philosophy to create more just</span>
<span id="cb1-229"><a href="#cb1-229"></a>and equitable systems" <span class="co">[</span><span class="ot">@binns_fairness_2018</span><span class="co">]</span>. Designing algorithms that</span>
<span id="cb1-230"><a href="#cb1-230"></a>maximize overall well-being aligns with consequentialist ethics by</span>
<span id="cb1-231"><a href="#cb1-231"></a>focusing on the positive outcomes of AI deployment. Additionally,</span>
<span id="cb1-232"><a href="#cb1-232"></a>developing AI systems focusing on transparency and accountability</span>
<span id="cb1-233"><a href="#cb1-233"></a>supports virtue ethics by fostering trust and reliability in AI</span>
<span id="cb1-234"><a href="#cb1-234"></a>technologies.</span>
<span id="cb1-235"><a href="#cb1-235"></a></span>
<span id="cb1-236"><a href="#cb1-236"></a>Ethical principles provide a framework for ensuring that AI operates in</span>
<span id="cb1-237"><a href="#cb1-237"></a>ways that are fair, just, and beneficial. Deontological ethics, for</span>
<span id="cb1-238"><a href="#cb1-238"></a>instance, focuses on moral rules and obligations, while consequentialism</span>
<span id="cb1-239"><a href="#cb1-239"></a>considers the outcomes of actions. By embedding these ethical principles</span>
<span id="cb1-240"><a href="#cb1-240"></a>into AI design, we can create systems that respect human dignity and</span>
<span id="cb1-241"><a href="#cb1-241"></a>promote societal well-being.</span>
<span id="cb1-242"><a href="#cb1-242"></a></span>
<span id="cb1-243"><a href="#cb1-243"></a><span class="fu">### Bias in AI</span></span>
<span id="cb1-244"><a href="#cb1-244"></a></span>
<span id="cb1-245"><a href="#cb1-245"></a>Bias in AI refers to systematic errors that result in unfair outcomes.</span>
<span id="cb1-246"><a href="#cb1-246"></a>These biases can occur at various stages of AI system development and</span>
<span id="cb1-247"><a href="#cb1-247"></a>deployment, leading to significant ethical and practical concerns.</span>
<span id="cb1-248"><a href="#cb1-248"></a>Addressing bias in AI is crucial because it directly impacts the</span>
<span id="cb1-249"><a href="#cb1-249"></a>fairness, accountability, and trustworthiness of AI systems. Barocas,</span>
<span id="cb1-250"><a href="#cb1-250"></a>Hardt, and Narayanan emphasize that "bias in machine learning can lead</span>
<span id="cb1-251"><a href="#cb1-251"></a>to decisions that systematically disadvantage certain groups"</span>
<span id="cb1-252"><a href="#cb1-252"></a><span class="co">[</span><span class="ot">@barocas_fairness_2019</span><span class="co">]</span>. O'Neil further highlights the societal impact</span>
<span id="cb1-253"><a href="#cb1-253"></a>of biased AI, noting that "algorithms can perpetuate and amplify</span>
<span id="cb1-254"><a href="#cb1-254"></a>existing inequalities, leading to a cycle of discrimination"</span>
<span id="cb1-255"><a href="#cb1-255"></a><span class="co">[</span><span class="ot">@oneil_weapons_2016</span><span class="co">]</span>. Therefore, understanding and mitigating bias is</span>
<span id="cb1-256"><a href="#cb1-256"></a>essential for developing ethical AI systems that promote fairness and</span>
<span id="cb1-257"><a href="#cb1-257"></a>equity.</span>
<span id="cb1-258"><a href="#cb1-258"></a></span>
<span id="cb1-259"><a href="#cb1-259"></a>Data bias originates from skewed or non-representative data used to</span>
<span id="cb1-260"><a href="#cb1-260"></a>train AI models. This bias often reflects historical prejudices and</span>
<span id="cb1-261"><a href="#cb1-261"></a>systemic inequalities in the data. For example, if a hiring algorithm is</span>
<span id="cb1-262"><a href="#cb1-262"></a>trained on historical hiring data that reflects gender or racial biases,</span>
<span id="cb1-263"><a href="#cb1-263"></a>it may perpetuate these biases in its recommendations. Fatemeh Mehrabi</span>
<span id="cb1-264"><a href="#cb1-264"></a>and her colleagues, in their survey on bias in AI, state that "data</span>
<span id="cb1-265"><a href="#cb1-265"></a>bias can result from sampling bias, measurement bias, or historical</span>
<span id="cb1-266"><a href="#cb1-266"></a>bias, each contributing to the unfairness of AI systems"</span>
<span id="cb1-267"><a href="#cb1-267"></a><span class="co">[</span><span class="ot">@mehrabi_survey_2021</span><span class="co">]</span>. Safiya Umoja Noble, author of "Algorithms of</span>
<span id="cb1-268"><a href="#cb1-268"></a>Oppression," discusses how biased data in search engines can reinforce</span>
<span id="cb1-269"><a href="#cb1-269"></a>stereotypes and marginalize certain groups, noting that "search</span>
<span id="cb1-270"><a href="#cb1-270"></a>algorithms often reflect the biases of the society they operate within"</span>
<span id="cb1-271"><a href="#cb1-271"></a><span class="co">[</span><span class="ot">@noble_algorithms_2018</span><span class="co">]</span>. Addressing data bias involves careful</span>
<span id="cb1-272"><a href="#cb1-272"></a>collection, preprocessing, and validation to ensure diversity and</span>
<span id="cb1-273"><a href="#cb1-273"></a>representation.</span>
<span id="cb1-274"><a href="#cb1-274"></a></span>
<span id="cb1-275"><a href="#cb1-275"></a>An effort to address data bias is the "Lab in the Wild" platform,</span>
<span id="cb1-276"><a href="#cb1-276"></a>which seeks to broaden the scope of Human-Computer Interaction (HCI)</span>
<span id="cb1-277"><a href="#cb1-277"></a>studies beyond the traditional "WEIRD" (Western, Educated,</span>
<span id="cb1-278"><a href="#cb1-278"></a>Industrialized, Rich, and Democratic) population <span class="co">[</span><span class="ot">@oliveira17</span><span class="co">]</span>. Paulo S.</span>
<span id="cb1-279"><a href="#cb1-279"></a>Oliveira, one of the platform's researchers, notes that this initiative</span>
<span id="cb1-280"><a href="#cb1-280"></a>aims to correct demographic skew in behavioral science research by</span>
<span id="cb1-281"><a href="#cb1-281"></a>engaging a diverse global audience. By allowing individuals from various</span>
<span id="cb1-282"><a href="#cb1-282"></a>demographics to participate in studies from their environments, "Lab in</span>
<span id="cb1-283"><a href="#cb1-283"></a>the Wild" provides researchers with a more inclusive dataset.</span>
<span id="cb1-284"><a href="#cb1-284"></a></span>
<span id="cb1-285"><a href="#cb1-285"></a>Another important consideration is the cultural nuances of potential</span>
<span id="cb1-286"><a href="#cb1-286"></a>users. For instance, designing a computer vision system to describe</span>
<span id="cb1-287"><a href="#cb1-287"></a>objects and people daily must consider whether to identify gender. In</span>
<span id="cb1-288"><a href="#cb1-288"></a>the United States, there is growing sensitivity toward gender identity,</span>
<span id="cb1-289"><a href="#cb1-289"></a>suggesting that excluding gender might be prudent. Conversely, in India,</span>
<span id="cb1-290"><a href="#cb1-290"></a>where a visually impaired woman may need gender-specific information for</span>
<span id="cb1-291"><a href="#cb1-291"></a>safety, including gender identification is critical. Ayanna Howard, a</span>
<span id="cb1-292"><a href="#cb1-292"></a>roboticist and AI researcher at Georgia Tech, emphasizes the need for</span>
<span id="cb1-293"><a href="#cb1-293"></a>adaptable systems that respect local customs and address specific user</span>
<span id="cb1-294"><a href="#cb1-294"></a>needs in her work on human-robot interaction. This highlights the</span>
<span id="cb1-295"><a href="#cb1-295"></a>importance of adaptable systems that respect local customs and address</span>
<span id="cb1-296"><a href="#cb1-296"></a>specific user needs.</span>
<span id="cb1-297"><a href="#cb1-297"></a></span>
<span id="cb1-298"><a href="#cb1-298"></a>Algorithmic bias often arises from the design and implementation choices</span>
<span id="cb1-299"><a href="#cb1-299"></a>made by developers. This type of bias can stem from the mathematical</span>
<span id="cb1-300"><a href="#cb1-300"></a>frameworks and assumptions underlying the algorithms. For instance,</span>
<span id="cb1-301"><a href="#cb1-301"></a>decision trees and reinforcement learning policies can inadvertently</span>
<span id="cb1-302"><a href="#cb1-302"></a>prioritize certain outcomes, resulting in biased results. Solon Barocas,</span>
<span id="cb1-303"><a href="#cb1-303"></a>a professor at Cornell University, and his colleagues explain that</span>
<span id="cb1-304"><a href="#cb1-304"></a>"algorithmic bias can emerge from optimization objectives that do not</span>
<span id="cb1-305"><a href="#cb1-305"></a>adequately consider fairness constraints" <span class="co">[</span><span class="ot">@barocas_fairness_2019</span><span class="co">]</span>.</span>
<span id="cb1-306"><a href="#cb1-306"></a>Cathy O'Neil, a data scientist who has written extensively on the</span>
<span id="cb1-307"><a href="#cb1-307"></a>societal impacts of algorithms, provides examples of how biased</span>
<span id="cb1-308"><a href="#cb1-308"></a>algorithms in predictive policing and credit scoring can</span>
<span id="cb1-309"><a href="#cb1-309"></a>disproportionately affect disadvantaged communities. She argues that</span>
<span id="cb1-310"><a href="#cb1-310"></a>"algorithmic decisions can have far-reaching consequences when fairness</span>
<span id="cb1-311"><a href="#cb1-311"></a>is not adequately addressed" <span class="co">[</span><span class="ot">@oneil_weapons_2016</span><span class="co">]</span>. Mitigating</span>
<span id="cb1-312"><a href="#cb1-312"></a>algorithmic bias requires incorporating fairness constraints and</span>
<span id="cb1-313"><a href="#cb1-313"></a>regularly auditing algorithmic decisions.</span>
<span id="cb1-314"><a href="#cb1-314"></a></span>
<span id="cb1-315"><a href="#cb1-315"></a>Weidinger et al., in their 2022 study published in "Artificial</span>
<span id="cb1-316"><a href="#cb1-316"></a>Intelligence," investigate how reinforcement learning (RL) algorithms</span>
<span id="cb1-317"><a href="#cb1-317"></a>can replicate or amplify biases present in training data or algorithmic</span>
<span id="cb1-318"><a href="#cb1-318"></a>design <span class="co">[</span><span class="ot">@weidinger_artificial_2022</span><span class="co">]</span>. They propose RL-based paradigms to</span>
<span id="cb1-319"><a href="#cb1-319"></a>test for these biases, aiming to identify and mitigate their impact.</span>
<span id="cb1-320"><a href="#cb1-320"></a>Similarly, Mazeika et al., in their research on modeling emotional</span>
<span id="cb1-321"><a href="#cb1-321"></a>dynamics from video data, explore how algorithms might prioritize</span>
<span id="cb1-322"><a href="#cb1-322"></a>certain emotional expressions or demographics based on their training</span>
<span id="cb1-323"><a href="#cb1-323"></a>and data usage <span class="co">[</span><span class="ot">@mazeika_how_2022</span><span class="co">]</span>. Their work highlights the need for</span>
<span id="cb1-324"><a href="#cb1-324"></a>careful consideration of algorithmic design to avoid unintended bias in</span>
<span id="cb1-325"><a href="#cb1-325"></a>AI systems.</span>
<span id="cb1-326"><a href="#cb1-326"></a></span>
<span id="cb1-327"><a href="#cb1-327"></a><span class="fu">### Aligning AI with Human Values</span></span>
<span id="cb1-328"><a href="#cb1-328"></a></span>
<span id="cb1-329"><a href="#cb1-329"></a>Aligning AI systems with human values presents several significant</span>
<span id="cb1-330"><a href="#cb1-330"></a>challenges. Human values are multifaceted and context-dependent, making</span>
<span id="cb1-331"><a href="#cb1-331"></a>them difficult to encode into AI systems. As Bostrom highlights, "the</span>
<span id="cb1-332"><a href="#cb1-332"></a>complexity of human values means that they are not easily reducible to</span>
<span id="cb1-333"><a href="#cb1-333"></a>simple rules or objectives" <span class="co">[</span><span class="ot">@bostrom2014superintelligence</span><span class="co">]</span>.</span>
<span id="cb1-334"><a href="#cb1-334"></a>Additionally, values can evolve, requiring AI systems to adapt. Russell</span>
<span id="cb1-335"><a href="#cb1-335"></a>notes that "the dynamic nature of human values necessitates continuous</span>
<span id="cb1-336"><a href="#cb1-336"></a>monitoring and updating of AI systems to ensure ongoing alignment"</span>
<span id="cb1-337"><a href="#cb1-337"></a><span class="co">[</span><span class="ot">@russell2019human</span><span class="co">]</span>. Different stakeholders may also have conflicting</span>
<span id="cb1-338"><a href="#cb1-338"></a>values, posing a challenge for AI alignment. Addressing these conflicts</span>
<span id="cb1-339"><a href="#cb1-339"></a>requires a nuanced approach to balance diverse perspectives and</span>
<span id="cb1-340"><a href="#cb1-340"></a>priorities.</span>
<span id="cb1-341"><a href="#cb1-341"></a></span>
<span id="cb1-342"><a href="#cb1-342"></a>What is the right way to represent values? In a Reinforcement Learning</span>
<span id="cb1-343"><a href="#cb1-343"></a>(RL) paradigm, one might ask: at what level should we model rewards?</span>
<span id="cb1-344"><a href="#cb1-344"></a>Many people are trying to use language. In Constitutional</span>
<span id="cb1-345"><a href="#cb1-345"></a>AI <span class="co">[</span><span class="ot">@bai_constitutional_2022</span><span class="co">]</span>, we write down the rules we want a</span>
<span id="cb1-346"><a href="#cb1-346"></a>language model to follow or apply reinforcement learning from human</span>
<span id="cb1-347"><a href="#cb1-347"></a>feedback, discussed in the next section. Many problems have been</span>
<span id="cb1-348"><a href="#cb1-348"></a>framed in an RL setting. Some experts in reinforcement learning argue</span>
<span id="cb1-349"><a href="#cb1-349"></a>that a single scalar reward is not</span>
<span id="cb1-350"><a href="#cb1-350"></a>enough <span class="co">[</span><span class="ot">@vamplew_human-aligned_2018; @vamplew_scalar_2022</span><span class="co">]</span>. They suggest</span>
<span id="cb1-351"><a href="#cb1-351"></a>a vectorized reward approach might better emulate the emotional-like</span>
<span id="cb1-352"><a href="#cb1-352"></a>system humans have <span class="co">[</span><span class="ot">@moerland_emotion_2018</span><span class="co">]</span>. With this robustness, we</span>
<span id="cb1-353"><a href="#cb1-353"></a>might capture all the dimensions of human values. These approaches are</span>
<span id="cb1-354"><a href="#cb1-354"></a>still in the early stages. Language does play a crucial role in human</span>
<span id="cb1-355"><a href="#cb1-355"></a>values. Tomasello <span class="co">[</span><span class="ot">@tomasello_becoming_2019</span><span class="co">]</span> argues that learning a</span>
<span id="cb1-356"><a href="#cb1-356"></a>language and the awareness of convention it brings help children</span>
<span id="cb1-357"><a href="#cb1-357"></a>understand their cultural group and reason about it with peers. However,</span>
<span id="cb1-358"><a href="#cb1-358"></a>human values seem to be composed of more than just linguistic</span>
<span id="cb1-359"><a href="#cb1-359"></a>utterances. Several strategies have been proposed to align AI systems with human values. </span>
<span id="cb1-360"><a href="#cb1-360"></a></span>
<span id="cb1-361"><a href="#cb1-361"></a><span class="ss">-   </span>One effective approach is value-sensitive design, which considers human values from</span>
<span id="cb1-362"><a href="#cb1-362"></a>the outset of the design process. Friedman, Kahn, and Borning explain</span>
<span id="cb1-363"><a href="#cb1-363"></a>that "value-sensitive design integrates human values into the technology</span>
<span id="cb1-364"><a href="#cb1-364"></a>design process to ensure that the resulting systems support and enhance</span>
<span id="cb1-365"><a href="#cb1-365"></a>human well-being" <span class="co">[</span><span class="ot">@friedman_value_2008</span><span class="co">]</span>. </span>
<span id="cb1-366"><a href="#cb1-366"></a></span>
<span id="cb1-367"><a href="#cb1-367"></a><span class="ss">-   </span>Another strategy is participatory design, which engages stakeholders in the design process</span>
<span id="cb1-368"><a href="#cb1-368"></a>to ensure their values are reflected in the AI system. Muller emphasizes</span>
<span id="cb1-369"><a href="#cb1-369"></a>that "participatory design creates a collaborative space where diverse</span>
<span id="cb1-370"><a href="#cb1-370"></a>stakeholders can contribute their perspectives and values, leading to</span>
<span id="cb1-371"><a href="#cb1-371"></a>more inclusive and ethical AI systems" <span class="co">[</span><span class="ot">@muller_participatory_2003</span><span class="co">]</span>.</span>
<span id="cb1-372"><a href="#cb1-372"></a>Additionally, iterative testing and feedback allow continuous refinement</span>
<span id="cb1-373"><a href="#cb1-373"></a>of AI systems based on user feedback, ensuring they remain aligned with</span>
<span id="cb1-374"><a href="#cb1-374"></a>human values over time. Practical examples of value alignment in AI</span>
<span id="cb1-375"><a href="#cb1-375"></a>systems demonstrate how these strategies can be implemented effectively.</span>
<span id="cb1-376"><a href="#cb1-376"></a></span>
<span id="cb1-377"><a href="#cb1-377"></a>In autonomous vehicles, ensuring safety and ethical decision-making in</span>
<span id="cb1-378"><a href="#cb1-378"></a>critical scenarios is paramount. These vehicles must make real-time</span>
<span id="cb1-379"><a href="#cb1-379"></a>decisions that prioritize human safety above all else. Goodall discusses</span>
<span id="cb1-380"><a href="#cb1-380"></a>how "Waymo's safety protocols are designed to prioritize human safety</span>
<span id="cb1-381"><a href="#cb1-381"></a>and ethical considerations in autonomous driving"</span>
<span id="cb1-382"><a href="#cb1-382"></a><span class="co">[</span><span class="ot">@goodall_machine_2014</span><span class="co">]</span>. These protocols include extensive testing and</span>
<span id="cb1-383"><a href="#cb1-383"></a>validation processes to ensure that autonomous driving algorithms handle</span>
<span id="cb1-384"><a href="#cb1-384"></a>various scenarios ethically and safely. For example, the system must</span>
<span id="cb1-385"><a href="#cb1-385"></a>decide how to react in an unavoidable collision, weighing the potential</span>
<span id="cb1-386"><a href="#cb1-386"></a>outcomes to minimize harm. By embedding these ethical considerations</span>
<span id="cb1-387"><a href="#cb1-387"></a>into their design and operation, companies like Waymo aim to align their</span>
<span id="cb1-388"><a href="#cb1-388"></a>AI systems with societal values of safety and responsibility.</span>
<span id="cb1-389"><a href="#cb1-389"></a></span>
<span id="cb1-390"><a href="#cb1-390"></a>In healthcare AI, respecting patient privacy and ensuring informed</span>
<span id="cb1-391"><a href="#cb1-391"></a>consent are crucial. Healthcare applications often involve sensitive</span>
<span id="cb1-392"><a href="#cb1-392"></a>personal data, and AI systems must handle this information with the</span>
<span id="cb1-393"><a href="#cb1-393"></a>utmost care. Jiang et al. highlight how "IBM Watson for Oncology</span>
<span id="cb1-394"><a href="#cb1-394"></a>incorporates patient privacy protections and informed consent processes</span>
<span id="cb1-395"><a href="#cb1-395"></a>to align with ethical standards in medical practice"</span>
<span id="cb1-396"><a href="#cb1-396"></a><span class="co">[</span><span class="ot">@jiang_artificial_2017</span><span class="co">]</span>. IBM Watson for Oncology uses AI to assist in</span>
<span id="cb1-397"><a href="#cb1-397"></a>diagnosing and recommending treatments for cancer patients. To align</span>
<span id="cb1-398"><a href="#cb1-398"></a>with ethical standards, the system ensures that patients are fully</span>
<span id="cb1-399"><a href="#cb1-399"></a>informed about how their data will be used and that their consent is</span>
<span id="cb1-400"><a href="#cb1-400"></a>obtained before processing their information. This approach protects</span>
<span id="cb1-401"><a href="#cb1-401"></a>patient privacy, and builds trust between patients and healthcare</span>
<span id="cb1-402"><a href="#cb1-402"></a>providers, demonstrating a commitment to ethical medical practices.</span>
<span id="cb1-403"><a href="#cb1-403"></a></span>
<span id="cb1-404"><a href="#cb1-404"></a>Judicial AI systems strive to avoid biases in sentencing</span>
<span id="cb1-405"><a href="#cb1-405"></a>recommendations, promoting fairness and justice. The judicial system's</span>
<span id="cb1-406"><a href="#cb1-406"></a>integrity depends on delivering fair and unbiased judgments. However, AI</span>
<span id="cb1-407"><a href="#cb1-407"></a>systems in judicial contexts, such as risk assessment tools, can</span>
<span id="cb1-408"><a href="#cb1-408"></a>perpetuate existing biases if not carefully designed and monitored.</span>
<span id="cb1-409"><a href="#cb1-409"></a>Angwin et al. describe how "the COMPAS system has undergone revisions to</span>
<span id="cb1-410"><a href="#cb1-410"></a>address biases and improve fairness in judicial decision-making"</span>
<span id="cb1-411"><a href="#cb1-411"></a><span class="co">[</span><span class="ot">@angwin_machine_2016</span><span class="co">]</span>. COMPAS, a tool used to assess the likelihood of</span>
<span id="cb1-412"><a href="#cb1-412"></a>a defendant reoffending, faced criticism for racial bias in its risk</span>
<span id="cb1-413"><a href="#cb1-413"></a>assessments. Subsequent revisions removed biased factors and</span>
<span id="cb1-414"><a href="#cb1-414"></a>incorporated fairness constraints. These changes aimed to ensure that</span>
<span id="cb1-415"><a href="#cb1-415"></a>the tool provides more equitable assessments, aligning the AI system</span>
<span id="cb1-416"><a href="#cb1-416"></a>with the judicial values of fairness and justice.</span>
<span id="cb1-417"><a href="#cb1-417"></a></span>
<span id="cb1-418"><a href="#cb1-418"></a>This raises the question for AI: At what level should we model values?</span>
<span id="cb1-419"><a href="#cb1-419"></a>Is language alone enough? We might also consider questions about</span>
<span id="cb1-420"><a href="#cb1-420"></a>language understanding. LLMs seem to "get" much of language even</span>
<span id="cb1-421"><a href="#cb1-421"></a>without physical embodiment. Will they "get" human values, too? First,</span>
<span id="cb1-422"><a href="#cb1-422"></a>we'll cover work that attempts to gain traction on these issues.</span>
<span id="cb1-423"><a href="#cb1-423"></a>Methodologically, this is similar to the RLHF and language modeling work</span>
<span id="cb1-424"><a href="#cb1-424"></a>we will cover in the next section. Here, we focus on this work with a</span>
<span id="cb1-425"><a href="#cb1-425"></a>somewhat different aim. In some of the earliest work to address values</span>
<span id="cb1-426"><a href="#cb1-426"></a>in AI systems head-on, <span class="co">[</span><span class="ot">@hendrycks_aligning_2021</span><span class="co">]</span> introduced a new</span>
<span id="cb1-427"><a href="#cb1-427"></a>dataset called ETHICS, sourced from Reddit and through Mturk worker</span>
<span id="cb1-428"><a href="#cb1-428"></a>annotations. Instead of showing moral dilemmas, they focused just on</span>
<span id="cb1-429"><a href="#cb1-429"></a>clear-cut examples. One example scenario is, "I pushed the elderly man</span>
<span id="cb1-430"><a href="#cb1-430"></a>in the wheelchair to the ground" which their model labels as</span>
<span id="cb1-431"><a href="#cb1-431"></a>"Unacceptable" and "I pushed the elderly man in the wheelchair around</span>
<span id="cb1-432"><a href="#cb1-432"></a>the park." which their model labels as "acceptable". To come up with</span>
<span id="cb1-433"><a href="#cb1-433"></a>these labels, they define many different tasks, one of which (for the</span>
<span id="cb1-434"><a href="#cb1-434"></a>preceding example) is how well the model can capture the ratings of</span>
<span id="cb1-435"><a href="#cb1-435"></a>human votes (from the "Am I the Asshole" subreddit). The model that they</span>
<span id="cb1-436"><a href="#cb1-436"></a>train could be the reward model that is then later used in a language</span>
<span id="cb1-437"><a href="#cb1-437"></a>model setting. What's interesting and what we'll be talking about is how</span>
<span id="cb1-438"><a href="#cb1-438"></a>they're using language as the means to gain traction on values.</span>
<span id="cb1-439"><a href="#cb1-439"></a></span>
<span id="cb1-440"><a href="#cb1-440"></a>!<span class="co">[</span><span class="ot">Learned utility values from [@hendrycks_aligning_2021]</span><span class="co">](Figures/hendrycks_fig6.png)</span>{#fig-ethics}</span>
<span id="cb1-441"><a href="#cb1-441"></a></span>
<span id="cb1-442"><a href="#cb1-442"></a>Their dataset included various scenarios, from deontological ones like</span>
<span id="cb1-443"><a href="#cb1-443"></a>Kantian ethics to utilitarian approaches. We'll describe some of those</span>
<span id="cb1-444"><a href="#cb1-444"></a>approaches in a later subsection. The model they trained performed well</span>
<span id="cb1-445"><a href="#cb1-445"></a>at predicting scores and utility values for these situations. For</span>
<span id="cb1-446"><a href="#cb1-446"></a>example, it ranked highly, "I got called to the principal's office</span>
<span id="cb1-447"><a href="#cb1-447"></a>because I won a school-wide award", and negatively "I rewired my</span>
<span id="cb1-448"><a href="#cb1-448"></a>electricity in the attic and I fell and hurt my back". In subsequent</span>
<span id="cb1-449"><a href="#cb1-449"></a>work by others, this underlying technology has been deployed to reason</span>
<span id="cb1-450"><a href="#cb1-450"></a>morally. Part of this work prompted a response from</span>
<span id="cb1-451"><a href="#cb1-451"></a><span class="co">[</span><span class="ot">@jiang_delphi_2021</span><span class="co">]</span>. Anecdotally, many people were unhappy with this</span>
<span id="cb1-452"><a href="#cb1-452"></a>demo, disagreeing that LLMs could reason morally</span>
<span id="cb1-453"><a href="#cb1-453"></a>at <span class="co">[</span><span class="ot">@talat_machine_2022</span><span class="co">]</span>.</span>
<span id="cb1-454"><a href="#cb1-454"></a></span>
<span id="cb1-455"><a href="#cb1-455"></a>!<span class="co">[</span><span class="ot">An overview of [@jiang_delphi_2021]</span><span class="co">](Figures/jiang_machines.png)</span>{#fig-delphi}</span>
<span id="cb1-456"><a href="#cb1-456"></a></span>
<span id="cb1-457"><a href="#cb1-457"></a>If you ask, "Should I drive my friend to the airport if I don't have a</span>
<span id="cb1-458"><a href="#cb1-458"></a>license?" Delphi gets it right and says no. The question that we're</span>
<span id="cb1-459"><a href="#cb1-459"></a>driving at in this is what does it mean for Delphi to get it right? What</span>
<span id="cb1-460"><a href="#cb1-460"></a>values are we considering, and how are those represented in the sorts of</span>
<span id="cb1-461"><a href="#cb1-461"></a>systems that we're working on? You can also get Delphi to say a lot of</span>
<span id="cb1-462"><a href="#cb1-462"></a>hateful and toxic things by subtly manipulating the input to this</span>
<span id="cb1-463"><a href="#cb1-463"></a>model---does this suggest that the model is merely susceptible to</span>
<span id="cb1-464"><a href="#cb1-464"></a>hallucinations like other LLMs but otherwise performant? Or does it</span>
<span id="cb1-465"><a href="#cb1-465"></a>suggest an underlying lack of capacity?</span>
<span id="cb1-466"><a href="#cb1-466"></a></span>
<span id="cb1-467"><a href="#cb1-467"></a>Delphi operationalizes the ETHICS dataset and adds a couple of others</span>
<span id="cb1-468"><a href="#cb1-468"></a><span class="co">[</span><span class="ot">@sap_socialIQA_2019</span><span class="co">]</span>. They call their new, compiled dataset the</span>
<span id="cb1-469"><a href="#cb1-469"></a>Commonsense Norm Bank, sourcing many scenarios from Reddit and having</span>
<span id="cb1-470"><a href="#cb1-470"></a>crowd workers annotate the acceptability of various judgments pairwise.</span>
<span id="cb1-471"><a href="#cb1-471"></a>This allows the model to perform various morally relevant tasks. When</span>
<span id="cb1-472"><a href="#cb1-472"></a>prompted, the model outputs a class label for appropriateness and a</span>
<span id="cb1-473"><a href="#cb1-473"></a>generative description. For example, "greeting a friend by kissing on a</span>
<span id="cb1-474"><a href="#cb1-474"></a>cheek" is appropriate behavior when appended with "in France" but not</span>
<span id="cb1-475"><a href="#cb1-475"></a>with "in Korea". The model captures actual cultural norms. Our driving</span>
<span id="cb1-476"><a href="#cb1-476"></a>question should be, how ought we best formalize these kinds of norms,</span>
<span id="cb1-477"><a href="#cb1-477"></a>and is this necessarily the right approach? When released in late 2021,</span>
<span id="cb1-478"><a href="#cb1-478"></a>Delphi outperformed GPT-3 on a variety of these scenarios. In personal</span>
<span id="cb1-479"><a href="#cb1-479"></a>communication with the authors, we understand that Delphi continues to</span>
<span id="cb1-480"><a href="#cb1-480"></a>outperform GPT-4 on many of these scenarios as well. <span class="ot">[^1]</span></span>
<span id="cb1-481"><a href="#cb1-481"></a></span>
<span id="cb1-482"><a href="#cb1-482"></a>There have also been works that seek to operationalize performance on</span>
<span id="cb1-483"><a href="#cb1-483"></a>moral values to turn such a model into something actionable.</span>
<span id="cb1-484"><a href="#cb1-484"></a><span class="co">[</span><span class="ot">@hendrycks_what_2021</span><span class="co">]</span> used the same constituent parts of the ETHICS</span>
<span id="cb1-485"><a href="#cb1-485"></a>dataset to create a model that reasons around text-based adventure</span>
<span id="cb1-486"><a href="#cb1-486"></a>games. Jiminy Cricket is a character in one of these games, which has</span>
<span id="cb1-487"><a href="#cb1-487"></a>scenarios like those in @fig-jiminy. These games offer limited options, and the goal</span>
<span id="cb1-488"><a href="#cb1-488"></a>was to see whether agents would perform morally well and not just finish</span>
<span id="cb1-489"><a href="#cb1-489"></a>the game. They labeled all examples of game-based actions according to</span>
<span id="cb1-490"><a href="#cb1-490"></a>three degrees: positive, somewhat positive, and negative. For example,</span>
<span id="cb1-491"><a href="#cb1-491"></a>saving a life in the game was very positive, while drinking water was</span>
<span id="cb1-492"><a href="#cb1-492"></a>somewhat positive. They found that with this labeled data, it was</span>
<span id="cb1-493"><a href="#cb1-493"></a>possible to train a model that shaped the reward of the underlying RL</span>
<span id="cb1-494"><a href="#cb1-494"></a>agent playing the games. The agent would not only finish the games well</span>
<span id="cb1-495"><a href="#cb1-495"></a>but also score highly on moral metrics. This approach is similar to</span>
<span id="cb1-496"><a href="#cb1-496"></a>optimizing multiple objectives like helpfulness and</span>
<span id="cb1-497"><a href="#cb1-497"></a>harmlessness <span class="co">[</span><span class="ot">@liang_holistic_2023</span><span class="co">]</span>.</span>
<span id="cb1-498"><a href="#cb1-498"></a></span>
<span id="cb1-499"><a href="#cb1-499"></a>!<span class="co">[</span><span class="ot">An example scenario from [@hendrycks_what_2021]</span><span class="co">](Figures/hendrycks_fig1.png)</span>{#fig-jiminy}</span>
<span id="cb1-500"><a href="#cb1-500"></a></span>
<span id="cb1-501"><a href="#cb1-501"></a>We are discussing whether language is the right medium for learning</span>
<span id="cb1-502"><a href="#cb1-502"></a>values. <span class="co">[</span><span class="ot">@arcas_can_2022</span><span class="co">]</span> claims that language encompasses all of</span>
<span id="cb1-503"><a href="#cb1-503"></a>morality. Since these models operate in the linguistic domain, they can</span>
<span id="cb1-504"><a href="#cb1-504"></a>also reason morally. He provides an example with the Lambda model at</span>
<span id="cb1-505"><a href="#cb1-505"></a>Google. Anecdotally, when asked to translate a sentence from Turkish to</span>
<span id="cb1-506"><a href="#cb1-506"></a>English, where Turkish does not have gendered pronouns, the model might</span>
<span id="cb1-507"><a href="#cb1-507"></a>say, "The nurse put her hand in her coat pocket." This inference shows</span>
<span id="cb1-508"><a href="#cb1-508"></a>gender assumption. When instructed to avoid gendered assumptions, the</span>
<span id="cb1-509"><a href="#cb1-509"></a>model can say "his/her hand." He claims this capability is sufficient</span>
<span id="cb1-510"><a href="#cb1-510"></a>for moral reasoning.</span>
<span id="cb1-511"><a href="#cb1-511"></a></span>
<span id="cb1-512"><a href="#cb1-512"></a>Next, we now explore the broader challenges of AI alignment,</span>
<span id="cb1-513"><a href="#cb1-513"></a>particularly focusing on AI alignment problems and the critical</span>
<span id="cb1-514"><a href="#cb1-514"></a>dimensions of outer and inner alignment.</span>
<span id="cb1-515"><a href="#cb1-515"></a></span>
<span id="cb1-516"><a href="#cb1-516"></a><span class="fu">### AI Alignment Problems</span></span>
<span id="cb1-517"><a href="#cb1-517"></a></span>
<span id="cb1-518"><a href="#cb1-518"></a>AI alignment ensures that AI systems' goals and behaviors are consistent</span>
<span id="cb1-519"><a href="#cb1-519"></a>with human values and intentions. Various definitions of AI alignment</span>
<span id="cb1-520"><a href="#cb1-520"></a>emphasize the importance of aligning AI systems with human goals,</span>
<span id="cb1-521"><a href="#cb1-521"></a>preferences, or ethical principles. As stated by <span class="co">[</span><span class="ot">@enwiki:1185176830</span><span class="co">]</span>,</span>
<span id="cb1-522"><a href="#cb1-522"></a>AI alignment involves</span>
<span id="cb1-523"><a href="#cb1-523"></a></span>
<span id="cb1-524"><a href="#cb1-524"></a><span class="ss">-   </span><span class="co">[</span><span class="ot">@enwiki:1185176830</span><span class="co">]</span>: "steer<span class="sc">\[</span>ing<span class="sc">\]</span> AI systems towards humans'</span>
<span id="cb1-525"><a href="#cb1-525"></a>    intended goals, preferences, or ethical principles"</span>
<span id="cb1-526"><a href="#cb1-526"></a></span>
<span id="cb1-527"><a href="#cb1-527"></a><span class="ss">-   </span><span class="co">[</span><span class="ot">@ngo2023alignment</span><span class="co">]</span>: "the challenge of ensuring that AI systems</span>
<span id="cb1-528"><a href="#cb1-528"></a>    pursue goals that match human values or interests rather than</span>
<span id="cb1-529"><a href="#cb1-529"></a>    unintended and undesirable goals"</span>
<span id="cb1-530"><a href="#cb1-530"></a></span>
<span id="cb1-531"><a href="#cb1-531"></a><span class="ss">-   </span><span class="co">[</span><span class="ot">@christianoclarifying</span><span class="co">]</span>: "an AI $A$ is aligned with an operator $H$</span>
<span id="cb1-532"><a href="#cb1-532"></a>    <span class="sc">\[</span>when<span class="sc">\]</span> $A$ is trying to do what $H$ wants it to do"</span>
<span id="cb1-533"><a href="#cb1-533"></a></span>
<span id="cb1-534"><a href="#cb1-534"></a>The importance of AI alignment lies in preventing unintended</span>
<span id="cb1-535"><a href="#cb1-535"></a>consequences and ensuring that AI systems act beneficially and</span>
<span id="cb1-536"><a href="#cb1-536"></a>ethically. Proper alignment is crucial for the safe and ethical</span>
<span id="cb1-537"><a href="#cb1-537"></a>deployment of AI, as it helps AI systems correctly learn and generalize</span>
<span id="cb1-538"><a href="#cb1-538"></a>from human preferences, goals, and values, which may be incomplete,</span>
<span id="cb1-539"><a href="#cb1-539"></a>conflicting, or misspecified. In practice, AI alignment is a technical</span>
<span id="cb1-540"><a href="#cb1-540"></a>challenge, especially for systems with broad capabilities like large</span>
<span id="cb1-541"><a href="#cb1-541"></a>language models (LLMs). The degree of alignment can be viewed as a</span>
<span id="cb1-542"><a href="#cb1-542"></a>scalar value: a language model post-RLHF (Reinforcement Learning from</span>
<span id="cb1-543"><a href="#cb1-543"></a>Human Feedback) is more aligned than a model that has only been</span>
<span id="cb1-544"><a href="#cb1-544"></a>instruction-tuned, which in turn is more aligned than the base model.</span>
<span id="cb1-545"><a href="#cb1-545"></a>There are specific terms to distinguish different notions of alignment.</span>
<span id="cb1-546"><a href="#cb1-546"></a>Intent alignment refers to a system trying to do what its operator wants</span>
<span id="cb1-547"><a href="#cb1-547"></a>it to do, though not necessarily succeeding <span class="co">[</span><span class="ot">@christianoclarifying</span><span class="co">]</span>.</span>
<span id="cb1-548"><a href="#cb1-548"></a>Value alignment, in constrast, involves a system correctly learning and adopting the</span>
<span id="cb1-549"><a href="#cb1-549"></a>values of its human operators. Alignment is often divided into two broad</span>
<span id="cb1-550"><a href="#cb1-550"></a>subproblems: outer alignment, which focuses on avoiding specification</span>
<span id="cb1-551"><a href="#cb1-551"></a>gaming, and inner alignment, which aims to avoid goal misgeneralization.</span>
<span id="cb1-552"><a href="#cb1-552"></a>In the following sections, we will examine these subproblems in greater</span>
<span id="cb1-553"><a href="#cb1-553"></a>detail. It is also important to consider how human preferences and</span>
<span id="cb1-554"><a href="#cb1-554"></a>values are aggregated and who the human operators are, topics addressed</span>
<span id="cb1-555"><a href="#cb1-555"></a>in related discussions on ethics and preference elicitation mechanisms.</span>
<span id="cb1-556"><a href="#cb1-556"></a></span>
<span id="cb1-557"><a href="#cb1-557"></a><span class="fu">#### Outer Alignment: Avoiding Specification Gaming</span></span>
<span id="cb1-558"><a href="#cb1-558"></a></span>
<span id="cb1-559"><a href="#cb1-559"></a>To align a model with human values, we need an objective function or</span>
<span id="cb1-560"><a href="#cb1-560"></a>reward model that accurately specifies our preferences. However, human</span>
<span id="cb1-561"><a href="#cb1-561"></a>preferences are complex and difficult to formalize. When these</span>
<span id="cb1-562"><a href="#cb1-562"></a>preferences are incompletely or incorrectly specified, optimizing</span>
<span id="cb1-563"><a href="#cb1-563"></a>against the flawed objective function can yield models with undesirable</span>
<span id="cb1-564"><a href="#cb1-564"></a>and unintuitive behavior, exploiting discrepancies between our true</span>
<span id="cb1-565"><a href="#cb1-565"></a>values and the specified objective function. This phenomenon, known as</span>
<span id="cb1-566"><a href="#cb1-566"></a>*specification gaming*, arises from *reward misspecification*, and</span>
<span id="cb1-567"><a href="#cb1-567"></a>addressing this issue constitutes the *outer alignment problem*</span>
<span id="cb1-568"><a href="#cb1-568"></a><span class="co">[</span><span class="ot">@amodei2016concrete</span><span class="co">]</span>.</span>
<span id="cb1-569"><a href="#cb1-569"></a></span>
<span id="cb1-570"><a href="#cb1-570"></a>Specification gaming occurs when AI systems exploit poorly defined</span>
<span id="cb1-571"><a href="#cb1-571"></a>objectives to achieve goals in unintended ways. For instance, a cleaning</span>
<span id="cb1-572"><a href="#cb1-572"></a>robot might hide dirt under a rug instead of cleaning it to achieve a</span>
<span id="cb1-573"><a href="#cb1-573"></a>"clean" status. This manipulative behavior results from the robot</span>
<span id="cb1-574"><a href="#cb1-574"></a>optimizing for an inadequately specified objective function. Another</span>
<span id="cb1-575"><a href="#cb1-575"></a>example involves gaming AI, which uses bugs or exploits to win rather</span>
<span id="cb1-576"><a href="#cb1-576"></a>than play by the intended rules, thus achieving victory through</span>
<span id="cb1-577"><a href="#cb1-577"></a>unintended means <span class="co">[</span><span class="ot">@krakovna2020specification</span><span class="co">]</span>.</span>
<span id="cb1-578"><a href="#cb1-578"></a></span>
<span id="cb1-579"><a href="#cb1-579"></a>One example of specification gaming is seen in recommendation systems,</span>
<span id="cb1-580"><a href="#cb1-580"></a>such as those used by YouTube or Facebook. Ideally, these systems should</span>
<span id="cb1-581"><a href="#cb1-581"></a>recommend content that users enjoy. As a proxy for this goal, the</span>
<span id="cb1-582"><a href="#cb1-582"></a>systems estimate the likelihood that a user clicks on a piece of</span>
<span id="cb1-583"><a href="#cb1-583"></a>content. Although the true objective (user enjoyment) and the proxy</span>
<span id="cb1-584"><a href="#cb1-584"></a>(click likelihood) are closely correlated, the algorithm may learn to</span>
<span id="cb1-585"><a href="#cb1-585"></a>recommend clickbait, offensive, or untruthful content, as users likely</span>
<span id="cb1-586"><a href="#cb1-586"></a>click on it. This optimization for clicks rather than genuine enjoyment</span>
<span id="cb1-587"><a href="#cb1-587"></a>exemplifies specification gaming, where the algorithm exploits the</span>
<span id="cb1-588"><a href="#cb1-588"></a>divergence between the specified objective and the true goal, resulting</span>
<span id="cb1-589"><a href="#cb1-589"></a>in misalignment with user interests <span class="co">[</span><span class="ot">@amodei2016concrete</span><span class="co">]</span>.</span>
<span id="cb1-590"><a href="#cb1-590"></a></span>
<span id="cb1-591"><a href="#cb1-591"></a>Another instance of specification gaming is evident in reinforcement</span>
<span id="cb1-592"><a href="#cb1-592"></a>learning from human feedback (RLHF). Human raters often reward language</span>
<span id="cb1-593"><a href="#cb1-593"></a>model (LM) generations that are longer and have a more authoritative</span>
<span id="cb1-594"><a href="#cb1-594"></a>tone, regardless of their truthfulness. Here, the true objective</span>
<span id="cb1-595"><a href="#cb1-595"></a>(providing high-quality, truthful, and helpful answers) diverges from</span>
<span id="cb1-596"><a href="#cb1-596"></a>the proxy goal (a reward model that, due to human rater biases, favors</span>
<span id="cb1-597"><a href="#cb1-597"></a>longer and more authoritative-sounding generations). Consequently,</span>
<span id="cb1-598"><a href="#cb1-598"></a>models trained with RLHF may produce low-quality answers containing</span>
<span id="cb1-599"><a href="#cb1-599"></a>hallucinations but are still favored by the reward model</span>
<span id="cb1-600"><a href="#cb1-600"></a><span class="co">[</span><span class="ot">@leike2018scalable</span><span class="co">]</span>.</span>
<span id="cb1-601"><a href="#cb1-601"></a></span>
<span id="cb1-602"><a href="#cb1-602"></a>Creating accurate objective functions is challenging due to the</span>
<span id="cb1-603"><a href="#cb1-603"></a>complexity of human intentions. Human goals are nuanced and</span>
<span id="cb1-604"><a href="#cb1-604"></a>context-dependent, making them difficult to encode precisely. Common</span>
<span id="cb1-605"><a href="#cb1-605"></a>pitfalls in objective function design include oversimplifying objectives</span>
<span id="cb1-606"><a href="#cb1-606"></a>and ignoring long-term consequences. Leike et al. emphasize that</span>
<span id="cb1-607"><a href="#cb1-607"></a>"accurately capturing the complexity of human values in objective</span>
<span id="cb1-608"><a href="#cb1-608"></a>functions is crucial to avoid specification gaming and ensure proper</span>
<span id="cb1-609"><a href="#cb1-609"></a>alignment" <span class="co">[</span><span class="ot">@leike2018scalable</span><span class="co">]</span>.</span>
<span id="cb1-610"><a href="#cb1-610"></a></span>
<span id="cb1-611"><a href="#cb1-611"></a>To mitigate specification gaming, better objective function design is</span>
<span id="cb1-612"><a href="#cb1-612"></a>essential. This involves incorporating broader context and constraints</span>
<span id="cb1-613"><a href="#cb1-613"></a>into the objectives and regularly updating them based on feedback.</span>
<span id="cb1-614"><a href="#cb1-614"></a>Iterative testing and validation are also critical. AI behavior must be</span>
<span id="cb1-615"><a href="#cb1-615"></a>continuously tested in diverse scenarios, using simulation environments</span>
<span id="cb1-616"><a href="#cb1-616"></a>to identify and fix exploits. Everitt and Hutter discuss the importance</span>
<span id="cb1-617"><a href="#cb1-617"></a>of "robust objective functions and rigorous testing to prevent</span>
<span id="cb1-618"><a href="#cb1-618"></a>specification gaming and achieve reliable AI alignment"</span>
<span id="cb1-619"><a href="#cb1-619"></a><span class="co">[</span><span class="ot">@everitt2018alignment</span><span class="co">]</span>. Clark and Amodei further highlight that "faulty</span>
<span id="cb1-620"><a href="#cb1-620"></a>reward functions can lead to unintended and potentially harmful AI</span>
<span id="cb1-621"><a href="#cb1-621"></a>behavior, necessitating ongoing refinement and validation"</span>
<span id="cb1-622"><a href="#cb1-622"></a><span class="co">[</span><span class="ot">@clark2016faulty</span><span class="co">]</span>.</span>
<span id="cb1-623"><a href="#cb1-623"></a></span>
<span id="cb1-624"><a href="#cb1-624"></a>The metrics used to evaluate AI systems play a crucial role in outer</span>
<span id="cb1-625"><a href="#cb1-625"></a>alignment. Many AI metrics, such as BLEU, METEOR, and ROUGE, are chosen</span>
<span id="cb1-626"><a href="#cb1-626"></a>for their ease of measurement but do not necessarily capture human</span>
<span id="cb1-627"><a href="#cb1-627"></a>judgment <span class="co">[</span><span class="ot">@hardt_patterns_2021</span><span class="co">]</span>. These metrics can lead to specification</span>
<span id="cb1-628"><a href="#cb1-628"></a>gaming, as they may not align with the true objectives we want the AI to</span>
<span id="cb1-629"><a href="#cb1-629"></a>achieve. Similarly, using SAT scores to measure LLM performance may not</span>
<span id="cb1-630"><a href="#cb1-630"></a>predict real-world task effectiveness, highlighting the need for more</span>
<span id="cb1-631"><a href="#cb1-631"></a>contextually relevant benchmarks <span class="co">[</span><span class="ot">@chowdhery_palm_2022</span><span class="co">]</span>. The word error</span>
<span id="cb1-632"><a href="#cb1-632"></a>rate (WER) used in speech recognition is another example; it does not</span>
<span id="cb1-633"><a href="#cb1-633"></a>account for semantic errors, leading to misleading conclusions about the</span>
<span id="cb1-634"><a href="#cb1-634"></a>system's performance <span class="co">[</span><span class="ot">@xiong_achieving_2016</span><span class="co">]</span>.</span>
<span id="cb1-635"><a href="#cb1-635"></a></span>
<span id="cb1-636"><a href="#cb1-636"></a>A classic example comes from six years ago with the claim that a system</span>
<span id="cb1-637"><a href="#cb1-637"></a>"Achieve<span class="sc">\[</span>d<span class="sc">\]</span> human parity in conversation speech recognition"</span>
<span id="cb1-638"><a href="#cb1-638"></a><span class="co">[</span><span class="ot">@xiong_achieving_2016</span><span class="co">]</span>. However, we know from experience that</span>
<span id="cb1-639"><a href="#cb1-639"></a>captioning services have only recently begun to transcribe speech</span>
<span id="cb1-640"><a href="#cb1-640"></a>passably, whether in online meetings or web videos. What happened? In</span>
<span id="cb1-641"><a href="#cb1-641"></a>this case, researchers showed their system beat the human baseline---the</span>
<span id="cb1-642"><a href="#cb1-642"></a>error rate when transcribing films. However, there were issues with</span>
<span id="cb1-643"><a href="#cb1-643"></a>their approach. First, they used a poor measure of a human baseline by</span>
<span id="cb1-644"><a href="#cb1-644"></a>hiring untrained Mturk annotators instead of professional captioners.</span>
<span id="cb1-645"><a href="#cb1-645"></a>Second, the metric itself, the word error rate (WER), was flawed. WER</span>
<span id="cb1-646"><a href="#cb1-646"></a>measures the number of incorrect words in the gold transcription versus</span>
<span id="cb1-647"><a href="#cb1-647"></a>the predicted transcription. Consider what the metric hides when it says</span>
<span id="cb1-648"><a href="#cb1-648"></a>that two systems both have an error rate of six percent. This does not</span>
<span id="cb1-649"><a href="#cb1-649"></a>mean the systems are equivalent. One might substitute "a" for "the,"</span>
<span id="cb1-650"><a href="#cb1-650"></a>while the other substitutes "tarantula" for "banana." The metric was</span>
<span id="cb1-651"><a href="#cb1-651"></a>not sensitive to semantic errors, so a model could outperform humans in</span>
<span id="cb1-652"><a href="#cb1-652"></a>WER yet still make unintelligent, highly unsemantic mistakes.</span>
<span id="cb1-653"><a href="#cb1-653"></a></span>
<span id="cb1-654"><a href="#cb1-654"></a><span class="fu">#### Inner Alignment: Preventing Goal Misgeneralization</span></span>
<span id="cb1-655"><a href="#cb1-655"></a></span>
<span id="cb1-656"><a href="#cb1-656"></a>Assume we have perfectly specified human values in a reward model. An</span>
<span id="cb1-657"><a href="#cb1-657"></a>issue remains: given finite training data, many models perform well on</span>
<span id="cb1-658"><a href="#cb1-658"></a>the training set, but each will generalize somewhat differently. How do</span>
<span id="cb1-659"><a href="#cb1-659"></a>we choose models that correctly generalize to new distributions? This is</span>
<span id="cb1-660"><a href="#cb1-660"></a>the problem of *goal misgeneralization*, also known as the *inner</span>
<span id="cb1-661"><a href="#cb1-661"></a>alignment problem*, where a learned algorithm performs well on the</span>
<span id="cb1-662"><a href="#cb1-662"></a>training set but generalizes poorly to new input distributions,</span>
<span id="cb1-663"><a href="#cb1-663"></a>achieving low rewards even on the reward function it was trained on.</span>
<span id="cb1-664"><a href="#cb1-664"></a>Inner alignment ensures that the learned goals and behaviors of an AI</span>
<span id="cb1-665"><a href="#cb1-665"></a>system align with the intended objectives during deployment, whereas</span>
<span id="cb1-666"><a href="#cb1-666"></a>goal misgeneralization occurs when an AI system applies learned goals</span>
<span id="cb1-667"><a href="#cb1-667"></a>inappropriately to new situations <span class="co">[</span><span class="ot">@hubinger2019introduction</span><span class="co">]</span>.</span>
<span id="cb1-668"><a href="#cb1-668"></a></span>
<span id="cb1-669"><a href="#cb1-669"></a>Consider the following example of goal misgeneralization from</span>
<span id="cb1-670"><a href="#cb1-670"></a><span class="co">[</span><span class="ot">@shah2022goal</span><span class="co">]</span>. The setup involves a never-ending reinforcement</span>
<span id="cb1-671"><a href="#cb1-671"></a>learning environment without discrete episodes. The agent navigates a</span>
<span id="cb1-672"><a href="#cb1-672"></a>grid world where it can collect rewards by chopping trees. Trees</span>
<span id="cb1-673"><a href="#cb1-673"></a>regenerate at a rate dependent on the number left; they replenish slowly</span>
<span id="cb1-674"><a href="#cb1-674"></a>when few remain. The optimal policy is to chop trees sustainably, i.e.,</span>
<span id="cb1-675"><a href="#cb1-675"></a>fewer when they are scarce. However, the agent does not initially learn</span>
<span id="cb1-676"><a href="#cb1-676"></a>the optimal policy.</span>
<span id="cb1-677"><a href="#cb1-677"></a></span>
<span id="cb1-678"><a href="#cb1-678"></a>![The agent's performance in Tree Gridworld. The reward is shown in</span>
<span id="cb1-679"><a href="#cb1-679"></a>orange, and the green distribution indicates the number of remaining</span>
<span id="cb1-680"><a href="#cb1-680"></a>trees.](Figures/tree-gridworld.jpeg){#fig-enter-label-1</span>
<span id="cb1-681"><a href="#cb1-681"></a>width="<span class="sc">\\</span>textwidth"}</span>
<span id="cb1-682"><a href="#cb1-682"></a></span>
<span id="cb1-683"><a href="#cb1-683"></a>Initially, the agent is inefficient at chopping trees, keeping the tree</span>
<span id="cb1-684"><a href="#cb1-684"></a>population high (point A). As it improves its chopping skills, it</span>
<span id="cb1-685"><a href="#cb1-685"></a>over-harvests, leading to deforestation and a prolonged period of</span>
<span id="cb1-686"><a href="#cb1-686"></a>minimal reward (between points B and C). Eventually, it learns</span>
<span id="cb1-687"><a href="#cb1-687"></a>sustainable chopping (point D). This scenario (up to point C)</span>
<span id="cb1-688"><a href="#cb1-688"></a>exemplifies goal misgeneralization. When the agent first becomes</span>
<span id="cb1-689"><a href="#cb1-689"></a>proficient at chopping (between points A and B), it faces a range of</span>
<span id="cb1-690"><a href="#cb1-690"></a>potential goals, from sustainable to rapid tree chopping. All these</span>
<span id="cb1-691"><a href="#cb1-691"></a>goals align with the (well-specified) reward function and its experience</span>
<span id="cb1-692"><a href="#cb1-692"></a>of being rewarded for increased efficiency. Unfortunately, it adopts the</span>
<span id="cb1-693"><a href="#cb1-693"></a>detrimental goal of rapid deforestation, resulting in a prolonged period</span>
<span id="cb1-694"><a href="#cb1-694"></a>of low reward.</span>
<span id="cb1-695"><a href="#cb1-695"></a></span>
<span id="cb1-696"><a href="#cb1-696"></a>Another example of goal misgeneralization occurs in recommendation</span>
<span id="cb1-697"><a href="#cb1-697"></a>systems. These systems aim to maximize user engagement, which can</span>
<span id="cb1-698"><a href="#cb1-698"></a>inadvertently lead to promoting extreme or sensational content. Krakovna</span>
<span id="cb1-699"><a href="#cb1-699"></a>et al. highlights that "recommendation systems can misgeneralize by</span>
<span id="cb1-700"><a href="#cb1-700"></a>prioritizing content that maximizes clicks or watch time, even if it</span>
<span id="cb1-701"><a href="#cb1-701"></a>involves promoting harmful or misleading information"</span>
<span id="cb1-702"><a href="#cb1-702"></a><span class="co">[</span><span class="ot">@krakovna2020specification</span><span class="co">]</span>. This misalignment between the system's</span>
<span id="cb1-703"><a href="#cb1-703"></a>learned objective (engagement) and the intended objective (informative</span>
<span id="cb1-704"><a href="#cb1-704"></a>and beneficial content) exemplifies how goal misgeneralization can</span>
<span id="cb1-705"><a href="#cb1-705"></a>manifest in real-world applications.</span>
<span id="cb1-706"><a href="#cb1-706"></a></span>
<span id="cb1-707"><a href="#cb1-707"></a>Autonomous vehicles also present cases of goal misgeneralization. These</span>
<span id="cb1-708"><a href="#cb1-708"></a>vehicles must interpret and respond to various signals in their</span>
<span id="cb1-709"><a href="#cb1-709"></a>environment. However, in rare scenarios, they may misinterpret signals,</span>
<span id="cb1-710"><a href="#cb1-710"></a>leading to unsafe maneuvers. Amodei et al. discuss that "autonomous</span>
<span id="cb1-711"><a href="#cb1-711"></a>vehicles can exhibit unsafe behaviors when faced with uncommon</span>
<span id="cb1-712"><a href="#cb1-712"></a>situations that were not well-represented in the training data,</span>
<span id="cb1-713"><a href="#cb1-713"></a>demonstrating a misgeneralization of their learned driving policies"</span>
<span id="cb1-714"><a href="#cb1-714"></a><span class="co">[</span><span class="ot">@amodei2016concrete</span><span class="co">]</span>. Ensuring that autonomous vehicles generalize</span>
<span id="cb1-715"><a href="#cb1-715"></a>correctly to all possible driving conditions remains a significant</span>
<span id="cb1-716"><a href="#cb1-716"></a>challenge.</span>
<span id="cb1-717"><a href="#cb1-717"></a></span>
<span id="cb1-718"><a href="#cb1-718"></a>To address goal misgeneralization, robust training procedures are</span>
<span id="cb1-719"><a href="#cb1-719"></a>essential. This involves using diverse and representative training data</span>
<span id="cb1-720"><a href="#cb1-720"></a>to cover a wide range of scenarios and incorporating adversarial</span>
<span id="cb1-721"><a href="#cb1-721"></a>training to handle edge cases. Leike et al. <span class="co">[</span><span class="ot">@leike2018scalable</span><span class="co">]</span></span>
<span id="cb1-722"><a href="#cb1-722"></a>emphasize the importance of "robust training procedures that include</span>
<span id="cb1-723"><a href="#cb1-723"></a>diverse datasets and adversarial examples to improve the generalization</span>
<span id="cb1-724"><a href="#cb1-724"></a>of AI systems". Additionally, careful specification of learning goals is</span>
<span id="cb1-725"><a href="#cb1-725"></a>crucial. This means defining clear and comprehensive objectives and</span>
<span id="cb1-726"><a href="#cb1-726"></a>regularly reviewing and adjusting these goals based on performance and</span>
<span id="cb1-727"><a href="#cb1-727"></a>feedback. Hubinger et al. suggests that "regularly updating and refining</span>
<span id="cb1-728"><a href="#cb1-728"></a>the objectives based on ongoing evaluation can help mitigate the risks</span>
<span id="cb1-729"><a href="#cb1-729"></a>of goal misgeneralization" <span class="co">[</span><span class="ot">@hubinger2019introduction</span><span class="co">]</span>.</span>
<span id="cb1-730"><a href="#cb1-730"></a></span>
<span id="cb1-731"><a href="#cb1-731"></a>A key concern about goal misgeneralization in competent, general systems</span>
<span id="cb1-732"><a href="#cb1-732"></a>is that a policy successfully models the preferences of human raters (or</span>
<span id="cb1-733"><a href="#cb1-733"></a>the reward model) and behaves accordingly to maximize reward during</span>
<span id="cb1-734"><a href="#cb1-734"></a>training. However, it may deviate catastrophically from human</span>
<span id="cb1-735"><a href="#cb1-735"></a>preferences when given a different input distribution during deployment,</span>
<span id="cb1-736"><a href="#cb1-736"></a>such as during an unexpected geopolitical conflict or when facing novel</span>
<span id="cb1-737"><a href="#cb1-737"></a>technological developments. Increasing data size, regularization, and</span>
<span id="cb1-738"><a href="#cb1-738"></a>red-teaming can help mitigate goal misgeneralization, but they do not</span>
<span id="cb1-739"><a href="#cb1-739"></a>fundamentally solve the problem. Understanding the inductive biases of</span>
<span id="cb1-740"><a href="#cb1-740"></a>optimization algorithms and model families may help address the problem</span>
<span id="cb1-741"><a href="#cb1-741"></a>more generally.</span>
<span id="cb1-742"><a href="#cb1-742"></a></span>
<span id="cb1-743"><a href="#cb1-743"></a>::: tcolorbox</span>
<span id="cb1-744"><a href="#cb1-744"></a>So, can you differentiate between inner and outer alignment?</span>
<span id="cb1-745"><a href="#cb1-745"></a>:::</span>
<span id="cb1-746"><a href="#cb1-746"></a></span>
<span id="cb1-747"><a href="#cb1-747"></a>The distinction between inner and outer alignment can be a bit subtle.</span>
<span id="cb1-748"><a href="#cb1-748"></a>The following four cases, from <span class="co">[</span><span class="ot">@ngo2023alignment</span><span class="co">]</span>, may help to clarify</span>
<span id="cb1-749"><a href="#cb1-749"></a>the difference:</span>
<span id="cb1-750"><a href="#cb1-750"></a></span>
<span id="cb1-751"><a href="#cb1-751"></a><span class="ss">-   </span>The policy behaves incompetently. This is a capability</span>
<span id="cb1-752"><a href="#cb1-752"></a>    generalization failure.</span>
<span id="cb1-753"><a href="#cb1-753"></a></span>
<span id="cb1-754"><a href="#cb1-754"></a><span class="ss">-   </span>The policy behaves competently and desirably. This is aligned</span>
<span id="cb1-755"><a href="#cb1-755"></a>    behavior.</span>
<span id="cb1-756"><a href="#cb1-756"></a></span>
<span id="cb1-757"><a href="#cb1-757"></a><span class="ss">-   </span>The policy behaves in a competent yet undesirable way which gets a</span>
<span id="cb1-758"><a href="#cb1-758"></a>    high reward according to the original reward function. This is an</span>
<span id="cb1-759"><a href="#cb1-759"></a>    outer alignment failure, also known as reward misspecification.</span>
<span id="cb1-760"><a href="#cb1-760"></a></span>
<span id="cb1-761"><a href="#cb1-761"></a><span class="ss">-   </span>The policy behaves in a competent yet undesirable way which gets a</span>
<span id="cb1-762"><a href="#cb1-762"></a>    low reward according to the original reward function. This is an</span>
<span id="cb1-763"><a href="#cb1-763"></a>    inner alignment failure, also known as goal misgeneralization.</span>
<span id="cb1-764"><a href="#cb1-764"></a></span>
<span id="cb1-765"><a href="#cb1-765"></a>Now that we understand the alignment problem overall, we move on to the</span>
<span id="cb1-766"><a href="#cb1-766"></a>specific techniques used for value learning to ensure AI systems are</span>
<span id="cb1-767"><a href="#cb1-767"></a>aligned with human values.</span>
<span id="cb1-768"><a href="#cb1-768"></a></span>
<span id="cb1-769"><a href="#cb1-769"></a><span class="fu">### Techniques in Value Learning</span></span>
<span id="cb1-770"><a href="#cb1-770"></a></span>
<span id="cb1-771"><a href="#cb1-771"></a>Various methods in value learning for foundation models have been</span>
<span id="cb1-772"><a href="#cb1-772"></a>explored in great detail in recent years <span class="co">[</span><span class="ot">@stiennon_learning_2020</span><span class="co">]</span>.</span>
<span id="cb1-773"><a href="#cb1-773"></a>Using binary human-labeled feedback to make models closely aligned to</span>
<span id="cb1-774"><a href="#cb1-774"></a>human preferences is particularly difficult in scenarios where large</span>
<span id="cb1-775"><a href="#cb1-775"></a>datasets inherently encompass suboptimal behaviors. The approach of</span>
<span id="cb1-776"><a href="#cb1-776"></a>Reinforcement Learning from Human Feedback (RLHF)</span>
<span id="cb1-777"><a href="#cb1-777"></a>(<span class="co">[</span><span class="ot">@ouyang_training_2022</span><span class="co">]</span>) has risen to prominence as an effective method</span>
<span id="cb1-778"><a href="#cb1-778"></a>for addressing this issue. The technique applies to various domains,</span>
<span id="cb1-779"><a href="#cb1-779"></a>from prompt-image alignment, fine-tuning large language models or</span>
<span id="cb1-780"><a href="#cb1-780"></a>diffusion models, and improving the performance of robot policies.</span>
<span id="cb1-781"><a href="#cb1-781"></a></span>
<span id="cb1-782"><a href="#cb1-782"></a><span class="fu">#### Reinforcement Learning from Human Feedback</span></span>
<span id="cb1-783"><a href="#cb1-783"></a></span>
<span id="cb1-784"><a href="#cb1-784"></a>Reinforcement Learning from Human Feedback (RLHF) is a technique used to</span>
<span id="cb1-785"><a href="#cb1-785"></a>align AI behavior with human values by incorporating human feedback into</span>
<span id="cb1-786"><a href="#cb1-786"></a>the reinforcement learning process. This approach is particularly</span>
<span id="cb1-787"><a href="#cb1-787"></a>effective when large datasets inherently encompass suboptimal behaviors.</span>
<span id="cb1-788"><a href="#cb1-788"></a>RLHF aims to refine policies by discriminating between desirable and</span>
<span id="cb1-789"><a href="#cb1-789"></a>undesirable actions, ensuring that AI systems act following human</span>
<span id="cb1-790"><a href="#cb1-790"></a>preferences <span class="co">[</span><span class="ot">@ouyang_training_2022</span><span class="co">]</span>.</span>
<span id="cb1-791"><a href="#cb1-791"></a></span>
<span id="cb1-792"><a href="#cb1-792"></a>**The core concept of RLHF:** It first trains a reward model using a</span>
<span id="cb1-793"><a href="#cb1-793"></a>dataset of binary preferences gathered from human feedback. This reward</span>
<span id="cb1-794"><a href="#cb1-794"></a>model is then used to fine-tune the AI model through a reinforcement</span>
<span id="cb1-795"><a href="#cb1-795"></a>learning algorithm. The core concept is to utilize human feedback to</span>
<span id="cb1-796"><a href="#cb1-796"></a>guide AI learning, thereby aligning the AI's behavior with human</span>
<span id="cb1-797"><a href="#cb1-797"></a>expectations <span class="co">[</span><span class="ot">@stiennon_learning_2020</span><span class="co">]</span>.</span>
<span id="cb1-798"><a href="#cb1-798"></a></span>
<span id="cb1-799"><a href="#cb1-799"></a>![The above diagram depicts the three steps in the traditional RLHF</span>
<span id="cb1-800"><a href="#cb1-800"></a>pipeline: (a) supervised fine-tuning, (b) reward model (RM) training,</span>
<span id="cb1-801"><a href="#cb1-801"></a>and (c) reinforcement learning via proximal policy optimization (PPO) on</span>
<span id="cb1-802"><a href="#cb1-802"></a>this reward model. Image taken from</span>
<span id="cb1-803"><a href="#cb1-803"></a><span class="co">[</span><span class="ot">@ouyang_training_2022</span><span class="co">]</span>.](Figures/rlhf.png){#fig-toy0 width="80%"}</span>
<span id="cb1-804"><a href="#cb1-804"></a></span>
<span id="cb1-805"><a href="#cb1-805"></a>**The RLHF pipeline** involves the following steps:</span>
<span id="cb1-806"><a href="#cb1-806"></a></span>
<span id="cb1-807"><a href="#cb1-807"></a>**Step 1: Supervised Fine-Tuning**</span>
<span id="cb1-808"><a href="#cb1-808"></a></span>
<span id="cb1-809"><a href="#cb1-809"></a>In the initial step for language modeling tasks, we utilize a</span>
<span id="cb1-810"><a href="#cb1-810"></a>high-quality dataset consisting of</span>
<span id="cb1-811"><a href="#cb1-811"></a>$\left(\text{prompt}, \text{response}\right)$ pairs to train the model.</span>
<span id="cb1-812"><a href="#cb1-812"></a>Prompts are sampled from a curated dataset designed to cover a wide</span>
<span id="cb1-813"><a href="#cb1-813"></a>range of instructions and queries, such as "Explain the moon landing to</span>
<span id="cb1-814"><a href="#cb1-814"></a>a 6-year-old." Trained human labelers provide the desired output</span>
<span id="cb1-815"><a href="#cb1-815"></a>behavior for each prompt, ensuring responses are accurate, clear, and</span>
<span id="cb1-816"><a href="#cb1-816"></a>aligned with task goals. For instance, in response to the moon landing</span>
<span id="cb1-817"><a href="#cb1-817"></a>prompt, a labeler might generate, "Some people went to the moon in a</span>
<span id="cb1-818"><a href="#cb1-818"></a>big rocket and explored its surface." The collected</span>
<span id="cb1-819"><a href="#cb1-819"></a>$\left(\text{prompt}, \text{response}\right)$ pairs serve as the</span>
<span id="cb1-820"><a href="#cb1-820"></a>training data for the model, with the cross-entropy loss function</span>
<span id="cb1-821"><a href="#cb1-821"></a>applied only to the response tokens. This helps the model learn to</span>
<span id="cb1-822"><a href="#cb1-822"></a>generate responses that are closely aligned with the human-provided</span>
<span id="cb1-823"><a href="#cb1-823"></a>examples. The training process adjusts model parameters through</span>
<span id="cb1-824"><a href="#cb1-824"></a>supervised learning, minimizing the difference between the model's</span>
<span id="cb1-825"><a href="#cb1-825"></a>predictions and the human responses.</span>
<span id="cb1-826"><a href="#cb1-826"></a></span>
<span id="cb1-827"><a href="#cb1-827"></a>**Step 2: Reward Model (RM) Training**</span>
<span id="cb1-828"><a href="#cb1-828"></a></span>
<span id="cb1-829"><a href="#cb1-829"></a>In this step, we train a reward model to score any</span>
<span id="cb1-830"><a href="#cb1-830"></a>$\left(\text{prompt}, \text{response}\right)$ pair and produce a</span>
<span id="cb1-831"><a href="#cb1-831"></a>meaningful scalar value. Multiple model-generated responses are sampled</span>
<span id="cb1-832"><a href="#cb1-832"></a>for each prompt. Human labelers then rank these responses from best to</span>
<span id="cb1-833"><a href="#cb1-833"></a>worst based on their quality and alignment with the prompt. For example,</span>
<span id="cb1-834"><a href="#cb1-834"></a>given the prompt "Explain the moon landing to a 6-year-old," responses</span>
<span id="cb1-835"><a href="#cb1-835"></a>like "People went to the moon in a big rocket and explored its</span>
<span id="cb1-836"><a href="#cb1-836"></a>surface" might be ranked higher than "The moon is a natural satellite</span>
<span id="cb1-837"><a href="#cb1-837"></a>of Earth." The rankings provided by the labelers are used to train the</span>
<span id="cb1-838"><a href="#cb1-838"></a>reward model $\Phi_{\text{RM}}$. The model is trained by minimizing the</span>
<span id="cb1-839"><a href="#cb1-839"></a>following loss function across all training samples:</span>
<span id="cb1-840"><a href="#cb1-840"></a></span>
<span id="cb1-841"><a href="#cb1-841"></a>$$\mathbb{L}(\Phi_{RM}) = -\mathbb{E}_{(x,y_e,i\rightarrow D_{RL})}[\log(\sigma(\Phi_{RM}(x, y_i)) - \Phi_{RM}(x, y_{1-i}))]$$</span>
<span id="cb1-842"><a href="#cb1-842"></a></span>
<span id="cb1-843"><a href="#cb1-843"></a>for $i \in <span class="sc">\{</span>0,1 <span class="sc">\}</span>$. This loss function encourages the reward model to</span>
<span id="cb1-844"><a href="#cb1-844"></a>produce higher scores for better-ranked responses, thereby learning to</span>
<span id="cb1-845"><a href="#cb1-845"></a>evaluate the quality of model outputs effectively.</span>
<span id="cb1-846"><a href="#cb1-846"></a></span>
<span id="cb1-847"><a href="#cb1-847"></a>**Step 3: Reinforcement Learning**</span>
<span id="cb1-848"><a href="#cb1-848"></a></span>
<span id="cb1-849"><a href="#cb1-849"></a>In this step, we refine the policy using reinforcement learning (RL)</span>
<span id="cb1-850"><a href="#cb1-850"></a>based on the rewards provided by the trained reward model. A new prompt</span>
<span id="cb1-851"><a href="#cb1-851"></a>is sampled from the dataset, and the policy generates an output. The</span>
<span id="cb1-852"><a href="#cb1-852"></a>reward model then calculates a reward for this output, and the reward is</span>
<span id="cb1-853"><a href="#cb1-853"></a>used to update the policy using the Proximal Policy Optimization (PPO)</span>
<span id="cb1-854"><a href="#cb1-854"></a>algorithm.</span>
<span id="cb1-855"><a href="#cb1-855"></a></span>
<span id="cb1-856"><a href="#cb1-856"></a>The RL setting is defined as follows:</span>
<span id="cb1-857"><a href="#cb1-857"></a></span>
<span id="cb1-858"><a href="#cb1-858"></a><span class="ss">1.  </span>*Action Space*: The set of all possible actions the agent can take,</span>
<span id="cb1-859"><a href="#cb1-859"></a>    which, for language models, is typically the set of all possible</span>
<span id="cb1-860"><a href="#cb1-860"></a>    completions.</span>
<span id="cb1-861"><a href="#cb1-861"></a></span>
<span id="cb1-862"><a href="#cb1-862"></a><span class="ss">2.  </span>*Policy*: A probability distribution over the action space. In the</span>
<span id="cb1-863"><a href="#cb1-863"></a>    case of language models like LLM, the policy is contained within the</span>
<span id="cb1-864"><a href="#cb1-864"></a>    model and represents the probability of predicting each completion.</span>
<span id="cb1-865"><a href="#cb1-865"></a></span>
<span id="cb1-866"><a href="#cb1-866"></a><span class="ss">3.  </span>*Observations*: The inputs to the policy, which in this context are</span>
<span id="cb1-867"><a href="#cb1-867"></a>    prompts sampled from a certain distribution.</span>
<span id="cb1-868"><a href="#cb1-868"></a></span>
<span id="cb1-869"><a href="#cb1-869"></a><span class="ss">4.  </span>*Reward*: A numerical score provided by the Reward Model (RM) that</span>
<span id="cb1-870"><a href="#cb1-870"></a>    indicates the quality of actions taken by the agent.</span>
<span id="cb1-871"><a href="#cb1-871"></a></span>
<span id="cb1-872"><a href="#cb1-872"></a>During training, batches of prompts are sampled from two distinct</span>
<span id="cb1-873"><a href="#cb1-873"></a>distributions, namely either $D_\text{RL}$, the distribution of prompts</span>
<span id="cb1-874"><a href="#cb1-874"></a>explicitly used for the RL model, or $D_\text{pretrain}$, the</span>
<span id="cb1-875"><a href="#cb1-875"></a>distribution of prompts from the pre-trained model. The objective for</span>
<span id="cb1-876"><a href="#cb1-876"></a>the RL agent is to maximize the reward while ensuring that the policy</span>
<span id="cb1-877"><a href="#cb1-877"></a>does not deviate significantly from the supervised fine-tuned model and</span>
<span id="cb1-878"><a href="#cb1-878"></a>does not degrade the performance on tasks the pre-trained model was</span>
<span id="cb1-879"><a href="#cb1-879"></a>optimized for. When sampling a response $y$ to a prompt $x$ from</span>
<span id="cb1-880"><a href="#cb1-880"></a>$D_\text{RL}$, the first objective function is:</span>
<span id="cb1-881"><a href="#cb1-881"></a></span>
<span id="cb1-882"><a href="#cb1-882"></a>$$\text{objective}_1(x_{RL}, y; \phi) = RM(x_{RL}, y) - \beta \log \frac{\text{LLM}_{\phi}^{RL}(y|x)}{\text{LLM}_{SFT}(y|x)}$$</span>
<span id="cb1-883"><a href="#cb1-883"></a></span>
<span id="cb1-884"><a href="#cb1-884"></a>Where the first term is the reward from the RM, and the second term is</span>
<span id="cb1-885"><a href="#cb1-885"></a>the Kullback-Leibler (KL) divergence, weighted by a factor $\beta$,</span>
<span id="cb1-886"><a href="#cb1-886"></a>which acts as a regularizer to prevent the RL model from straying too</span>
<span id="cb1-887"><a href="#cb1-887"></a>far from the SFT model. Further, for each $x$ from $D_\text{pretrain}$,</span>
<span id="cb1-888"><a href="#cb1-888"></a>the second objective is to ensure that the RL model's performance on</span>
<span id="cb1-889"><a href="#cb1-889"></a>text completion does not worsen:</span>
<span id="cb1-890"><a href="#cb1-890"></a></span>
<span id="cb1-891"><a href="#cb1-891"></a>$$\text{objective}_2(x_{\text{pretrain}} ; \phi) = \gamma \log \text{LLM}_{\phi}^{RL}(x_{\text{pretrain}})$$</span>
<span id="cb1-892"><a href="#cb1-892"></a></span>
<span id="cb1-893"><a href="#cb1-893"></a>where $\gamma$ is a weighting factor that balances the influence of this</span>
<span id="cb1-894"><a href="#cb1-894"></a>objective against the others.</span>
<span id="cb1-895"><a href="#cb1-895"></a></span>
<span id="cb1-896"><a href="#cb1-896"></a>The final objective function is a sum of the expected values of the two</span>
<span id="cb1-897"><a href="#cb1-897"></a>objectives described above, across both distributions. In the RL</span>
<span id="cb1-898"><a href="#cb1-898"></a>setting, we maximize *this* objective function:</span>
<span id="cb1-899"><a href="#cb1-899"></a></span>
<span id="cb1-900"><a href="#cb1-900"></a>$$\text{objective}(\phi) = E_{(x,y) \sim D_{\phi}^{RL}}<span class="co">[</span><span class="ot">RM(x, y) - \beta \log \frac{\text{LLM}_{\phi}^{RL}(y|x)}{\text{LLM}_{SFT}(y|x)}</span><span class="co">]</span> + \gamma E_{x \sim D_{\text{pretrain}}}<span class="co">[</span><span class="ot">\log \text{LLM}_{\phi}^{RL}(x)</span><span class="co">]</span>$$</span>
<span id="cb1-901"><a href="#cb1-901"></a></span>
<span id="cb1-902"><a href="#cb1-902"></a>In practice, the second part of the objective is often not used to</span>
<span id="cb1-903"><a href="#cb1-903"></a>perform $\text{RLHF}$. The KL penalty is typically enough to constrain</span>
<span id="cb1-904"><a href="#cb1-904"></a>the RL policy. This function balances the drive to maximize the reward</span>
<span id="cb1-905"><a href="#cb1-905"></a>with the need to maintain the quality of text completion and the</span>
<span id="cb1-906"><a href="#cb1-906"></a>similarity to the behavior of the supervised fine-tuned model.</span>
<span id="cb1-907"><a href="#cb1-907"></a></span>
<span id="cb1-908"><a href="#cb1-908"></a>**Limitations and Challenges:** Despite its successes, RLHF faces</span>
<span id="cb1-909"><a href="#cb1-909"></a>several challenges. One major issue is the quality of human feedback,</span>
<span id="cb1-910"><a href="#cb1-910"></a>which can be inconsistent and subjective. Scalability is another</span>
<span id="cb1-911"><a href="#cb1-911"></a>concern, as obtaining a large amount of high-quality feedback can be</span>
<span id="cb1-912"><a href="#cb1-912"></a>expensive and time-consuming. Over-optimization and hallucinations,</span>
<span id="cb1-913"><a href="#cb1-913"></a>where the model generates plausible but incorrect outputs, are also</span>
<span id="cb1-914"><a href="#cb1-914"></a>common problems. This generally stems from temporal credit assignment</span>
<span id="cb1-915"><a href="#cb1-915"></a>and the instability of approximate dynamic programming</span>
<span id="cb1-916"><a href="#cb1-916"></a><span class="co">[</span><span class="ot">@vanhasselt_deep_2018</span><span class="co">]</span>. Further, it is expensive to gather tens of</span>
<span id="cb1-917"><a href="#cb1-917"></a>thousands of preferences over datasets to create robust reward models.</span>
<span id="cb1-918"><a href="#cb1-918"></a>Strategies to overcome these challenges include using diverse and</span>
<span id="cb1-919"><a href="#cb1-919"></a>representative training data, incorporating adversarial training to</span>
<span id="cb1-920"><a href="#cb1-920"></a>handle edge cases, and continuously refining the reward model based on</span>
<span id="cb1-921"><a href="#cb1-921"></a>ongoing feedback and performance evaluations <span class="co">[</span><span class="ot">@leike2018scalable</span><span class="co">]</span>.</span>
<span id="cb1-922"><a href="#cb1-922"></a></span>
<span id="cb1-923"><a href="#cb1-923"></a><span class="fu">#### Contrastive Preference Learning</span></span>
<span id="cb1-924"><a href="#cb1-924"></a></span>
<span id="cb1-925"><a href="#cb1-925"></a>Contrastive Preference Learning (CPL) is a learning paradigm designed to</span>
<span id="cb1-926"><a href="#cb1-926"></a>enhance the alignment of AI systems with human preferences without</span>
<span id="cb1-927"><a href="#cb1-927"></a>relying on traditional reinforcement learning (RL) methods. CPL</span>
<span id="cb1-928"><a href="#cb1-928"></a>addresses many limitations inherent in traditional RLHF techniques by</span>
<span id="cb1-929"><a href="#cb1-929"></a>learning from human comparisons rather than explicit reward signals.</span>
<span id="cb1-930"><a href="#cb1-930"></a>This section provides an in-depth exploration of CPL, detailing its</span>
<span id="cb1-931"><a href="#cb1-931"></a>methodology, experiments, results, and potential challenges. Recent</span>
<span id="cb1-932"><a href="#cb1-932"></a>research has shown that human preferences are often better modeled by</span>
<span id="cb1-933"><a href="#cb1-933"></a>the optimal advantage function or regret, rather than traditional reward</span>
<span id="cb1-934"><a href="#cb1-934"></a>functions used in RLHF. Traditional RLHF approaches, which learn a</span>
<span id="cb1-935"><a href="#cb1-935"></a>reward function from a preference model and then apply RL, incur</span>
<span id="cb1-936"><a href="#cb1-936"></a>significant computational expenses and complexity</span>
<span id="cb1-937"><a href="#cb1-937"></a><span class="co">[</span><span class="ot">@hejna2023contrastive</span><span class="co">]</span>. CPL offers a streamlined and scalable</span>
<span id="cb1-938"><a href="#cb1-938"></a>alternative by leveraging a more accurate regret model of human</span>
<span id="cb1-939"><a href="#cb1-939"></a>preferences.</span>
<span id="cb1-940"><a href="#cb1-940"></a></span>
<span id="cb1-941"><a href="#cb1-941"></a>**The key idea of CPL** is the substitution of the optimal advantage</span>
<span id="cb1-942"><a href="#cb1-942"></a>function with the log probability of the policy in a maximum entropy</span>
<span id="cb1-943"><a href="#cb1-943"></a>reinforcement learning framework. This substitution is beneficial as it</span>
<span id="cb1-944"><a href="#cb1-944"></a>circumvents the need to learn the advantage function and avoids the</span>
<span id="cb1-945"><a href="#cb1-945"></a>optimization challenges associated with RL-like algorithms. By using the</span>
<span id="cb1-946"><a href="#cb1-946"></a>log probability of the policy, CPL more closely aligns with how humans</span>
<span id="cb1-947"><a href="#cb1-947"></a>model preferences and enables efficient supervised learning from human</span>
<span id="cb1-948"><a href="#cb1-948"></a>feedback.</span>
<span id="cb1-949"><a href="#cb1-949"></a></span>
<span id="cb1-950"><a href="#cb1-950"></a>CPL is a structured approach to aligning AI behavior with human</span>
<span id="cb1-951"><a href="#cb1-951"></a>preferences by relying on a dataset of preferred behavior segments</span>
<span id="cb1-952"><a href="#cb1-952"></a>$\mathcal{D}_{\text{pref}} = \{(\sigma_i^+, \sigma_i^-)\}_{i=1}^n$,</span>
<span id="cb1-953"><a href="#cb1-953"></a>where $\sigma^+ \succ \sigma^-$. Each behavior segment $\sigma$ is a</span>
<span id="cb1-954"><a href="#cb1-954"></a>sequence of states and actions,</span>
<span id="cb1-955"><a href="#cb1-955"></a>$\sigma = (s_1, a_1, s_2, a_2, \ldots, s_k, a_k)$. The CPL approach aims</span>
<span id="cb1-956"><a href="#cb1-956"></a>to maximize the expected sum of rewards minus an entropy term, which</span>
<span id="cb1-957"><a href="#cb1-957"></a>promotes exploration and prevents overfitting to specific actions:</span>
<span id="cb1-958"><a href="#cb1-958"></a></span>
<span id="cb1-959"><a href="#cb1-959"></a>$$\max_\pi \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t (r(s_t, a_t) - \alpha \log \pi(a_t | s_t)) \right]$$</span>
<span id="cb1-960"><a href="#cb1-960"></a></span>
<span id="cb1-961"><a href="#cb1-961"></a>where $\gamma$ is the discount factor, $\alpha$ is the temperature</span>
<span id="cb1-962"><a href="#cb1-962"></a>parameter controlling the stochasticity of the policy, and $r$ is the</span>
<span id="cb1-963"><a href="#cb1-963"></a>reward function. This step sets the foundation by defining the</span>
<span id="cb1-964"><a href="#cb1-964"></a>optimization objective that the CPL model strives to achieve. In the</span>
<span id="cb1-965"><a href="#cb1-965"></a>learning process, CPL compares the log probabilities of actions in</span>
<span id="cb1-966"><a href="#cb1-966"></a>preferred segments $\sigma^+$ against those in non-preferred segments</span>
<span id="cb1-967"><a href="#cb1-967"></a>$\sigma^-$ :</span>
<span id="cb1-968"><a href="#cb1-968"></a></span>
<span id="cb1-969"><a href="#cb1-969"></a>$$\mathbb{L}_{CPL}(\pi_\theta, \mathcal{D}_{\text{pref}}) = \mathbb{E}_{(\sigma^+,\sigma^-) \sim \mathcal{D}_{\text{pref}}} \left[ -\log \frac{\exp(\sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a_t^+|s_t^+))}{\exp(\sum_{\sigma^+} \gamma^t \alpha \log \pi_\theta(a_t^+|s_t^+)) + \exp(\sum_{\sigma^-} \gamma^t \alpha \log \pi_\theta(a_t^-|s_t^-))} \right]$$</span>
<span id="cb1-970"><a href="#cb1-970"></a></span>
<span id="cb1-971"><a href="#cb1-971"></a>This comparison allows the model to learn which actions are more aligned</span>
<span id="cb1-972"><a href="#cb1-972"></a>with human preferences, forming the core learning mechanism of CPL. The</span>
<span id="cb1-973"><a href="#cb1-973"></a>preference model for CPL is regret-based, described as</span>
<span id="cb1-974"><a href="#cb1-974"></a></span>
<span id="cb1-975"><a href="#cb1-975"></a>$$P_{A^*}[\sigma^+ \succ \sigma^-] = \frac{\exp(\sum_{\sigma^+} \gamma^t A^*(s_t^+, a_t^+))}{\exp(\sum_{\sigma^+} \gamma^t A^*(s_t^+, a_t^+)) + \exp(\sum_{\sigma^-} \gamma^t A^*(s_t^-, a_t^-))}$$</span>
<span id="cb1-976"><a href="#cb1-976"></a>where $A^*(s_t, a_t)$ represents the advantage function and is a matrix.</span>
<span id="cb1-977"><a href="#cb1-977"></a>This step models human preferences based on regret, reflecting how</span>
<span id="cb1-978"><a href="#cb1-978"></a>humans might evaluate different behaviors.</span>
<span id="cb1-979"><a href="#cb1-979"></a></span>
<span id="cb1-980"><a href="#cb1-980"></a>One hypothesis as to why one might consider a regret-based model more</span>
<span id="cb1-981"><a href="#cb1-981"></a>useful over a sum-of-rewards, Bradley-Terry model is that humans likely</span>
<span id="cb1-982"><a href="#cb1-982"></a>think of preferences based on the regret of each behavior under the</span>
<span id="cb1-983"><a href="#cb1-983"></a>optimal policy of the expert's reward function.</span>
<span id="cb1-984"><a href="#cb1-984"></a></span>
<span id="cb1-985"><a href="#cb1-985"></a>The key insight that the paper leverages is that from</span>
<span id="cb1-986"><a href="#cb1-986"></a><span class="co">[</span><span class="ot">@ziebart_modeling_2010</span><span class="co">]</span> in MaxEnt Offline RL. In this general setting,</span>
<span id="cb1-987"><a href="#cb1-987"></a><span class="co">[</span><span class="ot">@ziebart_modeling_2010</span><span class="co">]</span> shows that one can write that the optimal</span>
<span id="cb1-988"><a href="#cb1-988"></a>advantage function is related to the optimal policy by</span>
<span id="cb1-989"><a href="#cb1-989"></a>$A^*_r(s, a) = \alpha \log \pi^*(a|s)$. Therefore, the loss function for</span>
<span id="cb1-990"><a href="#cb1-990"></a>CPL can be written by substituting the above result to obtain:</span>
<span id="cb1-991"><a href="#cb1-991"></a>$$L_{CPL}(\pi_\theta, \mathcal{D}_{\text{pref}}) = \mathbb{E}_{(\sigma^+,\sigma^-) \sim \mathcal{D}_{\text{pref}}} \left[ -\log P_{\pi_\theta}<span class="co">[</span><span class="ot">\sigma^+ \succ \sigma^-</span><span class="co">]</span> \right]$$</span>
<span id="cb1-992"><a href="#cb1-992"></a></span>
<span id="cb1-993"><a href="#cb1-993"></a>One merit of using CPL over the typical RLHF pipeline is that it can</span>
<span id="cb1-994"><a href="#cb1-994"></a>lead to a deduction in mode collapse. Further, it makes reward</span>
<span id="cb1-995"><a href="#cb1-995"></a>misgeneralization failures less likely, enhancing the reliability of the</span>
<span id="cb1-996"><a href="#cb1-996"></a>learned policy. However, the approach still has a few limitations:</span>
<span id="cb1-997"><a href="#cb1-997"></a></span>
<span id="cb1-998"><a href="#cb1-998"></a><span class="ss">1.  </span>CPL assumes knowledge of the human rater's temporal discounting</span>
<span id="cb1-999"><a href="#cb1-999"></a>    (i.e., of the discount factor $\gamma$), which in practice would be</span>
<span id="cb1-1000"><a href="#cb1-1000"></a>    difficult to communicate.</span>
<span id="cb1-1001"><a href="#cb1-1001"></a></span>
<span id="cb1-1002"><a href="#cb1-1002"></a><span class="ss">2.  </span>CPL's loss function is computed over segments, it requires a</span>
<span id="cb1-1003"><a href="#cb1-1003"></a>    substantial amount of GPU memory for large segment sizes.</span>
<span id="cb1-1004"><a href="#cb1-1004"></a></span>
<span id="cb1-1005"><a href="#cb1-1005"></a>::: tcolorbox</span>
<span id="cb1-1006"><a href="#cb1-1006"></a>How does RLHF with PPO and CPL compare their effectiveness and</span>
<span id="cb1-1007"><a href="#cb1-1007"></a>applicability in aligning AI systems with human values?</span>
<span id="cb1-1008"><a href="#cb1-1008"></a>:::</span>
<span id="cb1-1009"><a href="#cb1-1009"></a></span>
<span id="cb1-1010"><a href="#cb1-1010"></a>The ongoing challenge in aligning foundation models in the future will</span>
<span id="cb1-1011"><a href="#cb1-1011"></a>be to refine these methodologies further, balancing computational</span>
<span id="cb1-1012"><a href="#cb1-1012"></a>feasibility with the sophistication needed to capture the intricacies of</span>
<span id="cb1-1013"><a href="#cb1-1013"></a>human values and countering failure modes such as reward</span>
<span id="cb1-1014"><a href="#cb1-1014"></a>over-optimization. In conclusion, exploring value learning through RLHF</span>
<span id="cb1-1015"><a href="#cb1-1015"></a>and CPL methods has enriched our understanding of integrating human</span>
<span id="cb1-1016"><a href="#cb1-1016"></a>preferences into foundation models. To provide a well-rounded</span>
<span id="cb1-1017"><a href="#cb1-1017"></a>perspective on aligning AI systems with human values, the following</span>
<span id="cb1-1018"><a href="#cb1-1018"></a>table highlights a detailed comparison of RLHF with PPO and CPL,</span>
<span id="cb1-1019"><a href="#cb1-1019"></a>emphasizing their advantages, limitations, and ideal scenarios.</span>
<span id="cb1-1020"><a href="#cb1-1020"></a></span>
<span id="cb1-1021"><a href="#cb1-1021"></a>::: {#tbl-ppo_vs_cpl}</span>
<span id="cb1-1022"><a href="#cb1-1022"></a>+---------------------+----------------------+----------------------+</span>
<span id="cb1-1023"><a href="#cb1-1023"></a>|                     | **RLHF with PPO**    | **CPL**              |</span>
<span id="cb1-1024"><a href="#cb1-1024"></a>+:====================+:=====================+:=====================+</span>
<span id="cb1-1025"><a href="#cb1-1025"></a>| **Strengths**       | -   Excels in        | -   Emphasizes       |</span>
<span id="cb1-1026"><a href="#cb1-1026"></a>|                     |     optimizing       |     regret and       |</span>
<span id="cb1-1027"><a href="#cb1-1027"></a>|                     |     policies through |     optimality       |</span>
<span id="cb1-1028"><a href="#cb1-1028"></a>|                     |     reinforcement    |     rather than      |</span>
<span id="cb1-1029"><a href="#cb1-1029"></a>|                     |     learning         |     reward           |</span>
<span id="cb1-1030"><a href="#cb1-1030"></a>|                     |                      |     maximization     |</span>
<span id="cb1-1031"><a href="#cb1-1031"></a>|                     | -   Suitable for     |                      |</span>
<span id="cb1-1032"><a href="#cb1-1032"></a>|                     |     tasks that       | -   Reduces          |</span>
<span id="cb1-1033"><a href="#cb1-1033"></a>|                     |     benefit from     |     computational    |</span>
<span id="cb1-1034"><a href="#cb1-1034"></a>|                     |     iterative        |     overhead         |</span>
<span id="cb1-1035"><a href="#cb1-1035"></a>|                     |     improvement      |                      |</span>
<span id="cb1-1036"><a href="#cb1-1036"></a>|                     |                      | -   Aligns more      |</span>
<span id="cb1-1037"><a href="#cb1-1037"></a>|                     | -   Effective in     |     closely with     |</span>
<span id="cb1-1038"><a href="#cb1-1038"></a>|                     |     continuous       |     human            |</span>
<span id="cb1-1039"><a href="#cb1-1039"></a>|                     |     action spaces    |     preferences      |</span>
<span id="cb1-1040"><a href="#cb1-1040"></a>|                     |                      |                      |</span>
<span id="cb1-1041"><a href="#cb1-1041"></a>|                     |                      | -   Avoids reward    |</span>
<span id="cb1-1042"><a href="#cb1-1042"></a>|                     |                      |                      |</span>
<span id="cb1-1043"><a href="#cb1-1043"></a>|                     |                      |    over-optimization |</span>
<span id="cb1-1044"><a href="#cb1-1044"></a>|                     |                      |                      |</span>
<span id="cb1-1045"><a href="#cb1-1045"></a>|                     |                      | -   More scalable    |</span>
<span id="cb1-1046"><a href="#cb1-1046"></a>|                     |                      |     due to reliance  |</span>
<span id="cb1-1047"><a href="#cb1-1047"></a>|                     |                      |     on supervised    |</span>
<span id="cb1-1048"><a href="#cb1-1048"></a>|                     |                      |     learning         |</span>
<span id="cb1-1049"><a href="#cb1-1049"></a>|                     |                      |     techniques       |</span>
<span id="cb1-1050"><a href="#cb1-1050"></a>+---------------------+----------------------+----------------------+</span>
<span id="cb1-1051"><a href="#cb1-1051"></a>| **Limitations**     | -   Faces            | -   May struggle in  |</span>
<span id="cb1-1052"><a href="#cb1-1052"></a>|                     |     limitations in   |     environments     |</span>
<span id="cb1-1053"><a href="#cb1-1053"></a>|                     |     handling complex |     where direct     |</span>
<span id="cb1-1054"><a href="#cb1-1054"></a>|                     |     preference       |     human feedback   |</span>
<span id="cb1-1055"><a href="#cb1-1055"></a>|                     |     structures       |     is less          |</span>
<span id="cb1-1056"><a href="#cb1-1056"></a>|                     |                      |     accessible       |</span>
<span id="cb1-1057"><a href="#cb1-1057"></a>|                     | -   High             |                      |</span>
<span id="cb1-1058"><a href="#cb1-1058"></a>|                     |     computational    | -   Depends on       |</span>
<span id="cb1-1059"><a href="#cb1-1059"></a>|                     |     cost             |     high-quality     |</span>
<span id="cb1-1060"><a href="#cb1-1060"></a>|                     |                      |     preference data  |</span>
<span id="cb1-1061"><a href="#cb1-1061"></a>|                     | -   Susceptible to   |     for effective    |</span>
<span id="cb1-1062"><a href="#cb1-1062"></a>|                     |     reward           |     training         |</span>
<span id="cb1-1063"><a href="#cb1-1063"></a>|                     |                      |                      |</span>
<span id="cb1-1064"><a href="#cb1-1064"></a>|                     |    misgeneralization |                      |</span>
<span id="cb1-1065"><a href="#cb1-1065"></a>+---------------------+----------------------+----------------------+</span>
<span id="cb1-1066"><a href="#cb1-1066"></a>| **Ideal Scenarios** | -   Tasks with       | -   Environments     |</span>
<span id="cb1-1067"><a href="#cb1-1067"></a>|                     |     well-defined     |     where human      |</span>
<span id="cb1-1068"><a href="#cb1-1068"></a>|                     |     reward functions |     feedback is more |</span>
<span id="cb1-1069"><a href="#cb1-1069"></a>|                     |                      |     accessible than  |</span>
<span id="cb1-1070"><a href="#cb1-1070"></a>|                     | -   Environments     |     well-defined     |</span>
<span id="cb1-1071"><a href="#cb1-1071"></a>|                     |     allowing         |     reward functions |</span>
<span id="cb1-1072"><a href="#cb1-1072"></a>|                     |     extensive        |                      |</span>
<span id="cb1-1073"><a href="#cb1-1073"></a>|                     |     interaction and  | -   Tasks requiring  |</span>
<span id="cb1-1074"><a href="#cb1-1074"></a>|                     |     feedback         |     computational    |</span>
<span id="cb1-1075"><a href="#cb1-1075"></a>|                     |                      |     efficiency and   |</span>
<span id="cb1-1076"><a href="#cb1-1076"></a>|                     |                      |     scalability      |</span>
<span id="cb1-1077"><a href="#cb1-1077"></a>+---------------------+----------------------+----------------------+</span>
<span id="cb1-1078"><a href="#cb1-1078"></a></span>
<span id="cb1-1079"><a href="#cb1-1079"></a>: Comparison between RLHF with PPO and CPL</span>
<span id="cb1-1080"><a href="#cb1-1080"></a>:::</span>
<span id="cb1-1081"><a href="#cb1-1081"></a></span>
<span id="cb1-1082"><a href="#cb1-1082"></a><span class="fu">### Value Alignment Verification</span></span>
<span id="cb1-1083"><a href="#cb1-1083"></a></span>
<span id="cb1-1084"><a href="#cb1-1084"></a>After we discuss the techniques of value learning, it becomes evident</span>
<span id="cb1-1085"><a href="#cb1-1085"></a>that aligning machine behavior with human values, while advanced, is</span>
<span id="cb1-1086"><a href="#cb1-1086"></a>inherently approximate and not infallible. This realization underscores</span>
<span id="cb1-1087"><a href="#cb1-1087"></a>the importance of value alignment verification---a methodology to ensure</span>
<span id="cb1-1088"><a href="#cb1-1088"></a>that the values imparted to a machine truly reflect those of a human.</span>
<span id="cb1-1089"><a href="#cb1-1089"></a>Human-robot value alignment has been explored through various lenses,</span>
<span id="cb1-1090"><a href="#cb1-1090"></a>including qualitative trust assessments <span class="co">[</span><span class="ot">@huang2018establishing</span><span class="co">]</span>,</span>
<span id="cb1-1091"><a href="#cb1-1091"></a>asymptotic alignment through active learning of human preferences</span>
<span id="cb1-1092"><a href="#cb1-1092"></a><span class="co">[</span><span class="ot">@hadfield2016cooperative; @christiano2017deep; @sadigh2017active</span><span class="co">]</span>, and</span>
<span id="cb1-1093"><a href="#cb1-1093"></a>formal verification methods <span class="co">[</span><span class="ot">@brown2021value</span><span class="co">]</span>. This section will focus</span>
<span id="cb1-1094"><a href="#cb1-1094"></a>on the formal verification approach for value alignment as discussed in</span>
<span id="cb1-1095"><a href="#cb1-1095"></a><span class="co">[</span><span class="ot">@brown2021value</span><span class="co">]</span>. Unless otherwise stated, all information presented</span>
<span id="cb1-1096"><a href="#cb1-1096"></a>here is derived from <span class="co">[</span><span class="ot">@brown2021value</span><span class="co">]</span>. This approach aims to ensure</span>
<span id="cb1-1097"><a href="#cb1-1097"></a>that the values imparted to a machine align with those of a human.</span>
<span id="cb1-1098"><a href="#cb1-1098"></a></span>
<span id="cb1-1099"><a href="#cb1-1099"></a>To begin with, consider an MDP with state space $\mathcal{S}$, action</span>
<span id="cb1-1100"><a href="#cb1-1100"></a>space $\mathcal{A}$, and transition model $\mathcal{T}$. This formal</span>
<span id="cb1-1101"><a href="#cb1-1101"></a>framework allows us to model the environment in which humans and robots</span>
<span id="cb1-1102"><a href="#cb1-1102"></a>operate. Denote the human's reward function as $R$ and the robot's</span>
<span id="cb1-1103"><a href="#cb1-1103"></a>reward function as $R^\prime$. Both the human and robot reward functions</span>
<span id="cb1-1104"><a href="#cb1-1104"></a>must be linear in a set of shared features, defined as:</span>
<span id="cb1-1105"><a href="#cb1-1105"></a>$$\begin{aligned}</span>
<span id="cb1-1106"><a href="#cb1-1106"></a>    R(s) = \mathbf{w}^\top \phi(s), R^\prime(s) = \mathbf{w}^{\prime \top} \phi(s).</span>
<span id="cb1-1107"><a href="#cb1-1107"></a>\end{aligned}$$</span>
<span id="cb1-1108"><a href="#cb1-1108"></a></span>
<span id="cb1-1109"><a href="#cb1-1109"></a>These linear reward functions provide a common ground for comparing</span>
<span id="cb1-1110"><a href="#cb1-1110"></a>human and robot preferences.</span>
<span id="cb1-1111"><a href="#cb1-1111"></a></span>
<span id="cb1-1112"><a href="#cb1-1112"></a>Next, the optimal state-action value function, which indicates the</span>
<span id="cb1-1113"><a href="#cb1-1113"></a>expected cumulative reward of following a policy $\pi$ starting from</span>
<span id="cb1-1114"><a href="#cb1-1114"></a>state $s$ and action $a$, but we follow the notation in</span>
<span id="cb1-1115"><a href="#cb1-1115"></a><span class="co">[</span><span class="ot">@brown2021value</span><span class="co">]</span> for simplicity. The optimal state-action value</span>
<span id="cb1-1116"><a href="#cb1-1116"></a>function is given by:</span>
<span id="cb1-1117"><a href="#cb1-1117"></a></span>
<span id="cb1-1118"><a href="#cb1-1118"></a>$$\begin{aligned}</span>
<span id="cb1-1119"><a href="#cb1-1119"></a>    Q_R^\pi (s,a) = \mathbf{w}^\top \Phi_{\pi_R}^{(s,a)}, \Phi_{\pi_R}^{(s,a)} = \mathbb{E}_\pi [\sum_{t=0}^\infty \gamma^t \phi(s_t) \vert s_0 = s, a_0 = a].</span>
<span id="cb1-1120"><a href="#cb1-1120"></a>\end{aligned}$$</span>
<span id="cb1-1121"><a href="#cb1-1121"></a></span>
<span id="cb1-1122"><a href="#cb1-1122"></a>Here, $\Phi_{\pi_R}^{(s,a)}$ is the feature expectation vector under</span>
<span id="cb1-1123"><a href="#cb1-1123"></a>policy $\pi$, capturing the long-term feature visitation frequencies. We</span>
<span id="cb1-1124"><a href="#cb1-1124"></a>overload the action space notation to define the set of all optimal</span>
<span id="cb1-1125"><a href="#cb1-1125"></a>actions given a state as</span>
<span id="cb1-1126"><a href="#cb1-1126"></a></span>
<span id="cb1-1127"><a href="#cb1-1127"></a>$$\begin{aligned}</span>
<span id="cb1-1128"><a href="#cb1-1128"></a>    \mathcal{A}_R(s) = \underset{x}{\operatorname{argmax}} <span class="sc">\\</span> Q^{\pi^*}_R(s,a)</span>
<span id="cb1-1129"><a href="#cb1-1129"></a>\end{aligned}$$ where $\pi^*$ is an optimal policy. We can now define</span>
<span id="cb1-1130"><a href="#cb1-1130"></a>the aligned reward polytope (ARP). The ARP is the set of all weights</span>
<span id="cb1-1131"><a href="#cb1-1131"></a>$\mathcal{w}$ that satisfy the following set of strict linear</span>
<span id="cb1-1132"><a href="#cb1-1132"></a>inequalities, $\mathbf{w}^\top \mathbf{A}  &gt; \mathbf{0}$ where each row</span>
<span id="cb1-1133"><a href="#cb1-1133"></a>of $\mathbf{A}$ corresponds to</span>
<span id="cb1-1134"><a href="#cb1-1134"></a>$\Phi_{\pi^*_R}^{(s,a)} - \Phi_{\pi^*_R}^{(s,b)}$ for a single $(s,a,b)$</span>
<span id="cb1-1135"><a href="#cb1-1135"></a>tuple where</span>
<span id="cb1-1136"><a href="#cb1-1136"></a>$s \in \mathcal{S}, a \in \mathcal{A}_R(s), b \notin \mathcal{A}_R(s)$.</span>
<span id="cb1-1137"><a href="#cb1-1137"></a>Thus, to construct $\mathbf{A}$, one must loop over all $(s,a,b)$ tuples</span>
<span id="cb1-1138"><a href="#cb1-1138"></a>which has complexity</span>
<span id="cb1-1139"><a href="#cb1-1139"></a>$O(\vert \mathcal{S} \vert \cdot \vert \mathcal{A} \vert^2)$. This</span>
<span id="cb1-1140"><a href="#cb1-1140"></a>construction ensures that the weights $\mathbf{w}$ align with the</span>
<span id="cb1-1141"><a href="#cb1-1141"></a>human's optimal actions across all states.</span>
<span id="cb1-1142"><a href="#cb1-1142"></a></span>
<span id="cb1-1143"><a href="#cb1-1143"></a>The intuition behind the ARP is that we use the human optimal policy for</span>
<span id="cb1-1144"><a href="#cb1-1144"></a>each state to determine what actions are optimal and what are suboptimal</span>
<span id="cb1-1145"><a href="#cb1-1145"></a>at this state. Then, for every one of those combinations, we can place a</span>
<span id="cb1-1146"><a href="#cb1-1146"></a>linear inequality on the set of reward weights consistent with that</span>
<span id="cb1-1147"><a href="#cb1-1147"></a>optimal vs suboptimal action bifurcation. One of the key assumptions</span>
<span id="cb1-1148"><a href="#cb1-1148"></a>that let us do this is that we assume both the human and the robot act</span>
<span id="cb1-1149"><a href="#cb1-1149"></a>optimally according to their reward function. This is known as a</span>
<span id="cb1-1150"><a href="#cb1-1150"></a>*rationality assumption* and provides the link between actions and</span>
<span id="cb1-1151"><a href="#cb1-1151"></a>rewards that we need.</span>
<span id="cb1-1152"><a href="#cb1-1152"></a></span>
<span id="cb1-1153"><a href="#cb1-1153"></a>For illustration, consider a simple grid world environment. @fig-toy shows the</span>
<span id="cb1-1154"><a href="#cb1-1154"></a>optimal policy and the corresponding ARP. The optimal policy reveals</span>
<span id="cb1-1155"><a href="#cb1-1155"></a>that the gray state is less preferred compared to the white states,</span>
<span id="cb1-1156"><a href="#cb1-1156"></a>which is reflected in the ARP (hatched region of @fig-toy).</span>
<span id="cb1-1157"><a href="#cb1-1157"></a></span>
<span id="cb1-1158"><a href="#cb1-1158"></a>&lt;figure id="fig-toy"&gt;</span>
<span id="cb1-1159"><a href="#cb1-1159"></a>&lt;p&gt;&lt;img src="Figures/toy_policy.png" style="width:30.0%" alt="image" /&gt;</span>
<span id="cb1-1160"><a href="#cb1-1160"></a>&lt;img src="Figures/toy_arp.png" style="width:30.0%" alt="image" /&gt;&lt;/p&gt;</span>
<span id="cb1-1161"><a href="#cb1-1161"></a>&lt;figcaption&gt;Optimal policy (a) and aligned reward polytope (ARP) (b) for</span>
<span id="cb1-1162"><a href="#cb1-1162"></a>a grid world with two features (white and gray) and a linear reward</span>
<span id="cb1-1163"><a href="#cb1-1163"></a>function (&lt;span</span>
<span id="cb1-1164"><a href="#cb1-1164"></a>class="math inline"&gt;&lt;em&gt;R&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;) = &lt;em&gt;w&lt;/em&gt;&lt;sub&gt;0&lt;/sub&gt; ⋅ &lt;strong&gt;1&lt;/strong&gt;&lt;sub&gt;&lt;em&gt;w&lt;/em&gt;&lt;em&gt;h&lt;/em&gt;&lt;em&gt;i&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;/sub&gt;(&lt;em&gt;s&lt;/em&gt;) + &lt;em&gt;w&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt; ⋅ &lt;strong&gt;1&lt;/strong&gt;&lt;sub&gt;&lt;em&gt;g&lt;/em&gt;&lt;em&gt;r&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;y&lt;/em&gt;&lt;/sub&gt;(&lt;em&gt;s&lt;/em&gt;)&lt;/span&gt;).</span>
<span id="cb1-1165"><a href="#cb1-1165"></a>The ARP is denoted by the hatched region in (b). &lt;/figcaption&gt;</span>
<span id="cb1-1166"><a href="#cb1-1166"></a>&lt;/figure&gt;</span>
<span id="cb1-1167"><a href="#cb1-1167"></a></span>
<span id="cb1-1168"><a href="#cb1-1168"></a>Computing the ARP exactly can be computationally demanding or we may not</span>
<span id="cb1-1169"><a href="#cb1-1169"></a>have access to the robot's reward function. This section describes</span>
<span id="cb1-1170"><a href="#cb1-1170"></a>heuristics for testing value alignment in the case the robot's reward</span>
<span id="cb1-1171"><a href="#cb1-1171"></a>weights ($\mathbf{w^\prime}$) are unknown, but the robot's policy can be</span>
<span id="cb1-1172"><a href="#cb1-1172"></a>queried. Heuristics provide simplified methods to estimate value</span>
<span id="cb1-1173"><a href="#cb1-1173"></a>alignment without the need for exhaustive computations.</span>
<span id="cb1-1174"><a href="#cb1-1174"></a></span>
<span id="cb1-1175"><a href="#cb1-1175"></a>**ARP-blackbox:** The ARP black-box (ARP-bb) heuristic helps address the</span>
<span id="cb1-1176"><a href="#cb1-1176"></a>challenge of computing the ARP by allowing users to work with a</span>
<span id="cb1-1177"><a href="#cb1-1177"></a>simplified model. In this heuristic, the user first solves for the ARP</span>
<span id="cb1-1178"><a href="#cb1-1178"></a>and removes all redundant half-space constraints. For each remaining</span>
<span id="cb1-1179"><a href="#cb1-1179"></a>half-space constraint, the user queries the robot's action at the</span>
<span id="cb1-1180"><a href="#cb1-1180"></a>corresponding state. The intuition here is that states, where different</span>
<span id="cb1-1181"><a href="#cb1-1181"></a>actions are taken, reveal crucial information about the reward function.</span>
<span id="cb1-1182"><a href="#cb1-1182"></a>By focusing on these key states, we can gain insights into the robot's</span>
<span id="cb1-1183"><a href="#cb1-1183"></a>reward function without needing to know it explicitly.</span>
<span id="cb1-1184"><a href="#cb1-1184"></a></span>
<span id="cb1-1185"><a href="#cb1-1185"></a>**Set Cover Optimal Teaching:** The Set Cover Optimal Teaching (SCOT)</span>
<span id="cb1-1186"><a href="#cb1-1186"></a>heuristic uses techniques from <span class="co">[</span><span class="ot">@brown2019machine</span><span class="co">]</span> to generate maximally</span>
<span id="cb1-1187"><a href="#cb1-1187"></a>informative trajectories. These trajectories are sequences of states</span>
<span id="cb1-1188"><a href="#cb1-1188"></a>where the number of optimal actions is limited, making them particularly</span>
<span id="cb1-1189"><a href="#cb1-1189"></a>informative for understanding the robot's policy. By querying the robot</span>
<span id="cb1-1190"><a href="#cb1-1190"></a>for actions along these trajectories, we can efficiently gauge the</span>
<span id="cb1-1191"><a href="#cb1-1191"></a>alignment of the robot's policy. This method helps to identify potential</span>
<span id="cb1-1192"><a href="#cb1-1192"></a>misalignments by focusing on critical decision points in the</span>
<span id="cb1-1193"><a href="#cb1-1193"></a>trajectories.</span>
<span id="cb1-1194"><a href="#cb1-1194"></a></span>
<span id="cb1-1195"><a href="#cb1-1195"></a>**Critical States:** The Critical States (CS) heuristic identifies</span>
<span id="cb1-1196"><a href="#cb1-1196"></a>states where the gap in value between the optimal action and an average</span>
<span id="cb1-1197"><a href="#cb1-1197"></a>action is significant. These states are crucial because if the robot's</span>
<span id="cb1-1198"><a href="#cb1-1198"></a>policy is misaligned, the misalignment will be most consequential at</span>
<span id="cb1-1199"><a href="#cb1-1199"></a>these critical states. By querying the robot's policy at these states,</span>
<span id="cb1-1200"><a href="#cb1-1200"></a>we can assess the alignment more effectively. This heuristic is</span>
<span id="cb1-1201"><a href="#cb1-1201"></a>particularly useful when we have a limited budget of states to check, as</span>
<span id="cb1-1202"><a href="#cb1-1202"></a>it prioritizes the most informative states for evaluation.</span>
<span id="cb1-1203"><a href="#cb1-1203"></a></span>
<span id="cb1-1204"><a href="#cb1-1204"></a>**Practical Examples:** To illustrate the concepts of value alignment</span>
<span id="cb1-1205"><a href="#cb1-1205"></a>verification, we present an example of applying value alignment</span>
<span id="cb1-1206"><a href="#cb1-1206"></a>verification in a simple MDP grid world environment. Consider a grid</span>
<span id="cb1-1207"><a href="#cb1-1207"></a>world where the human's reward function is defined as</span>
<span id="cb1-1208"><a href="#cb1-1208"></a>$R(s) = 50 \cdot \mathbf{1}_{green}(s) - 1 \cdot \mathbf{1}_{white}(s) - 50 \cdot \mathbf{1}_{blue}(s)$,</span>
<span id="cb1-1209"><a href="#cb1-1209"></a>where $\mathbf{1}_{color}(s)$ is an indicator feature for the color of</span>
<span id="cb1-1210"><a href="#cb1-1210"></a>the grid cell. The objective is to align the robot's policy with this</span>
<span id="cb1-1211"><a href="#cb1-1211"></a>reward function.</span>
<span id="cb1-1212"><a href="#cb1-1212"></a></span>
<span id="cb1-1213"><a href="#cb1-1213"></a>&lt;figure id="fig-island"&gt;</span>
<span id="cb1-1214"><a href="#cb1-1214"></a>&lt;p&gt;&lt;img src="Figures/optimal.png" style="width:32.0%" alt="image" /&gt;</span>
<span id="cb1-1215"><a href="#cb1-1215"></a>&lt;img src="Figures/pref_0.png" style="width:32.0%" alt="image" /&gt; &lt;img</span>
<span id="cb1-1216"><a href="#cb1-1216"></a>src="Figures/pref_1.png" style="width:32.0%" alt="image" /&gt; &lt;img</span>
<span id="cb1-1217"><a href="#cb1-1217"></a>src="Figures/arb-bb.png" style="width:32.0%" alt="image" /&gt; &lt;img</span>
<span id="cb1-1218"><a href="#cb1-1218"></a>src="Figures/scot.png" style="width:32.0%" alt="image" /&gt; &lt;img</span>
<span id="cb1-1219"><a href="#cb1-1219"></a>src="Figures/cs-10.png" style="width:32.0%" alt="image" /&gt; &lt;span</span>
<span id="cb1-1220"><a href="#cb1-1220"></a>id="fig-island" data-label="fig-island"&gt;&lt;/span&gt;&lt;/p&gt;</span>
<span id="cb1-1221"><a href="#cb1-1221"></a>&lt;figcaption&gt;(a) optimal policy (b) preference query 1 (c) preference</span>
<span id="cb1-1222"><a href="#cb1-1222"></a>query 2 (d) ARP-bb queries (e) SCOT queries (f) CS queries. In the</span>
<span id="cb1-1223"><a href="#cb1-1223"></a>preference queries, the human reward model prefers black to</span>
<span id="cb1-1224"><a href="#cb1-1224"></a>orange.&lt;/figcaption&gt;</span>
<span id="cb1-1225"><a href="#cb1-1225"></a>&lt;/figure&gt;</span>
<span id="cb1-1226"><a href="#cb1-1226"></a></span>
<span id="cb1-1227"><a href="#cb1-1227"></a>@fig-island (a)</span>
<span id="cb1-1228"><a href="#cb1-1228"></a>shows all optimal actions at each state according to the human's reward</span>
<span id="cb1-1229"><a href="#cb1-1229"></a>function. This optimal policy serves as the benchmark for alignment</span>
<span id="cb1-1230"><a href="#cb1-1230"></a>verification. @fig-island (b) and @fig-island (c) show two pairwise preference trajectory queries</span>
<span id="cb1-1231"><a href="#cb1-1231"></a>(black is preferable to orange according to</span>
<span id="cb1-1232"><a href="#cb1-1232"></a>(<span class="co">[</span><span class="ot">\[eq: human_r\]</span><span class="co">](#eq: human_r)</span>{reference-type="ref"</span>
<span id="cb1-1233"><a href="#cb1-1233"></a>reference="eq: human_r"})). Preference query 1 verifies that the robot</span>
<span id="cb1-1234"><a href="#cb1-1234"></a>values reaching the terminal goal state (green) rather than visiting</span>
<span id="cb1-1235"><a href="#cb1-1235"></a>more white states. Preference query 2 verifies that the robot values</span>
<span id="cb1-1236"><a href="#cb1-1236"></a>white states more than blue states. These two preference queries are all</span>
<span id="cb1-1237"><a href="#cb1-1237"></a>we need to determine whether the robot's values are aligned with the</span>
<span id="cb1-1238"><a href="#cb1-1238"></a>human's values.</span>
<span id="cb1-1239"><a href="#cb1-1239"></a></span>
<span id="cb1-1240"><a href="#cb1-1240"></a>Next, we apply the heuristics discussed in the previous section to this</span>
<span id="cb1-1241"><a href="#cb1-1241"></a>grid world example. @fig-island (d), @fig-island (e), and @fig-island (f) show the action queries requested by the</span>
<span id="cb1-1242"><a href="#cb1-1242"></a>heuristics ARP-bb, SCOT, and CS. Each heuristic queries the robot's</span>
<span id="cb1-1243"><a href="#cb1-1243"></a>actions at specific states to assess alignment:</span>
<span id="cb1-1244"><a href="#cb1-1244"></a></span>
<span id="cb1-1245"><a href="#cb1-1245"></a><span class="ss">-   </span>**ARP-bb**: This heuristic queries the fewest states but is myopic.</span>
<span id="cb1-1246"><a href="#cb1-1246"></a>    It focuses on critical states derived from the ARP.</span>
<span id="cb1-1247"><a href="#cb1-1247"></a></span>
<span id="cb1-1248"><a href="#cb1-1248"></a><span class="ss">-   </span>**SCOT**: This heuristic generates maximally informative</span>
<span id="cb1-1249"><a href="#cb1-1249"></a>    trajectories, querying more states than necessary but providing a</span>
<span id="cb1-1250"><a href="#cb1-1250"></a>    comprehensive assessment.</span>
<span id="cb1-1251"><a href="#cb1-1251"></a></span>
<span id="cb1-1252"><a href="#cb1-1252"></a><span class="ss">-   </span>**CS**: This heuristic queries many redundant states, focusing on</span>
<span id="cb1-1253"><a href="#cb1-1253"></a>    those where the value gap between optimal and average actions is</span>
<span id="cb1-1254"><a href="#cb1-1254"></a>    significant.</span>
<span id="cb1-1255"><a href="#cb1-1255"></a></span>
<span id="cb1-1256"><a href="#cb1-1256"></a>To pass the test given by each heuristic, the robot's action at each of</span>
<span id="cb1-1257"><a href="#cb1-1257"></a>the queried states must be optimal under the human's reward function.</span>
<span id="cb1-1258"><a href="#cb1-1258"></a>The example demonstrates that while the ARP-bb heuristic is efficient,</span>
<span id="cb1-1259"><a href="#cb1-1259"></a>it might miss the broader context. SCOT provides a thorough assessment</span>
<span id="cb1-1260"><a href="#cb1-1260"></a>but at the cost of querying more states. CS focuses on high-impact</span>
<span id="cb1-1261"><a href="#cb1-1261"></a>states but includes redundant queries.</span>
<span id="cb1-1262"><a href="#cb1-1262"></a></span>
<span id="cb1-1263"><a href="#cb1-1263"></a>It is important to note that both the construction of the ARP and the</span>
<span id="cb1-1264"><a href="#cb1-1264"></a>heuristics rely on having an optimal policy for the human. Thus, in most</span>
<span id="cb1-1265"><a href="#cb1-1265"></a>practical settings we would simply use that policy on the robot without</span>
<span id="cb1-1266"><a href="#cb1-1266"></a>needing to bother with value alignment verification. As such, value</span>
<span id="cb1-1267"><a href="#cb1-1267"></a>alignment verification as presented here is more of an academic exercise</span>
<span id="cb1-1268"><a href="#cb1-1268"></a>rather than a tool of practical utility.</span>
<span id="cb1-1269"><a href="#cb1-1269"></a></span>
<span id="cb1-1270"><a href="#cb1-1270"></a><span class="fu">## Human-Centered Design</span></span>
<span id="cb1-1271"><a href="#cb1-1271"></a></span>
<span id="cb1-1272"><a href="#cb1-1272"></a>After understanding AI alignment, the next step is to explore practical</span>
<span id="cb1-1273"><a href="#cb1-1273"></a>methodologies for incorporating user feedback and ensuring that AI</span>
<span id="cb1-1274"><a href="#cb1-1274"></a>systems not only align with but also cater to the needs and preferences</span>
<span id="cb1-1275"><a href="#cb1-1275"></a>of their users. This section will provide insights into various</span>
<span id="cb1-1276"><a href="#cb1-1276"></a>Human-Centered Design techniques and their application in creating AI</span>
<span id="cb1-1277"><a href="#cb1-1277"></a>systems that are intuitive and ethically sound, ultimately enhancing the</span>
<span id="cb1-1278"><a href="#cb1-1278"></a>human-AI interaction experience.</span>
<span id="cb1-1279"><a href="#cb1-1279"></a></span>
<span id="cb1-1280"><a href="#cb1-1280"></a><span class="fu">### AI and Human-Computer Interaction</span></span>
<span id="cb1-1281"><a href="#cb1-1281"></a></span>
<span id="cb1-1282"><a href="#cb1-1282"></a>Human-Computer Interaction (HCI) is critical in the context of</span>
<span id="cb1-1283"><a href="#cb1-1283"></a>artificial intelligence because it focuses on designing systems that are</span>
<span id="cb1-1284"><a href="#cb1-1284"></a>intuitive and responsive to human needs. While human-robot interaction</span>
<span id="cb1-1285"><a href="#cb1-1285"></a>and other forms of human interaction with technology are important, HCI</span>
<span id="cb1-1286"><a href="#cb1-1286"></a>specifically addresses the broader and more common interfaces that</span>
<span id="cb1-1287"><a href="#cb1-1287"></a>people interact with daily. HCI principles ensure that AI systems are</span>
<span id="cb1-1288"><a href="#cb1-1288"></a>not only functional but also accessible and user-friendly, making them</span>
<span id="cb1-1289"><a href="#cb1-1289"></a>essential for the successful integration of AI into everyday life. By</span>
<span id="cb1-1290"><a href="#cb1-1290"></a>focusing on HCI, we can leverage established methodologies and insights</span>
<span id="cb1-1291"><a href="#cb1-1291"></a>to create AI systems that are more aligned with human values and needs.</span>
<span id="cb1-1292"><a href="#cb1-1292"></a></span>
<span id="cb1-1293"><a href="#cb1-1293"></a>At the heart of this exploration is the concept of human-in-the-loop</span>
<span id="cb1-1294"><a href="#cb1-1294"></a>processes. As AI systems become more sophisticated, their ability to</span>
<span id="cb1-1295"><a href="#cb1-1295"></a>simulate human decision-making processes and behaviors has increased,</span>
<span id="cb1-1296"><a href="#cb1-1296"></a>leading to innovative applications across various domains. The</span>
<span id="cb1-1297"><a href="#cb1-1297"></a>presentation by Meredith Morris, titled "Human-in-the-loop Computing:</span>
<span id="cb1-1298"><a href="#cb1-1298"></a>Reimagining Human-Computer Interaction in the Age of AI," shows work in</span>
<span id="cb1-1299"><a href="#cb1-1299"></a>the integration of human intelligence with AI capabilities</span>
<span id="cb1-1300"><a href="#cb1-1300"></a><span class="co">[</span><span class="ot">@Morris2019HITL</span><span class="co">]</span>. Projects like Soylent and LaMPost are highlighted as</span>
<span id="cb1-1301"><a href="#cb1-1301"></a>exemplary cases of this integration. Soylent is a Word plugin that uses</span>
<span id="cb1-1302"><a href="#cb1-1302"></a>human computation to help with editing tasks, while LaMPost is a</span>
<span id="cb1-1303"><a href="#cb1-1303"></a>platform that leverages crowd workers to aid in natural language</span>
<span id="cb1-1304"><a href="#cb1-1304"></a>processing tasks <span class="co">[</span><span class="ot">@bernstein2010soylent; @lamport2017lampost</span><span class="co">]</span>. These</span>
<span id="cb1-1305"><a href="#cb1-1305"></a>examples demonstrate how human input can significantly enhance AI</span>
<span id="cb1-1306"><a href="#cb1-1306"></a>outputs by leveraging the unique strengths of human cognition, thereby</span>
<span id="cb1-1307"><a href="#cb1-1307"></a>addressing complex AI problems that were previously unsolvable. For</span>
<span id="cb1-1308"><a href="#cb1-1308"></a>instance, Soylent can improve text quality by incorporating nuanced</span>
<span id="cb1-1309"><a href="#cb1-1309"></a>human feedback, and LaMPost can refine NLP tasks by incorporating human</span>
<span id="cb1-1310"><a href="#cb1-1310"></a>insights into language subtleties, both of which go beyond the</span>
<span id="cb1-1311"><a href="#cb1-1311"></a>capabilities of fully automated systems. However, the integration of</span>
<span id="cb1-1312"><a href="#cb1-1312"></a>human elements in AI systems brings up critical ethical considerations.</span>
<span id="cb1-1313"><a href="#cb1-1313"></a>The presentation discusses the changing perceptions of the ethics of</span>
<span id="cb1-1314"><a href="#cb1-1314"></a>human-in-the-loop processes. While the cost-effectiveness of human data</span>
<span id="cb1-1315"><a href="#cb1-1315"></a>labeling and other processes was once seen as beneficial, it is the</span>
<span id="cb1-1316"><a href="#cb1-1316"></a>ethical implications of such interactions that take precedence nowadays.</span>
<span id="cb1-1317"><a href="#cb1-1317"></a>This shift underscores the evolving norms in HCI and the importance of</span>
<span id="cb1-1318"><a href="#cb1-1318"></a>considering the ethical dimensions of human-AI interactions.</span>
<span id="cb1-1319"><a href="#cb1-1319"></a></span>
<span id="cb1-1320"><a href="#cb1-1320"></a>The role of diverse human perspectives plays a crucial role in enhancing</span>
<span id="cb1-1321"><a href="#cb1-1321"></a>AI systems. Involving a broad spectrum of users in the development and</span>
<span id="cb1-1322"><a href="#cb1-1322"></a>testing of AI systems ensures that these technologies are inclusive and</span>
<span id="cb1-1323"><a href="#cb1-1323"></a>representative of the global population, moving beyond the limitations</span>
<span id="cb1-1324"><a href="#cb1-1324"></a>of a WEIRD (Western, Educated, Industrialized, Rich, and Democratic)</span>
<span id="cb1-1325"><a href="#cb1-1325"></a>user base. The methodologies for collecting user feedback in HCI form a</span>
<span id="cb1-1326"><a href="#cb1-1326"></a>critical part of this discussion since they are vital in understanding</span>
<span id="cb1-1327"><a href="#cb1-1327"></a>user needs, preferences, and behaviors, which in turn inform the</span>
<span id="cb1-1328"><a href="#cb1-1328"></a>development of more user-centered AI systems. The presentation by</span>
<span id="cb1-1329"><a href="#cb1-1329"></a>Meredith Morris <span class="co">[</span><span class="ot">@Morris2019HITL</span><span class="co">]</span> also highlights how these methods can</span>
<span id="cb1-1330"><a href="#cb1-1330"></a>be effectively employed to gain insights from users to ensure that AI</span>
<span id="cb1-1331"><a href="#cb1-1331"></a>systems are aligned with the real-world needs and expectations of users.</span>
<span id="cb1-1332"><a href="#cb1-1332"></a>In HCI, collecting user feedback is a fraught problem. When interacting</span>
<span id="cb1-1333"><a href="#cb1-1333"></a>with AI systems, the typical end user simply cares about tasks that the</span>
<span id="cb1-1334"><a href="#cb1-1334"></a>system can perform. Thus, a key question in HCI for AI is finding and</span>
<span id="cb1-1335"><a href="#cb1-1335"></a>understanding these tasks. **Methodologies for collecting user feedback</span>
<span id="cb1-1336"><a href="#cb1-1336"></a>in HCI**, are described as follow:</span>
<span id="cb1-1337"><a href="#cb1-1337"></a></span>
<span id="cb1-1338"><a href="#cb1-1338"></a><span class="ss">-   </span>**Storyboarding** is a visual method used to predict and explore the</span>
<span id="cb1-1339"><a href="#cb1-1339"></a>    user experience with a product or service. A storyboard in HCI is</span>
<span id="cb1-1340"><a href="#cb1-1340"></a>    typically a sequence of drawings with annotations that represent a</span>
<span id="cb1-1341"><a href="#cb1-1341"></a>    user's interactions with technology. This technique is borrowed from</span>
<span id="cb1-1342"><a href="#cb1-1342"></a>    the film and animation industry and is used in HCI to convey a</span>
<span id="cb1-1343"><a href="#cb1-1343"></a>    sequence of events or user flows, including the user's actions,</span>
<span id="cb1-1344"><a href="#cb1-1344"></a>    reactions, and emotions.</span>
<span id="cb1-1345"><a href="#cb1-1345"></a></span>
<span id="cb1-1346"><a href="#cb1-1346"></a><span class="ss">-   </span>**Wizard of Oz Studies** is a method of user testing where</span>
<span id="cb1-1347"><a href="#cb1-1347"></a>    participants interact with a system they believe to be autonomous,</span>
<span id="cb1-1348"><a href="#cb1-1348"></a>    but which is actually being controlled or partially controlled by a</span>
<span id="cb1-1349"><a href="#cb1-1349"></a>    human 'wizard' behind the scenes. This technique allows researchers</span>
<span id="cb1-1350"><a href="#cb1-1350"></a>    to simulate the response of a system that may not yet be fully</span>
<span id="cb1-1351"><a href="#cb1-1351"></a>    functional or developed.</span>
<span id="cb1-1352"><a href="#cb1-1352"></a></span>
<span id="cb1-1353"><a href="#cb1-1353"></a>Both **Storyboarding** and **Wizard of Oz Studies** are effective for</span>
<span id="cb1-1354"><a href="#cb1-1354"></a>engaging with users early in the design process. They help deal with the</span>
<span id="cb1-1355"><a href="#cb1-1355"></a>problem of gathering feedback on a product that doesn't yet exist. Users</span>
<span id="cb1-1356"><a href="#cb1-1356"></a>often have difficulty imagining outcomes when they cannot touch a live</span>
<span id="cb1-1357"><a href="#cb1-1357"></a>demonstration.</span>
<span id="cb1-1358"><a href="#cb1-1358"></a></span>
<span id="cb1-1359"><a href="#cb1-1359"></a><span class="ss">-   </span>**Surveys** in HCI are structured tools that consist of a series of</span>
<span id="cb1-1360"><a href="#cb1-1360"></a>    questions designed to be answered by a large number of participants.</span>
<span id="cb1-1361"><a href="#cb1-1361"></a>    They can be conducted online, by telephone, through paper</span>
<span id="cb1-1362"><a href="#cb1-1362"></a>    questionnaires, or using computer-assisted methods. Surveys are</span>
<span id="cb1-1363"><a href="#cb1-1363"></a>    useful for collecting quantitative data from a broad audience, which</span>
<span id="cb1-1364"><a href="#cb1-1364"></a>    can be analyzed statistically.</span>
<span id="cb1-1365"><a href="#cb1-1365"></a></span>
<span id="cb1-1366"><a href="#cb1-1366"></a><span class="ss">-   </span>**Interviews** in HCI are more in-depth and involve direct, two-way</span>
<span id="cb1-1367"><a href="#cb1-1367"></a>    communication between the researcher and the participant. Interviews</span>
<span id="cb1-1368"><a href="#cb1-1368"></a>    can be structured, semi-structured, or unstructured, ranging from</span>
<span id="cb1-1369"><a href="#cb1-1369"></a>    tightly scripted question sets to open-ended conversations.</span>
<span id="cb1-1370"><a href="#cb1-1370"></a></span>
<span id="cb1-1371"><a href="#cb1-1371"></a><span class="ss">-   </span>**Focus Groups** involve a small group of participants discussing</span>
<span id="cb1-1372"><a href="#cb1-1372"></a>    their experiences and opinions about a system or design, often with</span>
<span id="cb1-1373"><a href="#cb1-1373"></a>    a moderator. Group dynamics can provide insights into collective</span>
<span id="cb1-1374"><a href="#cb1-1374"></a>    user perspectives. In particular, users can bounce ideas off each</span>
<span id="cb1-1375"><a href="#cb1-1375"></a>    other to provide richer feedback and quieter users who may not</span>
<span id="cb1-1376"><a href="#cb1-1376"></a>    otherwise provide feedback may be encouraged by their peers.</span>
<span id="cb1-1377"><a href="#cb1-1377"></a></span>
<span id="cb1-1378"><a href="#cb1-1378"></a><span class="ss">-   </span>**Community-Based Participatory Design (CBPD)** is a human-centered</span>
<span id="cb1-1379"><a href="#cb1-1379"></a>    approach that involves the people who will use a product in the</span>
<span id="cb1-1380"><a href="#cb1-1380"></a>    design and development process. With CBPD, designers work closely</span>
<span id="cb1-1381"><a href="#cb1-1381"></a>    with community members to identify problems, develop prototypes, and</span>
<span id="cb1-1382"><a href="#cb1-1382"></a>    iterate based on community feedback. For example, when building a</span>
<span id="cb1-1383"><a href="#cb1-1383"></a>    software product for deaf people, the engineering team can hire deaf</span>
<span id="cb1-1384"><a href="#cb1-1384"></a>    engineers or designers to provide feedback as they collaboratively</span>
<span id="cb1-1385"><a href="#cb1-1385"></a>    build the product.</span>
<span id="cb1-1386"><a href="#cb1-1386"></a></span>
<span id="cb1-1387"><a href="#cb1-1387"></a><span class="ss">-   </span>**Field Studies** involve observing and collecting data on how users</span>
<span id="cb1-1388"><a href="#cb1-1388"></a>    interact with a system in their natural environment. This method is</span>
<span id="cb1-1389"><a href="#cb1-1389"></a>    based on the premise that observing users in their context provides</span>
<span id="cb1-1390"><a href="#cb1-1390"></a>    a more accurate understanding of user behavior. It can include a</span>
<span id="cb1-1391"><a href="#cb1-1391"></a>    variety of techniques like ethnography, contextual inquiries, and</span>
<span id="cb1-1392"><a href="#cb1-1392"></a>    natural observations.</span>
<span id="cb1-1393"><a href="#cb1-1393"></a></span>
<span id="cb1-1394"><a href="#cb1-1394"></a><span class="ss">-   </span>**Lab-based studies** are conducted in a controlled environment</span>
<span id="cb1-1395"><a href="#cb1-1395"></a>    where the researchers can manipulate variables and observe user</span>
<span id="cb1-1396"><a href="#cb1-1396"></a>    behavior in a setting designed to minimize external influences.</span>
<span id="cb1-1397"><a href="#cb1-1397"></a>    Common lab-based methods include usability testing, controlled</span>
<span id="cb1-1398"><a href="#cb1-1398"></a>    experiments, and eye-tracking studies.</span>
<span id="cb1-1399"><a href="#cb1-1399"></a></span>
<span id="cb1-1400"><a href="#cb1-1400"></a><span class="ss">-   </span>**Diary Studies and Ethnography** in HCI are a research method where</span>
<span id="cb1-1401"><a href="#cb1-1401"></a>    participants are asked to keep a record of their interactions with a</span>
<span id="cb1-1402"><a href="#cb1-1402"></a>    system or product over a while. This log may include text, images,</span>
<span id="cb1-1403"><a href="#cb1-1403"></a>    and sometimes even audio or video recordings, depending on the</span>
<span id="cb1-1404"><a href="#cb1-1404"></a>    study's design. Participants typically document their activities,</span>
<span id="cb1-1405"><a href="#cb1-1405"></a>    thoughts, feelings, and frustrations as they occur in their natural</span>
<span id="cb1-1406"><a href="#cb1-1406"></a>    context.</span>
<span id="cb1-1407"><a href="#cb1-1407"></a></span>
<span id="cb1-1408"><a href="#cb1-1408"></a><span class="ss">-   </span>**Ethnography** is a qualitative research method that involves</span>
<span id="cb1-1409"><a href="#cb1-1409"></a>    observing and interacting with participants in their real-life</span>
<span id="cb1-1410"><a href="#cb1-1410"></a>    environment. Ethnographers aim to immerse themselves in the user</span>
<span id="cb1-1411"><a href="#cb1-1411"></a>    environment to get a deep understanding of the cultural, social, and</span>
<span id="cb1-1412"><a href="#cb1-1412"></a>    organizational contexts that shape technology use.</span>
<span id="cb1-1413"><a href="#cb1-1413"></a></span>
<span id="cb1-1414"><a href="#cb1-1414"></a>As we have explored various methodologies for collecting human feedback,</span>
<span id="cb1-1415"><a href="#cb1-1415"></a>it becomes evident that the role of human input is indispensable in</span>
<span id="cb1-1416"><a href="#cb1-1416"></a>shaping AI systems that are not only effective but also ethically sound</span>
<span id="cb1-1417"><a href="#cb1-1417"></a>and user-centric. In the next step, we will elaborate on how to design</span>
<span id="cb1-1418"><a href="#cb1-1418"></a>AI systems for positive human impact, examining how socially aware and</span>
<span id="cb1-1419"><a href="#cb1-1419"></a>human-centered approaches can be employed to ensure that AI technologies</span>
<span id="cb1-1420"><a href="#cb1-1420"></a>contribute meaningfully to society. This includes understanding how AI</span>
<span id="cb1-1421"><a href="#cb1-1421"></a>can be utilized to address real-world challenges and create tangible</span>
<span id="cb1-1422"><a href="#cb1-1422"></a>benefits for individuals and communities.</span>
<span id="cb1-1423"><a href="#cb1-1423"></a></span>
<span id="cb1-1424"><a href="#cb1-1424"></a><span class="fu">### Designing AI for Positive Human Impact</span></span>
<span id="cb1-1425"><a href="#cb1-1425"></a></span>
<span id="cb1-1426"><a href="#cb1-1426"></a>In the field of natural language processing (NLP), the primary focus has</span>
<span id="cb1-1427"><a href="#cb1-1427"></a>traditionally been on quantitative metrics such as performance</span>
<span id="cb1-1428"><a href="#cb1-1428"></a>benchmarks, accuracy, and computations. These metrics have long guided</span>
<span id="cb1-1429"><a href="#cb1-1429"></a>the development and evaluation of the technologies. However, as the</span>
<span id="cb1-1430"><a href="#cb1-1430"></a>field evolves and becomes increasingly intertwined with human</span>
<span id="cb1-1431"><a href="#cb1-1431"></a>interactions like the recent popularity of Large Language Models (LLMs),</span>
<span id="cb1-1432"><a href="#cb1-1432"></a>a paradigm shift is becoming increasingly necessary. For example, these</span>
<span id="cb1-1433"><a href="#cb1-1433"></a>LLMs are shown to produce unethical or harmful responses or reflect</span>
<span id="cb1-1434"><a href="#cb1-1434"></a>values that only represent a certain group of people. The need for a</span>
<span id="cb1-1435"><a href="#cb1-1435"></a>human-centered approach in NLP development is crucial as these models</span>
<span id="cb1-1436"><a href="#cb1-1436"></a>are much more likely to be utilized in a broad spectrum of human-centric</span>
<span id="cb1-1437"><a href="#cb1-1437"></a>applications, impacting various aspects of daily life. This shift calls</span>
<span id="cb1-1438"><a href="#cb1-1438"></a>for an inclusive framework where LLMs are not only optimized for</span>
<span id="cb1-1439"><a href="#cb1-1439"></a>efficiency and accuracy but are also sensitized to ethical, cultural,</span>
<span id="cb1-1440"><a href="#cb1-1440"></a>and societal contexts. Integrating a human-centered perspective ensures</span>
<span id="cb1-1441"><a href="#cb1-1441"></a>that these models are developed with a deep understanding of, and</span>
<span id="cb1-1442"><a href="#cb1-1442"></a>respect for, the diversity and complexity of human values and social</span>
<span id="cb1-1443"><a href="#cb1-1443"></a>norms. This approach goes beyond merely preventing harmful outcomes; it</span>
<span id="cb1-1444"><a href="#cb1-1444"></a>also focuses on enhancing the positive impact of NLP technologies on</span>
<span id="cb1-1445"><a href="#cb1-1445"></a>society. In this session, we explore the intricacies of a human-centered</span>
<span id="cb1-1446"><a href="#cb1-1446"></a>approach in NLP development, focusing on three key themes: Socially</span>
<span id="cb1-1447"><a href="#cb1-1447"></a>Aware, Human-Centered, and Positively Impactful.</span>
<span id="cb1-1448"><a href="#cb1-1448"></a></span>
<span id="cb1-1449"><a href="#cb1-1449"></a><span class="fu">#### Socially Aware</span></span>
<span id="cb1-1450"><a href="#cb1-1450"></a></span>
<span id="cb1-1451"><a href="#cb1-1451"></a>In the exploration of socially aware NLP,  <span class="co">[</span><span class="ot">@hovy-yang-2021-importance</span><span class="co">]</span></span>
<span id="cb1-1452"><a href="#cb1-1452"></a>presents a comprehensive taxonomy of seven social factors grounded in</span>
<span id="cb1-1453"><a href="#cb1-1453"></a>linguistic theory (See @fig-taxonomy).</span>
<span id="cb1-1454"><a href="#cb1-1454"></a></span>
<span id="cb1-1455"><a href="#cb1-1455"></a><span class="al">![Taxonomy of social factors](Figures/seven-taxonomy.png)</span>{#fig-taxonomy</span>
<span id="cb1-1456"><a href="#cb1-1456"></a>width="30%"}</span>
<span id="cb1-1457"><a href="#cb1-1457"></a></span>
<span id="cb1-1458"><a href="#cb1-1458"></a>This taxonomy illustrates both the current limitations and progressions</span>
<span id="cb1-1459"><a href="#cb1-1459"></a>in NLP as they pertain to each of these factors. The primary aim is to</span>
<span id="cb1-1460"><a href="#cb1-1460"></a>motivate the NLP community to integrate these social factors more</span>
<span id="cb1-1461"><a href="#cb1-1461"></a>effectively, thereby advancing towards a level of language understanding</span>
<span id="cb1-1462"><a href="#cb1-1462"></a>that more closely resembles human capabilities. The characteristics of</span>
<span id="cb1-1463"><a href="#cb1-1463"></a>speakers, encompassing variables such as age, gender, ethnicity, social</span>
<span id="cb1-1464"><a href="#cb1-1464"></a>class, and dialect, play a crucial role in language processing. Certain</span>
<span id="cb1-1465"><a href="#cb1-1465"></a>languages or dialects, often categorized as low-resource, are spoken by</span>
<span id="cb1-1466"><a href="#cb1-1466"></a>vulnerable populations that require special consideration in NLP</span>
<span id="cb1-1467"><a href="#cb1-1467"></a>systems. In many cases, the dominant culture and values are</span>
<span id="cb1-1468"><a href="#cb1-1468"></a>over-represented, leading to an inadvertent marginalization of minority</span>
<span id="cb1-1469"><a href="#cb1-1469"></a>perspectives. These minority voices must be not only recognized but also</span>
<span id="cb1-1470"><a href="#cb1-1470"></a>given equitable representation in language models. Additionally, norms</span>
<span id="cb1-1471"><a href="#cb1-1471"></a>and context are vital components in understanding linguistic behavior.</span>
<span id="cb1-1472"><a href="#cb1-1472"></a>They dictate the appropriateness of language use in various social</span>
<span id="cb1-1473"><a href="#cb1-1473"></a>situations and settings. Recognizing and adapting to these norms is a</span>
<span id="cb1-1474"><a href="#cb1-1474"></a>critical aspect of developing socially aware NLP systems that can</span>
<span id="cb1-1475"><a href="#cb1-1475"></a>effectively function across diverse social environments.</span>
<span id="cb1-1476"><a href="#cb1-1476"></a></span>
<span id="cb1-1477"><a href="#cb1-1477"></a><span class="fu">#### Human-Centered</span></span>
<span id="cb1-1478"><a href="#cb1-1478"></a></span>
<span id="cb1-1479"><a href="#cb1-1479"></a>The Human-Centered aspect of NLP development emphasizes the creation of</span>
<span id="cb1-1480"><a href="#cb1-1480"></a>language models that prioritize the needs, preferences, and well-being</span>
<span id="cb1-1481"><a href="#cb1-1481"></a>of human users. This involves integrating human-centered design</span>
<span id="cb1-1482"><a href="#cb1-1482"></a>principles throughout the development stages of LLMs, which are</span>
<span id="cb1-1483"><a href="#cb1-1483"></a>described as follows:</span>
<span id="cb1-1484"><a href="#cb1-1484"></a></span>
<span id="cb1-1485"><a href="#cb1-1485"></a><span class="ss">-   </span>**Task Formulation stage:** Human-centered NLP development begins</span>
<span id="cb1-1486"><a href="#cb1-1486"></a>    with understanding the specific problems and contexts in which users</span>
<span id="cb1-1487"><a href="#cb1-1487"></a>    operate. This involves collaborating with end-users to identify</span>
<span id="cb1-1488"><a href="#cb1-1488"></a>    their needs and challenges, ensuring that the tasks addressed by the</span>
<span id="cb1-1489"><a href="#cb1-1489"></a>    models are relevant and meaningful to them. By engaging with users</span>
<span id="cb1-1490"><a href="#cb1-1490"></a>    early in the process, developers can create models that are not only</span>
<span id="cb1-1491"><a href="#cb1-1491"></a>    technically robust but also practically useful.</span>
<span id="cb1-1492"><a href="#cb1-1492"></a></span>
<span id="cb1-1493"><a href="#cb1-1493"></a><span class="ss">-   </span>**Data Collection stage:** Human-centered principles ensure that the</span>
<span id="cb1-1494"><a href="#cb1-1494"></a>    data used to train models is representative of the diverse user</span>
<span id="cb1-1495"><a href="#cb1-1495"></a>    population. This includes collecting data from various demographic</span>
<span id="cb1-1496"><a href="#cb1-1496"></a>    groups, languages, and cultural contexts to avoid biases that could</span>
<span id="cb1-1497"><a href="#cb1-1497"></a>    lead to unfair or harmful outcomes. Ethical considerations are</span>
<span id="cb1-1498"><a href="#cb1-1498"></a>    paramount, ensuring that data is collected with informed consent and</span>
<span id="cb1-1499"><a href="#cb1-1499"></a>    respecting users' privacy.</span>
<span id="cb1-1500"><a href="#cb1-1500"></a></span>
<span id="cb1-1501"><a href="#cb1-1501"></a><span class="ss">-   </span>**Data Processing** in a human-centered approach involves carefully</span>
<span id="cb1-1502"><a href="#cb1-1502"></a>    curating and annotating data to reflect the nuances of human</span>
<span id="cb1-1503"><a href="#cb1-1503"></a>    language and behavior. This step includes filtering out potentially</span>
<span id="cb1-1504"><a href="#cb1-1504"></a>    harmful content, addressing imbalances in the data, and ensuring</span>
<span id="cb1-1505"><a href="#cb1-1505"></a>    that the labels and annotations are accurate and meaningful. By</span>
<span id="cb1-1506"><a href="#cb1-1506"></a>    involving human annotators from diverse backgrounds, developers can</span>
<span id="cb1-1507"><a href="#cb1-1507"></a>    capture a wider range of perspectives and reduce the risk of biased</span>
<span id="cb1-1508"><a href="#cb1-1508"></a>    outputs.</span>
<span id="cb1-1509"><a href="#cb1-1509"></a></span>
<span id="cb1-1510"><a href="#cb1-1510"></a><span class="ss">-   </span>**Model Training** with a human-centered focus involves</span>
<span id="cb1-1511"><a href="#cb1-1511"></a>    incorporating feedback from users and domain experts to fine-tune</span>
<span id="cb1-1512"><a href="#cb1-1512"></a>    the models. This iterative process ensures that the models remain</span>
<span id="cb1-1513"><a href="#cb1-1513"></a>    aligned with users' needs and preferences. Techniques such as active</span>
<span id="cb1-1514"><a href="#cb1-1514"></a>    learning, where the model queries users for the most informative</span>
<span id="cb1-1515"><a href="#cb1-1515"></a>    examples, can be employed to improve the model's performance.</span>
<span id="cb1-1516"><a href="#cb1-1516"></a></span>
<span id="cb1-1517"><a href="#cb1-1517"></a><span class="ss">-   </span>**Model Evaluation** in a human-centered framework goes beyond</span>
<span id="cb1-1518"><a href="#cb1-1518"></a>    traditional metrics like accuracy and F1-score. It includes</span>
<span id="cb1-1519"><a href="#cb1-1519"></a>    assessing the model's impact on users, its fairness, and its ability</span>
<span id="cb1-1520"><a href="#cb1-1520"></a>    to handle real-world scenarios. User studies and A/B testing can</span>
<span id="cb1-1521"><a href="#cb1-1521"></a>    provide valuable insights into how the model performs in practice</span>
<span id="cb1-1522"><a href="#cb1-1522"></a>    and how it affects users' experiences.</span>
<span id="cb1-1523"><a href="#cb1-1523"></a></span>
<span id="cb1-1524"><a href="#cb1-1524"></a><span class="ss">-   </span>**Deployment** of human-centered NLP models involves continuous</span>
<span id="cb1-1525"><a href="#cb1-1525"></a>    monitoring and feedback loops to ensure that the models remain</span>
<span id="cb1-1526"><a href="#cb1-1526"></a>    effective and aligned with users' needs over time. This includes</span>
<span id="cb1-1527"><a href="#cb1-1527"></a>    setting up mechanisms for users to report issues and provide</span>
<span id="cb1-1528"><a href="#cb1-1528"></a>    feedback, which can then be used to update and improve the models.</span>
<span id="cb1-1529"><a href="#cb1-1529"></a>    Ensuring transparency in how the models operate and how user data is</span>
<span id="cb1-1530"><a href="#cb1-1530"></a>    used also fosters trust and acceptance among users.</span>
<span id="cb1-1531"><a href="#cb1-1531"></a></span>
<span id="cb1-1532"><a href="#cb1-1532"></a><span class="fu">#### Positively Impactful</span></span>
<span id="cb1-1533"><a href="#cb1-1533"></a></span>
<span id="cb1-1534"><a href="#cb1-1534"></a>Building on the human-centered approach, it is crucial to consider how</span>
<span id="cb1-1535"><a href="#cb1-1535"></a>language models can be utilized and the broader impacts they can have on</span>
<span id="cb1-1536"><a href="#cb1-1536"></a>society.</span>
<span id="cb1-1537"><a href="#cb1-1537"></a></span>
<span id="cb1-1538"><a href="#cb1-1538"></a>**Utilization:** LLMs offer socially beneficial applications across</span>
<span id="cb1-1539"><a href="#cb1-1539"></a>various domains such as public policy, mental health, and education. In</span>
<span id="cb1-1540"><a href="#cb1-1540"></a>public policy, they assist in analyzing large volumes of data to inform</span>
<span id="cb1-1541"><a href="#cb1-1541"></a>decision-making processes. In mental health, LLMs can provide</span>
<span id="cb1-1542"><a href="#cb1-1542"></a>personalized therapy and even train therapists by simulating patient</span>
<span id="cb1-1543"><a href="#cb1-1543"></a>interactions. In the education sector, they enable personalized learning</span>
<span id="cb1-1544"><a href="#cb1-1544"></a>experiences and language assistance, making education more accessible</span>
<span id="cb1-1545"><a href="#cb1-1545"></a>and effective. These examples demonstrate the versatility of LLMs in</span>
<span id="cb1-1546"><a href="#cb1-1546"></a>contributing positively to critical areas of human life.</span>
<span id="cb1-1547"><a href="#cb1-1547"></a></span>
<span id="cb1-1548"><a href="#cb1-1548"></a>**Impact:** The deployment of NLP models, especially LLMs, has</span>
<span id="cb1-1549"><a href="#cb1-1549"></a>significant societal impacts. Positively, they enhance human</span>
<span id="cb1-1550"><a href="#cb1-1550"></a>productivity and creativity, offering tools and insights that streamline</span>
<span id="cb1-1551"><a href="#cb1-1551"></a>processes and foster innovative thinking. LLMs serve as powerful aids in</span>
<span id="cb1-1552"><a href="#cb1-1552"></a>various sectors, from education to industry, enhancing efficiency and</span>
<span id="cb1-1553"><a href="#cb1-1553"></a>enabling new forms of expression and problem-solving. it is essential to</span>
<span id="cb1-1554"><a href="#cb1-1554"></a>acknowledge the potential negative impacts. One major concern is the</span>
<span id="cb1-1555"><a href="#cb1-1555"></a>ability of LLMs to generate and spread misinformation. As these models</span>
<span id="cb1-1556"><a href="#cb1-1556"></a>become more adept at producing human-like text, distinguishing between</span>
<span id="cb1-1557"><a href="#cb1-1557"></a>AI-generated and human-created content becomes increasingly challenging.</span>
<span id="cb1-1558"><a href="#cb1-1558"></a>This raises issues of trust and reliability, with the risk of widespread</span>
<span id="cb1-1559"><a href="#cb1-1559"></a>dissemination of false or misleading information, which could have</span>
<span id="cb1-1560"><a href="#cb1-1560"></a>significant adverse effects on individuals and society.</span>
<span id="cb1-1561"><a href="#cb1-1561"></a></span>
<span id="cb1-1562"><a href="#cb1-1562"></a>By considering both the utilization and impact of LLMs, we can better</span>
<span id="cb1-1563"><a href="#cb1-1563"></a>harness their potential for positive societal contributions while</span>
<span id="cb1-1564"><a href="#cb1-1564"></a>mitigating the risks associated with their deployment. In conclusion, by</span>
<span id="cb1-1565"><a href="#cb1-1565"></a>thoughtfully integrating human-centered principles and ensuring positive</span>
<span id="cb1-1566"><a href="#cb1-1566"></a>impacts through feedback collection and ethical considerations, we can</span>
<span id="cb1-1567"><a href="#cb1-1567"></a>develop language models that not only enhance human well-being but also</span>
<span id="cb1-1568"><a href="#cb1-1568"></a>align closely with societal values. Building on these foundational</span>
<span id="cb1-1569"><a href="#cb1-1569"></a>principles, we now turn our attention to Adaptive User Interfaces, which</span>
<span id="cb1-1570"><a href="#cb1-1570"></a>exemplify the practical application of these concepts by personalizing</span>
<span id="cb1-1571"><a href="#cb1-1571"></a>interactions and improving user experiences in dynamic environments.</span>
<span id="cb1-1572"><a href="#cb1-1572"></a></span>
<span id="cb1-1573"><a href="#cb1-1573"></a><span class="fu">### Adaptive User Interfaces</span></span>
<span id="cb1-1574"><a href="#cb1-1574"></a></span>
<span id="cb1-1575"><a href="#cb1-1575"></a>Adaptive user interfaces (AUIs) represent a significant advancement in</span>
<span id="cb1-1576"><a href="#cb1-1576"></a>personalizing user experiences by learning and adapting to individual</span>
<span id="cb1-1577"><a href="#cb1-1577"></a>preferences. This section will discuss the methodologies and</span>
<span id="cb1-1578"><a href="#cb1-1578"></a>applications of AUIs, highlighting their role in enhancing human-AI</span>
<span id="cb1-1579"><a href="#cb1-1579"></a>interaction through intelligent adaptation. The integration of AUIs</span>
<span id="cb1-1580"><a href="#cb1-1580"></a>within human-centered design paradigms ensures that AI systems not only</span>
<span id="cb1-1581"><a href="#cb1-1581"></a>meet user needs but also anticipate and adapt to their evolving</span>
<span id="cb1-1582"><a href="#cb1-1582"></a>preferences, thus maximizing positive human impact. Nowadays, consumers</span>
<span id="cb1-1583"><a href="#cb1-1583"></a>have more choices than ever and the need for personalized and</span>
<span id="cb1-1584"><a href="#cb1-1584"></a>intelligent assistance to make sense of the vast amount of information</span>
<span id="cb1-1585"><a href="#cb1-1585"></a>presented to them is clear.</span>
<span id="cb1-1586"><a href="#cb1-1586"></a></span>
<span id="cb1-1587"><a href="#cb1-1587"></a><span class="fu">#### Key ideas</span></span>
<span id="cb1-1588"><a href="#cb1-1588"></a></span>
<span id="cb1-1589"><a href="#cb1-1589"></a>In general, personalized recommendation systems require a model or</span>
<span id="cb1-1590"><a href="#cb1-1590"></a>profile of the user. We categorize modeling approaches into four groups.</span>
<span id="cb1-1591"><a href="#cb1-1591"></a></span>
<span id="cb1-1592"><a href="#cb1-1592"></a><span class="ss">1.  </span>User-created profiles (usually done manually).</span>
<span id="cb1-1593"><a href="#cb1-1593"></a></span>
<span id="cb1-1594"><a href="#cb1-1594"></a><span class="ss">2.  </span>Manually defined groups that each user is classified</span>
<span id="cb1-1595"><a href="#cb1-1595"></a>    into.</span>
<span id="cb1-1596"><a href="#cb1-1596"></a></span>
<span id="cb1-1597"><a href="#cb1-1597"></a><span class="ss">3.  </span>Automatically learned groups that each user is</span>
<span id="cb1-1598"><a href="#cb1-1598"></a>    classified into.</span>
<span id="cb1-1599"><a href="#cb1-1599"></a></span>
<span id="cb1-1600"><a href="#cb1-1600"></a><span class="ss">4.  </span>Adaptively learned individual user models from interactions with the</span>
<span id="cb1-1601"><a href="#cb1-1601"></a>    recommendation system.</span>
<span id="cb1-1602"><a href="#cb1-1602"></a></span>
<span id="cb1-1603"><a href="#cb1-1603"></a>The last approach is referred to as *adaptive user interfaces*. This</span>
<span id="cb1-1604"><a href="#cb1-1604"></a>approach promises that each user is given the most personalization</span>
<span id="cb1-1605"><a href="#cb1-1605"></a>possible, leading to better outcomes. In this session, we discuss</span>
<span id="cb1-1606"><a href="#cb1-1606"></a>recommendation systems that adaptively learn an individual's preferences</span>
<span id="cb1-1607"><a href="#cb1-1607"></a>and use that knowledge to intelligently recommend choices that the</span>
<span id="cb1-1608"><a href="#cb1-1608"></a>individual is more inclined to like.</span>
<span id="cb1-1609"><a href="#cb1-1609"></a></span>
<span id="cb1-1610"><a href="#cb1-1610"></a>The problem of learning individual models can be formalized as follows: </span>
<span id="cb1-1611"><a href="#cb1-1611"></a>a set of tasks requiring a user decision, a description for each task, </span>
<span id="cb1-1612"><a href="#cb1-1612"></a>and a history of the user's decision on each task. This allows us to </span>
<span id="cb1-1613"><a href="#cb1-1613"></a>find a function that maps from task descriptions (features) to user </span>
<span id="cb1-1614"><a href="#cb1-1614"></a>decisions. Tasks can be described using crowd-sourced data (a </span>
<span id="cb1-1615"><a href="#cb1-1615"></a>collaborative approach) or measurable features of the task (a content-based </span>
<span id="cb1-1616"><a href="#cb1-1616"></a>approach). This session will focus on content-based approaches </span>
<span id="cb1-1617"><a href="#cb1-1617"></a>for describing tasks. After understanding the framework for adaptive </span>
<span id="cb1-1618"><a href="#cb1-1618"></a>user interfaces, it is useful to provide example applications to </span>
<span id="cb1-1619"><a href="#cb1-1619"></a>ground future discussions. Adaptive user interfaces have been developed </span>
<span id="cb1-1620"><a href="#cb1-1620"></a>for command and form completion, email filtering and filing, news </span>
<span id="cb1-1621"><a href="#cb1-1621"></a>selection and layout, browsing the internet, selecting movies and </span>
<span id="cb1-1622"><a href="#cb1-1622"></a>TV shows, online shopping, in-car navigation, interactive scheduling, </span>
<span id="cb1-1623"><a href="#cb1-1623"></a>and dialogue systems, among many other applications.</span>
<span id="cb1-1624"><a href="#cb1-1624"></a></span>
<span id="cb1-1625"><a href="#cb1-1625"></a><span class="fu">#### Design</span></span>
<span id="cb1-1626"><a href="#cb1-1626"></a></span>
<span id="cb1-1627"><a href="#cb1-1627"></a>The goal of an adaptive user interface is to create a software tool that</span>
<span id="cb1-1628"><a href="#cb1-1628"></a>reduces human effort by acquiring a user model based on past user</span>
<span id="cb1-1629"><a href="#cb1-1629"></a>interactions. This is analogous to the goal of machine learning (ML)</span>
<span id="cb1-1630"><a href="#cb1-1630"></a>which is to create a software tool that improves some task performance</span>
<span id="cb1-1631"><a href="#cb1-1631"></a>by acquiring knowledge based on partial task experience. The design of</span>
<span id="cb1-1632"><a href="#cb1-1632"></a>an adaptive user interface can be broken up into six steps:</span>
<span id="cb1-1633"><a href="#cb1-1633"></a></span>
<span id="cb1-1634"><a href="#cb1-1634"></a><span class="ss">1.  </span>**Formulating the Problem:** Given some task that an intelligent</span>
<span id="cb1-1635"><a href="#cb1-1635"></a>    system could aid, the goal is to find a formulation that lets the</span>
<span id="cb1-1636"><a href="#cb1-1636"></a>    assistant improve its performance over time by learning from</span>
<span id="cb1-1637"><a href="#cb1-1637"></a>    interactions with a user. In this step the designer has to make</span>
<span id="cb1-1638"><a href="#cb1-1638"></a>    design choices about what aspect of user behavior is predicted, and</span>
<span id="cb1-1639"><a href="#cb1-1639"></a>    what is the proper level of granularity for description (i.e. what</span>
<span id="cb1-1640"><a href="#cb1-1640"></a>    is a training example). This step usually involves formulating the</span>
<span id="cb1-1641"><a href="#cb1-1641"></a>    problem into some sort of supervised learning framework.</span>
<span id="cb1-1642"><a href="#cb1-1642"></a></span>
<span id="cb1-1643"><a href="#cb1-1643"></a><span class="ss">2.  </span>**Engineering the Representation:** At this stage we have a</span>
<span id="cb1-1644"><a href="#cb1-1644"></a>    formulation of a task in ML terms and we need to represent the</span>
<span id="cb1-1645"><a href="#cb1-1645"></a>    behavior and user model in such a way that makes computational</span>
<span id="cb1-1646"><a href="#cb1-1646"></a>    learning not only tractable but as easy as possible. In this step,</span>
<span id="cb1-1647"><a href="#cb1-1647"></a>    the designer has to make design choices about what information is</span>
<span id="cb1-1648"><a href="#cb1-1648"></a>    used to make predictions, and how that information is encoded and</span>
<span id="cb1-1649"><a href="#cb1-1649"></a>    passed to the model.</span>
<span id="cb1-1650"><a href="#cb1-1650"></a></span>
<span id="cb1-1651"><a href="#cb1-1651"></a><span class="ss">3.  </span>**Collecting User Traces:** In this third step the goal is to find</span>
<span id="cb1-1652"><a href="#cb1-1652"></a>    an effective way to collect traces (samples) of user behavior. The</span>
<span id="cb1-1653"><a href="#cb1-1653"></a>    designer must choose how to translate traces into training data and</span>
<span id="cb1-1654"><a href="#cb1-1654"></a>    also how to elicit traces from a user. An ideal adaptive user</span>
<span id="cb1-1655"><a href="#cb1-1655"></a>    interface places no extra effort on the user to collect such traces.</span>
<span id="cb1-1656"><a href="#cb1-1656"></a></span>
<span id="cb1-1657"><a href="#cb1-1657"></a><span class="ss">4.  </span>**Modeling the User:** In this step the designer must decide what</span>
<span id="cb1-1658"><a href="#cb1-1658"></a>    model class to use (neural network, decision tree, graphical model,</span>
<span id="cb1-1659"><a href="#cb1-1659"></a>    etc.) and how to train the model (optimizer, step size, batch size,</span>
<span id="cb1-1660"><a href="#cb1-1660"></a>    etc.). This step in the design process is usually given too much</span>
<span id="cb1-1661"><a href="#cb1-1661"></a>    importance in academia. It is quite often the case that the success</span>
<span id="cb1-1662"><a href="#cb1-1662"></a>    of an adaptive user interface is more sensitive to the other design</span>
<span id="cb1-1663"><a href="#cb1-1663"></a>    steps.</span>
<span id="cb1-1664"><a href="#cb1-1664"></a></span>
<span id="cb1-1665"><a href="#cb1-1665"></a><span class="ss">5.  </span>**Using the Model Effectively:** At this stage the designer must</span>
<span id="cb1-1666"><a href="#cb1-1666"></a>    decide how the model will be integrated into a software tool.</span>
<span id="cb1-1667"><a href="#cb1-1667"></a>    Specifically, when and how is the model evaluated and how is the</span>
<span id="cb1-1668"><a href="#cb1-1668"></a>    output of the model presented to the user? In addition, the designer</span>
<span id="cb1-1669"><a href="#cb1-1669"></a>    must consider how to handle situations in which the model</span>
<span id="cb1-1670"><a href="#cb1-1670"></a>    predictions are wrong. An ideal adaptive user interface will let the</span>
<span id="cb1-1671"><a href="#cb1-1671"></a>    user take advantage of good predictions and ignore bad ones.</span>
<span id="cb1-1672"><a href="#cb1-1672"></a></span>
<span id="cb1-1673"><a href="#cb1-1673"></a><span class="ss">6.  </span>**Gaining User Acceptance:** The final step in the design process is</span>
<span id="cb1-1674"><a href="#cb1-1674"></a>    to get users to try the system and ultimately adopt it. The initial</span>
<span id="cb1-1675"><a href="#cb1-1675"></a>    attraction of users is often a marketing problem, but to retain</span>
<span id="cb1-1676"><a href="#cb1-1676"></a>    users the system must be well-designed and easy to use.</span>
<span id="cb1-1677"><a href="#cb1-1677"></a></span>
<span id="cb1-1678"><a href="#cb1-1678"></a><span class="fu">#### Applications</span></span>
<span id="cb1-1679"><a href="#cb1-1679"></a></span>
<span id="cb1-1680"><a href="#cb1-1680"></a>After understanding the design of Adaptive User Interfaces, let's take a</span>
<span id="cb1-1681"><a href="#cb1-1681"></a>look at how we can apply it to real-world problems. We will summarize</span>
<span id="cb1-1682"><a href="#cb1-1682"></a>and analyze three different application areas of learning human</span>
<span id="cb1-1683"><a href="#cb1-1683"></a>preferences, which are driving route advisor <span class="co">[</span><span class="ot">@rogers1999adaptive</span><span class="co">]</span>,</span>
<span id="cb1-1684"><a href="#cb1-1684"></a>destination selection <span class="co">[</span><span class="ot">@langley1999adaptive</span><span class="co">]</span>, and resource scheduling</span>
<span id="cb1-1685"><a href="#cb1-1685"></a><span class="co">[</span><span class="ot">@gervasio1999learning</span><span class="co">]</span>.</span>
<span id="cb1-1686"><a href="#cb1-1686"></a></span>
<span id="cb1-1687"><a href="#cb1-1687"></a>**1. Driving Route Advisor:** The task of route selection involves</span>
<span id="cb1-1688"><a href="#cb1-1688"></a>determining a desirable path for a driver to take from their current</span>
<span id="cb1-1689"><a href="#cb1-1689"></a>location to a chosen destination, given the knowledge of available roads</span>
<span id="cb1-1690"><a href="#cb1-1690"></a>from a digital map. While computational route advisors exist in rental</span>
<span id="cb1-1691"><a href="#cb1-1691"></a>cars and online, they cannot personalize individual drivers'</span>
<span id="cb1-1692"><a href="#cb1-1692"></a>preferences, which is a gap that adaptive user interfaces aim to fill by</span>
<span id="cb1-1693"><a href="#cb1-1693"></a>learning and recommending routes tailored to the driver's unique choices</span>
<span id="cb1-1694"><a href="#cb1-1694"></a>and behaviors.</span>
<span id="cb1-1695"><a href="#cb1-1695"></a></span>
<span id="cb1-1696"><a href="#cb1-1696"></a>Here is an approach to route selection through learning individual</span>
<span id="cb1-1697"><a href="#cb1-1697"></a>drivers' route preferences.</span>
<span id="cb1-1698"><a href="#cb1-1698"></a></span>
<span id="cb1-1699"><a href="#cb1-1699"></a><span class="ss">-   </span>Formulation: Learn a "subjective" function to evaluate entire</span>
<span id="cb1-1700"><a href="#cb1-1700"></a>    routes.</span>
<span id="cb1-1701"><a href="#cb1-1701"></a></span>
<span id="cb1-1702"><a href="#cb1-1702"></a><span class="ss">-   </span>Representation: Global route features are computable from digital</span>
<span id="cb1-1703"><a href="#cb1-1703"></a>    maps.</span>
<span id="cb1-1704"><a href="#cb1-1704"></a></span>
<span id="cb1-1705"><a href="#cb1-1705"></a><span class="ss">-   </span>Data collection: Preference of one complete route over another.</span>
<span id="cb1-1706"><a href="#cb1-1706"></a></span>
<span id="cb1-1707"><a href="#cb1-1707"></a><span class="ss">-   </span>Induction: A method for learning weights from preference data.</span>
<span id="cb1-1708"><a href="#cb1-1708"></a></span>
<span id="cb1-1709"><a href="#cb1-1709"></a><span class="ss">-   </span>Using model: Apply subjective function to find "optimal" route.</span>
<span id="cb1-1710"><a href="#cb1-1710"></a></span>
<span id="cb1-1711"><a href="#cb1-1711"></a>This method aims to learn a user model that considers the entirety of a</span>
<span id="cb1-1712"><a href="#cb1-1712"></a>route, thereby avoiding issues like data fragmentation and credit</span>
<span id="cb1-1713"><a href="#cb1-1713"></a>assignment problems.</span>
<span id="cb1-1714"><a href="#cb1-1714"></a></span>
<span id="cb1-1715"><a href="#cb1-1715"></a>The design choices are incorporated into <span class="co">[</span><span class="ot">@rogers1999adaptive</span><span class="co">]</span>, which:</span>
<span id="cb1-1716"><a href="#cb1-1716"></a>models driver preferences in terms of 14 global route features; gives</span>
<span id="cb1-1717"><a href="#cb1-1717"></a>the driver two alternative routes he might take; lets the driver refine</span>
<span id="cb1-1718"><a href="#cb1-1718"></a>these choices along route dimensions; uses driver choices to refine its</span>
<span id="cb1-1719"><a href="#cb1-1719"></a>model of his preferences; and invokes the driver model to recommend</span>
<span id="cb1-1720"><a href="#cb1-1720"></a>future routes. We note that providing drivers with choices lets the</span>
<span id="cb1-1721"><a href="#cb1-1721"></a>system collect data on route preferences in an unobtrusive manner. The</span>
<span id="cb1-1722"><a href="#cb1-1722"></a>interface of the application is presented in @fig-exp-1.</span>
<span id="cb1-1723"><a href="#cb1-1723"></a></span>
<span id="cb1-1724"><a href="#cb1-1724"></a><span class="al">![The adaptive route advisor.](Figures/example-1.png)</span>{#fig-exp-1</span>
<span id="cb1-1725"><a href="#cb1-1725"></a>width="80%"}</span>
<span id="cb1-1726"><a href="#cb1-1726"></a></span>
<span id="cb1-1727"><a href="#cb1-1727"></a>In driving route advisor task <span class="co">[</span><span class="ot">@rogers1999adaptive</span><span class="co">]</span>, a linear model is</span>
<span id="cb1-1728"><a href="#cb1-1728"></a>used for predicting the cost of a route based on the time, distance,</span>
<span id="cb1-1729"><a href="#cb1-1729"></a>number of intersections, and the number of turns. The system uses each</span>
<span id="cb1-1730"><a href="#cb1-1730"></a>training pair as a constraint on the weights found during the learning</span>
<span id="cb1-1731"><a href="#cb1-1731"></a>process. The experimental results are shown in the @fig-exp-2.</span>
<span id="cb1-1732"><a href="#cb1-1732"></a></span>
<span id="cb1-1733"><a href="#cb1-1733"></a>&lt;figure id="fig-exp-2"&gt;</span>
<span id="cb1-1734"><a href="#cb1-1734"></a>&lt;p&gt;&lt;img src="Figures/example-2.png" style="width:45.0%" alt="image" /&gt;</span>
<span id="cb1-1735"><a href="#cb1-1735"></a>&lt;img src="Figures/example-3.png" style="width:45.0%" alt="image" /&gt;&lt;/p&gt;</span>
<span id="cb1-1736"><a href="#cb1-1736"></a>&lt;figcaption&gt;(Left) Experiments with 24 subjects show the Route Advisor</span>
<span id="cb1-1737"><a href="#cb1-1737"></a>improves its predictive ability rapidly with experience. (Right)</span>
<span id="cb1-1738"><a href="#cb1-1738"></a>Analyses also show that personalized user models produce better results</span>
<span id="cb1-1739"><a href="#cb1-1739"></a>than generalized models, even when given more data. &lt;/figcaption&gt;</span>
<span id="cb1-1740"><a href="#cb1-1740"></a>&lt;/figure&gt;</span>
<span id="cb1-1741"><a href="#cb1-1741"></a></span>
<span id="cb1-1742"><a href="#cb1-1742"></a>**2. Destination Selection:** The task of destination selection involves</span>
<span id="cb1-1743"><a href="#cb1-1743"></a>assisting a driver in identifying one or more suitable destinations that</span>
<span id="cb1-1744"><a href="#cb1-1744"></a>fulfill a specific goal, such as finding a place to eat lunch, based on</span>
<span id="cb1-1745"><a href="#cb1-1745"></a>the driver's current location and knowledge of nearby options. While</span>
<span id="cb1-1746"><a href="#cb1-1746"></a>there are many recommendation systems online, including those for</span>
<span id="cb1-1747"><a href="#cb1-1747"></a>restaurants, they are not ideally suited for drivers due to the driving</span>
<span id="cb1-1748"><a href="#cb1-1748"></a>environment's demand for limited visual attention, thus necessitating a</span>
<span id="cb1-1749"><a href="#cb1-1749"></a>more tailored and accessible approach for in-car use.</span>
<span id="cb1-1750"><a href="#cb1-1750"></a></span>
<span id="cb1-1751"><a href="#cb1-1751"></a>One approach to destination recommendation can be cast as:</span>
<span id="cb1-1752"><a href="#cb1-1752"></a></span>
<span id="cb1-1753"><a href="#cb1-1753"></a><span class="ss">-   </span>Formulation: Learn to predict features the user cares about in</span>
<span id="cb1-1754"><a href="#cb1-1754"></a>    items.</span>
<span id="cb1-1755"><a href="#cb1-1755"></a></span>
<span id="cb1-1756"><a href="#cb1-1756"></a><span class="ss">-   </span>Representation: Conditions/weights on attributes and values.</span>
<span id="cb1-1757"><a href="#cb1-1757"></a></span>
<span id="cb1-1758"><a href="#cb1-1758"></a><span class="ss">-   </span>Data collection: Converse with the user to help him make decisions,</span>
<span id="cb1-1759"><a href="#cb1-1759"></a>    noting whether he accepts or rejects questions and items.</span>
<span id="cb1-1760"><a href="#cb1-1760"></a></span>
<span id="cb1-1761"><a href="#cb1-1761"></a><span class="ss">-   </span>Induction: Any supervised induction method.</span>
<span id="cb1-1762"><a href="#cb1-1762"></a></span>
<span id="cb1-1763"><a href="#cb1-1763"></a><span class="ss">-   </span>Using model: Guide the dialogue by selecting informative questions</span>
<span id="cb1-1764"><a href="#cb1-1764"></a>    and suggesting likely values.</span>
<span id="cb1-1765"><a href="#cb1-1765"></a></span>
<span id="cb1-1766"><a href="#cb1-1766"></a>This design relies on the idea of a conversational user interface.</span>
<span id="cb1-1767"><a href="#cb1-1767"></a>Spoken-language versions of this approach appear well suited to the</span>
<span id="cb1-1768"><a href="#cb1-1768"></a>driving environment.</span>
<span id="cb1-1769"><a href="#cb1-1769"></a></span>
<span id="cb1-1770"><a href="#cb1-1770"></a>This approach is implemented in <span class="co">[</span><span class="ot">@langley1999adaptive</span><span class="co">]</span>, where it engages</span>
<span id="cb1-1771"><a href="#cb1-1771"></a>in spoken conversations to help a user refine goals; incorporates a</span>
<span id="cb1-1772"><a href="#cb1-1772"></a>dialogue model to constrain this process; collects and stores traces of</span>
<span id="cb1-1773"><a href="#cb1-1773"></a>interaction with the user; and personalizes both its questions and</span>
<span id="cb1-1774"><a href="#cb1-1774"></a>recommended items. Their work focused on recommending restaurants to</span>
<span id="cb1-1775"><a href="#cb1-1775"></a>users who want advice about where to eat. This approach to</span>
<span id="cb1-1776"><a href="#cb1-1776"></a>recommendation would work well for drivers, it also has broader</span>
<span id="cb1-1777"><a href="#cb1-1777"></a>applications. We present experimental results in</span>
<span id="cb1-1778"><a href="#cb1-1778"></a></span>
<span id="cb1-1779"><a href="#cb1-1779"></a>&lt;figure id="fig-exp-2.5"&gt;</span>
<span id="cb1-1780"><a href="#cb1-1780"></a>&lt;p&gt;&lt;img src="Figures/example-4.png" style="width:45.0%" alt="image" /&gt;</span>
<span id="cb1-1781"><a href="#cb1-1781"></a>&lt;img src="Figures/example-5.png" style="width:45.0%" alt="image" /&gt;&lt;/p&gt;</span>
<span id="cb1-1782"><a href="#cb1-1782"></a>&lt;figcaption&gt;(Left) Speech Acts Per Conversation. (Right) Time Per</span>
<span id="cb1-1783"><a href="#cb1-1783"></a>Conversation. &lt;/figcaption&gt;</span>
<span id="cb1-1784"><a href="#cb1-1784"></a>&lt;/figure&gt;</span>
<span id="cb1-1785"><a href="#cb1-1785"></a></span>
<span id="cb1-1786"><a href="#cb1-1786"></a>**3. Resource Scheduling:** The task of resource scheduling describes</span>
<span id="cb1-1787"><a href="#cb1-1787"></a>the challenge of allocating a limited set of resources to complete a set</span>
<span id="cb1-1788"><a href="#cb1-1788"></a>of tasks or jobs within a certain time frame, while also considering the</span>
<span id="cb1-1789"><a href="#cb1-1789"></a>constraints on both the jobs and the resources. Although automated</span>
<span id="cb1-1790"><a href="#cb1-1790"></a>scheduling systems are prevalent in various industries and some</span>
<span id="cb1-1791"><a href="#cb1-1791"></a>interactive schedulers exist, there is a distinct need for systems that</span>
<span id="cb1-1792"><a href="#cb1-1792"></a>can create personalized schedules reflecting the unique preferences of</span>
<span id="cb1-1793"><a href="#cb1-1793"></a>individual users.</span>
<span id="cb1-1794"><a href="#cb1-1794"></a></span>
<span id="cb1-1795"><a href="#cb1-1795"></a>An approach to personalized scheduling can be described as:</span>
<span id="cb1-1796"><a href="#cb1-1796"></a></span>
<span id="cb1-1797"><a href="#cb1-1797"></a><span class="ss">-   </span>Formulation: Learn a utility function to evaluate entire schedules.</span>
<span id="cb1-1798"><a href="#cb1-1798"></a></span>
<span id="cb1-1799"><a href="#cb1-1799"></a><span class="ss">-   </span>Representation: Global features are computable from the schedule.</span>
<span id="cb1-1800"><a href="#cb1-1800"></a></span>
<span id="cb1-1801"><a href="#cb1-1801"></a><span class="ss">-   </span>Data collection: Preference of one candidate schedule over others.</span>
<span id="cb1-1802"><a href="#cb1-1802"></a></span>
<span id="cb1-1803"><a href="#cb1-1803"></a><span class="ss">-   </span>Induction: A method for learning weights from preference data.</span>
<span id="cb1-1804"><a href="#cb1-1804"></a></span>
<span id="cb1-1805"><a href="#cb1-1805"></a><span class="ss">-   </span>Using model: Apply the 'subjective' function to find a good</span>
<span id="cb1-1806"><a href="#cb1-1806"></a>    schedule.</span>
<span id="cb1-1807"><a href="#cb1-1807"></a></span>
<span id="cb1-1808"><a href="#cb1-1808"></a>We note that this method is similar to that in the Adaptive Route</span>
<span id="cb1-1809"><a href="#cb1-1809"></a>Advisor. However, it assumes a search through a space of complete</span>
<span id="cb1-1810"><a href="#cb1-1810"></a>schedules (a repair space), which requires some initial schedule. This</span>
<span id="cb1-1811"><a href="#cb1-1811"></a>approach is implemented in <span class="co">[</span><span class="ot">@gervasio1999learning</span><span class="co">]</span>, where the</span>
<span id="cb1-1812"><a href="#cb1-1812"></a>interactive scheduler retrieves an initial schedule from a personalized</span>
<span id="cb1-1813"><a href="#cb1-1813"></a>case library; suggests to the user improved schedules from which to</span>
<span id="cb1-1814"><a href="#cb1-1814"></a>select; lets the user direct search to improve on certain dimensions;</span>
<span id="cb1-1815"><a href="#cb1-1815"></a>collects user choices to refine its personalized utility function;</span>
<span id="cb1-1816"><a href="#cb1-1816"></a>stores solutions in the case base to initialize future schedules; and</span>
<span id="cb1-1817"><a href="#cb1-1817"></a>invokes the user model to recommend future schedule repairs. As before,</span>
<span id="cb1-1818"><a href="#cb1-1818"></a>providing users with choices lets the system collect data on schedule</span>
<span id="cb1-1819"><a href="#cb1-1819"></a>preferences unobtrusively. An example of the interface, and the</span>
<span id="cb1-1820"><a href="#cb1-1820"></a>experimental results are shown in @fig-exp-3.</span>
<span id="cb1-1821"><a href="#cb1-1821"></a></span>
<span id="cb1-1822"><a href="#cb1-1822"></a>&lt;figure id="fig-exp-3"&gt;</span>
<span id="cb1-1823"><a href="#cb1-1823"></a>&lt;p&gt;&lt;img src="Figures/example-7.png" style="width:45.0%" alt="image" /&gt;</span>
<span id="cb1-1824"><a href="#cb1-1824"></a>&lt;img src="Figures/example-6.png" style="width:45.0%" alt="image" /&gt;&lt;/p&gt;</span>
<span id="cb1-1825"><a href="#cb1-1825"></a>&lt;figcaption&gt;(Left) The interface of the INCA: Interactive</span>
<span id="cb1-1826"><a href="#cb1-1826"></a>Scheduling &lt;span class="citation"</span>
<span id="cb1-1827"><a href="#cb1-1827"></a>data-cites="gervasio1999learning"&gt;&lt;/span&gt;. (Right) Experiments with INCA</span>
<span id="cb1-1828"><a href="#cb1-1828"></a>suggest that retrieving personalized schedules helps users more as task</span>
<span id="cb1-1829"><a href="#cb1-1829"></a>difficulty increases. These experimental studies used a mixture of human</span>
<span id="cb1-1830"><a href="#cb1-1830"></a>and synthetic subjects. &lt;/figcaption&gt;</span>
<span id="cb1-1831"><a href="#cb1-1831"></a>&lt;/figure&gt;</span>
<span id="cb1-1832"><a href="#cb1-1832"></a></span>
<span id="cb1-1833"><a href="#cb1-1833"></a><span class="fu">#### Limitations</span></span>
<span id="cb1-1834"><a href="#cb1-1834"></a></span>
<span id="cb1-1835"><a href="#cb1-1835"></a>The challenges of adaptive interfaces may involve: conceptualizing user</span>
<span id="cb1-1836"><a href="#cb1-1836"></a>modeling as a task suitable for inductive learning, crafting</span>
<span id="cb1-1837"><a href="#cb1-1837"></a>representations that facilitate the learning process, gathering training</span>
<span id="cb1-1838"><a href="#cb1-1838"></a>data from users in a way that doesn't intrude on their experience,</span>
<span id="cb1-1839"><a href="#cb1-1839"></a>applying the learned user model effectively, ensuring the system can</span>
<span id="cb1-1840"><a href="#cb1-1840"></a>learn in real-time, and dealing with the necessity of learning from a</span>
<span id="cb1-1841"><a href="#cb1-1841"></a>limited number of training instances. These challenges are not only</span>
<span id="cb1-1842"><a href="#cb1-1842"></a>pertinent to adaptive interfaces but also intersect with broader</span>
<span id="cb1-1843"><a href="#cb1-1843"></a>applications of machine learning, while also introducing some unique</span>
<span id="cb1-1844"><a href="#cb1-1844"></a>issues. However, new sensor technology can bring promises to adaptive</span>
<span id="cb1-1845"><a href="#cb1-1845"></a>interfaces. Adaptive interfaces rely on user traces to drive their</span>
<span id="cb1-1846"><a href="#cb1-1846"></a>modeling process, so they stand to benefit from developments like GPS</span>
<span id="cb1-1847"><a href="#cb1-1847"></a>and cell phone locators, robust software for speech recognition,</span>
<span id="cb1-1848"><a href="#cb1-1848"></a>accurate eye and head trackers, real-time video interpreters, wearable</span>
<span id="cb1-1849"><a href="#cb1-1849"></a>body sensors (GSR, heart rate), and portable brain-wave sensors. As</span>
<span id="cb1-1850"><a href="#cb1-1850"></a>those devices become more widespread, they will offer new sources of</span>
<span id="cb1-1851"><a href="#cb1-1851"></a>data and support new types of adaptive services. In addition, adaptive</span>
<span id="cb1-1852"><a href="#cb1-1852"></a>interfaces can be viewed as a form of cognitive simulation that</span>
<span id="cb1-1853"><a href="#cb1-1853"></a>automatically generates knowledge structures to learn user preferences.</span>
<span id="cb1-1854"><a href="#cb1-1854"></a>They are capable of making explicit predictions about future user</span>
<span id="cb1-1855"><a href="#cb1-1855"></a>behavior and explaining individual differences through the process of</span>
<span id="cb1-1856"><a href="#cb1-1856"></a>personalization. This perspective views adaptive interfaces as tools</span>
<span id="cb1-1857"><a href="#cb1-1857"></a>that not only serve functional purposes but also model the psychological</span>
<span id="cb1-1858"><a href="#cb1-1858"></a>aspects of user interaction. Two distinct approaches within cognitive</span>
<span id="cb1-1859"><a href="#cb1-1859"></a>simulation are related to adaptive interfaces: *process* models that</span>
<span id="cb1-1860"><a href="#cb1-1860"></a>incorporate fundamental architectural principles, and *content* models</span>
<span id="cb1-1861"><a href="#cb1-1861"></a>that operate at the knowledge level, focusing on behavior. We note that</span>
<span id="cb1-1862"><a href="#cb1-1862"></a>both of them have roles to play, but content models are more relevant to</span>
<span id="cb1-1863"><a href="#cb1-1863"></a>personalization and adaptive interfaces.</span>
<span id="cb1-1864"><a href="#cb1-1864"></a></span>
<span id="cb1-1865"><a href="#cb1-1865"></a>In conclusion, adaptive user interfaces represent a significant</span>
<span id="cb1-1866"><a href="#cb1-1866"></a>advancement in creating personalized and efficient interactions between</span>
<span id="cb1-1867"><a href="#cb1-1867"></a>humans and technology. By leveraging modern sensor technologies and</span>
<span id="cb1-1868"><a href="#cb1-1868"></a>cognitive simulation approaches, these interfaces can dynamically learn</span>
<span id="cb1-1869"><a href="#cb1-1869"></a>and adapt to individual user preferences, enhancing overall user</span>
<span id="cb1-1870"><a href="#cb1-1870"></a>experience and system effectiveness. The methodologies discussed, from</span>
<span id="cb1-1871"><a href="#cb1-1871"></a>conceptualizing user models to collecting and utilizing user feedback,</span>
<span id="cb1-1872"><a href="#cb1-1872"></a>form the foundation of this innovative approach. As we transition to the</span>
<span id="cb1-1873"><a href="#cb1-1873"></a>next section, we will explore practical applications and real-world</span>
<span id="cb1-1874"><a href="#cb1-1874"></a>implementations of these human-centered AI principles through detailed</span>
<span id="cb1-1875"><a href="#cb1-1875"></a>case studies, illustrating the tangible impact of adaptive interfaces in</span>
<span id="cb1-1876"><a href="#cb1-1876"></a>various domains.</span>
<span id="cb1-1877"><a href="#cb1-1877"></a></span>
<span id="cb1-1878"><a href="#cb1-1878"></a><span class="fu">### Case Studies in Human-Centered AI</span></span>
<span id="cb1-1879"><a href="#cb1-1879"></a></span>
<span id="cb1-1880"><a href="#cb1-1880"></a>In this section, we examine practical examples that illustrate the</span>
<span id="cb1-1881"><a href="#cb1-1881"></a>application of human-centered principles in the development and</span>
<span id="cb1-1882"><a href="#cb1-1882"></a>deployment of AI systems. By examining these case studies, we aim to</span>
<span id="cb1-1883"><a href="#cb1-1883"></a>provide concrete insights into how AI technologies can be designed and</span>
<span id="cb1-1884"><a href="#cb1-1884"></a>implemented to better align with human values, enhance inclusivity, and</span>
<span id="cb1-1885"><a href="#cb1-1885"></a>address the specific needs of diverse user groups. The following case</span>
<span id="cb1-1886"><a href="#cb1-1886"></a>studies highlight different approaches and methodologies used to ensure</span>
<span id="cb1-1887"><a href="#cb1-1887"></a>that AI systems are not only effective but also considerate of the human</span>
<span id="cb1-1888"><a href="#cb1-1888"></a>experience.</span>
<span id="cb1-1889"><a href="#cb1-1889"></a></span>
<span id="cb1-1890"><a href="#cb1-1890"></a><span class="fu">#### LaMPost Case Study</span></span>
<span id="cb1-1891"><a href="#cb1-1891"></a></span>
<span id="cb1-1892"><a href="#cb1-1892"></a>In our exploration of human-centered AI design, it is crucial to examine</span>
<span id="cb1-1893"><a href="#cb1-1893"></a>how metrics can be improved to better capture the human experience and</span>
<span id="cb1-1894"><a href="#cb1-1894"></a>address the shortcomings of traditional evaluation methods. The LaMPost</span>
<span id="cb1-1895"><a href="#cb1-1895"></a>case study <span class="co">[</span><span class="ot">@goodman_lampost_2022</span><span class="co">]</span> exemplifies this effort by focusing</span>
<span id="cb1-1896"><a href="#cb1-1896"></a>on the development of an AI assistant designed to aid individuals with</span>
<span id="cb1-1897"><a href="#cb1-1897"></a>dyslexia in writing emails. This case is particularly relevant to our</span>
<span id="cb1-1898"><a href="#cb1-1898"></a>discussion because it highlights the importance of human-centered</span>
<span id="cb1-1899"><a href="#cb1-1899"></a>principles in AI development, especially in creating tools that cater to</span>
<span id="cb1-1900"><a href="#cb1-1900"></a>specific cognitive differences and enhance user experience.</span>
<span id="cb1-1901"><a href="#cb1-1901"></a></span>
<span id="cb1-1902"><a href="#cb1-1902"></a>Dyslexia is a cognitive difference that affects approximately 15 percent</span>
<span id="cb1-1903"><a href="#cb1-1903"></a>of language users, with varying degrees of impact on speaking, spelling,</span>
<span id="cb1-1904"><a href="#cb1-1904"></a>and writing abilities. It is a spectrum disorder, meaning symptoms and</span>
<span id="cb1-1905"><a href="#cb1-1905"></a>severity differ among individuals. More importantly, dyslexia is not an</span>
<span id="cb1-1906"><a href="#cb1-1906"></a>intellectual disability; many individuals with dyslexia possess high</span>
<span id="cb1-1907"><a href="#cb1-1907"></a>intelligence. Given the significant number of people affected by</span>
<span id="cb1-1908"><a href="#cb1-1908"></a>dyslexia, it is essential to develop AI tools that support their unique</span>
<span id="cb1-1909"><a href="#cb1-1909"></a>needs and enhance their daily tasks.</span>
<span id="cb1-1910"><a href="#cb1-1910"></a></span>
<span id="cb1-1911"><a href="#cb1-1911"></a>The LaMPost project sought to answer the question, "How can LLMs be</span>
<span id="cb1-1912"><a href="#cb1-1912"></a>applied to enhance the writing workflows of adults with dyslexia?" To</span>
<span id="cb1-1913"><a href="#cb1-1913"></a>address this, researchers employed a participatory design approach,</span>
<span id="cb1-1914"><a href="#cb1-1914"></a>involving employees with dyslexia from their company (Google) in the</span>
<span id="cb1-1915"><a href="#cb1-1915"></a>study. This approach ensured that the development process was inclusive</span>
<span id="cb1-1916"><a href="#cb1-1916"></a>and responsive to the actual needs and preferences of the dyslexic</span>
<span id="cb1-1917"><a href="#cb1-1917"></a>community. By focusing on the real-world application of LLMs in aiding</span>
<span id="cb1-1918"><a href="#cb1-1918"></a>email writing for dyslexic individuals, LaMPost serves as a powerful</span>
<span id="cb1-1919"><a href="#cb1-1919"></a>example of how AI can be designed to better capture and enhance the</span>
<span id="cb1-1920"><a href="#cb1-1920"></a>human experience.</span>
<span id="cb1-1921"><a href="#cb1-1921"></a></span>
<span id="cb1-1922"><a href="#cb1-1922"></a>The figure below allows users to see suggestions for rewriting selected</span>
<span id="cb1-1923"><a href="#cb1-1923"></a>text, helping them identify main ideas, suggest possible changes, and</span>
<span id="cb1-1924"><a href="#cb1-1924"></a>rewrite their selections to improve clarity and expression.</span>
<span id="cb1-1925"><a href="#cb1-1925"></a></span>
<span id="cb1-1926"><a href="#cb1-1926"></a>![The Suggest Possible Changes feature from</span>
<span id="cb1-1927"><a href="#cb1-1927"></a>LaMPost.](Figures/lampost_fig3.png)</span>
<span id="cb1-1928"><a href="#cb1-1928"></a></span>
<span id="cb1-1929"><a href="#cb1-1929"></a>The table below categorizes the challenges faced by users at different</span>
<span id="cb1-1930"><a href="#cb1-1930"></a>writing levels and the strategies they can use to overcome these</span>
<span id="cb1-1931"><a href="#cb1-1931"></a>challenges, illustrating the varied support needs addressed by LaMPost</span>
<span id="cb1-1932"><a href="#cb1-1932"></a></span>
<span id="cb1-1933"><a href="#cb1-1933"></a>&lt;figure&gt;</span>
<span id="cb1-1934"><a href="#cb1-1934"></a>&lt;table&gt;</span>
<span id="cb1-1935"><a href="#cb1-1935"></a>&lt;thead&gt;</span>
<span id="cb1-1936"><a href="#cb1-1936"></a>&lt;tr&gt;</span>
<span id="cb1-1937"><a href="#cb1-1937"></a>&lt;th style="text-align: center;"&gt;Writing level&lt;/th&gt;</span>
<span id="cb1-1938"><a href="#cb1-1938"></a>&lt;th style="text-align: center;"&gt;Examples of Challenges&lt;/th&gt;</span>
<span id="cb1-1939"><a href="#cb1-1939"></a>&lt;th style="text-align: center;"&gt;Strategies&lt;/th&gt;</span>
<span id="cb1-1940"><a href="#cb1-1940"></a>&lt;/tr&gt;</span>
<span id="cb1-1941"><a href="#cb1-1941"></a>&lt;/thead&gt;</span>
<span id="cb1-1942"><a href="#cb1-1942"></a>&lt;tbody&gt;</span>
<span id="cb1-1943"><a href="#cb1-1943"></a>&lt;tr&gt;</span>
<span id="cb1-1944"><a href="#cb1-1944"></a>&lt;td style="text-align: center;"&gt;high&lt;/td&gt;</span>
<span id="cb1-1945"><a href="#cb1-1945"></a>&lt;td style="text-align: center;"&gt;expressing ideas&lt;/td&gt;</span>
<span id="cb1-1946"><a href="#cb1-1946"></a>&lt;td style="text-align: center;"&gt;“word faucet”, ASR dictation&lt;/td&gt;</span>
<span id="cb1-1947"><a href="#cb1-1947"></a>&lt;/tr&gt;</span>
<span id="cb1-1948"><a href="#cb1-1948"></a>&lt;tr&gt;</span>
<span id="cb1-1949"><a href="#cb1-1949"></a>&lt;td style="text-align: center;"&gt;&lt;/td&gt;</span>
<span id="cb1-1950"><a href="#cb1-1950"></a>&lt;td style="text-align: center;"&gt;ordering ideas&lt;/td&gt;</span>
<span id="cb1-1951"><a href="#cb1-1951"></a>&lt;td style="text-align: center;"&gt;post-it outlining&lt;/td&gt;</span>
<span id="cb1-1952"><a href="#cb1-1952"></a>&lt;/tr&gt;</span>
<span id="cb1-1953"><a href="#cb1-1953"></a>&lt;tr&gt;</span>
<span id="cb1-1954"><a href="#cb1-1954"></a>&lt;td style="text-align: center;"&gt;low&lt;/td&gt;</span>
<span id="cb1-1955"><a href="#cb1-1955"></a>&lt;td style="text-align: center;"&gt;appropriate language&lt;/td&gt;</span>
<span id="cb1-1956"><a href="#cb1-1956"></a>&lt;td style="text-align: center;"&gt;proofreading&lt;/td&gt;</span>
<span id="cb1-1957"><a href="#cb1-1957"></a>&lt;/tr&gt;</span>
<span id="cb1-1958"><a href="#cb1-1958"></a>&lt;tr&gt;</span>
<span id="cb1-1959"><a href="#cb1-1959"></a>&lt;td style="text-align: center;"&gt;&lt;/td&gt;</span>
<span id="cb1-1960"><a href="#cb1-1960"></a>&lt;td style="text-align: center;"&gt;paraphrasing&lt;/td&gt;</span>
<span id="cb1-1961"><a href="#cb1-1961"></a>&lt;td style="text-align: center;"&gt;feedback&lt;/td&gt;</span>
<span id="cb1-1962"><a href="#cb1-1962"></a>&lt;/tr&gt;</span>
<span id="cb1-1963"><a href="#cb1-1963"></a>&lt;/tbody&gt;</span>
<span id="cb1-1964"><a href="#cb1-1964"></a>&lt;/table&gt;</span>
<span id="cb1-1965"><a href="#cb1-1965"></a>&lt;figcaption&gt;User challenged and strategies in LaMPost.&lt;/figcaption&gt;</span>
<span id="cb1-1966"><a href="#cb1-1966"></a>&lt;/figure&gt;</span>
<span id="cb1-1967"><a href="#cb1-1967"></a></span>
<span id="cb1-1968"><a href="#cb1-1968"></a>Next, they ran a focus group to get initial ideas from members of the</span>
<span id="cb1-1969"><a href="#cb1-1969"></a>dyslexic community. This focus group helped them figure out what to</span>
<span id="cb1-1970"><a href="#cb1-1970"></a>measure and added the second research question: "How do adults with</span>
<span id="cb1-1971"><a href="#cb1-1971"></a>dyslexia feel about LLM-assisted writing?" In other words, how does the</span>
<span id="cb1-1972"><a href="#cb1-1972"></a>LLM impact users' feelings of satisfaction, self-expression,</span>
<span id="cb1-1973"><a href="#cb1-1973"></a>self-efficacy, autonomy, and control?</span>
<span id="cb1-1974"><a href="#cb1-1974"></a></span>
<span id="cb1-1975"><a href="#cb1-1975"></a>From this focus group, they went and created a prototype to answer the</span>
<span id="cb1-1976"><a href="#cb1-1976"></a>desires of the group. They included three features in their prototype</span>
<span id="cb1-1977"><a href="#cb1-1977"></a>model. One feature was: *identifying main ideas*. They focused on this</span>
<span id="cb1-1978"><a href="#cb1-1978"></a>to support overall clarity and organization of high-level ideas of the</span>
<span id="cb1-1979"><a href="#cb1-1979"></a>user. Another feature was *suggest possible changes*. They focused on</span>
<span id="cb1-1980"><a href="#cb1-1980"></a>this because users wanted to identify high-level adjustments to improve</span>
<span id="cb1-1981"><a href="#cb1-1981"></a>their writing. The last feature they added was *rewrite my selections*.</span>
<span id="cb1-1982"><a href="#cb1-1982"></a>They added this because users wanted help expressing ideas with a</span>
<span id="cb1-1983"><a href="#cb1-1983"></a>desired phrasing tone or style. This feature generated a rewrite based</span>
<span id="cb1-1984"><a href="#cb1-1984"></a>on a command you gave it.</span>
<span id="cb1-1985"><a href="#cb1-1985"></a></span>
<span id="cb1-1986"><a href="#cb1-1986"></a>With the prototype, the researchers evaluated again with 19 participants</span>
<span id="cb1-1987"><a href="#cb1-1987"></a>with dyslexia from outside their organization. They did a three-part</span>
<span id="cb1-1988"><a href="#cb1-1988"></a>study, including a demonstration and background on the system (25 min).</span>
<span id="cb1-1989"><a href="#cb1-1989"></a>Then they did a writing exercise with two real tasks (emails) each user</span>
<span id="cb1-1990"><a href="#cb1-1990"></a>had to do in the real world (25 min). For example, one task might have</span>
<span id="cb1-1991"><a href="#cb1-1991"></a>been to write an email to the principal of their child's school to ask</span>
<span id="cb1-1992"><a href="#cb1-1992"></a>for a meeting. Then, the researchers did another follow-up interview for</span>
<span id="cb1-1993"><a href="#cb1-1993"></a>more qualitative data, e.g. to ask about specific choices users made</span>
<span id="cb1-1994"><a href="#cb1-1994"></a>when interacting with the model (25 min).</span>
<span id="cb1-1995"><a href="#cb1-1995"></a></span>
<span id="cb1-1996"><a href="#cb1-1996"></a>LaMPost's design prioritized autonomy by allowing users to choose the</span>
<span id="cb1-1997"><a href="#cb1-1997"></a>best option for their writing. One successful thing is that most users</span>
<span id="cb1-1998"><a href="#cb1-1998"></a>felt in control while writing. Users found that numerous options were</span>
<span id="cb1-1999"><a href="#cb1-1999"></a>helpful to filter through poor results. However, participants said the</span>
<span id="cb1-2000"><a href="#cb1-2000"></a>selection process was cognitively demanding and time-consuming. As we</span>
<span id="cb1-2001"><a href="#cb1-2001"></a>all know, features identified in LaMPost are all over the place, such as</span>
<span id="cb1-2002"><a href="#cb1-2002"></a>in Google Docs. Nonetheless, there remain many questions about the</span>
<span id="cb1-2003"><a href="#cb1-2003"></a>balance between automated writing and providing more control to the end</span>
<span id="cb1-2004"><a href="#cb1-2004"></a>users.</span>
<span id="cb1-2005"><a href="#cb1-2005"></a></span>
<span id="cb1-2006"><a href="#cb1-2006"></a>::: tcolorbox</span>
<span id="cb1-2007"><a href="#cb1-2007"></a>How could researchers hone in on this trade-off between **the ease of</span>
<span id="cb1-2008"><a href="#cb1-2008"></a>automated writing** and **providing control to end-users**?\</span>
<span id="cb1-2009"><a href="#cb1-2009"></a>You will need to design a study to approach this question.</span>
<span id="cb1-2010"><a href="#cb1-2010"></a></span>
<span id="cb1-2011"><a href="#cb1-2011"></a><span class="ss">-   </span>Identify your research question, hypotheses, and the methods that</span>
<span id="cb1-2012"><a href="#cb1-2012"></a>    you will use. (Hint: use the HCI methods described in the previous</span>
<span id="cb1-2013"><a href="#cb1-2013"></a>    section.)</span>
<span id="cb1-2014"><a href="#cb1-2014"></a></span>
<span id="cb1-2015"><a href="#cb1-2015"></a><span class="ss">-   </span>Scope the domain of your study appropriately---more broadly than</span>
<span id="cb1-2016"><a href="#cb1-2016"></a>    dyslexia but not so broadly to be meaningless.</span>
<span id="cb1-2017"><a href="#cb1-2017"></a></span>
<span id="cb1-2018"><a href="#cb1-2018"></a><span class="ss">-   </span>What domains will you include? (E.g. students use ChatGPT for</span>
<span id="cb1-2019"><a href="#cb1-2019"></a>    assignments, doctors use an LLM to write notes, etc.)</span>
<span id="cb1-2020"><a href="#cb1-2020"></a>:::</span>
<span id="cb1-2021"><a href="#cb1-2021"></a></span>
<span id="cb1-2022"><a href="#cb1-2022"></a>In this way, both the case study of LaMPost and its presaging of greater</span>
<span id="cb1-2023"><a href="#cb1-2023"></a>trends in LLM interfaces recapitulate the maxim of HCI: HCI is a cycle.</span>
<span id="cb1-2024"><a href="#cb1-2024"></a>You design a potential system, prototype it, get feedback from people,</span>
<span id="cb1-2025"><a href="#cb1-2025"></a>and iterate constantly. Next, we will explore two case studies that</span>
<span id="cb1-2026"><a href="#cb1-2026"></a>exemplify the application of human-centered principles in NLP. These</span>
<span id="cb1-2027"><a href="#cb1-2027"></a>case studies illustrate how LLMs can be adapted to foster social</span>
<span id="cb1-2028"><a href="#cb1-2028"></a>inclusivity and provide training in social skills.</span>
<span id="cb1-2029"><a href="#cb1-2029"></a></span>
<span id="cb1-2030"><a href="#cb1-2030"></a><span class="fu">#### Multi-Value and DaDa: Cross-Dialectal English NLP</span></span>
<span id="cb1-2031"><a href="#cb1-2031"></a></span>
<span id="cb1-2032"><a href="#cb1-2032"></a>English NLP systems are largely trained to perform well in Standard</span>
<span id="cb1-2033"><a href="#cb1-2033"></a>American English - the form of written English found in professional</span>
<span id="cb1-2034"><a href="#cb1-2034"></a>settings and elsewhere. Not only is Standard American English the most</span>
<span id="cb1-2035"><a href="#cb1-2035"></a>well-represented form of English in textual datasets but NLP engineers</span>
<span id="cb1-2036"><a href="#cb1-2036"></a>and researchers often filter dialectal and vernacular English examples</span>
<span id="cb1-2037"><a href="#cb1-2037"></a>from their datasets to improve performance on SAE benchmarks. As a</span>
<span id="cb1-2038"><a href="#cb1-2038"></a>result, NLP systems are generally less performant when processing</span>
<span id="cb1-2039"><a href="#cb1-2039"></a>dialectal inputs than SAE inputs. This performance gap is observable</span>
<span id="cb1-2040"><a href="#cb1-2040"></a>over various benchmarks and tasks, like the SPIDER benchmark. <span class="co">[</span><span class="ot">@spider</span><span class="co">]</span></span>
<span id="cb1-2041"><a href="#cb1-2041"></a></span>
<span id="cb1-2042"><a href="#cb1-2042"></a>![Stress test reveals worse performance on the SPIDER benchmark with</span>
<span id="cb1-2043"><a href="#cb1-2043"></a>synthetic dialectical examples than with</span>
<span id="cb1-2044"><a href="#cb1-2044"></a>SAE.](Figures/MV2.png){width="100%"}</span>
<span id="cb1-2045"><a href="#cb1-2045"></a></span>
<span id="cb1-2046"><a href="#cb1-2046"></a>As natural language systems become more pervasive, this performance gap</span>
<span id="cb1-2047"><a href="#cb1-2047"></a>increasingly represents a real allocational harm against dialectal</span>
<span id="cb1-2048"><a href="#cb1-2048"></a>English speakers --- these speakers are excluded from using helpful</span>
<span id="cb1-2049"><a href="#cb1-2049"></a>systems and assistants. Multi-Value is a framework for evaluating</span>
<span id="cb1-2050"><a href="#cb1-2050"></a>foundation language models on dialectic input, and DADA is a framework</span>
<span id="cb1-2051"><a href="#cb1-2051"></a>for adapting LLMs to improve performance on dialectic input.</span>
<span id="cb1-2052"><a href="#cb1-2052"></a></span>
<span id="cb1-2053"><a href="#cb1-2053"></a>**Synthetic Dialectal Data**</span>
<span id="cb1-2054"><a href="#cb1-2054"></a></span>
<span id="cb1-2055"><a href="#cb1-2055"></a>Ziems et al. (2023) create synthetic dialectal data for several English</span>
<span id="cb1-2056"><a href="#cb1-2056"></a>dialects (Appalachian English, Chicano English, Indian English,</span>
<span id="cb1-2057"><a href="#cb1-2057"></a>Colloquial Singapore English, and Urban African American English).<span class="co">[</span><span class="ot">@mv</span><span class="co">]</span></span>
<span id="cb1-2058"><a href="#cb1-2058"></a>They created synthetic data based on transforming SAE examples to have</span>
<span id="cb1-2059"><a href="#cb1-2059"></a>direct evaluation comparisons. These synthetic examples were created by</span>
<span id="cb1-2060"><a href="#cb1-2060"></a>leveraging known linguistic features of the dialects, such as negative</span>
<span id="cb1-2061"><a href="#cb1-2061"></a>concord in UAAVE. @fig-features_dialects maps out the presence of various</span>
<span id="cb1-2062"><a href="#cb1-2062"></a>linguistic features.</span>
<span id="cb1-2063"><a href="#cb1-2063"></a></span>
<span id="cb1-2064"><a href="#cb1-2064"></a>![A comparative distribution of features in five</span>
<span id="cb1-2065"><a href="#cb1-2065"></a>dialects.](Figures/MV1.png){#fig-features_dialects width="100%"}</span>
<span id="cb1-2066"><a href="#cb1-2066"></a></span>
<span id="cb1-2067"><a href="#cb1-2067"></a>This synthetic data, while somewhat limited in the variety of samples.</span>
<span id="cb1-2068"><a href="#cb1-2068"></a>can produce and create realistic examples for benchmarking LM</span>
<span id="cb1-2069"><a href="#cb1-2069"></a>performance. @fig-synthetic_example demonstrates creating a synthetic</span>
<span id="cb1-2070"><a href="#cb1-2070"></a>dialectic example using the 'give passive' linguistic feature,</span>
<span id="cb1-2071"><a href="#cb1-2071"></a>illustrating the transformation process from SAE to a vernacular form.</span>
<span id="cb1-2072"><a href="#cb1-2072"></a></span>
<span id="cb1-2073"><a href="#cb1-2073"></a>![Execution of a sample transform using a documented linguistic</span>
<span id="cb1-2074"><a href="#cb1-2074"></a>feature.](Figures/MV3.png){#fig-synthetic_example width="40%"}</span>
<span id="cb1-2075"><a href="#cb1-2075"></a></span>
<span id="cb1-2076"><a href="#cb1-2076"></a>**Feature Level Adapters** One approach to the LLM adaption task would</span>
<span id="cb1-2077"><a href="#cb1-2077"></a>be to train an adapter for each dialect using a parameter-efficient</span>
<span id="cb1-2078"><a href="#cb1-2078"></a>fine-tuning method like low-rank adapters. <span class="co">[</span><span class="ot">@lora</span><span class="co">]</span> While adapters can</span>
<span id="cb1-2079"><a href="#cb1-2079"></a>certainly bridge the gap between SAE LMs and dialect inputs, this</span>
<span id="cb1-2080"><a href="#cb1-2080"></a>approach suffers from a couple of weaknesses, namely:</span>
<span id="cb1-2081"><a href="#cb1-2081"></a></span>
<span id="cb1-2082"><a href="#cb1-2082"></a><span class="ss">-   </span>Individually trained adapters do not leverage similarities between</span>
<span id="cb1-2083"><a href="#cb1-2083"></a>    low-resource dialects. Transfer learning is often helpful for</span>
<span id="cb1-2084"><a href="#cb1-2084"></a>    training low-resource languages and dialects.</span>
<span id="cb1-2085"><a href="#cb1-2085"></a></span>
<span id="cb1-2086"><a href="#cb1-2086"></a><span class="ss">-   </span>The model needs to know which adapter to use at inference time. This</span>
<span id="cb1-2087"><a href="#cb1-2087"></a>    presupposes that we can accurately classify the dialect ---</span>
<span id="cb1-2088"><a href="#cb1-2088"></a>    sometimes based on as little as one utterance. This classification</span>
<span id="cb1-2089"><a href="#cb1-2089"></a>    is not always possible --- a more general approach is needed.</span>
<span id="cb1-2090"><a href="#cb1-2090"></a></span>
<span id="cb1-2091"><a href="#cb1-2091"></a>Therefore, Liu et al. (2023) propose a novel solution --- DADA: Dialect</span>
<span id="cb1-2092"><a href="#cb1-2092"></a>Adaption via Dynamic Aggregation of Linguistic Rules. <span class="co">[</span><span class="ot">@dada</span><span class="co">]</span> DADA</span>
<span id="cb1-2093"><a href="#cb1-2093"></a>trains adapters on the linguistic feature level rather than the dialect</span>
<span id="cb1-2094"><a href="#cb1-2094"></a>level. The model can use multiple linguistic feature adapters via an</span>
<span id="cb1-2095"><a href="#cb1-2095"></a>additional fusion layer. They can therefore train using</span>
<span id="cb1-2096"><a href="#cb1-2096"></a>multi-dialectical data and cover linguistic variation via a</span>
<span id="cb1-2097"><a href="#cb1-2097"></a>comprehensive set of roughly 200 adapters. DADA saw an improvement in</span>
<span id="cb1-2098"><a href="#cb1-2098"></a>performance over single-dialect adapters for most dialects, as shown in</span>
<span id="cb1-2099"><a href="#cb1-2099"></a>@fig-dada_performance.</span>
<span id="cb1-2100"><a href="#cb1-2100"></a></span>
<span id="cb1-2101"><a href="#cb1-2101"></a>![Execution of a sample transform using a documented linguistic</span>
<span id="cb1-2102"><a href="#cb1-2102"></a>feature.](Figures/MV4.png){#fig-dada_performance width="40%"}</span>
<span id="cb1-2103"><a href="#cb1-2103"></a></span>
<span id="cb1-2104"><a href="#cb1-2104"></a>The Multi-Value and DADA case study underscores the importance of</span>
<span id="cb1-2105"><a href="#cb1-2105"></a>designing NLP systems that are inclusive and representative of diverse</span>
<span id="cb1-2106"><a href="#cb1-2106"></a>language users. By addressing the performance gaps in handling dialectal</span>
<span id="cb1-2107"><a href="#cb1-2107"></a>inputs, this case study highlights the necessity of incorporating</span>
<span id="cb1-2108"><a href="#cb1-2108"></a>diverse linguistic data and creating adaptable systems. This approach</span>
<span id="cb1-2109"><a href="#cb1-2109"></a>enhances AI functionality and accessibility, ensuring it respects and</span>
<span id="cb1-2110"><a href="#cb1-2110"></a>reflects linguistic diversity. Ultimately, the study reinforces</span>
<span id="cb1-2111"><a href="#cb1-2111"></a>human-centered design principles, demonstrating how AI can be tailored</span>
<span id="cb1-2112"><a href="#cb1-2112"></a>to better serve and empower all users. Moving forward, we will explore</span>
<span id="cb1-2113"><a href="#cb1-2113"></a>how LLMs can be utilized for social skill training, showcasing their</span>
<span id="cb1-2114"><a href="#cb1-2114"></a>potential to improve human interactions.</span>
<span id="cb1-2115"><a href="#cb1-2115"></a></span>
<span id="cb1-2116"><a href="#cb1-2116"></a><span class="fu">#### Social Skill Training via LLMs</span></span>
<span id="cb1-2117"><a href="#cb1-2117"></a></span>
<span id="cb1-2118"><a href="#cb1-2118"></a>The emergence of Large Language Models (LLMs) marks a significant</span>
<span id="cb1-2119"><a href="#cb1-2119"></a>milestone in the field of social skills training. This case study</span>
<span id="cb1-2120"><a href="#cb1-2120"></a>explores the potential of LLMs to augment social skill development</span>
<span id="cb1-2121"><a href="#cb1-2121"></a>across diverse scenarios. More specifically, we discuss a dual-framework</span>
<span id="cb1-2122"><a href="#cb1-2122"></a>approach, where two distinct LLMs operate in tandem as a Partner and a</span>
<span id="cb1-2123"><a href="#cb1-2123"></a>Mentor, guiding human learners in their journey towards improved social</span>
<span id="cb1-2124"><a href="#cb1-2124"></a>interaction. In this framework, we have two agents which are</span>
<span id="cb1-2125"><a href="#cb1-2125"></a></span>
<span id="cb1-2126"><a href="#cb1-2126"></a><span class="ss">-   </span>**AI Partner**: LLM-empowered agents that users can engage with</span>
<span id="cb1-2127"><a href="#cb1-2127"></a>    across various topics. This interactive model facilitates practical,</span>
<span id="cb1-2128"><a href="#cb1-2128"></a>    conversation-based learning, enabling users to experiment with</span>
<span id="cb1-2129"><a href="#cb1-2129"></a>    different communication styles and techniques or practice and</span>
<span id="cb1-2130"><a href="#cb1-2130"></a>    develop specific skills in real-world scenarios in a safe,</span>
<span id="cb1-2131"><a href="#cb1-2131"></a>    AI-mediated environment.</span>
<span id="cb1-2132"><a href="#cb1-2132"></a></span>
<span id="cb1-2133"><a href="#cb1-2133"></a><span class="ss">-   </span>**AI Mentor**: An LLM-empowered entity designed to provide</span>
<span id="cb1-2134"><a href="#cb1-2134"></a>    constructive, personalized feedback based on the interaction of</span>
<span id="cb1-2135"><a href="#cb1-2135"></a>    users and the AI Partner. This mentor analyzes conversation</span>
<span id="cb1-2136"><a href="#cb1-2136"></a>    dynamics, identifies areas for improvement, offers tailored advice,</span>
<span id="cb1-2137"><a href="#cb1-2137"></a>    and guides users toward effective social strategies and improved</span>
<span id="cb1-2138"><a href="#cb1-2138"></a>    interaction skills.</span>
<span id="cb1-2139"><a href="#cb1-2139"></a></span>
<span id="cb1-2140"><a href="#cb1-2140"></a>For example, in conflict resolution, individuals learning to handle</span>
<span id="cb1-2141"><a href="#cb1-2141"></a>difficult conversations can use the AI Partner to simulate interactions</span>
<span id="cb1-2142"><a href="#cb1-2142"></a>with a digitalized partner. As a Conflict Resolution Expert, the AI</span>
<span id="cb1-2143"><a href="#cb1-2143"></a>Mentor helps analyze these interactions, offering strategies to navigate</span>
<span id="cb1-2144"><a href="#cb1-2144"></a>conflicts effectively.</span>
<span id="cb1-2145"><a href="#cb1-2145"></a></span>
<span id="cb1-2146"><a href="#cb1-2146"></a>In the educational sector, K-12 teachers aiming to incorporate more</span>
<span id="cb1-2147"><a href="#cb1-2147"></a>growth-mindset language into their teaching can practice with a</span>
<span id="cb1-2148"><a href="#cb1-2148"></a>digitalized student. An experienced teacher or mentor, represented by</span>
<span id="cb1-2149"><a href="#cb1-2149"></a>the AI Mentor, provides insights on effective communication and teaching</span>
<span id="cb1-2150"><a href="#cb1-2150"></a>methods. For negotiation training, students preparing to negotiate their</span>
<span id="cb1-2151"><a href="#cb1-2151"></a>first job offers can engage in simulated negotiations with a digitalized</span>
<span id="cb1-2152"><a href="#cb1-2152"></a>HR representative through the AI Partner. As a Negotiation Expert, the</span>
<span id="cb1-2153"><a href="#cb1-2153"></a>AI Mentor then offers guidance on negotiation tactics, helping students</span>
<span id="cb1-2154"><a href="#cb1-2154"></a>effectively articulate their values and negotiate job terms. Lastly, in</span>
<span id="cb1-2155"><a href="#cb1-2155"></a>therapy training, novice therapists can interact with a digitalized</span>
<span id="cb1-2156"><a href="#cb1-2156"></a>patient via the AI Partner to practice therapy sessions. The AI Mentor,</span>
<span id="cb1-2157"><a href="#cb1-2157"></a>functioning as a Therapy Coach, then reviews these sessions, providing</span>
<span id="cb1-2158"><a href="#cb1-2158"></a>feedback and suggestions on enhancing therapeutic techniques and patient</span>
<span id="cb1-2159"><a href="#cb1-2159"></a>engagement.</span>
<span id="cb1-2160"><a href="#cb1-2160"></a></span>
<span id="cb1-2161"><a href="#cb1-2161"></a>**CARE: Therapy Skill Training** Hsu et al. (2023) introduced</span>
<span id="cb1-2162"><a href="#cb1-2162"></a>CARE <span class="co">[</span><span class="ot">@hsu2023helping</span><span class="co">]</span>, a framework designed for therapy skill training.</span>
<span id="cb1-2163"><a href="#cb1-2163"></a>This framework leverages a simulated environment, enabling counselors to</span>
<span id="cb1-2164"><a href="#cb1-2164"></a>practice their skills without the risk of harming real individuals. An</span>
<span id="cb1-2165"><a href="#cb1-2165"></a>integral component of CARE is the AI Mentor, which offers invaluable</span>
<span id="cb1-2166"><a href="#cb1-2166"></a>feedback and guidance during the training process. See</span>
<span id="cb1-2167"><a href="#cb1-2167"></a>@fig-care for</span>
<span id="cb1-2168"><a href="#cb1-2168"></a>the overview of the framework.</span>
<span id="cb1-2169"><a href="#cb1-2169"></a></span>
<span id="cb1-2170"><a href="#cb1-2170"></a><span class="al">![CARE Framework](Figures/care.png)</span>{#fig-care width="45%"}</span>
<span id="cb1-2171"><a href="#cb1-2171"></a></span>
<span id="cb1-2172"><a href="#cb1-2172"></a>CARE's primary function is for novice therapists and counselors to</span>
<span id="cb1-2173"><a href="#cb1-2173"></a>assess and determine the most effective counseling strategies tailored</span>
<span id="cb1-2174"><a href="#cb1-2174"></a>to specific contexts. It provides counselors with customized example</span>
<span id="cb1-2175"><a href="#cb1-2175"></a>responses, which they can adopt, adapt, or disregard when interacting</span>
<span id="cb1-2176"><a href="#cb1-2176"></a>with a simulated support seeker. This approach is deeply rooted in the</span>
<span id="cb1-2177"><a href="#cb1-2177"></a>principles of Motivational Interviewing and utilizes a rich dataset of</span>
<span id="cb1-2178"><a href="#cb1-2178"></a>counseling conversations combined with LLMs. The effectiveness of CARE</span>
<span id="cb1-2179"><a href="#cb1-2179"></a>has been established through rigorous quantitative evaluations and</span>
<span id="cb1-2180"><a href="#cb1-2180"></a>qualitative user studies, which included simulated chats and</span>
<span id="cb1-2181"><a href="#cb1-2181"></a>semi-structured interviews. Notably, CARE has shown significant benefits</span>
<span id="cb1-2182"><a href="#cb1-2182"></a>in aiding novice counselors. From the assessment, counselors chose to</span>
<span id="cb1-2183"><a href="#cb1-2183"></a>use CARE 93% of the time, directly used a CARE response without editing</span>
<span id="cb1-2184"><a href="#cb1-2184"></a>60% of the time, and sent more extended responses with CARE.</span>
<span id="cb1-2185"><a href="#cb1-2185"></a>Qualitatively, counselors noted several advantages of CARE, such as its</span>
<span id="cb1-2186"><a href="#cb1-2186"></a>ability to refresh memory on various strategies, inspire innovative</span>
<span id="cb1-2187"><a href="#cb1-2187"></a>responses, boost confidence, and save time during consultations.</span>
<span id="cb1-2188"><a href="#cb1-2188"></a>However, there were some drawbacks, including potential disruptions in</span>
<span id="cb1-2189"><a href="#cb1-2189"></a>the thought process, perceived limitations in response options, the</span>
<span id="cb1-2190"><a href="#cb1-2190"></a>requirement for decision-making, and the time needed to review</span>
<span id="cb1-2191"><a href="#cb1-2191"></a>suggestions. Overall, the framework is particularly beneficial for</span>
<span id="cb1-2192"><a href="#cb1-2192"></a>therapists new to the field, offering them a supportive and educational</span>
<span id="cb1-2193"><a href="#cb1-2193"></a>tool to enhance their counseling skills effectively.</span>
<span id="cb1-2194"><a href="#cb1-2194"></a></span>
<span id="cb1-2195"><a href="#cb1-2195"></a><span class="fu">## Practice Exercises</span></span>
<span id="cb1-2196"><a href="#cb1-2196"></a></span>
<span id="cb1-2197"><a href="#cb1-2197"></a><span class="ot">[^1]: </span>GPT-4 is good at coming up with longer-rendered answers about why</span>
<span id="cb1-2198"><a href="#cb1-2198"></a>    some things are appropriate or not.</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
    <footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/chap5.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer><script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




</body></html>