<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Learning – Machine Learning from Human Preferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../src/chap4.html" rel="next">
<link href="../src/chap2.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-07ba0ad10f5680c660e360ac31d2f3b6.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8b864f0777c60eecff11d75b6b2e1175.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-fa3d1c749edcb96cd5cb7d620f3e5237.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-6f24586c8b15e78d85e3983c622e3e8a.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../src/chap3.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning from Human Preferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sangttruong/mlhp" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Machine-Learning-from-Human-Preferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap3.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Elicitation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Decisions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Aggregation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Beyond Preferences</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/ack.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>license.qmd</p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-supervised-learning-problem" id="toc-the-supervised-learning-problem" class="nav-link active" data-scroll-target="#the-supervised-learning-problem"><span class="header-section-number">3.1</span> The Supervised Learning Problem</a></li>
  <li><a href="#point-estimation-via-maximum-likelihood" id="toc-point-estimation-via-maximum-likelihood" class="nav-link" data-scroll-target="#point-estimation-via-maximum-likelihood"><span class="header-section-number">3.2</span> Point Estimation via Maximum Likelihood</a></li>
  <li><a href="#posterior-estimation" id="toc-posterior-estimation" class="nav-link" data-scroll-target="#posterior-estimation"><span class="header-section-number">3.3</span> Posterior Estimation</a></li>
  <li><a href="#case-study-learning-from-human-feedback-in-robotics" id="toc-case-study-learning-from-human-feedback-in-robotics" class="nav-link" data-scroll-target="#case-study-learning-from-human-feedback-in-robotics"><span class="header-section-number">3.4</span> Case Study: Learning from Human Feedback in Robotics</a>
  <ul class="collapse">
  <li><a href="#learning-from-demonstrations-inverse-reinforcement-learning" id="toc-learning-from-demonstrations-inverse-reinforcement-learning" class="nav-link" data-scroll-target="#learning-from-demonstrations-inverse-reinforcement-learning"><span class="header-section-number">3.4.1</span> Learning from Demonstrations (Inverse Reinforcement Learning)</a></li>
  <li><a href="#learning-from-preferences-and-rankings-of-trajectories" id="toc-learning-from-preferences-and-rankings-of-trajectories" class="nav-link" data-scroll-target="#learning-from-preferences-and-rankings-of-trajectories"><span class="header-section-number">3.4.2</span> Learning from Preferences and Rankings of Trajectories</a></li>
  <li><a href="#learning-from-trajectory-evaluations-critiques-and-ratings" id="toc-learning-from-trajectory-evaluations-critiques-and-ratings" class="nav-link" data-scroll-target="#learning-from-trajectory-evaluations-critiques-and-ratings"><span class="header-section-number">3.4.3</span> Learning from Trajectory Evaluations (Critiques and Ratings)</a></li>
  <li><a href="#learning-from-physical-corrections" id="toc-learning-from-physical-corrections" class="nav-link" data-scroll-target="#learning-from-physical-corrections"><span class="header-section-number">3.4.4</span> Learning from Physical Corrections</a></li>
  <li><a href="#combining-multiple-feedback-types" id="toc-combining-multiple-feedback-types" class="nav-link" data-scroll-target="#combining-multiple-feedback-types"><span class="header-section-number">3.4.5</span> Combining Multiple Feedback Types</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">3.5</span> Summary</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/chap3.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Learning</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Designing a good utility function (or reward function) by hand for a complex AI or robotics task is notoriously difficult and error-prone. Instead of manually specifying what is “good” behavior, we can learn a utility function from human preferences. In this chapter, we explore how an agent can infer a human’s underlying utility function (their preferences or reward criteria) from various forms of feedback. We discuss both supervised learning and Bayesian approaches to utility learning, and examine techniques motivated by robotics—learning from demonstrations, physical corrections, trajectory evaluations, and pairwise comparisons. Throughout, we include mathematical formulations and code examples to illustrate the learning process.</p>
<section id="the-supervised-learning-problem" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="the-supervised-learning-problem"><span class="header-section-number">3.1</span> The Supervised Learning Problem</h2>
<p>Supervised learning approaches treat human feedback as labeled data to directly fit a utility function. The core idea is to assume there exists a true utility function <span class="math inline">\(u^*(x)\)</span> (over states, outcomes, or trajectories <span class="math inline">\(x\)</span>) that explains a human’s choices. We then choose a parameterized model <span class="math inline">\(u_\theta(x)\)</span> and adjust <span class="math inline">\(\theta\)</span> so that <span class="math inline">\(u_\theta\)</span> agrees with the human-provided preferences.</p>
<p>A common feedback format is pairwise comparisons: the human is shown two options (outcomes or trajectories) <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> and indicates which is preferred. We can model the probability that the human prefers <span class="math inline">\(A\)</span> over <span class="math inline">\(B\)</span> using a logistic or Bradley–Terry model:</p>
<p><span class="math display">\[
P(A \succ B \mid u_\theta) \;=\; \sigma\!\Big(u_\theta(A) - u_\theta(B)\Big)\,,
\]</span></p>
<p>where <span class="math inline">\(\sigma(z)=\frac{1}{1+e^{-z}}\)</span> is the sigmoid function. This implies the human is more likely to prefer <span class="math inline">\(A\)</span> if <span class="math inline">\(u_\theta(A)\)</span> is much larger than <span class="math inline">\(u_\theta(B)\)</span>.</p>
<p>At the heart of learning from human preferences lies a latent utility function — a function that assigns numerical value to states, trajectories, or outcomes according to a human’s (possibly unspoken) preferences. The goal of a learning algorithm is to infer this function from observed feedback, which may come in the form of demonstrations, ratings, rankings, or pairwise comparisons. But how exactly do we represent and update our belief about this hidden utility function?</p>
<p>Two major paradigms in statistical learning provide different answers: point estimation and posterior estimation.</p>
<p>In point estimation, we seek a single “best guess” for the utility function — typically a function <span class="math inline">\(u_\theta(x)\)</span> from a parameterized family (e.g.&nbsp;linear models, neural nets), with parameters <span class="math inline">\(\theta \in \mathbb{R}^d\)</span>. Given data <span class="math inline">\(\mathcal{D}\)</span> from human feedback (e.g.&nbsp;preferences), we choose the parameter <span class="math inline">\(\hat{\theta}\)</span> that best explains the observed behavior. Formally:</p>
<p><span class="math display">\[
\hat{\theta} = \arg\max_\theta \; p(\mathcal{D} \mid \theta)
\]</span></p>
<p>This is maximum likelihood estimation (MLE): we pick the parameters that make the observed data most probable under our model. Once <span class="math inline">\(\hat{\theta}\)</span> is selected, we treat <span class="math inline">\(u_{\hat{\theta}}\)</span> as the agent’s utility function, and optimize or sample behavior accordingly. This approach is straightforward and computationally efficient. It is the foundation of most supervised learning methods (like logistic regression or deep learning), and it provides a natural interpretation: we’re directly finding the utility function that agrees with the human feedback. However, it discards uncertainty: it assumes the data is sufficient to pin down a single utility function, which may not be true in practice.</p>
<p>In contrast, posterior estimation takes a fully Bayesian view. Instead of committing to one estimate, we maintain a distribution over utility functions. That is, we place a prior <span class="math inline">\(p(\theta)\)</span> over parameters (or over functions <span class="math inline">\(u\)</span> more generally), and update this to a posterior after observing data <span class="math inline">\(\mathcal{D}\)</span>:</p>
<p><span class="math display">\[
p(\theta \mid \mathcal{D}) \;=\; \frac{p(\mathcal{D} \mid \theta)\, p(\theta)}{p(\mathcal{D})}
\]</span></p>
<p>This posterior expresses our uncertainty over which utility functions are compatible with the human feedback. From this distribution, we can make predictions (e.g., using the posterior mean utility), quantify confidence, or even actively select new queries to reduce uncertainty (active learning). For instance, if we model utilities with a Gaussian Process (GP), then the posterior over <span class="math inline">\(u(x)\)</span> is also a GP after observing comparisons or evaluations. If we use a neural network for <span class="math inline">\(u_\theta(x)\)</span>, we can approximate the posterior with ensembles, variational inference, or MCMC. Posterior estimation is especially valuable when human feedback is sparse, noisy, or ambiguous — as is often the case in real-world preference learning. It allows the agent to reason about what it doesn’t know and to take cautious or exploratory actions accordingly.</p>
<p>The next two sections instantiate these two perspectives. In Section 4.1, we explore point estimation via supervised learning — treating preference data as labeled examples and fitting a utility model. In Section 4.2, we shift to posterior estimation with Bayesian methods like Gaussian processes and Bayesian neural networks, which model both our current estimate and the uncertainty around it.</p>
</section>
<section id="point-estimation-via-maximum-likelihood" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="point-estimation-via-maximum-likelihood"><span class="header-section-number">3.2</span> Point Estimation via Maximum Likelihood</h2>
<p>Given a dataset of comparisons <span class="math inline">\(D=\{(A_i, B_i, y_i)\}\)</span> (with <span class="math inline">\(y_i=1\)</span> if <span class="math inline">\(A_i\)</span> was preferred and <span class="math inline">\(0\)</span> if <span class="math inline">\(B_i\)</span> was preferred), we can fit <span class="math inline">\(\theta\)</span> by maximizing the likelihood of the human’s choices. Equivalently, we minimize a binary cross-entropy loss:</p>
<p><span class="math display">\[
\mathcal{L}(\theta) = -\sum_{i} \Big[\,y_i \log \sigma(u_\theta(A_i)\!-\!u_\theta(B_i)) + (1-y_i)\log(1-\sigma(u_\theta(A_i)\!-\!u_\theta(B_i)))\Big]\,,
\]</span></p>
<p>often with a regularization term to prevent overfitting. This is a straightforward supervised learning problem – essentially logistic regression – on pairwise difference features.</p>
<p>Example: Suppose a human’s utility for an outcome can be described by a quadratic function (unknown to the learning algorithm). We collect some pairwise preferences and then train a utility model <span class="math inline">\(u_\theta(x)\)</span> to predict those preferences. The code below simulates this scenario:</p>
<div id="a81d4a4e" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co"># True utility function (unknown to learner), e.g. u*(x) = -(x-5)^2 + constant </span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="kw">def</span> true_utility(x):</span>
<span id="cb1-5"><a href="#cb1-5"></a>    <span class="cf">return</span> <span class="op">-</span>(x<span class="op">-</span><span class="dv">5</span>)<span class="op">**</span><span class="dv">2</span>  <span class="co"># (peak at x=5)</span></span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co"># Generate synthetic pairwise preference data</span></span>
<span id="cb1-8"><a href="#cb1-8"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-9"><a href="#cb1-9"></a>n_pairs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb1-10"><a href="#cb1-10"></a>X1 <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">10</span>, size<span class="op">=</span>n_pairs)  <span class="co"># 20 random x-values</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>X2 <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">10</span>, size<span class="op">=</span>n_pairs)  <span class="co"># 20 more random x-values</span></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co"># Determine preferences according to true utility</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>prefs <span class="op">=</span> (true_utility(X1) <span class="op">&gt;</span> true_utility(X2)).astype(<span class="bu">int</span>)  <span class="co"># 1 if X1 preferred, else 0</span></span>
<span id="cb1-14"><a href="#cb1-14"></a></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="co"># Parametric model for utility: u_theta(x) = w0 + w1*x + w2*x^2  (quadratic form)</span></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="co"># Initialize weights</span></span>
<span id="cb1-17"><a href="#cb1-17"></a>w <span class="op">=</span> np.zeros(<span class="dv">3</span>)</span>
<span id="cb1-18"><a href="#cb1-18"></a>lr <span class="op">=</span> <span class="fl">0.01</span>       <span class="co"># learning rate</span></span>
<span id="cb1-19"><a href="#cb1-19"></a>reg <span class="op">=</span> <span class="fl">1e-3</span>      <span class="co"># L2 regularization strength</span></span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb1-21"><a href="#cb1-21"></a>    <span class="co"># Compute predictions via logistic model</span></span>
<span id="cb1-22"><a href="#cb1-22"></a>    util_diff <span class="op">=</span> (w[<span class="dv">0</span>] <span class="op">+</span> w[<span class="dv">1</span>]<span class="op">*</span>X1 <span class="op">+</span> w[<span class="dv">2</span>]<span class="op">*</span>X1<span class="op">**</span><span class="dv">2</span>) <span class="op">-</span> (w[<span class="dv">0</span>] <span class="op">+</span> w[<span class="dv">1</span>]<span class="op">*</span>X2 <span class="op">+</span> w[<span class="dv">2</span>]<span class="op">*</span>X2<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-23"><a href="#cb1-23"></a>    pred <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>util_diff))      <span class="co"># σ(w·(phi(X1)-phi(X2)))</span></span>
<span id="cb1-24"><a href="#cb1-24"></a>    <span class="co"># Gradient of cross-entropy loss</span></span>
<span id="cb1-25"><a href="#cb1-25"></a>    grad <span class="op">=</span> np.array([<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>])</span>
<span id="cb1-26"><a href="#cb1-26"></a>    error <span class="op">=</span> pred <span class="op">-</span> prefs  <span class="co"># (sigma - y)</span></span>
<span id="cb1-27"><a href="#cb1-27"></a>    <span class="co"># Features for X1 and X2</span></span>
<span id="cb1-28"><a href="#cb1-28"></a>    phi1 <span class="op">=</span> np.vstack([np.ones(n_pairs), X1, X1<span class="op">**</span><span class="dv">2</span>]).T</span>
<span id="cb1-29"><a href="#cb1-29"></a>    phi2 <span class="op">=</span> np.vstack([np.ones(n_pairs), X2, X2<span class="op">**</span><span class="dv">2</span>]).T</span>
<span id="cb1-30"><a href="#cb1-30"></a>    phi_diff <span class="op">=</span> phi1 <span class="op">-</span> phi2</span>
<span id="cb1-31"><a href="#cb1-31"></a>    <span class="co"># Gradient: derivative of loss w.rt w = (sigma - y)*φ_diff (averaged) + reg</span></span>
<span id="cb1-32"><a href="#cb1-32"></a>    grad <span class="op">=</span> phi_diff.T.dot(error) <span class="op">/</span> n_pairs <span class="op">+</span> reg <span class="op">*</span> w</span>
<span id="cb1-33"><a href="#cb1-33"></a>    <span class="co"># Update weights</span></span>
<span id="cb1-34"><a href="#cb1-34"></a>    w <span class="op">-=</span> lr <span class="op">*</span> grad</span>
<span id="cb1-35"><a href="#cb1-35"></a></span>
<span id="cb1-36"><a href="#cb1-36"></a><span class="bu">print</span>(<span class="st">"Learned weights:"</span>, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Learned weights: [ 0.          2.74417195 -0.22129969]</code></pre>
</div>
</div>
<p>After training, we can compare the learned utility function <span class="math inline">\(u_\theta(x)\)</span> to the true utility <span class="math inline">\(u^*(x)\)</span>. Below we plot the two functions:</p>
<div id="5be29fec" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2"></a></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="co"># Plot true vs learned utility curves</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>xs <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">200</span>)</span>
<span id="cb3-5"><a href="#cb3-5"></a>true_vals <span class="op">=</span> true_utility(xs)</span>
<span id="cb3-6"><a href="#cb3-6"></a>learned_vals <span class="op">=</span> w[<span class="dv">0</span>] <span class="op">+</span> w[<span class="dv">1</span>]<span class="op">*</span>xs <span class="op">+</span> w[<span class="dv">2</span>]<span class="op">*</span>xs<span class="op">**</span><span class="dv">2</span></span>
<span id="cb3-7"><a href="#cb3-7"></a></span>
<span id="cb3-8"><a href="#cb3-8"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">4</span>))</span>
<span id="cb3-9"><a href="#cb3-9"></a>plt.plot(xs, true_vals, label<span class="op">=</span><span class="st">"True Utility"</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb3-10"><a href="#cb3-10"></a>plt.plot(xs, learned_vals, label<span class="op">=</span><span class="st">"Learned Utility"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb3-11"><a href="#cb3-11"></a>plt.xlabel(<span class="st">"State x"</span>)</span>
<span id="cb3-12"><a href="#cb3-12"></a>plt.ylabel(<span class="st">"Utility"</span>)</span>
<span id="cb3-13"><a href="#cb3-13"></a>plt.title(<span class="st">"True vs. Learned Utility Function"</span>)</span>
<span id="cb3-14"><a href="#cb3-14"></a>plt.legend()</span>
<span id="cb3-15"><a href="#cb3-15"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chap3_files/figure-html/cell-3-output-1.png" width="521" height="376" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The learned curve closely matches the true utility up to an arbitrary scaling factor (utility is only defined up to affine transform when inferred from comparisons). The algorithm successfully recovered a utility function that orders states almost the same as the true utility <span class="math inline">\(u^*(x)\)</span>. In general, learning from comparisons can infer the <em>relative</em> utility of options (which item is preferred), although the absolute scale of <span class="math inline">\(u_\theta\)</span> is unidentifiable without further assumptions. Supervised learning on preferences has been widely used for ranking problems and preference-based reward learning.</p>
<p>While the above approach learns a utility and could then use it for decision-making, an alternative in some domains is to directly optimize the policy based on preferences, bypassing an explicit reward model. Direct Preference Optimization (DPO) is a recent technique introduced for aligning language models to human preferences <span class="citation" data-cites="rafailov2023direct">(<a href="#ref-rafailov2023direct" role="doc-biblioref">Rafailov et al. 2023</a>)</span>. The insight of DPO is that we can fold the reward model into the policy itself, and then update the policy via a simple supervised objective on preference data.</p>
<p>In the context of DPO for language models, one has a base policy <span class="math inline">\(\pi_{\text{ref}}\)</span> (e.g.&nbsp;the pre-trained model) and a new policy <span class="math inline">\(\pi_\theta\)</span> we want to fine-tune. Given a human preference between two model outputs (responses) <span class="math inline">\(y_+\)</span> (preferred) vs <span class="math inline">\(y_-\)</span> for the same prompt <span class="math inline">\(x\)</span>, DPO adjusts <span class="math inline">\(\pi_\theta\)</span> to increase the log-probability of <span class="math inline">\(y_+\)</span> and decrease that of <span class="math inline">\(y_-\)</span>. Formally, the <em>DPO loss</em> for a single preference is:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{DPO}}(\theta) \;=\; -\log \sigma\!\Big(\underbrace{\log \pi_\theta(y_+|x) - \log \pi_\theta(y_-|x)}_{\text{policy preference score}}\Big)\,.
\]</span></p>
<p>Intuitively, this is the same logistic loss as before, now directly in terms of the policy’s log-likelihood scores for each option. The overall objective is the average of this loss over a dataset of human preference comparisons. Optimizing this loss via gradient descent on <span class="math inline">\(\pi_\theta\)</span> effectively <em>bakes in</em> a reward model: <span class="math inline">\(\log \pi_\theta(y|x) - \log \pi_{\text{ref}}(y|x)\)</span> plays the role of a learned reward. In fact, one can show that DPO is implicitly fitting a reward function that <span class="math inline">\(\pi_\theta\)</span> is maximizing. Crucially, DPO does <em>not</em> require a separate reinforcement learning loop; it treats preference data as direct supervision for the policy, using a simple binary classification objective.</p>
<p>In practice, DPO has been shown to achieve similar or better alignment performance compared to reinforcement learning from human feedback (RLHF) while being more stable and easier to implement. It avoids the need to sample from the model during training or tune delicate hyperparameters of RL. Conceptually, DPO demonstrates that if we structure our utility model cleverly (here, as the log-ratio of policy and reference), we can extract an optimal policy in closed-form and learn utilities via supervised learning.</p>
<p>DPO’s formulation is related to the Bradley–Terry model for preferences and to the idea of <em>maximum entropy</em> policies. If we imagine a reward function <span class="math inline">\(R_\theta(y|x) = \log \pi_\theta(y|x) - \log \pi_{\text{ref}}(y|x)\)</span>, DPO’s loss is pushing <span class="math inline">\(R_\theta(y_+) &gt; R_\theta(y_-)\)</span> for preferred pairs. The optimal <span class="math inline">\(\pi_\theta\)</span> under this criterion turns out to be <span class="math inline">\(\pi_\theta(y|x) \propto \pi_{\text{ref}}(y|x)\exp\{R_\theta(y|x)\}\)</span>. In other words, <span class="math inline">\(\pi_\theta\)</span> is proportional to the reference policy re-weighted by an exponentiated reward – a result consistent with the principle of maximum entropy (softmax) optimization. DPO thus blurs the line between learning a utility function and learning a policy; it directly outputs a policy that (implicitly) encodes the learned utility.</p>
<p>The pairwise logistic approach can be extended to other feedback types. If humans provide numeric <em>ratings</em> or <em>scores</em> for options, one can treat utility learning as a regression problem: fit <span class="math inline">\(u_\theta(x)\)</span> to predict those scores (perhaps with a suitable bounded output or ordinal regression if scores are ordinal). If humans rank multiple options at once, algorithms like <em>RankNet</em> or <em>RankSVM</em> generalize the pairwise approach to listwise ranking losses. All these methods boil down to defining a loss that penalizes disagreements between the predicted utility order and the human-provided preferences, then optimizing <span class="math inline">\(\theta\)</span> to minimize that loss.</p>
<p>Supervised learning of utility is powerful due to its simplicity, but it typically provides point estimates of <span class="math inline">\(u_\theta\)</span>. Next, we consider Bayesian approaches that maintain uncertainty over the utility function.</p>
</section>
<section id="posterior-estimation" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="posterior-estimation"><span class="header-section-number">3.3</span> Posterior Estimation</h2>
<p>When feedback data is sparse, as is common in preference learning, it can be advantageous to model uncertainty over the utility function. Bayesian approaches place a prior on the utility function and update a posterior as human feedback is observed. This yields not only a best-guess utility function but also a measure of confidence or uncertainty, which is valuable for active learning (deciding which queries to ask next) and for safety (knowing when the learned reward might be wrong).</p>
<p>A popular Bayesian approach assumes that the human’s utility function can be modeled as a Gaussian Process (GP) – a distribution over functions. A GP prior is defined by a mean function (often taken to be 0 for convenience) and a kernel (covariance function) <span class="math inline">\(k(x,x')\)</span> which encodes assumptions about the smoothness or structure of the utility function. For example, one might assume <span class="math inline">\(u(x)\)</span> is a smooth function of state, and choose a radial basis function (RBF) kernel <span class="math inline">\(k(x,x') = \sigma_f^2 \exp(-\|x-x'\|^2/(2\ell^2))\)</span> with some length-scale <span class="math inline">\(\ell\)</span>.</p>
<p>After observing some preference data, Bayes’ rule gives a posterior over the function <span class="math inline">\(u(x)\)</span>. In the case of pairwise comparisons, the likelihood of a comparison <span class="math inline">\((A \succ B)\)</span> given an underlying utility function <span class="math inline">\(u\)</span> can be modeled via the same logistic function: <span class="math inline">\(P(A \succ B \mid u) = \sigma(u(A)-u(B))\)</span>. Combining this likelihood with the GP prior is analytically intractable (due to the non-Gaussian logistic likelihood), but one can use approximation techniques (Laplace approximation or MCMC) to obtain a posterior GP. The result is a <em>Gaussian process preference model</em> that can predict the utility of any new option with an uncertainty interval.</p>
<p>If we have direct evaluations of utility (e.g., the human provides a numeric reward for some states), the GP inference is simpler – it reduces to standard GP regression. However, in many real-world scenarios, humans are better at making relative judgments than assigning absolute utility values. This change in feedback type transforms the inference problem fundamentally. Instead of having a Gaussian likelihood (as in standard GP regression), we now have a non-Gaussian likelihood, typically modeled using a probit or logistic function. The observed data no longer provide direct samples of the latent utility function, but instead impose constraints on the <em>relative</em> ordering of latent values.</p>
<p>Due to this non-Gaussian likelihood, exact Bayesian inference is no longer tractable: the posterior over the latent utility function given the pairwise data does not have a closed-form expression. The GP prior is still Gaussian, but the posterior becomes non-Gaussian and multi-modal, particularly as the number of comparisons grows.</p>
<p>To address this, we must turn to approximate inference methods. One common and computationally efficient choice is the Laplace approximation, which approximates the true posterior with a Gaussian centered at the maximum a posteriori (MAP) estimate. This involves: 1. Finding the mode of the posterior (i.e., the most probable utility values given the data), 2. Approximating the curvature of the log-posterior around this mode using the Hessian (second derivative), 3. Using this local curvature to construct a Gaussian approximation.</p>
<p>While not exact, this method works well in practice, especially when the posterior is unimodal and the number of comparison pairs is moderate. Other alternatives such as variational inference or sampling-based methods (e.g., Hamiltonian Monte Carlo) can yield more accurate results but often require more complex implementation and computational resources.</p>
<div id="34978a49" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb4-5"><a href="#cb4-5"></a></span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="co"># --- True latent utility function ---</span></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="kw">def</span> true_u(x):</span>
<span id="cb4-8"><a href="#cb4-8"></a>    <span class="cf">return</span> np.sin(x) <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> x</span>
<span id="cb4-9"><a href="#cb4-9"></a></span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="co"># --- RBF Kernel function ---</span></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="kw">def</span> rbf_kernel(x1, x2, length_scale<span class="op">=</span><span class="fl">0.8</span>, sigma_f<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb4-12"><a href="#cb4-12"></a>    x1, x2 <span class="op">=</span> np.atleast_2d(x1).T, np.atleast_2d(x2).T</span>
<span id="cb4-13"><a href="#cb4-13"></a>    sqdist <span class="op">=</span> (x1 <span class="op">-</span> x2.T) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb4-14"><a href="#cb4-14"></a>    <span class="cf">return</span> sigma_f<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> sqdist <span class="op">/</span> length_scale<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb4-15"><a href="#cb4-15"></a></span>
<span id="cb4-16"><a href="#cb4-16"></a><span class="co"># --- Generate synthetic preference data ---</span></span>
<span id="cb4-17"><a href="#cb4-17"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-18"><a href="#cb4-18"></a>num_pairs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb4-19"><a href="#cb4-19"></a>X_candidates <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb4-20"><a href="#cb4-20"></a>true_utilities <span class="op">=</span> true_u(X_candidates)</span>
<span id="cb4-21"><a href="#cb4-21"></a></span>
<span id="cb4-22"><a href="#cb4-22"></a><span class="co"># Sample preference pairs</span></span>
<span id="cb4-23"><a href="#cb4-23"></a>idx_pairs <span class="op">=</span> np.random.choice(<span class="bu">len</span>(X_candidates), size<span class="op">=</span>(num_pairs, <span class="dv">2</span>), replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-24"><a href="#cb4-24"></a>X_pref_pairs <span class="op">=</span> []</span>
<span id="cb4-25"><a href="#cb4-25"></a><span class="cf">for</span> i, j <span class="kw">in</span> idx_pairs:</span>
<span id="cb4-26"><a href="#cb4-26"></a>    xi, xj <span class="op">=</span> X_candidates[i], X_candidates[j]</span>
<span id="cb4-27"><a href="#cb4-27"></a>    <span class="cf">if</span> true_utilities[i] <span class="op">&gt;</span> true_utilities[j]:</span>
<span id="cb4-28"><a href="#cb4-28"></a>        X_pref_pairs.append((xi, xj))</span>
<span id="cb4-29"><a href="#cb4-29"></a>    <span class="cf">else</span>:</span>
<span id="cb4-30"><a href="#cb4-30"></a>        X_pref_pairs.append((xj, xi))</span>
<span id="cb4-31"><a href="#cb4-31"></a>X_pref_pairs <span class="op">=</span> np.array(X_pref_pairs)</span>
<span id="cb4-32"><a href="#cb4-32"></a></span>
<span id="cb4-33"><a href="#cb4-33"></a><span class="co"># --- Unique x values and indexing ---</span></span>
<span id="cb4-34"><a href="#cb4-34"></a>X_all <span class="op">=</span> np.unique(X_pref_pairs.flatten())</span>
<span id="cb4-35"><a href="#cb4-35"></a>n <span class="op">=</span> <span class="bu">len</span>(X_all)</span>
<span id="cb4-36"><a href="#cb4-36"></a>x_to_idx <span class="op">=</span> {x: i <span class="cf">for</span> i, x <span class="kw">in</span> <span class="bu">enumerate</span>(X_all)}</span>
<span id="cb4-37"><a href="#cb4-37"></a></span>
<span id="cb4-38"><a href="#cb4-38"></a><span class="co"># --- GP prior kernel matrix ---</span></span>
<span id="cb4-39"><a href="#cb4-39"></a>length_scale <span class="op">=</span> <span class="fl">0.8</span></span>
<span id="cb4-40"><a href="#cb4-40"></a>sigma_f <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb4-41"><a href="#cb4-41"></a>sigma_noise <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb4-42"><a href="#cb4-42"></a>K <span class="op">=</span> rbf_kernel(X_all, X_all, length_scale, sigma_f) <span class="op">+</span> sigma_noise <span class="op">*</span> np.eye(n)</span>
<span id="cb4-43"><a href="#cb4-43"></a></span>
<span id="cb4-44"><a href="#cb4-44"></a><span class="co"># --- Negative log-posterior function ---</span></span>
<span id="cb4-45"><a href="#cb4-45"></a><span class="kw">def</span> neg_log_posterior(f):</span>
<span id="cb4-46"><a href="#cb4-46"></a>    prior_term <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> f.T <span class="op">@</span> np.linalg.solve(K, f)</span>
<span id="cb4-47"><a href="#cb4-47"></a>    lik_term <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb4-48"><a href="#cb4-48"></a>    <span class="cf">for</span> xi, xj <span class="kw">in</span> X_pref_pairs:</span>
<span id="cb4-49"><a href="#cb4-49"></a>        fi, fj <span class="op">=</span> f[x_to_idx[xi]], f[x_to_idx[xj]]</span>
<span id="cb4-50"><a href="#cb4-50"></a>        delta <span class="op">=</span> (fi <span class="op">-</span> fj) <span class="op">/</span> np.sqrt(<span class="dv">2</span>)</span>
<span id="cb4-51"><a href="#cb4-51"></a>        lik_term <span class="op">-=</span> np.log(norm.cdf(delta) <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb4-52"><a href="#cb4-52"></a>    <span class="cf">return</span> prior_term <span class="op">+</span> lik_term</span>
<span id="cb4-53"><a href="#cb4-53"></a></span>
<span id="cb4-54"><a href="#cb4-54"></a><span class="co"># --- MAP estimation of latent utilities ---</span></span>
<span id="cb4-55"><a href="#cb4-55"></a>f_init <span class="op">=</span> np.zeros(n)</span>
<span id="cb4-56"><a href="#cb4-56"></a>res <span class="op">=</span> minimize(neg_log_posterior, f_init, method<span class="op">=</span><span class="st">"L-BFGS-B"</span>)</span>
<span id="cb4-57"><a href="#cb4-57"></a>f_map <span class="op">=</span> res.x</span>
<span id="cb4-58"><a href="#cb4-58"></a></span>
<span id="cb4-59"><a href="#cb4-59"></a><span class="co"># --- Laplace approximation: compute W (Hessian of neg log likelihood) ---</span></span>
<span id="cb4-60"><a href="#cb4-60"></a>W <span class="op">=</span> np.zeros((n, n))</span>
<span id="cb4-61"><a href="#cb4-61"></a><span class="cf">for</span> xi, xj <span class="kw">in</span> X_pref_pairs:</span>
<span id="cb4-62"><a href="#cb4-62"></a>    i, j <span class="op">=</span> x_to_idx[xi], x_to_idx[xj]</span>
<span id="cb4-63"><a href="#cb4-63"></a>    fi, fj <span class="op">=</span> f_map[i], f_map[j]</span>
<span id="cb4-64"><a href="#cb4-64"></a>    delta <span class="op">=</span> (fi <span class="op">-</span> fj) <span class="op">/</span> np.sqrt(<span class="dv">2</span>)</span>
<span id="cb4-65"><a href="#cb4-65"></a>    phi <span class="op">=</span> norm.pdf(delta)</span>
<span id="cb4-66"><a href="#cb4-66"></a>    Phi <span class="op">=</span> norm.cdf(delta) <span class="op">+</span> <span class="fl">1e-6</span></span>
<span id="cb4-67"><a href="#cb4-67"></a>    w <span class="op">=</span> (phi <span class="op">/</span> Phi)<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> delta <span class="op">*</span> phi <span class="op">/</span> Phi</span>
<span id="cb4-68"><a href="#cb4-68"></a>    w <span class="op">/=</span> <span class="dv">2</span>  <span class="co"># adjust for sqrt(2)</span></span>
<span id="cb4-69"><a href="#cb4-69"></a>    W[i, i] <span class="op">+=</span> w</span>
<span id="cb4-70"><a href="#cb4-70"></a>    W[j, j] <span class="op">+=</span> w</span>
<span id="cb4-71"><a href="#cb4-71"></a>    W[i, j] <span class="op">-=</span> w</span>
<span id="cb4-72"><a href="#cb4-72"></a>    W[j, i] <span class="op">-=</span> w</span>
<span id="cb4-73"><a href="#cb4-73"></a></span>
<span id="cb4-74"><a href="#cb4-74"></a><span class="co"># --- Posterior covariance approximation ---</span></span>
<span id="cb4-75"><a href="#cb4-75"></a>L <span class="op">=</span> np.linalg.cholesky(K)</span>
<span id="cb4-76"><a href="#cb4-76"></a>K_inv <span class="op">=</span> np.linalg.solve(L.T, np.linalg.solve(L, np.eye(n)))</span>
<span id="cb4-77"><a href="#cb4-77"></a>H <span class="op">=</span> K_inv <span class="op">+</span> W</span>
<span id="cb4-78"><a href="#cb4-78"></a>H_inv <span class="op">=</span> np.linalg.inv(H)</span>
<span id="cb4-79"><a href="#cb4-79"></a></span>
<span id="cb4-80"><a href="#cb4-80"></a><span class="co"># --- Prediction at test points ---</span></span>
<span id="cb4-81"><a href="#cb4-81"></a>X_test <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">200</span>)</span>
<span id="cb4-82"><a href="#cb4-82"></a>K_s <span class="op">=</span> rbf_kernel(X_all, X_test, length_scale, sigma_f)</span>
<span id="cb4-83"><a href="#cb4-83"></a>K_ss_diag <span class="op">=</span> np.diag(rbf_kernel(X_test, X_test, length_scale, sigma_f))</span>
<span id="cb4-84"><a href="#cb4-84"></a></span>
<span id="cb4-85"><a href="#cb4-85"></a><span class="co"># Posterior mean and variance</span></span>
<span id="cb4-86"><a href="#cb4-86"></a>posterior_mean <span class="op">=</span> K_s.T <span class="op">@</span> K_inv <span class="op">@</span> f_map</span>
<span id="cb4-87"><a href="#cb4-87"></a>temp <span class="op">=</span> np.linalg.solve(H, K_s)</span>
<span id="cb4-88"><a href="#cb4-88"></a>posterior_var <span class="op">=</span> K_ss_diag <span class="op">-</span> np.<span class="bu">sum</span>(K_s <span class="op">*</span> temp, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-89"><a href="#cb4-89"></a>posterior_std <span class="op">=</span> np.sqrt(np.maximum(posterior_var, <span class="dv">0</span>))</span>
<span id="cb4-90"><a href="#cb4-90"></a></span>
<span id="cb4-91"><a href="#cb4-91"></a><span class="co"># --- Visualization ---</span></span>
<span id="cb4-92"><a href="#cb4-92"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb4-93"><a href="#cb4-93"></a>plt.plot(X_test, true_u(X_test), <span class="st">'k--'</span>, label<span class="op">=</span><span class="st">"True utility"</span>)</span>
<span id="cb4-94"><a href="#cb4-94"></a>plt.plot(X_test, posterior_mean, <span class="st">'b-'</span>, label<span class="op">=</span><span class="st">"Posterior mean"</span>)</span>
<span id="cb4-95"><a href="#cb4-95"></a>plt.fill_between(X_test,</span>
<span id="cb4-96"><a href="#cb4-96"></a>                 posterior_mean <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> posterior_std,</span>
<span id="cb4-97"><a href="#cb4-97"></a>                 posterior_mean <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> posterior_std,</span>
<span id="cb4-98"><a href="#cb4-98"></a>                 color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">"95% CI"</span>)</span>
<span id="cb4-99"><a href="#cb4-99"></a>plt.scatter(X_all, [true_u(x) <span class="cf">for</span> x <span class="kw">in</span> X_all], c<span class="op">=</span><span class="st">'red'</span>, marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">"Observed x"</span>)</span>
<span id="cb4-100"><a href="#cb4-100"></a>plt.title(<span class="st">"GP Preference Learning (Laplace Approximation, 100 Pairs)"</span>)</span>
<span id="cb4-101"><a href="#cb4-101"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb4-102"><a href="#cb4-102"></a>plt.ylabel(<span class="st">"Utility"</span>)</span>
<span id="cb4-103"><a href="#cb4-103"></a>plt.legend()</span>
<span id="cb4-104"><a href="#cb4-104"></a>plt.tight_layout()</span>
<span id="cb4-105"><a href="#cb4-105"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chap3_files/figure-html/cell-4-output-1.png" width="757" height="372" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>(<a href="">image</a>) <em>Gaussian Process posterior for a utility function (blue mean with 95% confidence band) after observing 5 points of noisy utility data (red ×). The true utility function (black dashed) is non-trivial. The GP correctly captures the function’s value around observed regions and expresses high uncertainty in the unobserved middle region. In practice, this uncertainty could guide an algorithm to query more feedback in the region <span class="math inline">\(x\approx [4,7]\)</span> to reduce ambiguity.</em></p>
<p>Gaussian processes are a flexible way to learn utility functions. They naturally handle irregular data and provide principled uncertainty estimates. GP-based preference learning has been applied to tasks like interactive Bayesian optimization, where an algorithm seeks to find the maximum of <span class="math inline">\(u(x)\)</span> by iteratively querying a human which of two options is better <span class="citation" data-cites="christiano2023deep">(<a href="#ref-christiano2023deep" role="doc-biblioref">Christiano et al. 2023</a>)</span>.</p>
<p>Instead of GPs, one can use Bayesian neural networks or ensemble methods to model uncertainty in <span class="math inline">\(u_\theta(x)\)</span>. For instance, a neural network can be trained on preference data, and techniques like Monte Carlo dropout or deep ensembles can provide uncertainty estimates for its predictions. These approaches scale to high-dimensional inputs (where GPs may be less practical) while still capturing epistemic uncertainty about the utility.</p>
<p>One principled way to capture uncertainty in Bayesian neural networks is via Markov Chain Monte Carlo (MCMC) methods, which seek to approximate the posterior distribution over model parameters given the data. In this setting, we place a prior over the neural network weights, <span class="math inline">\(p(\theta)\)</span>, and define a likelihood function based on observed preferences—typically using a probabilistic choice model such as the Bradley-Terry or probit model. Given a dataset <span class="math inline">\(\mathcal{D} = \{(x_i, x_j) : x_i \succ x_j\}\)</span>, the posterior is defined as</p>
<p><span class="math display">\[
p(\theta \mid \mathcal{D}) \propto p(\mathcal{D} \mid \theta) \cdot p(\theta),
\]</span></p>
<p>where <span class="math inline">\(p(\mathcal{D} \mid \theta)\)</span> is the likelihood of observing the pairwise comparisons under the utility function <span class="math inline">\(u_\theta(x)\)</span>, and <span class="math inline">\(p(\theta)\)</span> is the prior over the parameters.</p>
<p>Unlike Gaussian processes, for which posterior inference is tractable in closed form under Gaussian likelihoods, inference in BNNs with non-Gaussian likelihoods is generally intractable. This is due to the non-conjugate nature of the neural network likelihood and the high-dimensional, nonlinear structure of the weight space. As a result, approximate inference methods are required.</p>
<p>MCMC provides a general-purpose approach to approximate sampling from the posterior. The key idea is to construct a Markov chain whose stationary distribution is the target posterior. One of the most widely used algorithms is the Metropolis-Hastings (MH) algorithm. Given a current state <span class="math inline">\(\theta_t\)</span>, a new proposal <span class="math inline">\(\theta'\)</span> is generated from a proposal distribution <span class="math inline">\(q(\theta' \mid \theta_t)\)</span>, and accepted with probability</p>
<p><span class="math display">\[
A = \min\left(1, \frac{p(\mathcal{D} \mid \theta') \, p(\theta') \, q(\theta_t \mid \theta')}{p(\mathcal{D} \mid \theta_t) \, p(\theta_t) \, q(\theta' \mid \theta_t)}\right).
\]</span></p>
<p>When the proposal distribution is symmetric, i.e., <span class="math inline">\(q(\theta' \mid \theta_t) = q(\theta_t \mid \theta')\)</span>, the acceptance probability simplifies to a ratio of posterior densities. Over time, the chain yields samples <span class="math inline">\(\theta^{(1)}, \dots, \theta^{(T)} \sim p(\theta \mid \mathcal{D})\)</span>, which can be used to compute posterior predictive estimates for the utility function:</p>
<p><span class="math display">\[
\mathbb{E}[u(x)] \approx \frac{1}{T} \sum_{t=1}^T u_{\theta^{(t)}}(x),
\]</span></p>
<p>with corresponding uncertainty estimates captured via the variance of the predictions across samples.</p>
<p>MCMC methods are particularly appealing for preference learning because they directly quantify epistemic uncertainty in the utility function, which is crucial for downstream tasks such as decision-making, active learning, and safe exploration. Furthermore, MCMC makes no restrictive assumptions on the form of the posterior and can be used with non-convex and multi-modal distributions that arise from complex neural network architectures.</p>
<p>However, MCMC also faces significant computational challenges in practice. First, the convergence of the Markov chain can be slow, especially in high-dimensional parameter spaces. Second, naive random-walk proposals (as in the basic Metropolis-Hastings algorithm) may suffer from low acceptance rates and poor mixing. More advanced MCMC methods such as Hamiltonian Monte Carlo (HMC) and No-U-Turn Sampling (NUTS) can help address these issues by using gradient information to propose more efficient moves through the parameter space.</p>
<p>Despite these limitations, MCMC remains a valuable tool for principled Bayesian inference in preference modeling, particularly in settings where uncertainty quantification is critical and computational cost is acceptable. In lower-dimensional settings or as a pedagogical tool, even simple MH-based approaches can offer intuitive and effective approximations to the posterior over preference functions.</p>
<div id="b20cfe8e" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="co"># --- True latent utility function ---</span></span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="kw">def</span> true_u(x):</span>
<span id="cb5-7"><a href="#cb5-7"></a>    <span class="cf">return</span> np.sin(x) <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> x</span>
<span id="cb5-8"><a href="#cb5-8"></a></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="co"># --- Generate synthetic preference data ---</span></span>
<span id="cb5-10"><a href="#cb5-10"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb5-11"><a href="#cb5-11"></a>X <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">40</span>)</span>
<span id="cb5-12"><a href="#cb5-12"></a>y_true <span class="op">=</span> true_u(X)</span>
<span id="cb5-13"><a href="#cb5-13"></a></span>
<span id="cb5-14"><a href="#cb5-14"></a><span class="co"># Create pairwise comparisons</span></span>
<span id="cb5-15"><a href="#cb5-15"></a>pairs <span class="op">=</span> []</span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>):</span>
<span id="cb5-17"><a href="#cb5-17"></a>    i, j <span class="op">=</span> np.random.choice(<span class="bu">len</span>(X), <span class="dv">2</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-18"><a href="#cb5-18"></a>    <span class="cf">if</span> y_true[i] <span class="op">&gt;</span> y_true[j]:</span>
<span id="cb5-19"><a href="#cb5-19"></a>        pairs.append((X[i], X[j], <span class="dv">1</span>))  <span class="co"># x_i preferred over x_j</span></span>
<span id="cb5-20"><a href="#cb5-20"></a>    <span class="cf">else</span>:</span>
<span id="cb5-21"><a href="#cb5-21"></a>        pairs.append((X[j], X[i], <span class="dv">1</span>))</span>
<span id="cb5-22"><a href="#cb5-22"></a></span>
<span id="cb5-23"><a href="#cb5-23"></a><span class="co"># --- Define a deep neural network: 3 hidden layers ---</span></span>
<span id="cb5-24"><a href="#cb5-24"></a><span class="kw">def</span> init_deep_params(hidden_dims<span class="op">=</span>[<span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">10</span>]):</span>
<span id="cb5-25"><a href="#cb5-25"></a>    params <span class="op">=</span> {}</span>
<span id="cb5-26"><a href="#cb5-26"></a>    layer_dims <span class="op">=</span> [<span class="dv">1</span>] <span class="op">+</span> hidden_dims <span class="op">+</span> [<span class="dv">1</span>]</span>
<span id="cb5-27"><a href="#cb5-27"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(layer_dims) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb5-28"><a href="#cb5-28"></a>        W_key <span class="op">=</span> <span class="ss">f"W</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span></span>
<span id="cb5-29"><a href="#cb5-29"></a>        b_key <span class="op">=</span> <span class="ss">f"b</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span></span>
<span id="cb5-30"><a href="#cb5-30"></a>        params[W_key] <span class="op">=</span> np.random.randn(layer_dims[i<span class="op">+</span><span class="dv">1</span>], layer_dims[i]) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb5-31"><a href="#cb5-31"></a>        params[b_key] <span class="op">=</span> np.zeros((layer_dims[i<span class="op">+</span><span class="dv">1</span>], <span class="dv">1</span>))</span>
<span id="cb5-32"><a href="#cb5-32"></a>    <span class="cf">return</span> params</span>
<span id="cb5-33"><a href="#cb5-33"></a></span>
<span id="cb5-34"><a href="#cb5-34"></a><span class="kw">def</span> deep_forward(x, params):</span>
<span id="cb5-35"><a href="#cb5-35"></a>    x <span class="op">=</span> x.reshape(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb5-36"><a href="#cb5-36"></a>    num_layers <span class="op">=</span> <span class="bu">len</span>(params) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb5-37"><a href="#cb5-37"></a>    h <span class="op">=</span> x</span>
<span id="cb5-38"><a href="#cb5-38"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_layers):</span>
<span id="cb5-39"><a href="#cb5-39"></a>        h <span class="op">=</span> np.tanh(params[<span class="ss">f"W</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>] <span class="op">@</span> h <span class="op">+</span> params[<span class="ss">f"b</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>])</span>
<span id="cb5-40"><a href="#cb5-40"></a>    out <span class="op">=</span> params[<span class="ss">f"W</span><span class="sc">{</span>num_layers<span class="sc">}</span><span class="ss">"</span>] <span class="op">@</span> h <span class="op">+</span> params[<span class="ss">f"b</span><span class="sc">{</span>num_layers<span class="sc">}</span><span class="ss">"</span>]</span>
<span id="cb5-41"><a href="#cb5-41"></a>    <span class="cf">return</span> out.squeeze()</span>
<span id="cb5-42"><a href="#cb5-42"></a></span>
<span id="cb5-43"><a href="#cb5-43"></a><span class="kw">def</span> deep_utility(x, params):</span>
<span id="cb5-44"><a href="#cb5-44"></a>    <span class="cf">return</span> np.array([deep_forward(np.array([xi]), params) <span class="cf">for</span> xi <span class="kw">in</span> x])</span>
<span id="cb5-45"><a href="#cb5-45"></a></span>
<span id="cb5-46"><a href="#cb5-46"></a><span class="co"># --- Log likelihood (Bradley-Terry) ---</span></span>
<span id="cb5-47"><a href="#cb5-47"></a><span class="kw">def</span> deep_log_likelihood(params, pairs):</span>
<span id="cb5-48"><a href="#cb5-48"></a>    ll <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb5-49"><a href="#cb5-49"></a>    <span class="cf">for</span> xi, xj, _ <span class="kw">in</span> pairs:</span>
<span id="cb5-50"><a href="#cb5-50"></a>        ui <span class="op">=</span> deep_forward(np.array([xi]), params)</span>
<span id="cb5-51"><a href="#cb5-51"></a>        uj <span class="op">=</span> deep_forward(np.array([xj]), params)</span>
<span id="cb5-52"><a href="#cb5-52"></a>        ll <span class="op">+=</span> np.log(norm.cdf((ui <span class="op">-</span> uj) <span class="op">/</span> np.sqrt(<span class="dv">2</span>)) <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb5-53"><a href="#cb5-53"></a>    <span class="cf">return</span> ll</span>
<span id="cb5-54"><a href="#cb5-54"></a></span>
<span id="cb5-55"><a href="#cb5-55"></a><span class="co"># --- Gaussian prior on weights ---</span></span>
<span id="cb5-56"><a href="#cb5-56"></a><span class="kw">def</span> deep_log_prior(params):</span>
<span id="cb5-57"><a href="#cb5-57"></a>    lp <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb5-58"><a href="#cb5-58"></a>    <span class="cf">for</span> v <span class="kw">in</span> params.values():</span>
<span id="cb5-59"><a href="#cb5-59"></a>        lp <span class="op">-=</span> <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(v<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb5-60"><a href="#cb5-60"></a>    <span class="cf">return</span> lp</span>
<span id="cb5-61"><a href="#cb5-61"></a></span>
<span id="cb5-62"><a href="#cb5-62"></a><span class="co"># --- Proposal distribution ---</span></span>
<span id="cb5-63"><a href="#cb5-63"></a><span class="kw">def</span> deep_propose(params, sigma<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb5-64"><a href="#cb5-64"></a>    new_params <span class="op">=</span> {}</span>
<span id="cb5-65"><a href="#cb5-65"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> params.items():</span>
<span id="cb5-66"><a href="#cb5-66"></a>        new_params[k] <span class="op">=</span> v <span class="op">+</span> np.random.randn(<span class="op">*</span>v.shape) <span class="op">*</span> sigma</span>
<span id="cb5-67"><a href="#cb5-67"></a>    <span class="cf">return</span> new_params</span>
<span id="cb5-68"><a href="#cb5-68"></a></span>
<span id="cb5-69"><a href="#cb5-69"></a><span class="co"># --- Metropolis-Hastings sampling ---</span></span>
<span id="cb5-70"><a href="#cb5-70"></a><span class="kw">def</span> deep_mh(init_params, pairs, num_iters<span class="op">=</span><span class="dv">2000</span>, burn_in<span class="op">=</span><span class="dv">500</span>):</span>
<span id="cb5-71"><a href="#cb5-71"></a>    samples <span class="op">=</span> []</span>
<span id="cb5-72"><a href="#cb5-72"></a>    current <span class="op">=</span> init_params</span>
<span id="cb5-73"><a href="#cb5-73"></a>    current_lp <span class="op">=</span> deep_log_likelihood(current, pairs) <span class="op">+</span> deep_log_prior(current)</span>
<span id="cb5-74"><a href="#cb5-74"></a></span>
<span id="cb5-75"><a href="#cb5-75"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_iters):</span>
<span id="cb5-76"><a href="#cb5-76"></a>        proposal <span class="op">=</span> deep_propose(current)</span>
<span id="cb5-77"><a href="#cb5-77"></a>        proposal_lp <span class="op">=</span> deep_log_likelihood(proposal, pairs) <span class="op">+</span> deep_log_prior(proposal)</span>
<span id="cb5-78"><a href="#cb5-78"></a>        accept_prob <span class="op">=</span> np.exp(proposal_lp <span class="op">-</span> current_lp)</span>
<span id="cb5-79"><a href="#cb5-79"></a>        <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> accept_prob:</span>
<span id="cb5-80"><a href="#cb5-80"></a>            current <span class="op">=</span> proposal</span>
<span id="cb5-81"><a href="#cb5-81"></a>            current_lp <span class="op">=</span> proposal_lp</span>
<span id="cb5-82"><a href="#cb5-82"></a>        <span class="cf">if</span> i <span class="op">&gt;=</span> burn_in:</span>
<span id="cb5-83"><a href="#cb5-83"></a>            samples.append(current)</span>
<span id="cb5-84"><a href="#cb5-84"></a></span>
<span id="cb5-85"><a href="#cb5-85"></a>    <span class="cf">return</span> samples</span>
<span id="cb5-86"><a href="#cb5-86"></a></span>
<span id="cb5-87"><a href="#cb5-87"></a><span class="co"># --- Run MCMC ---</span></span>
<span id="cb5-88"><a href="#cb5-88"></a>deep_samples <span class="op">=</span> deep_mh(init_deep_params(), pairs, num_iters<span class="op">=</span><span class="dv">2000</span>, burn_in<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb5-89"><a href="#cb5-89"></a></span>
<span id="cb5-90"><a href="#cb5-90"></a><span class="co"># --- Posterior predictions ---</span></span>
<span id="cb5-91"><a href="#cb5-91"></a>X_test <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">200</span>)</span>
<span id="cb5-92"><a href="#cb5-92"></a>deep_preds <span class="op">=</span> np.array([deep_utility(X_test, s) <span class="cf">for</span> s <span class="kw">in</span> deep_samples])</span>
<span id="cb5-93"><a href="#cb5-93"></a>deep_mean <span class="op">=</span> deep_preds.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-94"><a href="#cb5-94"></a>deep_std <span class="op">=</span> deep_preds.std(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-95"><a href="#cb5-95"></a></span>
<span id="cb5-96"><a href="#cb5-96"></a><span class="co"># --- Plot results ---</span></span>
<span id="cb5-97"><a href="#cb5-97"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb5-98"><a href="#cb5-98"></a>plt.plot(X_test, true_u(X_test), <span class="st">'k--'</span>, label<span class="op">=</span><span class="st">'True utility'</span>)</span>
<span id="cb5-99"><a href="#cb5-99"></a>plt.plot(X_test, deep_mean, <span class="st">'b-'</span>, label<span class="op">=</span><span class="st">'BNN (3-layer) mean'</span>)</span>
<span id="cb5-100"><a href="#cb5-100"></a>plt.fill_between(X_test, deep_mean <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> deep_std, deep_mean <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> deep_std,</span>
<span id="cb5-101"><a href="#cb5-101"></a>                 color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">'95% CI'</span>)</span>
<span id="cb5-102"><a href="#cb5-102"></a>plt.title(<span class="st">"3-layer Bayesian Neural Network via MCMC on Preference Data"</span>)</span>
<span id="cb5-103"><a href="#cb5-103"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb5-104"><a href="#cb5-104"></a>plt.ylabel(<span class="st">"Utility"</span>)</span>
<span id="cb5-105"><a href="#cb5-105"></a>plt.legend()</span>
<span id="cb5-106"><a href="#cb5-106"></a>plt.tight_layout()</span>
<span id="cb5-107"><a href="#cb5-107"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chap3_files/figure-html/cell-5-output-1.png" width="757" height="372" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Another Bayesian approach is Bayesian Inverse Reinforcement Learning (IRL), where a prior is placed on the parameters of a reward function and Bayes’ rule is used to update this distribution given demonstrations or preferences <span class="citation" data-cites="iliad2019learning">(<a href="#ref-iliad2019learning" role="doc-biblioref"><strong>iliad2019learning?</strong></a>)</span>. Early work like Ramachandran &amp; Amir (2007) treated IRL as Bayesian inference, using MCMC to sample likely reward functions consistent with demonstrations. Such methods yield a posterior over reward functions, reflecting ambiguity when multiple rewards explain the human’s behavior.</p>
<p>In summary, Bayesian utility learning methods acknowledge that with limited human feedback, many possible utility functions might be compatible with the data. They keep track of this ambiguity, which is crucial for making cautious decisions and for actively gathering more feedback.</p>
</section>
<section id="case-study-learning-from-human-feedback-in-robotics" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="case-study-learning-from-human-feedback-in-robotics"><span class="header-section-number">3.4</span> Case Study: Learning from Human Feedback in Robotics</h2>
<p>Thus far, we discussed preference learning in general terms. We now focus on robotics, where an agent must learn a <em>reward/utility function</em> that captures the human’s objectives for a <em>sequential decision-making</em> task. Robotics brings additional challenges: the utility often depends on a trajectory of states and actions, and feedback can come in multiple forms. We outline several key forms of human feedback for robot learning and how to learn from them:</p>
<ul>
<li>Learning from demonstrations – inferring utility from expert demonstrations of the task.</li>
<li>Learning from physical corrections – updating utility when a human physically intervenes in the robot’s behavior.</li>
<li>Learning from trajectory evaluations – using human-provided scores or critiques of full trajectories.</li>
<li>Learning from pairwise trajectory comparisons – inferring reward from which of two trajectories a human prefers.</li>
</ul>
<p>These are not mutually exclusive; in practice, combinations can be very powerful <span class="citation" data-cites="iliad2019learning">(<a href="#ref-iliad2019learning" role="doc-biblioref"><strong>iliad2019learning?</strong></a>)</span>. We describe each mode and how utility functions can be derived.</p>
<section id="learning-from-demonstrations-inverse-reinforcement-learning" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="learning-from-demonstrations-inverse-reinforcement-learning"><span class="header-section-number">3.4.1</span> Learning from Demonstrations (Inverse Reinforcement Learning)</h3>
<p>In Learning from Demonstrations, also known as Inverse Reinforcement Learning, the human provides examples of desired behavior (e.g.&nbsp;teleoperating a robot to show how to perform a task). The assumption is that the demonstrator is approximately optimizing some latent reward function <span class="math inline">\(R^*(s,a)\)</span> (or utility for trajectories). IRL algorithms then search for a reward function <span class="math inline">\(R_\theta\)</span> under which the given demonstrations <span class="math inline">\(\tau_{demo}\)</span> have high expected return <span class="citation" data-cites="iliad2019learning">(<a href="#ref-iliad2019learning" role="doc-biblioref"><strong>iliad2019learning?</strong></a>)</span>.</p>
<p>One classic approach is <em>Maximum Margin IRL</em>, which finds a reward function that makes the return of the demonstration trajectories higher than that of any other trajectories by a large margin. Another is <em>Maximum Entropy IRL</em>, which models the demonstrator as noisily optimal (Boltzmann-rational) <span class="citation" data-cites="iliad2019learning">(<a href="#ref-iliad2019learning" role="doc-biblioref"><strong>iliad2019learning?</strong></a>)</span>. In MaxEnt IRL, the probability of a trajectory <span class="math inline">\(\tau\)</span> under reward parameters <span class="math inline">\(\theta\)</span> is modeled as:</p>
<p><span class="math display">\[
P(\tau \mid \theta) = \frac{\exp\{R_\theta(\tau)\}}{\displaystyle \sum_{\tau'} \exp\{R_\theta(\tau')\}} \,,
\]</span></p>
<p>where <span class="math inline">\(R_\theta(\tau) = \sum_{t} r_\theta(s_t, a_t)\)</span> is the cumulative reward of <span class="math inline">\(\tau\)</span>. The IRL algorithm then adjusts <span class="math inline">\(\theta\)</span> to maximize the likelihood of the human demonstrations (while often using techniques to approximate the denominator, since summing over all trajectories is intractable). The end result is a reward function <span class="math inline">\(R_\theta(s,a)\)</span> that rationalizes the demonstrations.</p>
<p><em>Key challenge:</em> unless demonstrations are <em>optimal</em> and cover the space well, IRL might recover an ambiguous or incorrect reward. In robotics, humans often have difficulty providing flawless demonstrations (due to hard-to-use interfaces or limited expertise) <span class="citation" data-cites="iliad2019learning">(<a href="#ref-iliad2019learning" role="doc-biblioref"><strong>iliad2019learning?</strong></a>)</span>. For example, users teleoperating a robot arm might move jerkily or only accomplish part of the task <span class="citation" data-cites="iliad2019learning">(<a href="#ref-iliad2019learning" role="doc-biblioref"><strong>iliad2019learning?</strong></a>)</span>. This makes sole reliance on demonstrations problematic. Nonetheless, demonstration data can provide a strong prior: it shows at least one way to succeed (or partial preferences for certain behaviors).</p>
</section>
<section id="learning-from-preferences-and-rankings-of-trajectories" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="learning-from-preferences-and-rankings-of-trajectories"><span class="header-section-number">3.4.2</span> Learning from Preferences and Rankings of Trajectories</h3>
<p>When high-quality demonstrations are hard to obtain, preference queries on trajectories are a viable alternative <span class="citation" data-cites="iliad2019learning">(<a href="#ref-iliad2019learning" role="doc-biblioref"><strong>iliad2019learning?</strong></a>)</span>. In preference-based learning for robotics, the robot (or algorithm) presents two (or more) trajectories of the task outcome, and the human chooses which one is better. Each such comparison provides a bit of information about the true underlying reward. By asking many queries, the algorithm can home in on the reward function that explains the human’s choices <span class="citation" data-cites="iliad2019learning">(<a href="#ref-iliad2019learning" role="doc-biblioref"><strong>iliad2019learning?</strong></a>)</span>.</p>
<p>A concrete example is an agent learning to do a backflip in simulation <span class="citation" data-cites="christiano2023deep">(<a href="#ref-christiano2023deep" role="doc-biblioref">Christiano et al. 2023</a>)</span>. The agent initially performs random flails. The system then repeatedly shows the human <em>two video clips</em> of the agent’s behavior and asks which is closer to a proper backflip. From these comparisons, a reward model is learned that assigns higher value to behaviors more like backflips <span class="citation" data-cites="christiano2023deep">(<a href="#ref-christiano2023deep" role="doc-biblioref">Christiano et al. 2023</a>)</span>. The agent then uses reinforcement learning to optimize this learned reward, gradually performing better backflips. This process continues, with the human being asked comparisons on trajectories where the algorithm is most uncertain (to maximally inform the reward model) <span class="citation" data-cites="christiano2023deep">(<a href="#ref-christiano2023deep" role="doc-biblioref">Christiano et al. 2023</a>)</span>.</p>
<p>(<a href="https://openai.com/index/learning-from-human-preferences/">Learning from human preferences | OpenAI</a>) <em>Framework for learning from human preferences in robotics: a reward predictor (utility function) is learned from human feedback on trajectory comparisons, and an RL algorithm uses this learned reward to improve the policy <span class="citation" data-cites="christiano2023deep">(<a href="#ref-christiano2023deep" role="doc-biblioref">Christiano et al. 2023</a>)</span>. The loop is iterative: as the policy improves, new queries focus on areas of uncertainty to refine the reward model.</em></p>
<p>Such preference-based reward learning has enabled complex skills without explicitly programmed rewards <span class="citation" data-cites="christiano2023deep">(<a href="#ref-christiano2023deep" role="doc-biblioref">Christiano et al. 2023</a>)</span>. Notably, Christiano <em>et al.</em> (2017) showed that an agent can learn Atari game policies and robotic manipulations from a few hundred comparison queries, achieving goals that are hard to specify but easy to judge <span class="citation" data-cites="christiano2023deep">(<a href="#ref-christiano2023deep" role="doc-biblioref">Christiano et al. 2023</a>)</span>. Preferences are often easier for humans than demonstrations: choosing between options is simpler than generating one from scratch <span class="citation" data-cites="iliad2019learning">(<a href="#ref-iliad2019learning" role="doc-biblioref"><strong>iliad2019learning?</strong></a>)</span>. However, preference learning can be slow if each query only yields one bit of information. Active learning and combining preferences with other feedback can greatly improve efficiency <span class="citation" data-cites="iliad2019learning">(<a href="#ref-iliad2019learning" role="doc-biblioref"><strong>iliad2019learning?</strong></a>)</span>.</p>
</section>
<section id="learning-from-trajectory-evaluations-critiques-and-ratings" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="learning-from-trajectory-evaluations-critiques-and-ratings"><span class="header-section-number">3.4.3</span> Learning from Trajectory Evaluations (Critiques and Ratings)</h3>
<p>Sometimes humans provide feedback in the form of <em>evaluative scores</em> or critiques on full trajectories (or partial trajectories). For example, after a robot finishes an attempt at a task, the human might give a reward signal (e.g.&nbsp;+1/-1, or a rating 1–5 stars, or say “too slow” vs “good job”). This is the premise of the TAMER framework (Training an Agent via Evaluative Reinforcement) and related approaches, where a human’s scalar reward signals are directly treated as the reward function for the agent in reinforcement learning.</p>
<p>From a utility learning perspective, such feedback can be used to directly fit a utility model <span class="math inline">\(u_\theta\)</span> that predicts the human’s rating for a given trajectory. For instance, if a human provides a score <span class="math inline">\(H(\tau)\)</span> for trajectory <span class="math inline">\(\tau\)</span>, one can treat it as a training target for <span class="math inline">\(u_\theta(\tau)\)</span> (possibly under a regression loss). However, because humans are inconsistent and may not precisely quantify their preferences, it’s often useful to model <span class="math inline">\(H(\tau)\)</span> as a noisy realization of the underlying utility, rather than a perfect label. A Bayesian approach could treat <span class="math inline">\(H(\tau)\)</span> as a noisy observation of <span class="math inline">\(u(\tau)\)</span> and update a posterior for <span class="math inline">\(u\)</span>. Alternatively, classification approaches can be used (e.g.&nbsp;treat trajectories into “liked” vs “disliked” based on thresholded ratings).</p>
<p>A challenge with trajectory-level feedback is <em>credit assignment</em>: the human’s single score must be attributed to the entire sequence of actions. Algorithms like COACH (Continuous cOaching of Automated Control Handlers) address this by allowing humans to give feedback at intermediate steps, thereby guiding the agent which specific part of the behavior was good or bad. In either case, learning from trajectory evaluations turns the human into a <em>reward function provider</em>, and the learning algorithm’s job is to infer the latent reward function that the human’s evaluations are trying to convey.</p>
</section>
<section id="learning-from-physical-corrections" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="learning-from-physical-corrections"><span class="header-section-number">3.4.4</span> Learning from Physical Corrections</h3>
<p>Robots that physically collaborate with humans can receive physical corrections: the human may push the robot or otherwise intervene to adjust its behavior. Such corrections provide insight into the human’s desired utility. For example, if a household robot is carrying a fragile object too recklessly and the human physically slows it down or re-routes it, that indicates the human’s reward favors safety over speed at that moment.</p>
<p>Learning from physical corrections can be formalized in different ways. One approach is to treat a correction as a demonstration on a small segment: the human’s intervention suggests a better action or trajectory than what the robot was doing. This can be converted into a comparison: “the trajectory after correction is preferred over the original trajectory” for that time segment. The robot can then update its reward function <span class="math inline">\(\theta\)</span> to satisfy <span class="math inline">\(R_\theta(\text{human-corrected behavior}) &gt; R_\theta(\text{robot’s initial behavior})\)</span>. Repeated corrections yield a dataset of such pairwise preferences, focused on the states where the robot was wrong <span class="citation" data-cites="losey2021corrections">(<a href="#ref-losey2021corrections" role="doc-biblioref"><strong>losey2021corrections?</strong></a>)</span>.</p>
<p>Another approach is to infer the human’s intent through the sequence of corrections. Research by Losey <em>et al.</em> (2021) formalized learning from <em>sequences</em> of physical corrections, noting that each correction is not independent: a series of pushes might only make sense in aggregate <span class="citation" data-cites="losey2021corrections">(<a href="#ref-losey2021corrections" role="doc-biblioref"><strong>losey2021corrections?</strong></a>)</span>. By analyzing the cumulative effect of multiple interventions, the algorithm can deduce the underlying objective more accurately (e.g.&nbsp;the human consistently steers the robot away from the table edges, implying a high negative reward for collisions). Their algorithm introduced an auxiliary reward term to capture the human’s trade-off: they will correct the robot if the immediate mistake is worth fixing relative to long-term performance <span class="citation" data-cites="losey2021corrections">(<a href="#ref-losey2021corrections" role="doc-biblioref"><strong>losey2021corrections?</strong></a>)</span>. The conclusion was that reasoning over the sequence of corrections improved learning of the human’s objective <span class="citation" data-cites="losey2021corrections">(<a href="#ref-losey2021corrections" role="doc-biblioref"><strong>losey2021corrections?</strong></a>)</span>.</p>
<p>Physical corrections are intuitive for humans – we often instinctively guide others or objects when they err. For the robot, interpreting this guidance requires converting it into constraints or examples for the utility function. It is a powerful signal because it is <em>active</em>: the human is not just telling preferences but directly imparting the desired direction of change.</p>
</section>
<section id="combining-multiple-feedback-types" class="level3" data-number="3.4.5">
<h3 data-number="3.4.5" class="anchored" data-anchor-id="combining-multiple-feedback-types"><span class="header-section-number">3.4.5</span> Combining Multiple Feedback Types</h3>
<p>Each feedback modality has strengths and weaknesses. Demonstrations provide a lot of information but can be hard to perform; preferences are easy for humans but yield information slowly; corrections are very informative locally but require physical interaction; trajectory evaluations are straightforward but coarse. Combining these modes can lead to faster and more robust reward learning <span class="citation" data-cites="iliad2019learning">(<a href="#ref-iliad2019learning" role="doc-biblioref"><strong>iliad2019learning?</strong></a>)</span>. For example, the DemPref algorithm <span class="citation" data-cites="iliad2019learning">(<a href="#ref-iliad2019learning" role="doc-biblioref"><strong>iliad2019learning?</strong></a>)</span> first uses demonstrations to get an initial rough reward model, then uses preference queries to refine it quickly <span class="citation" data-cites="iliad2019learning">(<a href="#ref-iliad2019learning" role="doc-biblioref"><strong>iliad2019learning?</strong></a>)</span>. In user studies, such combined approaches learned better rewards with fewer queries than using either alone <span class="citation" data-cites="iliad2019learning">(<a href="#ref-iliad2019learning" role="doc-biblioref"><strong>iliad2019learning?</strong></a>)</span>.</p>
<p>In practical robot learning systems, one might start by asking for a demonstration. If the demo is suboptimal, the system can then ask preference questions on alternative attempts to clarify the true goal. During actual execution, if the human intervenes, the robot updates its reward function on the fly to avoid repeating the mistake. This <em>interactive reward learning</em> loop continues until the robot’s behavior aligns with human intent.</p>
</section>
</section>
<section id="summary" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="summary"><span class="header-section-number">3.5</span> Summary</h2>
<p>Learning utility functions from human preferences enables value alignment: aligning an AI system’s objectives with what humans actually want, rather than what we <em>think</em> we want in abstract. We covered how supervised learning can extract utilities from comparisons or scores, and how Bayesian methods like Gaussian processes and Bayesian neural nets can capture uncertainty in our inferences. In robotics, we saw that feedback can come in many forms – demonstrations, comparisons, corrections, evaluations – each providing a unique window into the human’s utility function. By intelligently combining these signals, robots can efficiently learn complex reward functions that would be extremely difficult to hand-code.</p>
<p>Key takeaways and best practices include:</p>
<ul>
<li><em>Use the right feedback for the problem:</em> If optimal examples are available, demonstrations jump-start learning. If not, pairwise preferences or scalar critiques might be easier to obtain.</li>
<li><em>Model uncertainty:</em> Knowing what the system doesn’t know (via a Bayesian model) allows for smart query selection and avoids overconfidently optimizing the wrong objective.</li>
<li><em>Iterate with the human:</em> Preference learning is fundamentally an interactive process. An agent can query a human in ambiguous cases and continuously refine the utility estimate <span class="citation" data-cites="christiano2023deep">(<a href="#ref-christiano2023deep" role="doc-biblioref">Christiano et al. 2023</a>)</span>.</li>
<li><em>Validate the learned utility:</em> Once a reward is learned, testing the robot’s policy and having humans verify or correct it is crucial. Even a few manual corrections can reveal if the learned reward misses a key aspect, allowing further refinement.</li>
<li><em>Be aware of scaling and bias:</em> Human feedback can be noisy or biased. Techniques like DPO suggest ways to simplify learning and avoid instability, but one should monitor for issues like reward hacking or unintended solutions, intervening with additional feedback as needed.</li>
</ul>
<p>Learning from human preferences is a rich area of ongoing research. It lies at the intersection of machine learning, human-computer interaction, and ethics. As AI systems become more advanced, the importance of teaching them <em>our</em> utility functions (and not mis-specified proxies) grows. The methods discussed in this chapter are building blocks toward AI that truly understands and pursues what humans value, acquired through learning <em>with</em> humans in the loop rather than in isolation. By mastering these techniques, we move closer to AI and robots that can be trusted to make decisions aligned with human preferences and well-being.</p>
<!--
### Reward Learning in Robotics

To help set up our basic reward learning problem, consider a user and a
robot. The user's preferences or goals can be represented by an internal
reward function, R($\xi$), which the robot needs to learn. Since the
reward function isn't explicit, there are a variety of ways that the
robot can learn this reward function, which we will discuss in the next
section. An example method of learning a reward function from human data
is using pairwise comparison. Consider the robot example from section
one, but now, the robot shows the human two possible trajectories
$\xi_A$ and $\xi_B$ as depicted in the diagram below.

![Two different trajectories taken by a robot to prompt
user ranking.](Figures/robots.png){#fig-reward-robot-1 width="70%"}

The user is show both the trajectories above and asked to rank which one
is better. Based on iterations of multiple trajectories and ranking, the
robot is able to learn the user's internal reward function. There quite
a lot of ways that models can learn a reward function from human data.
Here's a list [@myers2021learning] of some of them:

1.  Pairwise comparison: This is the method that we saw illustrated in
    the previous example. The robot is able to learn based on a
    comparison ranking provided by the user.

2.  Expert demonstrations: Experts perform the task and the robot learns
    the optimal reward function from these demonstrations.

3.  Sub-optimal demonstrations: The robot is provided with
    demonstrations that are not quite as good as the expert
    demonstrations but it is still able to learn a noisy reward function
    from the demonstrations.

4.  Physical Corrections: While the robot is performing the task, at
    each point in its trajectory (or at an arbitrary point in its
    trajectory) its arm is corrected to a more suitable position. Based
    on these corrections, the robot is able to learn the reward
    function.

5.  Ranking: This method is similar to pairwise comparison but involves
    more trajectories than 2. All the trajectories may have subtle
    differences from each other, but these differences help provide
    insight to the model.

6.  Trajectory Assessment: Given a single trajectory, the user rates how
    close it is to optimal, typically using a ranking scale.

    Each of these methods allows the robot to refine its understanding
    of the user's reward function, but their effectiveness can vary
    depending on the application. For instance, expert demonstrations
    tend to produce more reliable results but may not always be feasible
    in everyday tasks. Pairwise comparison and ranking methods offer
    more flexibility but might require a higher number of iterations.

### Direct Preference Optimization

A modern method for estimating the parameters of a human preference
model is direct preference optimization [@rafailov2023direct], which is
used in the context of aligning language models to human preferences. A
recent approach [@christiano2023deep] first trains a reward model that
captures human preferences and then uses proximal policy optimization to
train a language model-based policy to reflect those learned
preferences. Direct Preference Optimization (DPO), on the other hand,
removes the need for a reward model by directly using the model
likelihood of two outcomes (a preferred or highly-ranked sequence and an
unpreferred or low-ranked sequence) to capture the preference
represented in the data. DPO provides a simpler framework than its
reinforcement learning approach and results in comparable performance
with improved stability. Furthermore, it obviates the need to train a
reward model, instead using a language model policy and human preference
dataset to align the policy directly to human preferences.

<!--
Through our exploration of human preference models, we will ground ourselves in
building a health coaching system that can provide meal recommendations aligned with a user's dietary needs and preferences. Examples of scenarios which can benefit from a model of how humans make choices include:

1.  **Health coaching:** Humans express their preferences every time
    they pick lunch for consumption. Humans may have several goals
    related to nutrition, such as weight loss and improving
    concentration. We can learn how a given individual or set of
    individuals prefer to eat to provide personalized recommendations to
    help them attain their goals. This chapter will use this use case to
    ground human preference modeling in a real-life application.

2.  **Social media:** Platforms have a far greater amount of content
    than one can consume in a lifetime, yet such products must aim to
    maximize user engagement. To accomplish this, we can learn what
    specific things people like to see in their feeds to optimize the
    value they gain out of their time on social media. For example, the
    video feed social media platform [TikTok](https://www.tiktok.com/)
    has had viral adoption due to its notorious ability to personalize a
    feed for its users based on their preferences.

3.  **Shopping:** Retail corporations largely aim to maximize revenue by
    making it easy for people to make purchases. Recommendation systems
    on online shopping platforms provide a mechanism for curating
    specific items based on an individual's previous purchases (or even
    browsing history) to make shoppers aware of items they may like and,
    therefore, purchase.

Two key models used in pairwise sampling are the Thurstonian and Bradley-Terry models [@cattelan2012]. The Thurstonian model assumes each item $i$ has a true score $u_i$ following a normal distribution. The difference $d_{ij} = u_i - u_j$ is also normally distributed. The probability that item $i$ is preferred over item $j$ is given by $P(i \succ j) = \Phi \left( \frac{u_i - u_j}{\sqrt{2\sigma^2}} \right)$, where $\Phi$ is the cumulative normal distribution function. The denominator $\sqrt{2\sigma^2}$ is the standard deviation of the difference $d_{ij} = u_i - u_j$ when $u_i$ and $u_j$ are normally distributed with variance $\sigma^2$[@cattelan2012]. The Bradley-Terry model defines the probability of preference based on latent scores $\beta_i$ and $\beta_j$. The probability that item $i$ is preferred over item $j$ is $P(i \succ j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}$. This model is used to estimate relative strengths or preferences based on latent scores. [@cattelan2012].

::: {#tbl-philosophy}
  -----------------------------------------------------------------------
  Application                         Human Preference
  ----------------------------------- -----------------------------------
  Computer vision: train a neural     This is how humans process images
  network to predict bounding boxes   by identifying the position and
  delineating all instances of dogs   geometry of the things we see in
  in an image                         them

  Natural language processing: train  Coherent text is itself a
  a model to generate coherent text   human-created and defined concept,
                                      and we prefer that any
                                      synthetically generated text
                                      matches that of humans

  Computer vision: train a diffusion  Humans prefer that images
  model to generate realistic images  accurately capture the world as
  of nature                           observed by humans, and this
                                      generative model should reflect the
                                      details that comprise that
                                      preference
  -----------------------------------------------------------------------

  : Examples of machine learning tasks and their interpretation as
  modeling human preferences.
:::
-->
<!--
Game theory provides a mathematical framework for analyzing strategic
interactions among rational agents. These models help in understanding
and predicting human behavior by considering multiple criteria and the
associated trade-offs. They enhance the understanding of preferences
across multiple criteria and allow for richer and more accurate feedback
through structured comparisons. Game-theory framings capture the
complexity of preferences and interactions in decision-making processes
[@bhatia2020preference].

The most popular form of preference elicitation involves pairwise
comparisons. Users are asked to choose between two options, such as
product A or product B. This method is used in various applications like
search engines, recommender systems, and interactive robotics. Key
concepts include the Von Neumann Winner and the Blackwell Winner. The
Von Neumann Winner refers to a distribution over objects that beats or
ties every other object in the collection under the expected utility
assumption. The Blackwell Winner generalizes the Von Neumann Winner for
multi-criteria problems using a target set for acceptable payoff vectors
[@bhatia2020preference].

Game-theory framings provide a framework for preference learning along
multiple criteria. These models use tools from vector-valued payoffs in
game theory, with Blackwell's approach being a key concept. This
approach allows for a more comprehensive understanding of preferences by
considering multiple criteria simultaneously [@bhatia2020preference].

In game-theory framings, pairwise preferences are modeled as random
variables. Comparisons between objects along different criteria are
captured in a preference tensor $P$. This tensor models the probability
that one object is preferred over another along a specific criterion,
allowing for a detailed understanding of preferences across multiple
dimensions [@bhatia2020preference].

The preference tensor $P$ captures object comparisons along different
criteria. It is defined as:
$$P(i_1, i_2; j) = P(i_1 \succ i_2 \text{ along criterion } j)$$ where
$P(i_2, i_1; j) = 1 - P(i_1, i_2; j)$. These values are aggregated to
form an overall preference matrix $P_{ov}$ [@bhatia2020preference].

The Blackwell Winner is defined using a target set $S$ of acceptable
score vectors. The goal is to find a distribution $\pi^*$ such that
$P(\pi^*, \pi) \in S$ for all $\pi$. This method minimizes the maximum
distance to the target set, providing a robust solution to
multi-criteria preference problems [@bhatia2020preference].

The optimization problem for finding the Blackwell Winner is defined as:
$$\pi(P, S, \|\cdot\|) = \arg \min_{\pi \in \Delta_d} \left[ \max_{\pi' \in \Delta_d} \rho(P(\pi, \pi'), S) \right]$$
where $\rho(u, v) = \|u - v\|$. This measures the distance to the target
set, ensuring that the selected distribution is as close as possible to
the ideal preference vector [@bhatia2020preference].
-->


<!-- -->

</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-christiano2023deep" class="csl-entry" role="listitem">
Christiano, Paul, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2023. <span>“Deep Reinforcement Learning from Human Preferences.”</span> <a href="https://arxiv.org/abs/1706.03741">https://arxiv.org/abs/1706.03741</a>.
</div>
<div id="ref-rafailov2023direct" class="csl-entry" role="listitem">
Rafailov, Rafael, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. <span>“Direct Preference Optimization: Your Language Model Is Secretly a Reward Model.”</span> <a href="https://arxiv.org/abs/2305.18290">https://arxiv.org/abs/2305.18290</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../src/chap2.html" class="pagination-link" aria-label="Background">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../src/chap4.html" class="pagination-link" aria-label="Elicitation">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Elicitation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb6" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb6-1"><a href="#cb6-1"></a><span class="co">---</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="an">title:</span><span class="co"> Learning</span></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="an">format:</span><span class="co"> html</span></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="an">filters:</span></span>
<span id="cb6-5"><a href="#cb6-5"></a><span class="co">  - pyodide</span></span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="an">execute:</span></span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="co">  engine: pyodide</span></span>
<span id="cb6-8"><a href="#cb6-8"></a><span class="co">  pyodide:</span></span>
<span id="cb6-9"><a href="#cb6-9"></a><span class="co">    auto: true</span></span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a><span class="co">---</span></span>
<span id="cb6-12"><a href="#cb6-12"></a></span>
<span id="cb6-13"><a href="#cb6-13"></a>Designing a good utility function (or reward function) by hand for a complex AI or robotics task is notoriously difficult and error-prone. Instead of manually specifying what is “good” behavior, we can learn a utility function from human preferences. In this chapter, we explore how an agent can infer a human’s underlying utility function (their preferences or reward criteria) from various forms of feedback. We discuss both supervised learning and Bayesian approaches to utility learning, and examine techniques motivated by robotics—learning from demonstrations, physical corrections, trajectory evaluations, and pairwise comparisons. Throughout, we include mathematical formulations and code examples to illustrate the learning process.</span>
<span id="cb6-14"><a href="#cb6-14"></a></span>
<span id="cb6-15"><a href="#cb6-15"></a><span class="fu">## The Supervised Learning Problem</span></span>
<span id="cb6-16"><a href="#cb6-16"></a></span>
<span id="cb6-17"><a href="#cb6-17"></a>Supervised learning approaches treat human feedback as labeled data to directly fit a utility function. The core idea is to assume there exists a true utility function $u^*(x)$ (over states, outcomes, or trajectories $x$) that explains a human’s choices. We then choose a parameterized model $u_\theta(x)$ and adjust $\theta$ so that $u_\theta$ agrees with the human-provided preferences.</span>
<span id="cb6-18"><a href="#cb6-18"></a></span>
<span id="cb6-19"><a href="#cb6-19"></a>A common feedback format is pairwise comparisons: the human is shown two options (outcomes or trajectories) $A$ and $B$ and indicates which is preferred. We can model the probability that the human prefers $A$ over $B$ using a logistic or Bradley–Terry model:</span>
<span id="cb6-20"><a href="#cb6-20"></a></span>
<span id="cb6-21"><a href="#cb6-21"></a>$$</span>
<span id="cb6-22"><a href="#cb6-22"></a>P(A \succ B \mid u_\theta) \;=\; \sigma<span class="sc">\!</span>\Big(u_\theta(A) - u_\theta(B)\Big)\,,</span>
<span id="cb6-23"><a href="#cb6-23"></a>$$</span>
<span id="cb6-24"><a href="#cb6-24"></a></span>
<span id="cb6-25"><a href="#cb6-25"></a>where $\sigma(z)=\frac{1}{1+e^{-z}}$ is the sigmoid function. This implies the human is more likely to prefer $A$ if $u_\theta(A)$ is much larger than $u_\theta(B)$.</span>
<span id="cb6-26"><a href="#cb6-26"></a></span>
<span id="cb6-27"><a href="#cb6-27"></a>At the heart of learning from human preferences lies a latent utility function — a function that assigns numerical value to states, trajectories, or outcomes according to a human’s (possibly unspoken) preferences. The goal of a learning algorithm is to infer this function from observed feedback, which may come in the form of demonstrations, ratings, rankings, or pairwise comparisons. But how exactly do we represent and update our belief about this hidden utility function?</span>
<span id="cb6-28"><a href="#cb6-28"></a></span>
<span id="cb6-29"><a href="#cb6-29"></a>Two major paradigms in statistical learning provide different answers: point estimation and posterior estimation.</span>
<span id="cb6-30"><a href="#cb6-30"></a></span>
<span id="cb6-31"><a href="#cb6-31"></a>In point estimation, we seek a single "best guess" for the utility function — typically a function $u_\theta(x)$ from a parameterized family (e.g. linear models, neural nets), with parameters $\theta \in \mathbb{R}^d$. Given data $\mathcal{D}$ from human feedback (e.g. preferences), we choose the parameter $\hat{\theta}$ that best explains the observed behavior. Formally:</span>
<span id="cb6-32"><a href="#cb6-32"></a></span>
<span id="cb6-33"><a href="#cb6-33"></a>$$</span>
<span id="cb6-34"><a href="#cb6-34"></a>\hat{\theta} = \arg\max_\theta \; p(\mathcal{D} \mid \theta)</span>
<span id="cb6-35"><a href="#cb6-35"></a>$$</span>
<span id="cb6-36"><a href="#cb6-36"></a></span>
<span id="cb6-37"><a href="#cb6-37"></a>This is maximum likelihood estimation (MLE): we pick the parameters that make the observed data most probable under our model. Once $\hat{\theta}$ is selected, we treat $u_{\hat{\theta}}$ as the agent’s utility function, and optimize or sample behavior accordingly. This approach is straightforward and computationally efficient. It is the foundation of most supervised learning methods (like logistic regression or deep learning), and it provides a natural interpretation: we’re directly finding the utility function that agrees with the human feedback. However, it discards uncertainty: it assumes the data is sufficient to pin down a single utility function, which may not be true in practice.</span>
<span id="cb6-38"><a href="#cb6-38"></a></span>
<span id="cb6-39"><a href="#cb6-39"></a>In contrast, posterior estimation takes a fully Bayesian view. Instead of committing to one estimate, we maintain a distribution over utility functions. That is, we place a prior $p(\theta)$ over parameters (or over functions $u$ more generally), and update this to a posterior after observing data $\mathcal{D}$:</span>
<span id="cb6-40"><a href="#cb6-40"></a></span>
<span id="cb6-41"><a href="#cb6-41"></a>$$</span>
<span id="cb6-42"><a href="#cb6-42"></a>p(\theta \mid \mathcal{D}) \;=\; \frac{p(\mathcal{D} \mid \theta)\, p(\theta)}{p(\mathcal{D})}</span>
<span id="cb6-43"><a href="#cb6-43"></a>$$</span>
<span id="cb6-44"><a href="#cb6-44"></a></span>
<span id="cb6-45"><a href="#cb6-45"></a>This posterior expresses our uncertainty over which utility functions are compatible with the human feedback. From this distribution, we can make predictions (e.g., using the posterior mean utility), quantify confidence, or even actively select new queries to reduce uncertainty (active learning). For instance, if we model utilities with a Gaussian Process (GP), then the posterior over $u(x)$ is also a GP after observing comparisons or evaluations. If we use a neural network for $u_\theta(x)$, we can approximate the posterior with ensembles, variational inference, or MCMC. Posterior estimation is especially valuable when human feedback is sparse, noisy, or ambiguous — as is often the case in real-world preference learning. It allows the agent to reason about what it doesn’t know and to take cautious or exploratory actions accordingly.</span>
<span id="cb6-46"><a href="#cb6-46"></a></span>
<span id="cb6-47"><a href="#cb6-47"></a>The next two sections instantiate these two perspectives. In Section 4.1, we explore point estimation via supervised learning — treating preference data as labeled examples and fitting a utility model. In Section 4.2, we shift to posterior estimation with Bayesian methods like Gaussian processes and Bayesian neural networks, which model both our current estimate and the uncertainty around it.</span>
<span id="cb6-48"><a href="#cb6-48"></a></span>
<span id="cb6-49"><a href="#cb6-49"></a><span class="fu">## Point Estimation via Maximum Likelihood</span></span>
<span id="cb6-50"><a href="#cb6-50"></a>Given a dataset of comparisons $D=<span class="sc">\{</span>(A_i, B_i, y_i)<span class="sc">\}</span>$ (with $y_i=1$ if $A_i$ was preferred and $0$ if $B_i$ was preferred), we can fit $\theta$ by maximizing the likelihood of the human’s choices. Equivalently, we minimize a binary cross-entropy loss:</span>
<span id="cb6-51"><a href="#cb6-51"></a></span>
<span id="cb6-52"><a href="#cb6-52"></a>$$</span>
<span id="cb6-53"><a href="#cb6-53"></a>\mathcal{L}(\theta) = -\sum_{i} \Big<span class="co">[</span><span class="ot">\,y_i \log \sigma(u_\theta(A_i)\!-\!u_\theta(B_i)) + (1-y_i)\log(1-\sigma(u_\theta(A_i)\!-\!u_\theta(B_i)))\Big</span><span class="co">]</span>\,,</span>
<span id="cb6-54"><a href="#cb6-54"></a>$$</span>
<span id="cb6-55"><a href="#cb6-55"></a></span>
<span id="cb6-56"><a href="#cb6-56"></a>often with a regularization term to prevent overfitting. This is a straightforward supervised learning problem – essentially logistic regression – on pairwise difference features.</span>
<span id="cb6-57"><a href="#cb6-57"></a></span>
<span id="cb6-58"><a href="#cb6-58"></a>Example: Suppose a human’s utility for an outcome can be described by a quadratic function (unknown to the learning algorithm). We collect some pairwise preferences and then train a utility model $u_\theta(x)$ to predict those preferences. The code below simulates this scenario:</span>
<span id="cb6-59"><a href="#cb6-59"></a></span>
<span id="cb6-62"><a href="#cb6-62"></a><span class="in">```{python}</span></span>
<span id="cb6-63"><a href="#cb6-63"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-64"><a href="#cb6-64"></a></span>
<span id="cb6-65"><a href="#cb6-65"></a><span class="co"># True utility function (unknown to learner), e.g. u*(x) = -(x-5)^2 + constant </span></span>
<span id="cb6-66"><a href="#cb6-66"></a><span class="kw">def</span> true_utility(x):</span>
<span id="cb6-67"><a href="#cb6-67"></a>    <span class="cf">return</span> <span class="op">-</span>(x<span class="op">-</span><span class="dv">5</span>)<span class="op">**</span><span class="dv">2</span>  <span class="co"># (peak at x=5)</span></span>
<span id="cb6-68"><a href="#cb6-68"></a></span>
<span id="cb6-69"><a href="#cb6-69"></a><span class="co"># Generate synthetic pairwise preference data</span></span>
<span id="cb6-70"><a href="#cb6-70"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb6-71"><a href="#cb6-71"></a>n_pairs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb6-72"><a href="#cb6-72"></a>X1 <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">10</span>, size<span class="op">=</span>n_pairs)  <span class="co"># 20 random x-values</span></span>
<span id="cb6-73"><a href="#cb6-73"></a>X2 <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">10</span>, size<span class="op">=</span>n_pairs)  <span class="co"># 20 more random x-values</span></span>
<span id="cb6-74"><a href="#cb6-74"></a><span class="co"># Determine preferences according to true utility</span></span>
<span id="cb6-75"><a href="#cb6-75"></a>prefs <span class="op">=</span> (true_utility(X1) <span class="op">&gt;</span> true_utility(X2)).astype(<span class="bu">int</span>)  <span class="co"># 1 if X1 preferred, else 0</span></span>
<span id="cb6-76"><a href="#cb6-76"></a></span>
<span id="cb6-77"><a href="#cb6-77"></a><span class="co"># Parametric model for utility: u_theta(x) = w0 + w1*x + w2*x^2  (quadratic form)</span></span>
<span id="cb6-78"><a href="#cb6-78"></a><span class="co"># Initialize weights</span></span>
<span id="cb6-79"><a href="#cb6-79"></a>w <span class="op">=</span> np.zeros(<span class="dv">3</span>)</span>
<span id="cb6-80"><a href="#cb6-80"></a>lr <span class="op">=</span> <span class="fl">0.01</span>       <span class="co"># learning rate</span></span>
<span id="cb6-81"><a href="#cb6-81"></a>reg <span class="op">=</span> <span class="fl">1e-3</span>      <span class="co"># L2 regularization strength</span></span>
<span id="cb6-82"><a href="#cb6-82"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb6-83"><a href="#cb6-83"></a>    <span class="co"># Compute predictions via logistic model</span></span>
<span id="cb6-84"><a href="#cb6-84"></a>    util_diff <span class="op">=</span> (w[<span class="dv">0</span>] <span class="op">+</span> w[<span class="dv">1</span>]<span class="op">*</span>X1 <span class="op">+</span> w[<span class="dv">2</span>]<span class="op">*</span>X1<span class="op">**</span><span class="dv">2</span>) <span class="op">-</span> (w[<span class="dv">0</span>] <span class="op">+</span> w[<span class="dv">1</span>]<span class="op">*</span>X2 <span class="op">+</span> w[<span class="dv">2</span>]<span class="op">*</span>X2<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb6-85"><a href="#cb6-85"></a>    pred <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>util_diff))      <span class="co"># σ(w·(phi(X1)-phi(X2)))</span></span>
<span id="cb6-86"><a href="#cb6-86"></a>    <span class="co"># Gradient of cross-entropy loss</span></span>
<span id="cb6-87"><a href="#cb6-87"></a>    grad <span class="op">=</span> np.array([<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>])</span>
<span id="cb6-88"><a href="#cb6-88"></a>    error <span class="op">=</span> pred <span class="op">-</span> prefs  <span class="co"># (sigma - y)</span></span>
<span id="cb6-89"><a href="#cb6-89"></a>    <span class="co"># Features for X1 and X2</span></span>
<span id="cb6-90"><a href="#cb6-90"></a>    phi1 <span class="op">=</span> np.vstack([np.ones(n_pairs), X1, X1<span class="op">**</span><span class="dv">2</span>]).T</span>
<span id="cb6-91"><a href="#cb6-91"></a>    phi2 <span class="op">=</span> np.vstack([np.ones(n_pairs), X2, X2<span class="op">**</span><span class="dv">2</span>]).T</span>
<span id="cb6-92"><a href="#cb6-92"></a>    phi_diff <span class="op">=</span> phi1 <span class="op">-</span> phi2</span>
<span id="cb6-93"><a href="#cb6-93"></a>    <span class="co"># Gradient: derivative of loss w.rt w = (sigma - y)*φ_diff (averaged) + reg</span></span>
<span id="cb6-94"><a href="#cb6-94"></a>    grad <span class="op">=</span> phi_diff.T.dot(error) <span class="op">/</span> n_pairs <span class="op">+</span> reg <span class="op">*</span> w</span>
<span id="cb6-95"><a href="#cb6-95"></a>    <span class="co"># Update weights</span></span>
<span id="cb6-96"><a href="#cb6-96"></a>    w <span class="op">-=</span> lr <span class="op">*</span> grad</span>
<span id="cb6-97"><a href="#cb6-97"></a></span>
<span id="cb6-98"><a href="#cb6-98"></a><span class="bu">print</span>(<span class="st">"Learned weights:"</span>, w)</span>
<span id="cb6-99"><a href="#cb6-99"></a><span class="in">```</span></span>
<span id="cb6-100"><a href="#cb6-100"></a></span>
<span id="cb6-101"><a href="#cb6-101"></a>After training, we can compare the learned utility function $u_\theta(x)$ to the true utility $u^*(x)$. Below we plot the two functions:</span>
<span id="cb6-102"><a href="#cb6-102"></a></span>
<span id="cb6-105"><a href="#cb6-105"></a><span class="in">```{python}</span></span>
<span id="cb6-106"><a href="#cb6-106"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-107"><a href="#cb6-107"></a></span>
<span id="cb6-108"><a href="#cb6-108"></a><span class="co"># Plot true vs learned utility curves</span></span>
<span id="cb6-109"><a href="#cb6-109"></a>xs <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">200</span>)</span>
<span id="cb6-110"><a href="#cb6-110"></a>true_vals <span class="op">=</span> true_utility(xs)</span>
<span id="cb6-111"><a href="#cb6-111"></a>learned_vals <span class="op">=</span> w[<span class="dv">0</span>] <span class="op">+</span> w[<span class="dv">1</span>]<span class="op">*</span>xs <span class="op">+</span> w[<span class="dv">2</span>]<span class="op">*</span>xs<span class="op">**</span><span class="dv">2</span></span>
<span id="cb6-112"><a href="#cb6-112"></a></span>
<span id="cb6-113"><a href="#cb6-113"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">4</span>))</span>
<span id="cb6-114"><a href="#cb6-114"></a>plt.plot(xs, true_vals, label<span class="op">=</span><span class="st">"True Utility"</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb6-115"><a href="#cb6-115"></a>plt.plot(xs, learned_vals, label<span class="op">=</span><span class="st">"Learned Utility"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb6-116"><a href="#cb6-116"></a>plt.xlabel(<span class="st">"State x"</span>)</span>
<span id="cb6-117"><a href="#cb6-117"></a>plt.ylabel(<span class="st">"Utility"</span>)</span>
<span id="cb6-118"><a href="#cb6-118"></a>plt.title(<span class="st">"True vs. Learned Utility Function"</span>)</span>
<span id="cb6-119"><a href="#cb6-119"></a>plt.legend()</span>
<span id="cb6-120"><a href="#cb6-120"></a>plt.show()</span>
<span id="cb6-121"><a href="#cb6-121"></a><span class="in">```</span></span>
<span id="cb6-122"><a href="#cb6-122"></a></span>
<span id="cb6-123"><a href="#cb6-123"></a>The learned curve closely matches the true utility up to an arbitrary scaling factor (utility is only defined up to affine transform when inferred from comparisons). The algorithm successfully recovered a utility function that orders states almost the same as the true utility $u^*(x)$. In general, learning from comparisons can infer the *relative* utility of options (which item is preferred), although the absolute scale of $u_\theta$ is unidentifiable without further assumptions. Supervised learning on preferences has been widely used for ranking problems and preference-based reward learning.</span>
<span id="cb6-124"><a href="#cb6-124"></a></span>
<span id="cb6-125"><a href="#cb6-125"></a>While the above approach learns a utility and could then use it for decision-making, an alternative in some domains is to directly optimize the policy based on preferences, bypassing an explicit reward model. Direct Preference Optimization (DPO) is a recent technique introduced for aligning language models to human preferences <span class="co">[</span><span class="ot">@rafailov2023direct</span><span class="co">]</span>. The insight of DPO is that we can fold the reward model into the policy itself, and then update the policy via a simple supervised objective on preference data.</span>
<span id="cb6-126"><a href="#cb6-126"></a></span>
<span id="cb6-127"><a href="#cb6-127"></a>In the context of DPO for language models, one has a base policy $\pi_{\text{ref}}$ (e.g. the pre-trained model) and a new policy $\pi_\theta$ we want to fine-tune. Given a human preference between two model outputs (responses) $y_+$ (preferred) vs $y_-$ for the same prompt $x$, DPO adjusts $\pi_\theta$ to increase the log-probability of $y_+$ and decrease that of $y_-$. Formally, the *DPO loss* for a single preference is:</span>
<span id="cb6-128"><a href="#cb6-128"></a></span>
<span id="cb6-129"><a href="#cb6-129"></a>$$</span>
<span id="cb6-130"><a href="#cb6-130"></a>\mathcal{L}_{\text{DPO}}(\theta) \;=\; -\log \sigma\!\Big(\underbrace{\log \pi_\theta(y_+|x) - \log \pi_\theta(y_-|x)}_{\text{policy preference score}}\Big)\,.</span>
<span id="cb6-131"><a href="#cb6-131"></a>$$</span>
<span id="cb6-132"><a href="#cb6-132"></a></span>
<span id="cb6-133"><a href="#cb6-133"></a>Intuitively, this is the same logistic loss as before, now directly in terms of the policy’s log-likelihood scores for each option. The overall objective is the average of this loss over a dataset of human preference comparisons. Optimizing this loss via gradient descent on $\pi_\theta$ effectively *bakes in* a reward model: $\log \pi_\theta(y|x) - \log \pi_{\text{ref}}(y|x)$ plays the role of a learned reward. In fact, one can show that DPO is implicitly fitting a reward function that $\pi_\theta$ is maximizing. Crucially, DPO does *not* require a separate reinforcement learning loop; it treats preference data as direct supervision for the policy, using a simple binary classification objective.</span>
<span id="cb6-134"><a href="#cb6-134"></a></span>
<span id="cb6-135"><a href="#cb6-135"></a>In practice, DPO has been shown to achieve similar or better alignment performance compared to reinforcement learning from human feedback (RLHF) while being more stable and easier to implement. It avoids the need to sample from the model during training or tune delicate hyperparameters of RL. Conceptually, DPO demonstrates that if we structure our utility model cleverly (here, as the log-ratio of policy and reference), we can extract an optimal policy in closed-form and learn utilities via supervised learning.</span>
<span id="cb6-136"><a href="#cb6-136"></a></span>
<span id="cb6-137"><a href="#cb6-137"></a>DPO’s formulation is related to the Bradley–Terry model for preferences and to the idea of *maximum entropy* policies. If we imagine a reward function $R_\theta(y|x) = \log \pi_\theta(y|x) - \log \pi_{\text{ref}}(y|x)$, DPO’s loss is pushing $R_\theta(y_+) &gt; R_\theta(y_-)$ for preferred pairs. The optimal $\pi_\theta$ under this criterion turns out to be $\pi_\theta(y|x) \propto \pi_{\text{ref}}(y|x)\exp<span class="sc">\{</span>R_\theta(y|x)<span class="sc">\}</span>$. In other words, $\pi_\theta$ is proportional to the reference policy re-weighted by an exponentiated reward – a result consistent with the principle of maximum entropy (softmax) optimization. DPO thus blurs the line between learning a utility function and learning a policy; it directly outputs a policy that (implicitly) encodes the learned utility. </span>
<span id="cb6-138"><a href="#cb6-138"></a></span>
<span id="cb6-139"><a href="#cb6-139"></a>The pairwise logistic approach can be extended to other feedback types. If humans provide numeric *ratings* or *scores* for options, one can treat utility learning as a regression problem: fit $u_\theta(x)$ to predict those scores (perhaps with a suitable bounded output or ordinal regression if scores are ordinal). If humans rank multiple options at once, algorithms like *RankNet* or *RankSVM* generalize the pairwise approach to listwise ranking losses. All these methods boil down to defining a loss that penalizes disagreements between the predicted utility order and the human-provided preferences, then optimizing $\theta$ to minimize that loss.</span>
<span id="cb6-140"><a href="#cb6-140"></a></span>
<span id="cb6-141"><a href="#cb6-141"></a>Supervised learning of utility is powerful due to its simplicity, but it typically provides point estimates of $u_\theta$. Next, we consider Bayesian approaches that maintain uncertainty over the utility function.</span>
<span id="cb6-142"><a href="#cb6-142"></a></span>
<span id="cb6-143"><a href="#cb6-143"></a><span class="fu">## Posterior Estimation</span></span>
<span id="cb6-144"><a href="#cb6-144"></a></span>
<span id="cb6-145"><a href="#cb6-145"></a>When feedback data is sparse, as is common in preference learning, it can be advantageous to model uncertainty over the utility function. Bayesian approaches place a prior on the utility function and update a posterior as human feedback is observed. This yields not only a best-guess utility function but also a measure of confidence or uncertainty, which is valuable for active learning (deciding which queries to ask next) and for safety (knowing when the learned reward might be wrong).</span>
<span id="cb6-146"><a href="#cb6-146"></a></span>
<span id="cb6-147"><a href="#cb6-147"></a>A popular Bayesian approach assumes that the human’s utility function can be modeled as a Gaussian Process (GP) – a distribution over functions. A GP prior is defined by a mean function (often taken to be 0 for convenience) and a kernel (covariance function) $k(x,x')$ which encodes assumptions about the smoothness or structure of the utility function. For example, one might assume $u(x)$ is a smooth function of state, and choose a radial basis function (RBF) kernel $k(x,x') = \sigma_f^2 \exp(-\|x-x'\|^2/(2\ell^2))$ with some length-scale $\ell$.</span>
<span id="cb6-148"><a href="#cb6-148"></a></span>
<span id="cb6-149"><a href="#cb6-149"></a>After observing some preference data, Bayes’ rule gives a posterior over the function $u(x)$. In the case of pairwise comparisons, the likelihood of a comparison $(A \succ B)$ given an underlying utility function $u$ can be modeled via the same logistic function: $P(A \succ B \mid u) = \sigma(u(A)-u(B))$. Combining this likelihood with the GP prior is analytically intractable (due to the non-Gaussian logistic likelihood), but one can use approximation techniques (Laplace approximation or MCMC) to obtain a posterior GP. The result is a *Gaussian process preference model* that can predict the utility of any new option with an uncertainty interval.</span>
<span id="cb6-150"><a href="#cb6-150"></a></span>
<span id="cb6-151"><a href="#cb6-151"></a>If we have direct evaluations of utility (e.g., the human provides a numeric reward for some states), the GP inference is simpler – it reduces to standard GP regression. However, in many real-world scenarios, humans are better at making relative judgments than assigning absolute utility values. This change in feedback type transforms the inference problem fundamentally. Instead of having a Gaussian likelihood (as in standard GP regression), we now have a non-Gaussian likelihood, typically modeled using a probit or logistic function. The observed data no longer provide direct samples of the latent utility function, but instead impose constraints on the *relative* ordering of latent values.</span>
<span id="cb6-152"><a href="#cb6-152"></a></span>
<span id="cb6-153"><a href="#cb6-153"></a>Due to this non-Gaussian likelihood, exact Bayesian inference is no longer tractable: the posterior over the latent utility function given the pairwise data does not have a closed-form expression. The GP prior is still Gaussian, but the posterior becomes non-Gaussian and multi-modal, particularly as the number of comparisons grows.</span>
<span id="cb6-154"><a href="#cb6-154"></a></span>
<span id="cb6-155"><a href="#cb6-155"></a>To address this, we must turn to approximate inference methods. One common and computationally efficient choice is the Laplace approximation, which approximates the true posterior with a Gaussian centered at the maximum a posteriori (MAP) estimate. This involves:</span>
<span id="cb6-156"><a href="#cb6-156"></a><span class="ss">1. </span>Finding the mode of the posterior (i.e., the most probable utility values given the data),</span>
<span id="cb6-157"><a href="#cb6-157"></a><span class="ss">2. </span>Approximating the curvature of the log-posterior around this mode using the Hessian (second derivative),</span>
<span id="cb6-158"><a href="#cb6-158"></a><span class="ss">3. </span>Using this local curvature to construct a Gaussian approximation.</span>
<span id="cb6-159"><a href="#cb6-159"></a></span>
<span id="cb6-160"><a href="#cb6-160"></a>While not exact, this method works well in practice, especially when the posterior is unimodal and the number of comparison pairs is moderate. Other alternatives such as variational inference or sampling-based methods (e.g., Hamiltonian Monte Carlo) can yield more accurate results but often require more complex implementation and computational resources.</span>
<span id="cb6-161"><a href="#cb6-161"></a></span>
<span id="cb6-164"><a href="#cb6-164"></a><span class="in">```{python}</span></span>
<span id="cb6-165"><a href="#cb6-165"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-166"><a href="#cb6-166"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-167"><a href="#cb6-167"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb6-168"><a href="#cb6-168"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb6-169"><a href="#cb6-169"></a></span>
<span id="cb6-170"><a href="#cb6-170"></a><span class="co"># --- True latent utility function ---</span></span>
<span id="cb6-171"><a href="#cb6-171"></a><span class="kw">def</span> true_u(x):</span>
<span id="cb6-172"><a href="#cb6-172"></a>    <span class="cf">return</span> np.sin(x) <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> x</span>
<span id="cb6-173"><a href="#cb6-173"></a></span>
<span id="cb6-174"><a href="#cb6-174"></a><span class="co"># --- RBF Kernel function ---</span></span>
<span id="cb6-175"><a href="#cb6-175"></a><span class="kw">def</span> rbf_kernel(x1, x2, length_scale<span class="op">=</span><span class="fl">0.8</span>, sigma_f<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb6-176"><a href="#cb6-176"></a>    x1, x2 <span class="op">=</span> np.atleast_2d(x1).T, np.atleast_2d(x2).T</span>
<span id="cb6-177"><a href="#cb6-177"></a>    sqdist <span class="op">=</span> (x1 <span class="op">-</span> x2.T) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb6-178"><a href="#cb6-178"></a>    <span class="cf">return</span> sigma_f<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> sqdist <span class="op">/</span> length_scale<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb6-179"><a href="#cb6-179"></a></span>
<span id="cb6-180"><a href="#cb6-180"></a><span class="co"># --- Generate synthetic preference data ---</span></span>
<span id="cb6-181"><a href="#cb6-181"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb6-182"><a href="#cb6-182"></a>num_pairs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb6-183"><a href="#cb6-183"></a>X_candidates <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb6-184"><a href="#cb6-184"></a>true_utilities <span class="op">=</span> true_u(X_candidates)</span>
<span id="cb6-185"><a href="#cb6-185"></a></span>
<span id="cb6-186"><a href="#cb6-186"></a><span class="co"># Sample preference pairs</span></span>
<span id="cb6-187"><a href="#cb6-187"></a>idx_pairs <span class="op">=</span> np.random.choice(<span class="bu">len</span>(X_candidates), size<span class="op">=</span>(num_pairs, <span class="dv">2</span>), replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-188"><a href="#cb6-188"></a>X_pref_pairs <span class="op">=</span> []</span>
<span id="cb6-189"><a href="#cb6-189"></a><span class="cf">for</span> i, j <span class="kw">in</span> idx_pairs:</span>
<span id="cb6-190"><a href="#cb6-190"></a>    xi, xj <span class="op">=</span> X_candidates[i], X_candidates[j]</span>
<span id="cb6-191"><a href="#cb6-191"></a>    <span class="cf">if</span> true_utilities[i] <span class="op">&gt;</span> true_utilities[j]:</span>
<span id="cb6-192"><a href="#cb6-192"></a>        X_pref_pairs.append((xi, xj))</span>
<span id="cb6-193"><a href="#cb6-193"></a>    <span class="cf">else</span>:</span>
<span id="cb6-194"><a href="#cb6-194"></a>        X_pref_pairs.append((xj, xi))</span>
<span id="cb6-195"><a href="#cb6-195"></a>X_pref_pairs <span class="op">=</span> np.array(X_pref_pairs)</span>
<span id="cb6-196"><a href="#cb6-196"></a></span>
<span id="cb6-197"><a href="#cb6-197"></a><span class="co"># --- Unique x values and indexing ---</span></span>
<span id="cb6-198"><a href="#cb6-198"></a>X_all <span class="op">=</span> np.unique(X_pref_pairs.flatten())</span>
<span id="cb6-199"><a href="#cb6-199"></a>n <span class="op">=</span> <span class="bu">len</span>(X_all)</span>
<span id="cb6-200"><a href="#cb6-200"></a>x_to_idx <span class="op">=</span> {x: i <span class="cf">for</span> i, x <span class="kw">in</span> <span class="bu">enumerate</span>(X_all)}</span>
<span id="cb6-201"><a href="#cb6-201"></a></span>
<span id="cb6-202"><a href="#cb6-202"></a><span class="co"># --- GP prior kernel matrix ---</span></span>
<span id="cb6-203"><a href="#cb6-203"></a>length_scale <span class="op">=</span> <span class="fl">0.8</span></span>
<span id="cb6-204"><a href="#cb6-204"></a>sigma_f <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb6-205"><a href="#cb6-205"></a>sigma_noise <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb6-206"><a href="#cb6-206"></a>K <span class="op">=</span> rbf_kernel(X_all, X_all, length_scale, sigma_f) <span class="op">+</span> sigma_noise <span class="op">*</span> np.eye(n)</span>
<span id="cb6-207"><a href="#cb6-207"></a></span>
<span id="cb6-208"><a href="#cb6-208"></a><span class="co"># --- Negative log-posterior function ---</span></span>
<span id="cb6-209"><a href="#cb6-209"></a><span class="kw">def</span> neg_log_posterior(f):</span>
<span id="cb6-210"><a href="#cb6-210"></a>    prior_term <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> f.T <span class="op">@</span> np.linalg.solve(K, f)</span>
<span id="cb6-211"><a href="#cb6-211"></a>    lik_term <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb6-212"><a href="#cb6-212"></a>    <span class="cf">for</span> xi, xj <span class="kw">in</span> X_pref_pairs:</span>
<span id="cb6-213"><a href="#cb6-213"></a>        fi, fj <span class="op">=</span> f[x_to_idx[xi]], f[x_to_idx[xj]]</span>
<span id="cb6-214"><a href="#cb6-214"></a>        delta <span class="op">=</span> (fi <span class="op">-</span> fj) <span class="op">/</span> np.sqrt(<span class="dv">2</span>)</span>
<span id="cb6-215"><a href="#cb6-215"></a>        lik_term <span class="op">-=</span> np.log(norm.cdf(delta) <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb6-216"><a href="#cb6-216"></a>    <span class="cf">return</span> prior_term <span class="op">+</span> lik_term</span>
<span id="cb6-217"><a href="#cb6-217"></a></span>
<span id="cb6-218"><a href="#cb6-218"></a><span class="co"># --- MAP estimation of latent utilities ---</span></span>
<span id="cb6-219"><a href="#cb6-219"></a>f_init <span class="op">=</span> np.zeros(n)</span>
<span id="cb6-220"><a href="#cb6-220"></a>res <span class="op">=</span> minimize(neg_log_posterior, f_init, method<span class="op">=</span><span class="st">"L-BFGS-B"</span>)</span>
<span id="cb6-221"><a href="#cb6-221"></a>f_map <span class="op">=</span> res.x</span>
<span id="cb6-222"><a href="#cb6-222"></a></span>
<span id="cb6-223"><a href="#cb6-223"></a><span class="co"># --- Laplace approximation: compute W (Hessian of neg log likelihood) ---</span></span>
<span id="cb6-224"><a href="#cb6-224"></a>W <span class="op">=</span> np.zeros((n, n))</span>
<span id="cb6-225"><a href="#cb6-225"></a><span class="cf">for</span> xi, xj <span class="kw">in</span> X_pref_pairs:</span>
<span id="cb6-226"><a href="#cb6-226"></a>    i, j <span class="op">=</span> x_to_idx[xi], x_to_idx[xj]</span>
<span id="cb6-227"><a href="#cb6-227"></a>    fi, fj <span class="op">=</span> f_map[i], f_map[j]</span>
<span id="cb6-228"><a href="#cb6-228"></a>    delta <span class="op">=</span> (fi <span class="op">-</span> fj) <span class="op">/</span> np.sqrt(<span class="dv">2</span>)</span>
<span id="cb6-229"><a href="#cb6-229"></a>    phi <span class="op">=</span> norm.pdf(delta)</span>
<span id="cb6-230"><a href="#cb6-230"></a>    Phi <span class="op">=</span> norm.cdf(delta) <span class="op">+</span> <span class="fl">1e-6</span></span>
<span id="cb6-231"><a href="#cb6-231"></a>    w <span class="op">=</span> (phi <span class="op">/</span> Phi)<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> delta <span class="op">*</span> phi <span class="op">/</span> Phi</span>
<span id="cb6-232"><a href="#cb6-232"></a>    w <span class="op">/=</span> <span class="dv">2</span>  <span class="co"># adjust for sqrt(2)</span></span>
<span id="cb6-233"><a href="#cb6-233"></a>    W[i, i] <span class="op">+=</span> w</span>
<span id="cb6-234"><a href="#cb6-234"></a>    W[j, j] <span class="op">+=</span> w</span>
<span id="cb6-235"><a href="#cb6-235"></a>    W[i, j] <span class="op">-=</span> w</span>
<span id="cb6-236"><a href="#cb6-236"></a>    W[j, i] <span class="op">-=</span> w</span>
<span id="cb6-237"><a href="#cb6-237"></a></span>
<span id="cb6-238"><a href="#cb6-238"></a><span class="co"># --- Posterior covariance approximation ---</span></span>
<span id="cb6-239"><a href="#cb6-239"></a>L <span class="op">=</span> np.linalg.cholesky(K)</span>
<span id="cb6-240"><a href="#cb6-240"></a>K_inv <span class="op">=</span> np.linalg.solve(L.T, np.linalg.solve(L, np.eye(n)))</span>
<span id="cb6-241"><a href="#cb6-241"></a>H <span class="op">=</span> K_inv <span class="op">+</span> W</span>
<span id="cb6-242"><a href="#cb6-242"></a>H_inv <span class="op">=</span> np.linalg.inv(H)</span>
<span id="cb6-243"><a href="#cb6-243"></a></span>
<span id="cb6-244"><a href="#cb6-244"></a><span class="co"># --- Prediction at test points ---</span></span>
<span id="cb6-245"><a href="#cb6-245"></a>X_test <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">200</span>)</span>
<span id="cb6-246"><a href="#cb6-246"></a>K_s <span class="op">=</span> rbf_kernel(X_all, X_test, length_scale, sigma_f)</span>
<span id="cb6-247"><a href="#cb6-247"></a>K_ss_diag <span class="op">=</span> np.diag(rbf_kernel(X_test, X_test, length_scale, sigma_f))</span>
<span id="cb6-248"><a href="#cb6-248"></a></span>
<span id="cb6-249"><a href="#cb6-249"></a><span class="co"># Posterior mean and variance</span></span>
<span id="cb6-250"><a href="#cb6-250"></a>posterior_mean <span class="op">=</span> K_s.T <span class="op">@</span> K_inv <span class="op">@</span> f_map</span>
<span id="cb6-251"><a href="#cb6-251"></a>temp <span class="op">=</span> np.linalg.solve(H, K_s)</span>
<span id="cb6-252"><a href="#cb6-252"></a>posterior_var <span class="op">=</span> K_ss_diag <span class="op">-</span> np.<span class="bu">sum</span>(K_s <span class="op">*</span> temp, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-253"><a href="#cb6-253"></a>posterior_std <span class="op">=</span> np.sqrt(np.maximum(posterior_var, <span class="dv">0</span>))</span>
<span id="cb6-254"><a href="#cb6-254"></a></span>
<span id="cb6-255"><a href="#cb6-255"></a><span class="co"># --- Visualization ---</span></span>
<span id="cb6-256"><a href="#cb6-256"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb6-257"><a href="#cb6-257"></a>plt.plot(X_test, true_u(X_test), <span class="st">'k--'</span>, label<span class="op">=</span><span class="st">"True utility"</span>)</span>
<span id="cb6-258"><a href="#cb6-258"></a>plt.plot(X_test, posterior_mean, <span class="st">'b-'</span>, label<span class="op">=</span><span class="st">"Posterior mean"</span>)</span>
<span id="cb6-259"><a href="#cb6-259"></a>plt.fill_between(X_test,</span>
<span id="cb6-260"><a href="#cb6-260"></a>                 posterior_mean <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> posterior_std,</span>
<span id="cb6-261"><a href="#cb6-261"></a>                 posterior_mean <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> posterior_std,</span>
<span id="cb6-262"><a href="#cb6-262"></a>                 color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">"95% CI"</span>)</span>
<span id="cb6-263"><a href="#cb6-263"></a>plt.scatter(X_all, [true_u(x) <span class="cf">for</span> x <span class="kw">in</span> X_all], c<span class="op">=</span><span class="st">'red'</span>, marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">"Observed x"</span>)</span>
<span id="cb6-264"><a href="#cb6-264"></a>plt.title(<span class="st">"GP Preference Learning (Laplace Approximation, 100 Pairs)"</span>)</span>
<span id="cb6-265"><a href="#cb6-265"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb6-266"><a href="#cb6-266"></a>plt.ylabel(<span class="st">"Utility"</span>)</span>
<span id="cb6-267"><a href="#cb6-267"></a>plt.legend()</span>
<span id="cb6-268"><a href="#cb6-268"></a>plt.tight_layout()</span>
<span id="cb6-269"><a href="#cb6-269"></a>plt.show()</span>
<span id="cb6-270"><a href="#cb6-270"></a><span class="in">```</span></span>
<span id="cb6-271"><a href="#cb6-271"></a></span>
<span id="cb6-272"><a href="#cb6-272"></a> (<span class="co">[</span><span class="ot">image</span><span class="co">]()</span>) *Gaussian Process posterior for a utility function (blue mean with 95% confidence band) after observing 5 points of noisy utility data (red ×). The true utility function (black dashed) is non-trivial. The GP correctly captures the function’s value around observed regions and expresses high uncertainty in the unobserved middle region. In practice, this uncertainty could guide an algorithm to query more feedback in the region $x\approx [4,7]$ to reduce ambiguity.* </span>
<span id="cb6-273"><a href="#cb6-273"></a></span>
<span id="cb6-274"><a href="#cb6-274"></a>Gaussian processes are a flexible way to learn utility functions. They naturally handle irregular data and provide principled uncertainty estimates. GP-based preference learning has been applied to tasks like interactive Bayesian optimization, where an algorithm seeks to find the maximum of $u(x)$ by iteratively querying a human which of two options is better <span class="co">[</span><span class="ot">@christiano2023deep</span><span class="co">]</span>.</span>
<span id="cb6-275"><a href="#cb6-275"></a></span>
<span id="cb6-276"><a href="#cb6-276"></a>Instead of GPs, one can use Bayesian neural networks or ensemble methods to model uncertainty in $u_\theta(x)$. For instance, a neural network can be trained on preference data, and techniques like Monte Carlo dropout or deep ensembles can provide uncertainty estimates for its predictions. These approaches scale to high-dimensional inputs (where GPs may be less practical) while still capturing epistemic uncertainty about the utility.</span>
<span id="cb6-277"><a href="#cb6-277"></a></span>
<span id="cb6-278"><a href="#cb6-278"></a>One principled way to capture uncertainty in Bayesian neural networks is via Markov Chain Monte Carlo (MCMC) methods, which seek to approximate the posterior distribution over model parameters given the data. In this setting, we place a prior over the neural network weights, $p(\theta)$, and define a likelihood function based on observed preferences—typically using a probabilistic choice model such as the Bradley-Terry or probit model. Given a dataset $\mathcal{D} = <span class="sc">\{</span>(x_i, x_j) : x_i \succ x_j<span class="sc">\}</span>$, the posterior is defined as</span>
<span id="cb6-279"><a href="#cb6-279"></a></span>
<span id="cb6-280"><a href="#cb6-280"></a>$$</span>
<span id="cb6-281"><a href="#cb6-281"></a>p(\theta \mid \mathcal{D}) \propto p(\mathcal{D} \mid \theta) \cdot p(\theta),</span>
<span id="cb6-282"><a href="#cb6-282"></a>$$</span>
<span id="cb6-283"><a href="#cb6-283"></a></span>
<span id="cb6-284"><a href="#cb6-284"></a>where $p(\mathcal{D} \mid \theta)$ is the likelihood of observing the pairwise comparisons under the utility function $u_\theta(x)$, and $p(\theta)$ is the prior over the parameters.</span>
<span id="cb6-285"><a href="#cb6-285"></a></span>
<span id="cb6-286"><a href="#cb6-286"></a>Unlike Gaussian processes, for which posterior inference is tractable in closed form under Gaussian likelihoods, inference in BNNs with non-Gaussian likelihoods is generally intractable. This is due to the non-conjugate nature of the neural network likelihood and the high-dimensional, nonlinear structure of the weight space. As a result, approximate inference methods are required.</span>
<span id="cb6-287"><a href="#cb6-287"></a></span>
<span id="cb6-288"><a href="#cb6-288"></a>MCMC provides a general-purpose approach to approximate sampling from the posterior. The key idea is to construct a Markov chain whose stationary distribution is the target posterior. One of the most widely used algorithms is the Metropolis-Hastings (MH) algorithm. Given a current state $\theta_t$, a new proposal $\theta'$ is generated from a proposal distribution $q(\theta' \mid \theta_t)$, and accepted with probability</span>
<span id="cb6-289"><a href="#cb6-289"></a></span>
<span id="cb6-290"><a href="#cb6-290"></a>$$</span>
<span id="cb6-291"><a href="#cb6-291"></a>A = \min\left(1, \frac{p(\mathcal{D} \mid \theta') \, p(\theta') \, q(\theta_t \mid \theta')}{p(\mathcal{D} \mid \theta_t) \, p(\theta_t) \, q(\theta' \mid \theta_t)}\right).</span>
<span id="cb6-292"><a href="#cb6-292"></a>$$</span>
<span id="cb6-293"><a href="#cb6-293"></a></span>
<span id="cb6-294"><a href="#cb6-294"></a>When the proposal distribution is symmetric, i.e., $q(\theta' \mid \theta_t) = q(\theta_t \mid \theta')$, the acceptance probability simplifies to a ratio of posterior densities. Over time, the chain yields samples $\theta^{(1)}, \dots, \theta^{(T)} \sim p(\theta \mid \mathcal{D})$, which can be used to compute posterior predictive estimates for the utility function:</span>
<span id="cb6-295"><a href="#cb6-295"></a></span>
<span id="cb6-296"><a href="#cb6-296"></a>$$</span>
<span id="cb6-297"><a href="#cb6-297"></a>\mathbb{E}<span class="co">[</span><span class="ot">u(x)</span><span class="co">]</span> \approx \frac{1}{T} \sum_{t=1}^T u_{\theta^{(t)}}(x),</span>
<span id="cb6-298"><a href="#cb6-298"></a>$$</span>
<span id="cb6-299"><a href="#cb6-299"></a></span>
<span id="cb6-300"><a href="#cb6-300"></a>with corresponding uncertainty estimates captured via the variance of the predictions across samples.</span>
<span id="cb6-301"><a href="#cb6-301"></a></span>
<span id="cb6-302"><a href="#cb6-302"></a>MCMC methods are particularly appealing for preference learning because they directly quantify epistemic uncertainty in the utility function, which is crucial for downstream tasks such as decision-making, active learning, and safe exploration. Furthermore, MCMC makes no restrictive assumptions on the form of the posterior and can be used with non-convex and multi-modal distributions that arise from complex neural network architectures.</span>
<span id="cb6-303"><a href="#cb6-303"></a></span>
<span id="cb6-304"><a href="#cb6-304"></a>However, MCMC also faces significant computational challenges in practice. First, the convergence of the Markov chain can be slow, especially in high-dimensional parameter spaces. Second, naive random-walk proposals (as in the basic Metropolis-Hastings algorithm) may suffer from low acceptance rates and poor mixing. More advanced MCMC methods such as Hamiltonian Monte Carlo (HMC) and No-U-Turn Sampling (NUTS) can help address these issues by using gradient information to propose more efficient moves through the parameter space.</span>
<span id="cb6-305"><a href="#cb6-305"></a></span>
<span id="cb6-306"><a href="#cb6-306"></a>Despite these limitations, MCMC remains a valuable tool for principled Bayesian inference in preference modeling, particularly in settings where uncertainty quantification is critical and computational cost is acceptable. In lower-dimensional settings or as a pedagogical tool, even simple MH-based approaches can offer intuitive and effective approximations to the posterior over preference functions.</span>
<span id="cb6-307"><a href="#cb6-307"></a></span>
<span id="cb6-310"><a href="#cb6-310"></a><span class="in">```{python}</span></span>
<span id="cb6-311"><a href="#cb6-311"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-312"><a href="#cb6-312"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-313"><a href="#cb6-313"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb6-314"><a href="#cb6-314"></a></span>
<span id="cb6-315"><a href="#cb6-315"></a><span class="co"># --- True latent utility function ---</span></span>
<span id="cb6-316"><a href="#cb6-316"></a><span class="kw">def</span> true_u(x):</span>
<span id="cb6-317"><a href="#cb6-317"></a>    <span class="cf">return</span> np.sin(x) <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> x</span>
<span id="cb6-318"><a href="#cb6-318"></a></span>
<span id="cb6-319"><a href="#cb6-319"></a><span class="co"># --- Generate synthetic preference data ---</span></span>
<span id="cb6-320"><a href="#cb6-320"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb6-321"><a href="#cb6-321"></a>X <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">40</span>)</span>
<span id="cb6-322"><a href="#cb6-322"></a>y_true <span class="op">=</span> true_u(X)</span>
<span id="cb6-323"><a href="#cb6-323"></a></span>
<span id="cb6-324"><a href="#cb6-324"></a><span class="co"># Create pairwise comparisons</span></span>
<span id="cb6-325"><a href="#cb6-325"></a>pairs <span class="op">=</span> []</span>
<span id="cb6-326"><a href="#cb6-326"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>):</span>
<span id="cb6-327"><a href="#cb6-327"></a>    i, j <span class="op">=</span> np.random.choice(<span class="bu">len</span>(X), <span class="dv">2</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-328"><a href="#cb6-328"></a>    <span class="cf">if</span> y_true[i] <span class="op">&gt;</span> y_true[j]:</span>
<span id="cb6-329"><a href="#cb6-329"></a>        pairs.append((X[i], X[j], <span class="dv">1</span>))  <span class="co"># x_i preferred over x_j</span></span>
<span id="cb6-330"><a href="#cb6-330"></a>    <span class="cf">else</span>:</span>
<span id="cb6-331"><a href="#cb6-331"></a>        pairs.append((X[j], X[i], <span class="dv">1</span>))</span>
<span id="cb6-332"><a href="#cb6-332"></a></span>
<span id="cb6-333"><a href="#cb6-333"></a><span class="co"># --- Define a deep neural network: 3 hidden layers ---</span></span>
<span id="cb6-334"><a href="#cb6-334"></a><span class="kw">def</span> init_deep_params(hidden_dims<span class="op">=</span>[<span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">10</span>]):</span>
<span id="cb6-335"><a href="#cb6-335"></a>    params <span class="op">=</span> {}</span>
<span id="cb6-336"><a href="#cb6-336"></a>    layer_dims <span class="op">=</span> [<span class="dv">1</span>] <span class="op">+</span> hidden_dims <span class="op">+</span> [<span class="dv">1</span>]</span>
<span id="cb6-337"><a href="#cb6-337"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(layer_dims) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb6-338"><a href="#cb6-338"></a>        W_key <span class="op">=</span> <span class="ss">f"W</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span></span>
<span id="cb6-339"><a href="#cb6-339"></a>        b_key <span class="op">=</span> <span class="ss">f"b</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span></span>
<span id="cb6-340"><a href="#cb6-340"></a>        params[W_key] <span class="op">=</span> np.random.randn(layer_dims[i<span class="op">+</span><span class="dv">1</span>], layer_dims[i]) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb6-341"><a href="#cb6-341"></a>        params[b_key] <span class="op">=</span> np.zeros((layer_dims[i<span class="op">+</span><span class="dv">1</span>], <span class="dv">1</span>))</span>
<span id="cb6-342"><a href="#cb6-342"></a>    <span class="cf">return</span> params</span>
<span id="cb6-343"><a href="#cb6-343"></a></span>
<span id="cb6-344"><a href="#cb6-344"></a><span class="kw">def</span> deep_forward(x, params):</span>
<span id="cb6-345"><a href="#cb6-345"></a>    x <span class="op">=</span> x.reshape(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb6-346"><a href="#cb6-346"></a>    num_layers <span class="op">=</span> <span class="bu">len</span>(params) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb6-347"><a href="#cb6-347"></a>    h <span class="op">=</span> x</span>
<span id="cb6-348"><a href="#cb6-348"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_layers):</span>
<span id="cb6-349"><a href="#cb6-349"></a>        h <span class="op">=</span> np.tanh(params[<span class="ss">f"W</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>] <span class="op">@</span> h <span class="op">+</span> params[<span class="ss">f"b</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>])</span>
<span id="cb6-350"><a href="#cb6-350"></a>    out <span class="op">=</span> params[<span class="ss">f"W</span><span class="sc">{</span>num_layers<span class="sc">}</span><span class="ss">"</span>] <span class="op">@</span> h <span class="op">+</span> params[<span class="ss">f"b</span><span class="sc">{</span>num_layers<span class="sc">}</span><span class="ss">"</span>]</span>
<span id="cb6-351"><a href="#cb6-351"></a>    <span class="cf">return</span> out.squeeze()</span>
<span id="cb6-352"><a href="#cb6-352"></a></span>
<span id="cb6-353"><a href="#cb6-353"></a><span class="kw">def</span> deep_utility(x, params):</span>
<span id="cb6-354"><a href="#cb6-354"></a>    <span class="cf">return</span> np.array([deep_forward(np.array([xi]), params) <span class="cf">for</span> xi <span class="kw">in</span> x])</span>
<span id="cb6-355"><a href="#cb6-355"></a></span>
<span id="cb6-356"><a href="#cb6-356"></a><span class="co"># --- Log likelihood (Bradley-Terry) ---</span></span>
<span id="cb6-357"><a href="#cb6-357"></a><span class="kw">def</span> deep_log_likelihood(params, pairs):</span>
<span id="cb6-358"><a href="#cb6-358"></a>    ll <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb6-359"><a href="#cb6-359"></a>    <span class="cf">for</span> xi, xj, _ <span class="kw">in</span> pairs:</span>
<span id="cb6-360"><a href="#cb6-360"></a>        ui <span class="op">=</span> deep_forward(np.array([xi]), params)</span>
<span id="cb6-361"><a href="#cb6-361"></a>        uj <span class="op">=</span> deep_forward(np.array([xj]), params)</span>
<span id="cb6-362"><a href="#cb6-362"></a>        ll <span class="op">+=</span> np.log(norm.cdf((ui <span class="op">-</span> uj) <span class="op">/</span> np.sqrt(<span class="dv">2</span>)) <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb6-363"><a href="#cb6-363"></a>    <span class="cf">return</span> ll</span>
<span id="cb6-364"><a href="#cb6-364"></a></span>
<span id="cb6-365"><a href="#cb6-365"></a><span class="co"># --- Gaussian prior on weights ---</span></span>
<span id="cb6-366"><a href="#cb6-366"></a><span class="kw">def</span> deep_log_prior(params):</span>
<span id="cb6-367"><a href="#cb6-367"></a>    lp <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb6-368"><a href="#cb6-368"></a>    <span class="cf">for</span> v <span class="kw">in</span> params.values():</span>
<span id="cb6-369"><a href="#cb6-369"></a>        lp <span class="op">-=</span> <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(v<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb6-370"><a href="#cb6-370"></a>    <span class="cf">return</span> lp</span>
<span id="cb6-371"><a href="#cb6-371"></a></span>
<span id="cb6-372"><a href="#cb6-372"></a><span class="co"># --- Proposal distribution ---</span></span>
<span id="cb6-373"><a href="#cb6-373"></a><span class="kw">def</span> deep_propose(params, sigma<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb6-374"><a href="#cb6-374"></a>    new_params <span class="op">=</span> {}</span>
<span id="cb6-375"><a href="#cb6-375"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> params.items():</span>
<span id="cb6-376"><a href="#cb6-376"></a>        new_params[k] <span class="op">=</span> v <span class="op">+</span> np.random.randn(<span class="op">*</span>v.shape) <span class="op">*</span> sigma</span>
<span id="cb6-377"><a href="#cb6-377"></a>    <span class="cf">return</span> new_params</span>
<span id="cb6-378"><a href="#cb6-378"></a></span>
<span id="cb6-379"><a href="#cb6-379"></a><span class="co"># --- Metropolis-Hastings sampling ---</span></span>
<span id="cb6-380"><a href="#cb6-380"></a><span class="kw">def</span> deep_mh(init_params, pairs, num_iters<span class="op">=</span><span class="dv">2000</span>, burn_in<span class="op">=</span><span class="dv">500</span>):</span>
<span id="cb6-381"><a href="#cb6-381"></a>    samples <span class="op">=</span> []</span>
<span id="cb6-382"><a href="#cb6-382"></a>    current <span class="op">=</span> init_params</span>
<span id="cb6-383"><a href="#cb6-383"></a>    current_lp <span class="op">=</span> deep_log_likelihood(current, pairs) <span class="op">+</span> deep_log_prior(current)</span>
<span id="cb6-384"><a href="#cb6-384"></a></span>
<span id="cb6-385"><a href="#cb6-385"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_iters):</span>
<span id="cb6-386"><a href="#cb6-386"></a>        proposal <span class="op">=</span> deep_propose(current)</span>
<span id="cb6-387"><a href="#cb6-387"></a>        proposal_lp <span class="op">=</span> deep_log_likelihood(proposal, pairs) <span class="op">+</span> deep_log_prior(proposal)</span>
<span id="cb6-388"><a href="#cb6-388"></a>        accept_prob <span class="op">=</span> np.exp(proposal_lp <span class="op">-</span> current_lp)</span>
<span id="cb6-389"><a href="#cb6-389"></a>        <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> accept_prob:</span>
<span id="cb6-390"><a href="#cb6-390"></a>            current <span class="op">=</span> proposal</span>
<span id="cb6-391"><a href="#cb6-391"></a>            current_lp <span class="op">=</span> proposal_lp</span>
<span id="cb6-392"><a href="#cb6-392"></a>        <span class="cf">if</span> i <span class="op">&gt;=</span> burn_in:</span>
<span id="cb6-393"><a href="#cb6-393"></a>            samples.append(current)</span>
<span id="cb6-394"><a href="#cb6-394"></a></span>
<span id="cb6-395"><a href="#cb6-395"></a>    <span class="cf">return</span> samples</span>
<span id="cb6-396"><a href="#cb6-396"></a></span>
<span id="cb6-397"><a href="#cb6-397"></a><span class="co"># --- Run MCMC ---</span></span>
<span id="cb6-398"><a href="#cb6-398"></a>deep_samples <span class="op">=</span> deep_mh(init_deep_params(), pairs, num_iters<span class="op">=</span><span class="dv">2000</span>, burn_in<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb6-399"><a href="#cb6-399"></a></span>
<span id="cb6-400"><a href="#cb6-400"></a><span class="co"># --- Posterior predictions ---</span></span>
<span id="cb6-401"><a href="#cb6-401"></a>X_test <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">200</span>)</span>
<span id="cb6-402"><a href="#cb6-402"></a>deep_preds <span class="op">=</span> np.array([deep_utility(X_test, s) <span class="cf">for</span> s <span class="kw">in</span> deep_samples])</span>
<span id="cb6-403"><a href="#cb6-403"></a>deep_mean <span class="op">=</span> deep_preds.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-404"><a href="#cb6-404"></a>deep_std <span class="op">=</span> deep_preds.std(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-405"><a href="#cb6-405"></a></span>
<span id="cb6-406"><a href="#cb6-406"></a><span class="co"># --- Plot results ---</span></span>
<span id="cb6-407"><a href="#cb6-407"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb6-408"><a href="#cb6-408"></a>plt.plot(X_test, true_u(X_test), <span class="st">'k--'</span>, label<span class="op">=</span><span class="st">'True utility'</span>)</span>
<span id="cb6-409"><a href="#cb6-409"></a>plt.plot(X_test, deep_mean, <span class="st">'b-'</span>, label<span class="op">=</span><span class="st">'BNN (3-layer) mean'</span>)</span>
<span id="cb6-410"><a href="#cb6-410"></a>plt.fill_between(X_test, deep_mean <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> deep_std, deep_mean <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> deep_std,</span>
<span id="cb6-411"><a href="#cb6-411"></a>                 color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">'95% CI'</span>)</span>
<span id="cb6-412"><a href="#cb6-412"></a>plt.title(<span class="st">"3-layer Bayesian Neural Network via MCMC on Preference Data"</span>)</span>
<span id="cb6-413"><a href="#cb6-413"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb6-414"><a href="#cb6-414"></a>plt.ylabel(<span class="st">"Utility"</span>)</span>
<span id="cb6-415"><a href="#cb6-415"></a>plt.legend()</span>
<span id="cb6-416"><a href="#cb6-416"></a>plt.tight_layout()</span>
<span id="cb6-417"><a href="#cb6-417"></a>plt.show()</span>
<span id="cb6-418"><a href="#cb6-418"></a><span class="in">```</span></span>
<span id="cb6-419"><a href="#cb6-419"></a></span>
<span id="cb6-420"><a href="#cb6-420"></a>Another Bayesian approach is Bayesian Inverse Reinforcement Learning (IRL), where a prior is placed on the parameters of a reward function and Bayes’ rule is used to update this distribution given demonstrations or preferences <span class="co">[</span><span class="ot">@iliad2019learning</span><span class="co">]</span>. Early work like Ramachandran &amp; Amir (2007) treated IRL as Bayesian inference, using MCMC to sample likely reward functions consistent with demonstrations. Such methods yield a posterior over reward functions, reflecting ambiguity when multiple rewards explain the human’s behavior.</span>
<span id="cb6-421"><a href="#cb6-421"></a></span>
<span id="cb6-422"><a href="#cb6-422"></a>In summary, Bayesian utility learning methods acknowledge that with limited human feedback, many possible utility functions might be compatible with the data. They keep track of this ambiguity, which is crucial for making cautious decisions and for actively gathering more feedback.</span>
<span id="cb6-423"><a href="#cb6-423"></a></span>
<span id="cb6-424"><a href="#cb6-424"></a><span class="fu">## Case Study: Learning from Human Feedback in Robotics</span></span>
<span id="cb6-425"><a href="#cb6-425"></a></span>
<span id="cb6-426"><a href="#cb6-426"></a>Thus far, we discussed preference learning in general terms. We now focus on robotics, where an agent must learn a *reward/utility function* that captures the human’s objectives for a *sequential decision-making* task. Robotics brings additional challenges: the utility often depends on a trajectory of states and actions, and feedback can come in multiple forms. We outline several key forms of human feedback for robot learning and how to learn from them:</span>
<span id="cb6-427"><a href="#cb6-427"></a></span>
<span id="cb6-428"><a href="#cb6-428"></a><span class="ss">- </span>Learning from demonstrations – inferring utility from expert demonstrations of the task.</span>
<span id="cb6-429"><a href="#cb6-429"></a><span class="ss">- </span>Learning from physical corrections – updating utility when a human physically intervenes in the robot’s behavior.</span>
<span id="cb6-430"><a href="#cb6-430"></a><span class="ss">- </span>Learning from trajectory evaluations – using human-provided scores or critiques of full trajectories.</span>
<span id="cb6-431"><a href="#cb6-431"></a><span class="ss">- </span>Learning from pairwise trajectory comparisons – inferring reward from which of two trajectories a human prefers.</span>
<span id="cb6-432"><a href="#cb6-432"></a></span>
<span id="cb6-433"><a href="#cb6-433"></a>These are not mutually exclusive; in practice, combinations can be very powerful <span class="co">[</span><span class="ot">@iliad2019learning</span><span class="co">]</span>. We describe each mode and how utility functions can be derived.</span>
<span id="cb6-434"><a href="#cb6-434"></a></span>
<span id="cb6-435"><a href="#cb6-435"></a><span class="fu">### Learning from Demonstrations (Inverse Reinforcement Learning)</span></span>
<span id="cb6-436"><a href="#cb6-436"></a></span>
<span id="cb6-437"><a href="#cb6-437"></a>In Learning from Demonstrations, also known as Inverse Reinforcement Learning, the human provides examples of desired behavior (e.g. teleoperating a robot to show how to perform a task). The assumption is that the demonstrator is approximately optimizing some latent reward function $R^*(s,a)$ (or utility for trajectories). IRL algorithms then search for a reward function $R_\theta$ under which the given demonstrations $\tau_{demo}$ have high expected return <span class="co">[</span><span class="ot">@iliad2019learning</span><span class="co">]</span>. </span>
<span id="cb6-438"><a href="#cb6-438"></a></span>
<span id="cb6-439"><a href="#cb6-439"></a>One classic approach is *Maximum Margin IRL*, which finds a reward function that makes the return of the demonstration trajectories higher than that of any other trajectories by a large margin. Another is *Maximum Entropy IRL*, which models the demonstrator as noisily optimal (Boltzmann-rational) <span class="co">[</span><span class="ot">@iliad2019learning</span><span class="co">]</span>. In MaxEnt IRL, the probability of a trajectory $\tau$ under reward parameters $\theta$ is modeled as:</span>
<span id="cb6-440"><a href="#cb6-440"></a></span>
<span id="cb6-441"><a href="#cb6-441"></a>$$</span>
<span id="cb6-442"><a href="#cb6-442"></a>P(\tau \mid \theta) = \frac{\exp<span class="sc">\{</span>R_\theta(\tau)<span class="sc">\}</span>}{\displaystyle \sum_{\tau'} \exp<span class="sc">\{</span>R_\theta(\tau')<span class="sc">\}</span>} \,,</span>
<span id="cb6-443"><a href="#cb6-443"></a>$$</span>
<span id="cb6-444"><a href="#cb6-444"></a></span>
<span id="cb6-445"><a href="#cb6-445"></a>where $R_\theta(\tau) = \sum_{t} r_\theta(s_t, a_t)$ is the cumulative reward of $\tau$. The IRL algorithm then adjusts $\theta$ to maximize the likelihood of the human demonstrations (while often using techniques to approximate the denominator, since summing over all trajectories is intractable). The end result is a reward function $R_\theta(s,a)$ that rationalizes the demonstrations.</span>
<span id="cb6-446"><a href="#cb6-446"></a></span>
<span id="cb6-447"><a href="#cb6-447"></a>*Key challenge:* unless demonstrations are *optimal* and cover the space well, IRL might recover an ambiguous or incorrect reward. In robotics, humans often have difficulty providing flawless demonstrations (due to hard-to-use interfaces or limited expertise) <span class="co">[</span><span class="ot">@iliad2019learning</span><span class="co">]</span>. For example, users teleoperating a robot arm might move jerkily or only accomplish part of the task <span class="co">[</span><span class="ot">@iliad2019learning</span><span class="co">]</span>. This makes sole reliance on demonstrations problematic. Nonetheless, demonstration data can provide a strong prior: it shows at least one way to succeed (or partial preferences for certain behaviors).</span>
<span id="cb6-448"><a href="#cb6-448"></a></span>
<span id="cb6-449"><a href="#cb6-449"></a><span class="fu">### Learning from Preferences and Rankings of Trajectories</span></span>
<span id="cb6-450"><a href="#cb6-450"></a></span>
<span id="cb6-451"><a href="#cb6-451"></a>When high-quality demonstrations are hard to obtain, preference queries on trajectories are a viable alternative <span class="co">[</span><span class="ot">@iliad2019learning</span><span class="co">]</span>. In preference-based learning for robotics, the robot (or algorithm) presents two (or more) trajectories of the task outcome, and the human chooses which one is better. Each such comparison provides a bit of information about the true underlying reward. By asking many queries, the algorithm can home in on the reward function that explains the human’s choices <span class="co">[</span><span class="ot">@iliad2019learning</span><span class="co">]</span>.</span>
<span id="cb6-452"><a href="#cb6-452"></a></span>
<span id="cb6-453"><a href="#cb6-453"></a>A concrete example is an agent learning to do a backflip in simulation <span class="co">[</span><span class="ot">@christiano2023deep</span><span class="co">]</span>. The agent initially performs random flails. The system then repeatedly shows the human *two video clips* of the agent’s behavior and asks which is closer to a proper backflip. From these comparisons, a reward model is learned that assigns higher value to behaviors more like backflips <span class="co">[</span><span class="ot">@christiano2023deep</span><span class="co">]</span>. The agent then uses reinforcement learning to optimize this learned reward, gradually performing better backflips. This process continues, with the human being asked comparisons on trajectories where the algorithm is most uncertain (to maximally inform the reward model) <span class="co">[</span><span class="ot">@christiano2023deep</span><span class="co">]</span>.</span>
<span id="cb6-454"><a href="#cb6-454"></a></span>
<span id="cb6-455"><a href="#cb6-455"></a> (<span class="co">[</span><span class="ot">Learning from human preferences | OpenAI</span><span class="co">](https://openai.com/index/learning-from-human-preferences/)</span>) *Framework for learning from human preferences in robotics: a reward predictor (utility function) is learned from human feedback on trajectory comparisons, and an RL algorithm uses this learned reward to improve the policy [@christiano2023deep]. The loop is iterative: as the policy improves, new queries focus on areas of uncertainty to refine the reward model.* </span>
<span id="cb6-456"><a href="#cb6-456"></a></span>
<span id="cb6-457"><a href="#cb6-457"></a>Such preference-based reward learning has enabled complex skills without explicitly programmed rewards <span class="co">[</span><span class="ot">@christiano2023deep</span><span class="co">]</span>. Notably, Christiano *et al.* (2017) showed that an agent can learn Atari game policies and robotic manipulations from a few hundred comparison queries, achieving goals that are hard to specify but easy to judge <span class="co">[</span><span class="ot">@christiano2023deep</span><span class="co">]</span>. Preferences are often easier for humans than demonstrations: choosing between options is simpler than generating one from scratch <span class="co">[</span><span class="ot">@iliad2019learning</span><span class="co">]</span>. However, preference learning can be slow if each query only yields one bit of information. Active learning and combining preferences with other feedback can greatly improve efficiency <span class="co">[</span><span class="ot">@iliad2019learning</span><span class="co">]</span>.</span>
<span id="cb6-458"><a href="#cb6-458"></a></span>
<span id="cb6-459"><a href="#cb6-459"></a><span class="fu">### Learning from Trajectory Evaluations (Critiques and Ratings)</span></span>
<span id="cb6-460"><a href="#cb6-460"></a></span>
<span id="cb6-461"><a href="#cb6-461"></a>Sometimes humans provide feedback in the form of *evaluative scores* or critiques on full trajectories (or partial trajectories). For example, after a robot finishes an attempt at a task, the human might give a reward signal (e.g. +1/-1, or a rating 1–5 stars, or say “too slow” vs “good job”). This is the premise of the TAMER framework (Training an Agent via Evaluative Reinforcement) and related approaches, where a human’s scalar reward signals are directly treated as the reward function for the agent in reinforcement learning.</span>
<span id="cb6-462"><a href="#cb6-462"></a></span>
<span id="cb6-463"><a href="#cb6-463"></a>From a utility learning perspective, such feedback can be used to directly fit a utility model $u_\theta$ that predicts the human’s rating for a given trajectory. For instance, if a human provides a score $H(\tau)$ for trajectory $\tau$, one can treat it as a training target for $u_\theta(\tau)$ (possibly under a regression loss). However, because humans are inconsistent and may not precisely quantify their preferences, it’s often useful to model $H(\tau)$ as a noisy realization of the underlying utility, rather than a perfect label. A Bayesian approach could treat $H(\tau)$ as a noisy observation of $u(\tau)$ and update a posterior for $u$. Alternatively, classification approaches can be used (e.g. treat trajectories into “liked” vs “disliked” based on thresholded ratings).</span>
<span id="cb6-464"><a href="#cb6-464"></a></span>
<span id="cb6-465"><a href="#cb6-465"></a>A challenge with trajectory-level feedback is *credit assignment*: the human’s single score must be attributed to the entire sequence of actions. Algorithms like COACH (Continuous cOaching of Automated Control Handlers) address this by allowing humans to give feedback at intermediate steps, thereby guiding the agent which specific part of the behavior was good or bad. In either case, learning from trajectory evaluations turns the human into a *reward function provider*, and the learning algorithm’s job is to infer the latent reward function that the human’s evaluations are trying to convey.</span>
<span id="cb6-466"><a href="#cb6-466"></a></span>
<span id="cb6-467"><a href="#cb6-467"></a><span class="fu">### Learning from Physical Corrections</span></span>
<span id="cb6-468"><a href="#cb6-468"></a></span>
<span id="cb6-469"><a href="#cb6-469"></a>Robots that physically collaborate with humans can receive physical corrections: the human may push the robot or otherwise intervene to adjust its behavior. Such corrections provide insight into the human’s desired utility. For example, if a household robot is carrying a fragile object too recklessly and the human physically slows it down or re-routes it, that indicates the human’s reward favors safety over speed at that moment.</span>
<span id="cb6-470"><a href="#cb6-470"></a></span>
<span id="cb6-471"><a href="#cb6-471"></a>Learning from physical corrections can be formalized in different ways. One approach is to treat a correction as a demonstration on a small segment: the human’s intervention suggests a better action or trajectory than what the robot was doing. This can be converted into a comparison: “the trajectory after correction is preferred over the original trajectory” for that time segment. The robot can then update its reward function $\theta$ to satisfy $R_\theta(\text{human-corrected behavior}) &gt; R_\theta(\text{robot’s initial behavior})$. Repeated corrections yield a dataset of such pairwise preferences, focused on the states where the robot was wrong <span class="co">[</span><span class="ot">@losey2021corrections</span><span class="co">]</span>.</span>
<span id="cb6-472"><a href="#cb6-472"></a></span>
<span id="cb6-473"><a href="#cb6-473"></a>Another approach is to infer the human’s intent through the sequence of corrections. Research by Losey *et al.* (2021) formalized learning from *sequences* of physical corrections, noting that each correction is not independent: a series of pushes might only make sense in aggregate <span class="co">[</span><span class="ot">@losey2021corrections</span><span class="co">]</span>. By analyzing the cumulative effect of multiple interventions, the algorithm can deduce the underlying objective more accurately (e.g. the human consistently steers the robot away from the table edges, implying a high negative reward for collisions). Their algorithm introduced an auxiliary reward term to capture the human’s trade-off: they will correct the robot if the immediate mistake is worth fixing relative to long-term performance <span class="co">[</span><span class="ot">@losey2021corrections</span><span class="co">]</span>. The conclusion was that reasoning over the sequence of corrections improved learning of the human’s objective <span class="co">[</span><span class="ot">@losey2021corrections</span><span class="co">]</span>.</span>
<span id="cb6-474"><a href="#cb6-474"></a></span>
<span id="cb6-475"><a href="#cb6-475"></a>Physical corrections are intuitive for humans – we often instinctively guide others or objects when they err. For the robot, interpreting this guidance requires converting it into constraints or examples for the utility function. It is a powerful signal because it is *active*: the human is not just telling preferences but directly imparting the desired direction of change.</span>
<span id="cb6-476"><a href="#cb6-476"></a></span>
<span id="cb6-477"><a href="#cb6-477"></a><span class="fu">### Combining Multiple Feedback Types</span></span>
<span id="cb6-478"><a href="#cb6-478"></a></span>
<span id="cb6-479"><a href="#cb6-479"></a>Each feedback modality has strengths and weaknesses. Demonstrations provide a lot of information but can be hard to perform; preferences are easy for humans but yield information slowly; corrections are very informative locally but require physical interaction; trajectory evaluations are straightforward but coarse. Combining these modes can lead to faster and more robust reward learning <span class="co">[</span><span class="ot">@iliad2019learning</span><span class="co">]</span>. For example, the DemPref algorithm <span class="co">[</span><span class="ot">@iliad2019learning</span><span class="co">]</span> first uses demonstrations to get an initial rough reward model, then uses preference queries to refine it quickly <span class="co">[</span><span class="ot">@iliad2019learning</span><span class="co">]</span>. In user studies, such combined approaches learned better rewards with fewer queries than using either alone <span class="co">[</span><span class="ot">@iliad2019learning</span><span class="co">]</span>.</span>
<span id="cb6-480"><a href="#cb6-480"></a></span>
<span id="cb6-481"><a href="#cb6-481"></a>In practical robot learning systems, one might start by asking for a demonstration. If the demo is suboptimal, the system can then ask preference questions on alternative attempts to clarify the true goal. During actual execution, if the human intervenes, the robot updates its reward function on the fly to avoid repeating the mistake. This *interactive reward learning* loop continues until the robot’s behavior aligns with human intent.</span>
<span id="cb6-482"><a href="#cb6-482"></a></span>
<span id="cb6-483"><a href="#cb6-483"></a><span class="fu">## Summary</span></span>
<span id="cb6-484"><a href="#cb6-484"></a></span>
<span id="cb6-485"><a href="#cb6-485"></a>Learning utility functions from human preferences enables value alignment: aligning an AI system’s objectives with what humans actually want, rather than what we *think* we want in abstract. We covered how supervised learning can extract utilities from comparisons or scores, and how Bayesian methods like Gaussian processes and Bayesian neural nets can capture uncertainty in our inferences. In robotics, we saw that feedback can come in many forms – demonstrations, comparisons, corrections, evaluations – each providing a unique window into the human’s utility function. By intelligently combining these signals, robots can efficiently learn complex reward functions that would be extremely difficult to hand-code.</span>
<span id="cb6-486"><a href="#cb6-486"></a></span>
<span id="cb6-487"><a href="#cb6-487"></a>Key takeaways and best practices include:</span>
<span id="cb6-488"><a href="#cb6-488"></a></span>
<span id="cb6-489"><a href="#cb6-489"></a><span class="ss">- </span>*Use the right feedback for the problem:* If optimal examples are available, demonstrations jump-start learning. If not, pairwise preferences or scalar critiques might be easier to obtain.</span>
<span id="cb6-490"><a href="#cb6-490"></a><span class="ss">- </span>*Model uncertainty:* Knowing what the system doesn’t know (via a Bayesian model) allows for smart query selection and avoids overconfidently optimizing the wrong objective.</span>
<span id="cb6-491"><a href="#cb6-491"></a><span class="ss">- </span>*Iterate with the human:* Preference learning is fundamentally an interactive process. An agent can query a human in ambiguous cases and continuously refine the utility estimate <span class="co">[</span><span class="ot">@christiano2023deep</span><span class="co">]</span>.</span>
<span id="cb6-492"><a href="#cb6-492"></a><span class="ss">- </span>*Validate the learned utility:* Once a reward is learned, testing the robot’s policy and having humans verify or correct it is crucial. Even a few manual corrections can reveal if the learned reward misses a key aspect, allowing further refinement.</span>
<span id="cb6-493"><a href="#cb6-493"></a><span class="ss">- </span>*Be aware of scaling and bias:* Human feedback can be noisy or biased. Techniques like DPO suggest ways to simplify learning and avoid instability, but one should monitor for issues like reward hacking or unintended solutions, intervening with additional feedback as needed.</span>
<span id="cb6-494"><a href="#cb6-494"></a></span>
<span id="cb6-495"><a href="#cb6-495"></a>Learning from human preferences is a rich area of ongoing research. It lies at the intersection of machine learning, human-computer interaction, and ethics. As AI systems become more advanced, the importance of teaching them *our* utility functions (and not mis-specified proxies) grows. The methods discussed in this chapter are building blocks toward AI that truly understands and pursues what humans value, acquired through learning *with* humans in the loop rather than in isolation. By mastering these techniques, we move closer to AI and robots that can be trusted to make decisions aligned with human preferences and well-being. </span>
<span id="cb6-496"><a href="#cb6-496"></a></span>
<span id="cb6-497"><a href="#cb6-497"></a><span class="co">&lt;!--</span></span>
<span id="cb6-498"><a href="#cb6-498"></a><span class="al">###</span><span class="co"> Reward Learning in Robotics</span></span>
<span id="cb6-499"><a href="#cb6-499"></a></span>
<span id="cb6-500"><a href="#cb6-500"></a><span class="co">To help set up our basic reward learning problem, consider a user and a</span></span>
<span id="cb6-501"><a href="#cb6-501"></a><span class="co">robot. The user's preferences or goals can be represented by an internal</span></span>
<span id="cb6-502"><a href="#cb6-502"></a><span class="co">reward function, R($\xi$), which the robot needs to learn. Since the</span></span>
<span id="cb6-503"><a href="#cb6-503"></a><span class="co">reward function isn't explicit, there are a variety of ways that the</span></span>
<span id="cb6-504"><a href="#cb6-504"></a><span class="co">robot can learn this reward function, which we will discuss in the next</span></span>
<span id="cb6-505"><a href="#cb6-505"></a><span class="co">section. An example method of learning a reward function from human data</span></span>
<span id="cb6-506"><a href="#cb6-506"></a><span class="co">is using pairwise comparison. Consider the robot example from section</span></span>
<span id="cb6-507"><a href="#cb6-507"></a><span class="co">one, but now, the robot shows the human two possible trajectories</span></span>
<span id="cb6-508"><a href="#cb6-508"></a><span class="co">$\xi_A$ and $\xi_B$ as depicted in the diagram below.</span></span>
<span id="cb6-509"><a href="#cb6-509"></a></span>
<span id="cb6-510"><a href="#cb6-510"></a><span class="co">![Two different trajectories taken by a robot to prompt</span></span>
<span id="cb6-511"><a href="#cb6-511"></a><span class="co">user ranking.](Figures/robots.png){#fig-reward-robot-1 width="70%"}</span></span>
<span id="cb6-512"><a href="#cb6-512"></a></span>
<span id="cb6-513"><a href="#cb6-513"></a><span class="co">The user is show both the trajectories above and asked to rank which one</span></span>
<span id="cb6-514"><a href="#cb6-514"></a><span class="co">is better. Based on iterations of multiple trajectories and ranking, the</span></span>
<span id="cb6-515"><a href="#cb6-515"></a><span class="co">robot is able to learn the user's internal reward function. There quite</span></span>
<span id="cb6-516"><a href="#cb6-516"></a><span class="co">a lot of ways that models can learn a reward function from human data.</span></span>
<span id="cb6-517"><a href="#cb6-517"></a><span class="co">Here's a list [@myers2021learning] of some of them:</span></span>
<span id="cb6-518"><a href="#cb6-518"></a></span>
<span id="cb6-519"><a href="#cb6-519"></a><span class="co">1.  Pairwise comparison: This is the method that we saw illustrated in</span></span>
<span id="cb6-520"><a href="#cb6-520"></a><span class="co">    the previous example. The robot is able to learn based on a</span></span>
<span id="cb6-521"><a href="#cb6-521"></a><span class="co">    comparison ranking provided by the user.</span></span>
<span id="cb6-522"><a href="#cb6-522"></a></span>
<span id="cb6-523"><a href="#cb6-523"></a><span class="co">2.  Expert demonstrations: Experts perform the task and the robot learns</span></span>
<span id="cb6-524"><a href="#cb6-524"></a><span class="co">    the optimal reward function from these demonstrations.</span></span>
<span id="cb6-525"><a href="#cb6-525"></a></span>
<span id="cb6-526"><a href="#cb6-526"></a><span class="co">3.  Sub-optimal demonstrations: The robot is provided with</span></span>
<span id="cb6-527"><a href="#cb6-527"></a><span class="co">    demonstrations that are not quite as good as the expert</span></span>
<span id="cb6-528"><a href="#cb6-528"></a><span class="co">    demonstrations but it is still able to learn a noisy reward function</span></span>
<span id="cb6-529"><a href="#cb6-529"></a><span class="co">    from the demonstrations.</span></span>
<span id="cb6-530"><a href="#cb6-530"></a></span>
<span id="cb6-531"><a href="#cb6-531"></a><span class="co">4.  Physical Corrections: While the robot is performing the task, at</span></span>
<span id="cb6-532"><a href="#cb6-532"></a><span class="co">    each point in its trajectory (or at an arbitrary point in its</span></span>
<span id="cb6-533"><a href="#cb6-533"></a><span class="co">    trajectory) its arm is corrected to a more suitable position. Based</span></span>
<span id="cb6-534"><a href="#cb6-534"></a><span class="co">    on these corrections, the robot is able to learn the reward</span></span>
<span id="cb6-535"><a href="#cb6-535"></a><span class="co">    function.</span></span>
<span id="cb6-536"><a href="#cb6-536"></a></span>
<span id="cb6-537"><a href="#cb6-537"></a><span class="co">5.  Ranking: This method is similar to pairwise comparison but involves</span></span>
<span id="cb6-538"><a href="#cb6-538"></a><span class="co">    more trajectories than 2. All the trajectories may have subtle</span></span>
<span id="cb6-539"><a href="#cb6-539"></a><span class="co">    differences from each other, but these differences help provide</span></span>
<span id="cb6-540"><a href="#cb6-540"></a><span class="co">    insight to the model.</span></span>
<span id="cb6-541"><a href="#cb6-541"></a></span>
<span id="cb6-542"><a href="#cb6-542"></a><span class="co">6.  Trajectory Assessment: Given a single trajectory, the user rates how</span></span>
<span id="cb6-543"><a href="#cb6-543"></a><span class="co">    close it is to optimal, typically using a ranking scale.</span></span>
<span id="cb6-544"><a href="#cb6-544"></a></span>
<span id="cb6-545"><a href="#cb6-545"></a><span class="co">    Each of these methods allows the robot to refine its understanding</span></span>
<span id="cb6-546"><a href="#cb6-546"></a><span class="co">    of the user's reward function, but their effectiveness can vary</span></span>
<span id="cb6-547"><a href="#cb6-547"></a><span class="co">    depending on the application. For instance, expert demonstrations</span></span>
<span id="cb6-548"><a href="#cb6-548"></a><span class="co">    tend to produce more reliable results but may not always be feasible</span></span>
<span id="cb6-549"><a href="#cb6-549"></a><span class="co">    in everyday tasks. Pairwise comparison and ranking methods offer</span></span>
<span id="cb6-550"><a href="#cb6-550"></a><span class="co">    more flexibility but might require a higher number of iterations.</span></span>
<span id="cb6-551"><a href="#cb6-551"></a></span>
<span id="cb6-552"><a href="#cb6-552"></a><span class="al">###</span><span class="co"> Direct Preference Optimization</span></span>
<span id="cb6-553"><a href="#cb6-553"></a></span>
<span id="cb6-554"><a href="#cb6-554"></a><span class="co">A modern method for estimating the parameters of a human preference</span></span>
<span id="cb6-555"><a href="#cb6-555"></a><span class="co">model is direct preference optimization [@rafailov2023direct], which is</span></span>
<span id="cb6-556"><a href="#cb6-556"></a><span class="co">used in the context of aligning language models to human preferences. A</span></span>
<span id="cb6-557"><a href="#cb6-557"></a><span class="co">recent approach [@christiano2023deep] first trains a reward model that</span></span>
<span id="cb6-558"><a href="#cb6-558"></a><span class="co">captures human preferences and then uses proximal policy optimization to</span></span>
<span id="cb6-559"><a href="#cb6-559"></a><span class="co">train a language model-based policy to reflect those learned</span></span>
<span id="cb6-560"><a href="#cb6-560"></a><span class="co">preferences. Direct Preference Optimization (DPO), on the other hand,</span></span>
<span id="cb6-561"><a href="#cb6-561"></a><span class="co">removes the need for a reward model by directly using the model</span></span>
<span id="cb6-562"><a href="#cb6-562"></a><span class="co">likelihood of two outcomes (a preferred or highly-ranked sequence and an</span></span>
<span id="cb6-563"><a href="#cb6-563"></a><span class="co">unpreferred or low-ranked sequence) to capture the preference</span></span>
<span id="cb6-564"><a href="#cb6-564"></a><span class="co">represented in the data. DPO provides a simpler framework than its</span></span>
<span id="cb6-565"><a href="#cb6-565"></a><span class="co">reinforcement learning approach and results in comparable performance</span></span>
<span id="cb6-566"><a href="#cb6-566"></a><span class="co">with improved stability. Furthermore, it obviates the need to train a</span></span>
<span id="cb6-567"><a href="#cb6-567"></a><span class="co">reward model, instead using a language model policy and human preference</span></span>
<span id="cb6-568"><a href="#cb6-568"></a><span class="co">dataset to align the policy directly to human preferences.</span></span>
<span id="cb6-569"><a href="#cb6-569"></a></span>
<span id="cb6-570"><a href="#cb6-570"></a><span class="co">&lt;!--</span></span>
<span id="cb6-571"><a href="#cb6-571"></a><span class="co">Through our exploration of human preference models, we will ground ourselves in</span></span>
<span id="cb6-572"><a href="#cb6-572"></a><span class="co">building a health coaching system that can provide meal recommendations aligned with a user's dietary needs and preferences. Examples of scenarios which can benefit from a model of how humans make choices include:</span></span>
<span id="cb6-573"><a href="#cb6-573"></a></span>
<span id="cb6-574"><a href="#cb6-574"></a><span class="co">1.  **Health coaching:** Humans express their preferences every time</span></span>
<span id="cb6-575"><a href="#cb6-575"></a><span class="co">    they pick lunch for consumption. Humans may have several goals</span></span>
<span id="cb6-576"><a href="#cb6-576"></a><span class="co">    related to nutrition, such as weight loss and improving</span></span>
<span id="cb6-577"><a href="#cb6-577"></a><span class="co">    concentration. We can learn how a given individual or set of</span></span>
<span id="cb6-578"><a href="#cb6-578"></a><span class="co">    individuals prefer to eat to provide personalized recommendations to</span></span>
<span id="cb6-579"><a href="#cb6-579"></a><span class="co">    help them attain their goals. This chapter will use this use case to</span></span>
<span id="cb6-580"><a href="#cb6-580"></a><span class="co">    ground human preference modeling in a real-life application.</span></span>
<span id="cb6-581"><a href="#cb6-581"></a></span>
<span id="cb6-582"><a href="#cb6-582"></a><span class="co">2.  **Social media:** Platforms have a far greater amount of content</span></span>
<span id="cb6-583"><a href="#cb6-583"></a><span class="co">    than one can consume in a lifetime, yet such products must aim to</span></span>
<span id="cb6-584"><a href="#cb6-584"></a><span class="co">    maximize user engagement. To accomplish this, we can learn what</span></span>
<span id="cb6-585"><a href="#cb6-585"></a><span class="co">    specific things people like to see in their feeds to optimize the</span></span>
<span id="cb6-586"><a href="#cb6-586"></a><span class="co">    value they gain out of their time on social media. For example, the</span></span>
<span id="cb6-587"><a href="#cb6-587"></a><span class="co">    video feed social media platform [TikTok](https://www.tiktok.com/)</span></span>
<span id="cb6-588"><a href="#cb6-588"></a><span class="co">    has had viral adoption due to its notorious ability to personalize a</span></span>
<span id="cb6-589"><a href="#cb6-589"></a><span class="co">    feed for its users based on their preferences.</span></span>
<span id="cb6-590"><a href="#cb6-590"></a></span>
<span id="cb6-591"><a href="#cb6-591"></a><span class="co">3.  **Shopping:** Retail corporations largely aim to maximize revenue by</span></span>
<span id="cb6-592"><a href="#cb6-592"></a><span class="co">    making it easy for people to make purchases. Recommendation systems</span></span>
<span id="cb6-593"><a href="#cb6-593"></a><span class="co">    on online shopping platforms provide a mechanism for curating</span></span>
<span id="cb6-594"><a href="#cb6-594"></a><span class="co">    specific items based on an individual's previous purchases (or even</span></span>
<span id="cb6-595"><a href="#cb6-595"></a><span class="co">    browsing history) to make shoppers aware of items they may like and,</span></span>
<span id="cb6-596"><a href="#cb6-596"></a><span class="co">    therefore, purchase.</span></span>
<span id="cb6-597"><a href="#cb6-597"></a></span>
<span id="cb6-598"><a href="#cb6-598"></a><span class="co">Two key models used in pairwise sampling are the Thurstonian and Bradley-Terry models [@cattelan2012]. The Thurstonian model assumes each item $i$ has a true score $u_i$ following a normal distribution. The difference $d_{ij} = u_i - u_j$ is also normally distributed. The probability that item $i$ is preferred over item $j$ is given by $P(i \succ j) = \Phi \left( \frac{u_i - u_j}{\sqrt{2\sigma^2}} \right)$, where $\Phi$ is the cumulative normal distribution function. The denominator $\sqrt{2\sigma^2}$ is the standard deviation of the difference $d_{ij} = u_i - u_j$ when $u_i$ and $u_j$ are normally distributed with variance $\sigma^2$[@cattelan2012]. The Bradley-Terry model defines the probability of preference based on latent scores $\beta_i$ and $\beta_j$. The probability that item $i$ is preferred over item $j$ is $P(i \succ j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}$. This model is used to estimate relative strengths or preferences based on latent scores. [@cattelan2012].</span></span>
<span id="cb6-599"><a href="#cb6-599"></a></span>
<span id="cb6-600"><a href="#cb6-600"></a><span class="co">::: {#tbl-philosophy}</span></span>
<span id="cb6-601"><a href="#cb6-601"></a><span class="co">  -----------------------------------------------------------------------</span></span>
<span id="cb6-602"><a href="#cb6-602"></a><span class="co">  Application                         Human Preference</span></span>
<span id="cb6-603"><a href="#cb6-603"></a><span class="co">  ----------------------------------- -----------------------------------</span></span>
<span id="cb6-604"><a href="#cb6-604"></a><span class="co">  Computer vision: train a neural     This is how humans process images</span></span>
<span id="cb6-605"><a href="#cb6-605"></a><span class="co">  network to predict bounding boxes   by identifying the position and</span></span>
<span id="cb6-606"><a href="#cb6-606"></a><span class="co">  delineating all instances of dogs   geometry of the things we see in</span></span>
<span id="cb6-607"><a href="#cb6-607"></a><span class="co">  in an image                         them</span></span>
<span id="cb6-608"><a href="#cb6-608"></a></span>
<span id="cb6-609"><a href="#cb6-609"></a><span class="co">  Natural language processing: train  Coherent text is itself a</span></span>
<span id="cb6-610"><a href="#cb6-610"></a><span class="co">  a model to generate coherent text   human-created and defined concept,</span></span>
<span id="cb6-611"><a href="#cb6-611"></a><span class="co">                                      and we prefer that any</span></span>
<span id="cb6-612"><a href="#cb6-612"></a><span class="co">                                      synthetically generated text</span></span>
<span id="cb6-613"><a href="#cb6-613"></a><span class="co">                                      matches that of humans</span></span>
<span id="cb6-614"><a href="#cb6-614"></a></span>
<span id="cb6-615"><a href="#cb6-615"></a><span class="co">  Computer vision: train a diffusion  Humans prefer that images</span></span>
<span id="cb6-616"><a href="#cb6-616"></a><span class="co">  model to generate realistic images  accurately capture the world as</span></span>
<span id="cb6-617"><a href="#cb6-617"></a><span class="co">  of nature                           observed by humans, and this</span></span>
<span id="cb6-618"><a href="#cb6-618"></a><span class="co">                                      generative model should reflect the</span></span>
<span id="cb6-619"><a href="#cb6-619"></a><span class="co">                                      details that comprise that</span></span>
<span id="cb6-620"><a href="#cb6-620"></a><span class="co">                                      preference</span></span>
<span id="cb6-621"><a href="#cb6-621"></a><span class="co">  -----------------------------------------------------------------------</span></span>
<span id="cb6-622"><a href="#cb6-622"></a></span>
<span id="cb6-623"><a href="#cb6-623"></a><span class="co">  : Examples of machine learning tasks and their interpretation as</span></span>
<span id="cb6-624"><a href="#cb6-624"></a><span class="co">  modeling human preferences.</span></span>
<span id="cb6-625"><a href="#cb6-625"></a><span class="co">:::</span></span>
<span id="cb6-626"><a href="#cb6-626"></a><span class="co">--&gt;</span></span>
<span id="cb6-627"><a href="#cb6-627"></a></span>
<span id="cb6-628"><a href="#cb6-628"></a><span class="co">&lt;!--</span></span>
<span id="cb6-629"><a href="#cb6-629"></a><span class="co">Game theory provides a mathematical framework for analyzing strategic</span></span>
<span id="cb6-630"><a href="#cb6-630"></a><span class="co">interactions among rational agents. These models help in understanding</span></span>
<span id="cb6-631"><a href="#cb6-631"></a><span class="co">and predicting human behavior by considering multiple criteria and the</span></span>
<span id="cb6-632"><a href="#cb6-632"></a><span class="co">associated trade-offs. They enhance the understanding of preferences</span></span>
<span id="cb6-633"><a href="#cb6-633"></a><span class="co">across multiple criteria and allow for richer and more accurate feedback</span></span>
<span id="cb6-634"><a href="#cb6-634"></a><span class="co">through structured comparisons. Game-theory framings capture the</span></span>
<span id="cb6-635"><a href="#cb6-635"></a><span class="co">complexity of preferences and interactions in decision-making processes</span></span>
<span id="cb6-636"><a href="#cb6-636"></a><span class="co">[@bhatia2020preference].</span></span>
<span id="cb6-637"><a href="#cb6-637"></a></span>
<span id="cb6-638"><a href="#cb6-638"></a><span class="co">The most popular form of preference elicitation involves pairwise</span></span>
<span id="cb6-639"><a href="#cb6-639"></a><span class="co">comparisons. Users are asked to choose between two options, such as</span></span>
<span id="cb6-640"><a href="#cb6-640"></a><span class="co">product A or product B. This method is used in various applications like</span></span>
<span id="cb6-641"><a href="#cb6-641"></a><span class="co">search engines, recommender systems, and interactive robotics. Key</span></span>
<span id="cb6-642"><a href="#cb6-642"></a><span class="co">concepts include the Von Neumann Winner and the Blackwell Winner. The</span></span>
<span id="cb6-643"><a href="#cb6-643"></a><span class="co">Von Neumann Winner refers to a distribution over objects that beats or</span></span>
<span id="cb6-644"><a href="#cb6-644"></a><span class="co">ties every other object in the collection under the expected utility</span></span>
<span id="cb6-645"><a href="#cb6-645"></a><span class="co">assumption. The Blackwell Winner generalizes the Von Neumann Winner for</span></span>
<span id="cb6-646"><a href="#cb6-646"></a><span class="co">multi-criteria problems using a target set for acceptable payoff vectors</span></span>
<span id="cb6-647"><a href="#cb6-647"></a><span class="co">[@bhatia2020preference].</span></span>
<span id="cb6-648"><a href="#cb6-648"></a></span>
<span id="cb6-649"><a href="#cb6-649"></a><span class="co">Game-theory framings provide a framework for preference learning along</span></span>
<span id="cb6-650"><a href="#cb6-650"></a><span class="co">multiple criteria. These models use tools from vector-valued payoffs in</span></span>
<span id="cb6-651"><a href="#cb6-651"></a><span class="co">game theory, with Blackwell's approach being a key concept. This</span></span>
<span id="cb6-652"><a href="#cb6-652"></a><span class="co">approach allows for a more comprehensive understanding of preferences by</span></span>
<span id="cb6-653"><a href="#cb6-653"></a><span class="co">considering multiple criteria simultaneously [@bhatia2020preference].</span></span>
<span id="cb6-654"><a href="#cb6-654"></a></span>
<span id="cb6-655"><a href="#cb6-655"></a><span class="co">In game-theory framings, pairwise preferences are modeled as random</span></span>
<span id="cb6-656"><a href="#cb6-656"></a><span class="co">variables. Comparisons between objects along different criteria are</span></span>
<span id="cb6-657"><a href="#cb6-657"></a><span class="co">captured in a preference tensor $P$. This tensor models the probability</span></span>
<span id="cb6-658"><a href="#cb6-658"></a><span class="co">that one object is preferred over another along a specific criterion,</span></span>
<span id="cb6-659"><a href="#cb6-659"></a><span class="co">allowing for a detailed understanding of preferences across multiple</span></span>
<span id="cb6-660"><a href="#cb6-660"></a><span class="co">dimensions [@bhatia2020preference].</span></span>
<span id="cb6-661"><a href="#cb6-661"></a></span>
<span id="cb6-662"><a href="#cb6-662"></a><span class="co">The preference tensor $P$ captures object comparisons along different</span></span>
<span id="cb6-663"><a href="#cb6-663"></a><span class="co">criteria. It is defined as:</span></span>
<span id="cb6-664"><a href="#cb6-664"></a><span class="co">$$P(i_1, i_2; j) = P(i_1 \succ i_2 \text{ along criterion } j)$$ where</span></span>
<span id="cb6-665"><a href="#cb6-665"></a><span class="co">$P(i_2, i_1; j) = 1 - P(i_1, i_2; j)$. These values are aggregated to</span></span>
<span id="cb6-666"><a href="#cb6-666"></a><span class="co">form an overall preference matrix $P_{ov}$ [@bhatia2020preference].</span></span>
<span id="cb6-667"><a href="#cb6-667"></a></span>
<span id="cb6-668"><a href="#cb6-668"></a><span class="co">The Blackwell Winner is defined using a target set $S$ of acceptable</span></span>
<span id="cb6-669"><a href="#cb6-669"></a><span class="co">score vectors. The goal is to find a distribution $\pi^*$ such that</span></span>
<span id="cb6-670"><a href="#cb6-670"></a><span class="co">$P(\pi^*, \pi) \in S$ for all $\pi$. This method minimizes the maximum</span></span>
<span id="cb6-671"><a href="#cb6-671"></a><span class="co">distance to the target set, providing a robust solution to</span></span>
<span id="cb6-672"><a href="#cb6-672"></a><span class="co">multi-criteria preference problems [@bhatia2020preference].</span></span>
<span id="cb6-673"><a href="#cb6-673"></a></span>
<span id="cb6-674"><a href="#cb6-674"></a><span class="co">The optimization problem for finding the Blackwell Winner is defined as:</span></span>
<span id="cb6-675"><a href="#cb6-675"></a><span class="co">$$\pi(P, S, \|\cdot\|) = \arg \min_{\pi \in \Delta_d} \left[ \max_{\pi' \in \Delta_d} \rho(P(\pi, \pi'), S) \right]$$</span></span>
<span id="cb6-676"><a href="#cb6-676"></a><span class="co">where $\rho(u, v) = \|u - v\|$. This measures the distance to the target</span></span>
<span id="cb6-677"><a href="#cb6-677"></a><span class="co">set, ensuring that the selected distribution is as close as possible to</span></span>
<span id="cb6-678"><a href="#cb6-678"></a><span class="co">the ideal preference vector [@bhatia2020preference].</span></span>
<span id="cb6-679"><a href="#cb6-679"></a><span class="co">--&gt;</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
    <footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/chap3.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer><script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




</body></html>