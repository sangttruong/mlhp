<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Models of Preferences and Decisions – Machine Learning from Human Preferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../src/chap2.html" rel="next">
<link href="../index.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-07ba0ad10f5680c660e360ac31d2f3b6.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8b864f0777c60eecff11d75b6b2e1175.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-fa3d1c749edcb96cd5cb7d620f3e5237.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-6f24586c8b15e78d85e3983c622e3e8a.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/monaco-editor@0.46.0/min/vs/editor/editor.main.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous" referrerpolicy="no-referrer">
  
<style type="text/css">
.monaco-editor pre {
  background-color: unset !important;
}

.qpyodide-editor-toolbar {
  width: 100%;
  display: flex;
  justify-content: space-between;
  box-sizing: border-box;
}

.qpyodide-editor-toolbar-left-buttons, .qpyodide-editor-toolbar-right-buttons {
  display: flex;
}

.qpyodide-non-interactive-loading-container.qpyodide-cell-needs-evaluation, .qpyodide-non-interactive-loading-container.qpyodide-cell-evaluated {
  justify-content: center;
  display: flex;
  background-color: rgba(250, 250, 250, 0.65);
  border: 1px solid rgba(233, 236, 239, 0.65);
  border-radius: 0.5rem;
  margin-top: 15px;
  margin-bottom: 15px;
}

.qpyodide-r-project-logo {
  color: #2767B0; /* R Project's blue color */
}

.qpyodide-icon-status-spinner {
  color: #7894c4;
}

.qpyodide-icon-run-code {
  color: #0d9c29
}

.qpyodide-output-code-stdout {
  color: #111;
}

.qpyodide-output-code-stderr {
  color: #db4133;
}

.qpyodide-editor {
  border: 1px solid #EEEEEE;
}

.qpyodide-editor-toolbar {
  background-color: #EEEEEE;
  padding: 0.2rem 0.5rem;
}

.qpyodide-button {
  background-color: #EEEEEE;
  display: inline-block;
  font-weight: 400;
  line-height: 1;
  text-decoration: none;
  text-align: center;
  color: #000;
  border-color: #dee2e6;
  border: 1px solid rgba(0,0,0,0);
  padding: 0.375rem 0.75rem;
  font-size: .9rem;
  border-radius: 0.25rem;
  transition: color .15s ease-in-out,background-color .15s ease-in-out,border-color .15s ease-in-out,box-shadow .15s ease-in-out;
}

.qpyodide-button:hover {
  color: #000;
  background-color: #d9dce0;
  border-color: #c8ccd0;
}

.qpyodide-button:disabled,.qpyodide-button.disabled,fieldset:disabled .qpyodide-button {
  pointer-events: none;
  opacity: .65
}

.qpyodide-button-reset {
  color: #696969; /*#4682b4;*/
}

.qpyodide-button-copy {
  color: #696969;
}


/* Custom styling for RevealJS Presentations*/

/* Reset the style of the interactive area */
.reveal div.qpyodide-interactive-area {
  display: block;
  box-shadow: none;
  max-width: 100%;
  max-height: 100%;
  margin: 0;
  padding: 0;
} 

/* Provide space to entries */
.reveal div.qpyodide-output-code-area pre div {
  margin: 1px 2px 1px 10px;
}

/* Collapse the inside code tags to avoid extra space between line outputs */
.reveal pre div code.qpyodide-output-code-stdout, .reveal pre div code.qpyodide-output-code-stderr {
  padding: 0;
  display: contents;
}

.reveal pre div code.qpyodide-output-code-stdout {
  color: #111;
}

.reveal pre div code.qpyodide-output-code-stderr {
  color: #db4133;
}


/* Create a border around console and output (does not effect graphs) */
.reveal div.qpyodide-console-area {
  border: 1px solid #EEEEEE;
  box-shadow: 2px 2px 10px #EEEEEE;
}

/* Cap output height and allow text to scroll */
/* TODO: Is there a better way to fit contents/max it parallel to the monaco editor size? */
.reveal div.qpyodide-output-code-area pre {
  max-height: 400px;
  overflow: scroll;
}
</style>
<script type="module">
// Document level settings ----

// Determine if we need to install python packages
globalThis.qpyodideInstallPythonPackagesList = [''];

// Check to see if we have an empty array, if we do set to skip the installation.
globalThis.qpyodideSetupPythonPackages = !(qpyodideInstallPythonPackagesList.indexOf("") !== -1);

// Display a startup message?
globalThis.qpyodideShowStartupMessage = false;

// Describe the webR settings that should be used
globalThis.qpyodideCustomizedPyodideOptions = {
  "indexURL": "https://cdn.jsdelivr.net/pyodide/v0.27.2/full/",
  "env": {
    "HOME": "/home/pyodide",
  }, 
  stdout: (text) => {qpyodideAddToOutputArray(text, "out");},
  stderr: (text) => {qpyodideAddToOutputArray(text, "error");}
}

// Store cell data
globalThis.qpyodideCellDetails = [{"code":"import numpy as np\nnp.random.seed(0)\n\ndef ackley(X, a=20, b=0.2, c=2*np.pi):\n    \"\"\"\n    Compute the Ackley function.\n    Parameters:\n      X: A NumPy array of shape (n, d) where each row is a d-dimensional point.\n      a, b, c: Parameters of the Ackley function.\n    Returns:\n      A NumPy array of function values.\n    \"\"\"\n    X = np.atleast_2d(X)\n    d = X.shape[1]\n    sum_sq = np.sum(X ** 2, axis=1)\n    term1 = -a * np.exp(-b * np.sqrt(sum_sq / d))\n    term2 = -np.exp(np.sum(np.cos(c * X), axis=1) / d)\n    return term1 + term2 + a + np.e","id":1,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"import matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nccmap = LinearSegmentedColormap.from_list(\"ackley\", [\"#f76a05\", \"#FFF2C9\"])\nplt.rcParams.update({\n    \"font.size\": 14,\n    \"axes.labelsize\": 16,\n    \"xtick.labelsize\": 14,\n    \"ytick.labelsize\": 14,\n    \"legend.fontsize\": 14,\n    \"axes.titlesize\": 16,\n})\n\ndef draw_surface():\n    inps = np.linspace(-2, 2, 100)\n    X, Y = np.meshgrid(inps, inps)\n    grid = np.column_stack([X.ravel(), Y.ravel()])\n    Z = ackley(grid).reshape(X.shape)\n    \n    plt.figure(figsize=(6, 5))\n    contour = plt.contourf(X, Y, Z, 50, cmap=ccmap)\n    plt.contour(X, Y, Z, levels=15, colors='black', linewidths=0.5, alpha=0.6)\n    plt.colorbar(contour, label=r'$f(x)$', ticks=[0, 3, 6])\n    plt.xlim(-2, 2)\n    plt.ylim(-2, 2)\n    plt.xticks([-2, 0, 2])\n    plt.yticks([-2, 0, 2])\n    plt.xlabel(r'$x_1$')\n    plt.ylabel(r'$x_2$')","id":2,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"d = 2\nn_items = 800\nitems = np.random.randn(n_items, d)*0.5 + np.ones((n_items, d))*0.5\nrewards = ackley(items)\ny = (rewards > rewards.mean())\ndraw_surface()\nplt.scatter(items[:, 0], items[:, 1], c=y, cmap='coolwarm', alpha=0.5)\nplt.show()","id":3,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"from matplotlib.colors import LinearSegmentedColormap\nlikert_cmap = LinearSegmentedColormap.from_list(\"likert_scale\", [\"red\", \"blue\"], N=5)\nnormalized = (rewards - rewards.min()) / (rewards.max() - rewards.min())\nratings = np.round(normalized * 4).squeeze()\n\ndraw_surface()\nscatter = plt.scatter(items[:, 0], items[:, 1], c=ratings, cmap=likert_cmap, alpha=0.5)\nplt.show()","id":4,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"import numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\n\n# Set random seed for reproducibility (optional)\nnp.random.seed(42)\n\n# Number of users and items\nnum_users = 50\nnum_items = 100\n\n# Generate user-specific and item-specific rewards\ntheta_true = np.random.randn(num_users)\nz_true = np.random.randn(num_items)\n\n# Define the logistic (sigmoid) function\ndef sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-x))\n\n# Generate observed choices using the logistic function\n# Compute probability matrix: shape (num_users, num_items)\nprobs = sigmoid(theta_true[:, None] - z_true[None, :])\n# Sample binary responses (0 or 1) from a Bernoulli distribution\ndata = np.random.binomial(1, probs)\n\n# Mask out a fraction of the response matrix (80% observed, 20% missing)\nmask = np.random.rand(num_users, num_items) > 0.2  # boolean mask\n# Create a version of the data with missing values (not needed for optimization, but for reference)\ndata_masked = data.copy().astype(float)\ndata_masked[~mask] = np.nan\n\n# Count of observed entries (used for averaging)\nobserved_count = np.sum(mask)\n\n# We will optimize over parameters theta and z.\n# Initialize estimates (random starting points)\ntheta_init = np.random.randn(num_users)\nz_init = np.random.randn(num_items)\n\n# Pack parameters into a single vector for the optimizer.\n# First num_users elements are theta_est, next num_items are z_est.\nparams_init = np.concatenate([theta_init, z_init])\n\ndef objective(params):\n    \"\"\"\n    Computes the loss and gradient for the current parameters.\n    Loss is defined as the negative log likelihood (averaged over observed entries).\n    \"\"\"\n    # Unpack parameters\n    theta = params[:num_users]\n    z = params[num_users:]\n    \n    # Compute difference and estimated probabilities\n    diff = theta[:, None] - z[None, :]  # shape: (num_users, num_items)\n    sigma = sigmoid(diff)\n    \n    # To avoid log(0), clip probabilities a little bit\n    eps = 1e-8\n    sigma = np.clip(sigma, eps, 1 - eps)\n    \n    # Compute negative log likelihood only on observed entries\n    # For each observed entry: if data == 1 then -log(sigma) else -log(1-sigma)\n    log_likelihood = data * np.log(sigma) + (1 - data) * np.log(1 - sigma)\n    loss = -np.sum(mask * log_likelihood) / observed_count\n    \n    # Compute gradient with respect to the difference x = theta_i - z_j\n    # d(loss)/d(x) = sigma - data  (for observed entries, zero otherwise)\n    diff_grad = (sigma - data) * mask  # shape: (num_users, num_items)\n    \n    # Gradients for theta: sum over items (axis 1)\n    grad_theta = np.sum(diff_grad, axis=1) / observed_count\n    # Gradients for z: negative sum over users (axis 0)\n    grad_z = -np.sum(diff_grad, axis=0) / observed_count\n    \n    # Pack gradients back into a single vector\n    grad = np.concatenate([grad_theta, grad_z])\n    return loss, grad\n\n# Callback to track progress (optional)\niteration_progress = tqdm()\n\ndef callback(xk):\n    iteration_progress.update(1)\n\n# Optimize using L-BFGS-B\nresult = minimize(\n    fun=lambda params: objective(params),\n    x0=params_init,\n    method=\"L-BFGS-B\",\n    jac=True,\n    callback=callback,\n    options={\"maxiter\": 100, \"disp\": True}\n)\niteration_progress.close()\n\n# Extract the estimated parameters\ntheta_est = result.x[:num_users]\nz_est = result.x[num_users:]\n\n# Compute final estimated probabilities\nprobs_final = sigmoid(theta_est[:, None] - z_est[None, :])\n\n# Compute AUC ROC on observed (training) and missing (test) entries\ntrain_probs = probs_final[mask]\ntest_probs = probs_final[~mask]\ntrain_labels = data[mask]\ntest_labels = data[~mask]\n\nauc_train = roc_auc_score(train_labels, train_probs)\nauc_test = roc_auc_score(test_labels, test_probs)\n\nprint(f\"Train AUC: {auc_train:.4f}\")\nprint(f\"Test AUC: {auc_test:.4f}\")","id":5,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"n_pairs = 10000\npair_indices = np.random.randint(0, n_items, size=(n_pairs, 2))\n# Exclude pairs where both indices are the same\nmask = pair_indices[:, 0] != pair_indices[:, 1]\npair_indices = pair_indices[mask]\n\nscores = np.zeros(n_items, dtype=int)\nwins = rewards[pair_indices[:, 0]] > rewards[pair_indices[:, 1]]\n\n# For pairs where the first item wins:\n#   - Increase score for the first item by 1\n#   - Decrease score for the second item by 1\nnp.add.at(scores, pair_indices[wins, 0], 1)\nnp.add.at(scores, pair_indices[wins, 1], -1)\n\n# For pairs where the second item wins or it's a tie:\n#   - Decrease score for the first item by 1\n#   - Increase score for the second item by 1\nnp.add.at(scores, pair_indices[~wins, 0], -1)\nnp.add.at(scores, pair_indices[~wins, 1], 1)\n\n# Determine preferred and non-preferred items based on scores\npreferred = scores > 0\nnon_preferred = scores < 0\n\ndraw_surface()\nplt.scatter(items[preferred, 0], items[preferred, 1], c='blue', label='Preferred', alpha=0.5)\nplt.scatter(items[non_preferred, 0], items[non_preferred, 1], c='purple', label='Non-preferred', alpha=0.5)\nplt.legend()\nplt.show()","id":6,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"from sklearn.model_selection import train_test_split\nimport torch\n\nclass LogisticRegression:\n    def __init__(self):\n        self.weights = None  # Initialized during training\n\n    def train(self, X, y, learning_rate, num_iterations):\n        \"\"\"\n        Train the logistic regression model using full batch gradient-based optimization.\n\n        Parameters:\n        - X (torch.Tensor): Training data of shape (n_samples, n_features).\n        - y (torch.Tensor): Target labels of shape (n_samples,).\n        \"\"\"\n        n_samples, n_features = X.shape\n\n        # Initialize weights without the bias term\n        self.weights = torch.zeros(n_features)\n\n        for i in range(num_iterations):\n            # YOUR CODE HERE (~4-5 lines)\n                pass\n            # END OF YOUR CODE\n\n    def predict_probs(self, X):\n        \"\"\"\n        Predict probabilities for samples in X.\n\n        Parameters:\n        - X (torch.Tensor): Input data of shape (n_samples, n_features).\n\n        Returns:\n        - y_probs (torch.Tensor): Predicted probabilities.\n        \"\"\"\n        y_probs = None\n\n        # YOUR CODE HERE (~2-3 lines)\n        pass\n        # END OF YOUR CODE\n\n        return y_probs\n\n\nif __name__ == \"__main__\":\n    # Load in Llama3 embeddings of prompt + completions on RewardBench\n    chosen_embeddings = torch.load('data/chosen_embeddings.pt')\n    rejected_embeddings = torch.load('data/rejected_embeddings.pt')\n\n    # Subtract the embeddings according to the Bradley-Terry reward model setup presented in the problem \n    X = (chosen_embeddings - rejected_embeddings).to(torch.float)\n    y = torch.ones(X.shape[0])\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)  \n\n    # Tune the learning_rate and num_iterations\n    learning_rate = None\n    num_iterations = None\n    model = LogisticRegression()\n    model.train(X_train, y_train, learning_rate=learning_rate, num_iterations=num_iterations)\n    print(f\"Expected Train Accuracy: {model.predict_probs(X_train).mean()}\")\n    print(f\"Expected Validation Accuracy: {del.predict_probs(X_val).mean()}\") # Should reach at least 90%","id":7,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"import torch\nimport random\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nrandom.seed(42)\ntorch.manual_seed(42)\n\ndef stated_prob(z_values):\n    \"\"\"\n    Computes the probability of stated preferences based on z values.\n    \n    Args:\n        z_values (torch.Tensor): The z value(s), where z represents the true probability of voting for Alice.\n\n    Returns:\n        torch.Tensor: Probability for stated preferences, derived from z values.\n    \"\"\"\n    # YOUR CODE HERE (~1 line)\n    # END OF YOUR CODE\n\nclass VotingSimulation:\n    \"\"\"\n    A class to simulate the voting process where revealed and stated preferences are generated.\n    \n    Attributes:\n        R (int): Number of revealed preferences.\n        z (float): The true probability of voting for Alice.\n        revealed_preferences (torch.Tensor): Simulated revealed preferences of R voters using Bernoulli distribution.\n                                             Takes on 1 for Alice, and 0 for Bob.\n        stated_preferences (torch.Tensor): Simulated stated preferences, initialized as an empty tensor.\n                                           Takes on 1 for Alice, and 0 for Bob.\n    \"\"\"\n    def __init__(self, R, z):\n        self.R = R\n        self.z = z\n        self.revealed_preferences = None # YOUR CODE HERE (~1 line)\n        self.stated_preferences = torch.tensor([])\n\n    def add_survey(self):\n        \"\"\"\n        Simulates an additional stated preference based on stated_prob and adds it to the list.\n        This updates the self.stated_preferences tensor by concatenating on a new simulated survey result.\n        \"\"\"\n        # YOUR CODE HERE (~3 lines)\n        # END OF YOUR CODE\n\ndef log_likelihoods(revealed_preferences, stated_preferences, z_values):\n    \"\"\"\n    Computes the log likelihoods across both revealed and stated preferences.\n    Use your answer in part (a) to help.\n    \n    Args:\n        revealed_preferences (torch.Tensor): Tensor containing revealed preferences (0 or 1).\n        stated_preferences (torch.Tensor): Tensor containing stated preferences (0 or 1).\n        z_values (torch.Tensor): Tensor of underlying z values to calculate likelihood for.\n\n    Returns:\n        torch.Tensor: Log likelihood for each z value.\n    \"\"\"\n    # YOUR CODE HERE (~10-16 lines)\n    pass\n    # END OF YOUR CODE \n\ndef average_mae_mle(R, z, survey_count, num_sims, z_sweep):\n    \"\"\"\n    Runs multiple simulations to compute the average mean absolute error (MAE) of Maximum Likelihood Estimation (MLE) \n    for z after increasing number of surveys.\n    \n    Args:\n        R (int): Number of revealed preferences.\n        z (float): The true probability of voting for Alice.\n        survey_count (int): Number of additional surveys to perform.\n        num_sims (int): Number of simulation runs to average over.\n        z_sweep (torch.Tensor): Range of z values to consider for maximum likelihood estimation.\n\n    Returns:\n        torch.Tensor: Tensor of mean absolute errors averaged over simulations.\n                      Should have shape (survey_count, )\n    \"\"\"\n    all_errors = []\n    for _ in tqdm(range(num_sims)):\n        errors = []\n        vote_simulator = VotingSimulation(R=R, z=z)\n\n        for _ in range(survey_count):\n            revealed_preferences = vote_simulator.revealed_preferences\n            stated_preferences = vote_simulator.stated_preferences\n\n            # YOUR CODE HERE (~6-8 lines)\n            pass # Compute log_likelihoods across z_sweep. Argmax to find MLE for z. \n                 # Append the absolute error to errors and add a survey to the simulator.\n            # END OF YOUR CODE\n\n        errors_tensor = torch.stack(errors) \n        all_errors.append(errors_tensor)\n\n    # Calculate the average error across simulations \n    mean_errors = torch.stack(all_errors).mean(dim=0)\n    return mean_errors\n\nif __name__ == \"__main__\":\n    # DO NOT CHANGE!\n    max_surveys = 2000\n    z = 0.5\n    R = 10\n    num_sims = 100\n    z_sweep = torch.linspace(0.01, 0.99, 981)\n\n    # Compute and plot the errors. Attach this plot to part (d).\n    mean_errors = average_mae_mle(R, z, max_surveys, num_sims, z_sweep)\n    plt.plot(mean_errors)\n\n    plt.xlabel('Surveys Conducted')\n    plt.ylabel('Average Error')\n    plt.title(f'MLE MAE Error (z={z}, {num_sims} simulations)')\n    plt.show()","id":8,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\ndef likelihood(p: float) -> float:\n    \"\"\"\n    Computes the likelihood of 9 heads and 3 tails assuming p_heads is p.\n\n    Args:\n    p (float): A value between 0 and 1 representing the probability of heads.\n\n    Returns:\n    float: The likelihood value at p_heads=p. Return 0 if p is outside the range [0, 1].\n    \"\"\"\n    # YOUR CODE HERE (~1-3 lines)\n    pass\n    # END OF YOUR CODE\n\n\ndef propose(x_current: float, sigma: float) -> float:\n    \"\"\"\n    Proposes a new sample from the proposal distribution Q.\n    Here, Q is a normal distribution centered at x_current with standard deviation sigma.\n\n    Args:\n    x_current (float): The current value in the Markov chain.\n    sigma (float): Standard deviation of the normal proposal distribution.\n\n    Returns:\n    float: The proposed new sample.\n    \"\"\"\n    # YOUR CODE HERE (~1-3 lines)\n    pass\n    # END OF YOUR CODE\n\n\ndef acceptance_probability(x_current: float, x_proposed: float) -> float:\n    \"\"\"\n    Computes the acceptance probability A for the proposed sample.\n    Since the proposal distribution is symmetric, Q cancels out.\n\n    Args:\n    x_current (float): The current value in the Markov chain.\n    x_proposed (float): The proposed new value.\n\n    Returns:\n    float: The acceptance probability\n    \"\"\"\n    # YOUR CODE HERE (~4-6 lines)\n    pass\n    # END OF YOUR CODE\n\n\ndef metropolis_hastings(N: int, T: int, x_init: float, sigma: float) -> np.ndarray:\n    \"\"\"\n    Runs the Metropolis-Hastings algorithm to sample from a posterior distribution.\n\n    Args:\n    N (int): Total number of iterations.\n    T (int): Burn-in period (number of initial samples to discard).\n    x_init (float): Initial value of the chain.\n    sigma (float): Standard deviation of the proposal distribution.\n\n    Returns:\n    list: Samples collected after the burn-in period.\n    \"\"\"\n    samples = []\n    x_current = x_init\n\n    for t in range(N):\n        # YOUR CODE HERE (~7-10 lines)\n        # Use the propose and acceptance_probability functions to get x_{t+1} and store it in samples after the burn-in period T\n        pass\n        # END OF YOUR CODE\n\n    return samples\n\n\ndef plot_results(samples: np.ndarray) -> None:\n    \"\"\"\n    Plots the histogram of MCMC samples along with the true Beta(10, 4) PDF.\n\n    Args:\n    samples (np.ndarray): Array of samples collected from the Metropolis-Hastings algorithm.\n\n    Returns:\n    None\n    \"\"\"\n    # Histogram of the samples from the Metropolis-Hastings algorithm\n    plt.hist(samples, bins=50, density=True, alpha=0.5, label=\"MCMC Samples\")\n\n    # True Beta(10, 4) distribution for comparison\n    p = np.linspace(0, 1, 1000)\n    beta_pdf = beta.pdf(p, 10, 4)\n    plt.plot(p, beta_pdf, \"r-\", label=\"Beta(10, 4) PDF\")\n\n    plt.xlabel(\"p_heads\")\n    plt.ylabel(\"Density\")\n    plt.title(\"Metropolis-Hastings Sampling of Biased Coin Posterior\")\n    plt.legend()\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    # MCMC Parameters (DO NOT CHANGE!)\n    N = 50000  # Total number of iterations\n    T = 10000  # Burn-in period to discard\n    x_init = 0.5  # Initial guess for p_heads\n    sigma = 0.1  # Standard deviation of the proposal distribution\n\n    # Run Metropolis-Hastings and plot the results\n    samples = metropolis_hastings(N, T, x_init, sigma)\n    plot_results(samples)","id":9,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"import torch\nimport torch.distributions as dist\nimport math\nfrom tqdm import tqdm\nfrom typing import Tuple\n\ndef make_data(\n    true_p: torch.Tensor, true_weights_1: torch.Tensor, true_weights_2: torch.Tensor, num_movies: int, feature_dim: int\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Generates a synthetic movie dataset according to the CardinalStreams model.\n\n    Args:\n        true_p (torch.Tensor): Probability of coming from Group 1.\n        true_weights_1 (torch.Tensor): Weights for Group 1.\n        true_weights_2 (torch.Tensor): Weights for Group 2.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: A tuple containing the dataset and labels.\n    \"\"\"\n    # Create movie features\n    first_movie_features = torch.randn((num_movies, feature_dim))\n    second_movie_features = torch.randn((num_movies, feature_dim))\n\n    # Only care about difference of features for Bradley-Terry\n    dataset = first_movie_features - second_movie_features\n\n    # Get probabilities that first movie is preferred assuming Group 1 or Group 2\n    weight_1_probs = torch.sigmoid(dataset @ true_weights_1)\n    weight_2_probs = torch.sigmoid(dataset @ true_weights_2)\n\n    # Probability that first movie is preferred overall can be viewed as sum of conditioning on Group 1 and Group 2\n    first_movie_preferred_probs = (\n        true_p * weight_1_probs + (1 - true_p) * weight_2_probs\n    )\n    labels = dist.Bernoulli(first_movie_preferred_probs).sample()\n    return dataset, labels\n\n\ndef compute_likelihoods(\n    dataset: torch.Tensor,\n    labels: torch.Tensor,\n    p: torch.Tensor,\n    w_1: torch.Tensor,\n    w_2: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"\n    Computes the likelihood of each datapoint. Use your calculation from part (a) to help.\n\n    Args:\n        dataset (torch.Tensor): The dataset of differences between movie features.\n        labels (torch.Tensor): The labels where 1 indicates the first movie is preferred, and 0 indicates preference of the second movie.\n        p (torch.Tensor): The probability of coming from Group 1.\n        w_1 (torch.Tensor): Weights for Group 1.\n        w_2 (torch.Tensor): Weights for Group 2.\n\n    Returns:\n        torch.Tensor: The likelihoods for each datapoint. Should have shape (dataset.shape[0], )\n    \"\"\"\n    # YOUR CODE HERE (~6-8 lines)\n    pass\n    # END OF YOUR CODE\n\ndef compute_prior_density(\n    p: torch.Tensor, w_1: torch.Tensor, w_2: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"\n    Computes the prior density of the parameters.\n\n    Args:\n        p (torch.Tensor): The probability of preferring model 1.\n        w_1 (torch.Tensor): Weights for model 1.\n        w_2 (torch.Tensor): Weights for model 2.\n\n    Returns:\n        torch.Tensor: The prior densities of p, w_1, and w_2.\n    \"\"\"\n    # Adjusts p to stay in the range [0.3, 0.7] to prevent multiple equilibria issues at p=0 and p=1\n    p_prob = torch.tensor([2.5]) if 0.3 <= p <= 0.7 else torch.tensor([0.0])\n\n    def normal_pdf(x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Computes the PDF of the standard normal distribution at x.\"\"\"\n        return (1.0 / torch.sqrt(torch.tensor(2 * math.pi))) * torch.exp(-0.5 * x**2)\n\n    weights_1_prob = normal_pdf(w_1)\n    weights_2_prob = normal_pdf(w_2)\n\n    # Concatenate the densities\n    concatenated_prob = torch.cat([p_prob, weights_1_prob, weights_2_prob])\n    return concatenated_prob\n\n\ndef metropolis_hastings(\n    dataset: torch.Tensor,\n    labels: torch.Tensor,\n    sigma: float = 0.01,\n    num_iters: int = 30000,\n    burn_in: int = 20000,\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]:\n    \"\"\"\n    Performs the Metropolis-Hastings algorithm to sample from the posterior distribution.\n    DO NOT CHANGE THE DEFAULT VALUES!\n\n    Args:\n        dataset (torch.Tensor): The dataset of differences between movie features.\n        labels (torch.Tensor): The labels indicating which movie is preferred.\n        sigma (float, optional): Standard deviation for proposal distribution.\n            Defaults to 0.01.\n        num_iters (int, optional): Total number of iterations. Defaults to 30000.\n        burn_in (int, optional): Number of iterations to discard as burn-in.\n            Defaults to 20000.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]: Samples of p,\n        w_1, w_2, and the fraction of accepted proposals.\n    \"\"\"\n    feature_dim = dataset.shape[1]\n\n    # Initialize random starting parameters by sampling priors\n    curr_p = 0.3 + 0.4 * torch.rand(1)\n    curr_w_1 = torch.randn(feature_dim)\n    curr_w_2 = torch.randn(feature_dim)\n\n    # Keep track of samples and total number of accepted proposals\n    p_samples = []\n    w_1_samples = []\n    w_2_samples = []\n    accept_count = 0 \n\n    for T in tqdm(range(num_iters)):\n        # YOUR CODE HERE (~3 lines)\n        pass # Sample proposals for p, w_1, w_2\n        # END OF YOUR CODE\n\n        # YOUR CODE HERE (~4-6 lines)\n        pass # Compute likehoods and prior densities on both the proposed and current samples\n        # END OF YOUR CODE\n\n        # YOUR CODE HERE (~2-4 lines)\n        pass # Obtain the ratios of the likelihoods and prior densities between the proposed and current samples \n        # END OF YOUR CODE \n\n        # YOUR CODE HERE (~1-2 lines)\n        pass # Multiply all ratios (both likelihoods and prior densities) and use this to calculate the acceptance probability of the proposal\n        # END OF YOUR CODE\n\n        # YOUR CODE HERE (~4-6 lines)\n        pass # Sample randomness to determine whether the proposal should be accepted to update curr_p, curr_w_1, curr_w_2, and accept_count\n        # END OF YOUR CODE \n\n        # YOUR CODE HERE (~4-6 lines)\n        pass # Update p_samples, w_1_samples, w_2_samples if we have passed the burn in period T\n        # END OF YOUR CODE \n\n    fraction_accepted = accept_count / num_iters\n    print(f\"Fraction of accepted proposals: {fraction_accepted}\")\n    return (\n        torch.stack(p_samples),\n        torch.stack(w_1_samples),\n        torch.stack(w_2_samples),\n        fraction_accepted,\n    )\n\n\ndef evaluate_metropolis(num_sims: int, num_movies: int, feature_dim: int) -> None:\n    \"\"\"\n    Runs the Metropolis-Hastings algorithm N times and compare estimated parameters\n    with true parameters to obtain success rate. You should attain a success rate of around 90%. \n\n    Note that there are two successful equilibria to converge to. They are true_weights_1 and true_weights_2 with probabilities\n    p and 1-p in addition to true_weights_2 and true_weights_1 with probabilities 1-p and p. This is why even though it may appear your\n    predicted parameters don't match the true parameters, they are in fact equivalent. \n\n    Args:\n        num_sims (int): Number of simulations to run.\n\n    Returns:\n        None\n    \"\"\"\n    \n    success_count = 0\n    for _ in range(num_sims):\n        # Sample random ground truth parameters\n        true_p = 0.3 + 0.4 * torch.rand(1)\n        true_weights_1 = torch.randn(feature_dim)\n        true_weights_2 = torch.randn(feature_dim)\n\n        print(\"\\n---- MCMC Simulation ----\")\n        print(\"True parameters:\", true_p, true_weights_1, true_weights_2)\n\n        dataset, labels = make_data(true_p, true_weights_1, true_weights_2, num_movies, feature_dim)\n        p_samples, w_1_samples, w_2_samples, _ = metropolis_hastings(dataset, labels)\n\n        p_pred = p_samples.mean(dim=0)\n        w_1_pred = w_1_samples.mean(dim=0)\n        w_2_pred = w_2_samples.mean(dim=0)\n\n        print(\"Predicted parameters:\", p_pred, w_1_pred, w_2_pred)\n\n        # Do casework on two equilibria cases to check for success\n        p_diff_case_1 = torch.abs(p_pred - true_p)\n        p_diff_case_2 = torch.abs(p_pred - (1 - true_p))\n\n        w_1_diff_case_1 = torch.max(torch.abs(w_1_pred - true_weights_1))\n        w_1_diff_case_2 = torch.max(torch.abs(w_1_pred - true_weights_2))\n\n        w_2_diff_case_1 = torch.max(torch.abs(w_2_pred - true_weights_2))\n        w_2_diff_case_2 = torch.max(torch.abs(w_2_pred - true_weights_1))\n\n        pass_case_1 = (\n            p_diff_case_1 < 0.1 and w_1_diff_case_1 < 0.5 and w_2_diff_case_1 < 0.5\n        )\n        pass_case_2 = (\n            p_diff_case_2 < 0.1 and w_1_diff_case_2 < 0.5 and w_2_diff_case_2 < 0.5\n        )\n        passes = pass_case_1 or pass_case_2\n\n        print(f'Result: {\"Success\" if passes else \"FAILED\"}')\n        if passes:\n            success_count += 1\n    print(f'Success rate: {success_count / num_sims}')\n\n\nif __name__ == \"__main__\":\n    evaluate_metropolis(num_sims=10, num_movies=30000, feature_dim=10)","id":10,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}}];


</script>
<script type="module">
// Declare startupMessageqpyodide globally
globalThis.qpyodideStartupMessage = document.createElement("p");

// Function to set the button text
globalThis.qpyodideSetInteractiveButtonState = function(buttonText, enableCodeButton = true) {
  document.querySelectorAll(".qpyodide-button-run").forEach((btn) => {
    btn.innerHTML = buttonText;
    btn.disabled = !enableCodeButton;
  });
}

// Function to update the status message in non-interactive cells
globalThis.qpyodideUpdateStatusMessage = function(message) {
  document.querySelectorAll(".qpyodide-status-text.qpyodide-cell-needs-evaluation").forEach((elem) => {
    elem.innerText = message;
  });
}

// Function to update the status message
globalThis.qpyodideUpdateStatusHeader = function(message) {

  if (!qpyodideShowStartupMessage) return;

  qpyodideStartupMessage.innerHTML = message;
}

// Status header update with customized spinner message
globalThis.qpyodideUpdateStatusHeaderSpinner = function(message) {

  qpyodideUpdateStatusHeader(`
    <i class="fa-solid fa-spinner fa-spin qpyodide-icon-status-spinner"></i>
    <span>${message}</span>
  `);
}


// Function that attaches the document status message
function qpyodideDisplayStartupMessage(showStartupMessage) {
  if (!showStartupMessage) {
    return;
  }

  // Get references to header elements
  const headerHTML = document.getElementById("title-block-header");
  const headerRevealJS = document.getElementById("title-slide");

  // Create the outermost div element for metadata
  const quartoTitleMeta = document.createElement("div");
  quartoTitleMeta.classList.add("quarto-title-meta");

  // Create the first inner div element
  const firstInnerDiv = document.createElement("div");
  firstInnerDiv.setAttribute("id", "qpyodide-status-message-area");

  // Create the second inner div element for "Pyodide Status" heading and contents
  const secondInnerDiv = document.createElement("div");
  secondInnerDiv.setAttribute("id", "qpyodide-status-message-title");
  secondInnerDiv.classList.add("quarto-title-meta-heading");
  secondInnerDiv.innerText = "Pyodide Status";

  // Create another inner div for contents
  const secondInnerDivContents = document.createElement("div");
  secondInnerDivContents.setAttribute("id", "qpyodide-status-message-body");
  secondInnerDivContents.classList.add("quarto-title-meta-contents");

  // Describe the Pyodide state
  qpyodideStartupMessage.innerText = "🟡 Loading...";
  qpyodideStartupMessage.setAttribute("id", "qpyodide-status-message-text");
  // Add `aria-live` to auto-announce the startup status to screen readers
  qpyodideStartupMessage.setAttribute("aria-live", "assertive");

  // Append the startup message to the contents
  secondInnerDivContents.appendChild(qpyodideStartupMessage);

  // Combine the inner divs and contents
  firstInnerDiv.appendChild(secondInnerDiv);
  firstInnerDiv.appendChild(secondInnerDivContents);
  quartoTitleMeta.appendChild(firstInnerDiv);

  // Determine where to insert the quartoTitleMeta element
  if (headerHTML || headerRevealJS) {
    // Append to the existing "title-block-header" element or "title-slide" div
    (headerHTML || headerRevealJS).appendChild(quartoTitleMeta);
  } else {
    // If neither headerHTML nor headerRevealJS is found, insert after "Pyodide-monaco-editor-init" script
    const monacoScript = document.getElementById("qpyodide-monaco-editor-init");
    const header = document.createElement("header");
    header.setAttribute("id", "title-block-header");
    header.appendChild(quartoTitleMeta);
    monacoScript.after(header);
  }
}

qpyodideDisplayStartupMessage(qpyodideShowStartupMessage);
</script>
<script type="module">
// Create a logging setup
globalThis.qpyodideMessageArray = []

// Add messages to array
globalThis.qpyodideAddToOutputArray = function(message, type) {
  qpyodideMessageArray.push({ message, type });
}

// Function to reset the output array
globalThis.qpyodideResetOutputArray = function() {
  qpyodideMessageArray = [];
}

globalThis.qpyodideRetrieveOutput = function() {
  return qpyodideMessageArray.map(entry => entry.message).join('\n');
}

// Start a timer
const initializePyodideTimerStart = performance.now();

// Encase with a dynamic import statement
globalThis.qpyodideInstance = await import(
  qpyodideCustomizedPyodideOptions.indexURL + "pyodide.mjs").then(
   async({ loadPyodide }) => {

    console.log("Start loading Pyodide");
    
    // Populate Pyodide options with defaults or new values based on `pyodide`` meta
    let mainPyodide = await loadPyodide(
      qpyodideCustomizedPyodideOptions
    );
    
    // Setup a namespace for global scoping
    // await loadedPyodide.runPythonAsync("globalScope = {}"); 
    
    // Update status to reflect the next stage of the procedure
    qpyodideUpdateStatusHeaderSpinner("Initializing Python Packages");

    // Load the `micropip` package to allow installation of packages.
    await mainPyodide.loadPackage("micropip");
    await mainPyodide.runPythonAsync(`import micropip`);

    // Load the `pyodide_http` package to shim uses of `requests` and `urllib3`.
    // This allows for `pd.read_csv(url)` to work flawlessly.
    // Details: https://github.com/coatless-quarto/pyodide/issues/9
    await mainPyodide.loadPackage("pyodide_http");
    await mainPyodide.runPythonAsync(`
    import pyodide_http
    pyodide_http.patch_all()  # Patch all libraries
    `);

    // Load the `matplotlib` package with necessary environment hook
    await mainPyodide.loadPackage("matplotlib");

    // Set the backend for matplotlib to be interactive.
    await mainPyodide.runPythonAsync(`
    import matplotlib
    matplotlib.use("module://matplotlib_pyodide.html5_canvas_backend")
    from matplotlib import pyplot as plt
    `);

    // Unlock interactive buttons
    qpyodideSetInteractiveButtonState(
      `<i class="fa-solid fa-play qpyodide-icon-run-code"></i> <span>Run Code</span>`, 
      true
    );

    // Set document status to viable
    qpyodideUpdateStatusHeader(
      "🟢 Ready!"
    );

    // Assign Pyodide into the global environment
    globalThis.mainPyodide = mainPyodide;

    console.log("Completed loading Pyodide");
    return mainPyodide;
  }
);

// Stop timer
const initializePyodideTimerEnd = performance.now();

// Create a function to retrieve the promise object.
globalThis._qpyodideGetInstance = function() {
    return qpyodideInstance;
}

</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../src/chap1.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Models of Preferences and Decisions</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning from Human Preferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sangttruong/mlhp" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Machine-Learning-from-Human-Preferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap1.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Models of Preferences and Decisions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Active Learning of Preference Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Alignment of Policy with Preferences</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Aggregation of Preferences</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/ack.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>license.qmd</p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-foundations" id="toc-sec-foundations" class="nav-link active" data-scroll-target="#sec-foundations"><span class="header-section-number">2.1</span> The Construction of Preference</a>
  <ul class="collapse">
  <li><a href="#axiom-1-preference-models-model-choice" id="toc-axiom-1-preference-models-model-choice" class="nav-link" data-scroll-target="#axiom-1-preference-models-model-choice"><span class="header-section-number">2.1.1</span> Axiom 1. Construction of Choices Set: Luce’s Choice Axiom (Luce, 1959)</a></li>
  <li><a href="#axiom-2-preference-centers-around-reward" id="toc-axiom-2-preference-centers-around-reward" class="nav-link" data-scroll-target="#axiom-2-preference-centers-around-reward"><span class="header-section-number">2.1.2</span> Axiom 2. Preference Centers around Utility: Reciprocity (Block &amp; Marschak, 1960)</a></li>
  <li><a href="#axiom-3-preference-captures-decision-making" id="toc-axiom-3-preference-captures-decision-making" class="nav-link" data-scroll-target="#axiom-3-preference-captures-decision-making"><span class="header-section-number">2.1.3</span> Axiom 3. Preference captures decision-making: Wins as a Sufficient Statistic (Bühlmann &amp; Huber, 1963)</a></li>
  <li><a href="#human-rationality" id="toc-human-rationality" class="nav-link" data-scroll-target="#human-rationality"><span class="header-section-number">2.1.4</span> Axiom 4. Rationality: The Transitivity of odds (Good, 1955)</a></li>
  </ul></li>
  <li><a href="#preference-model" id="toc-preference-model" class="nav-link" data-scroll-target="#preference-model"><span class="header-section-number">2.2</span> Models of Preferences and Decisions</a>
  <ul class="collapse">
  <li><a href="#item-wise-model" id="toc-item-wise-model" class="nav-link" data-scroll-target="#item-wise-model"><span class="header-section-number">2.2.1</span> Item-wise Model</a></li>
  <li><a href="#pairwise-model" id="toc-pairwise-model" class="nav-link" data-scroll-target="#pairwise-model"><span class="header-section-number">2.2.2</span> Pairwise Model</a></li>
  <li><a href="#list-wise-model" id="toc-list-wise-model" class="nav-link" data-scroll-target="#list-wise-model"><span class="header-section-number">2.2.3</span> List-wise Model</a></li>
  </ul></li>
  <li><a href="#function-class" id="toc-function-class" class="nav-link" data-scroll-target="#function-class"><span class="header-section-number">2.3</span> The Utility Function Class</a>
  <ul class="collapse">
  <li><a href="#parametric-and-nonparametric-function-class" id="toc-parametric-and-nonparametric-function-class" class="nav-link" data-scroll-target="#parametric-and-nonparametric-function-class"><span class="header-section-number">2.3.1</span> Parametric and Nonparametric Function Class</a></li>
  <li><a href="#unimodal-and-multimodal-function-class" id="toc-unimodal-and-multimodal-function-class" class="nav-link" data-scroll-target="#unimodal-and-multimodal-function-class"><span class="header-section-number">2.3.2</span> Unimodal and Multimodal Function Class</a></li>
  <li><a href="#single-objective-and-multi-objective-utility" id="toc-single-objective-and-multi-objective-utility" class="nav-link" data-scroll-target="#single-objective-and-multi-objective-utility"><span class="header-section-number">2.3.3</span> Single Objective and Multi-Objective Utility</a></li>
  <li><a href="#pretraining" id="toc-pretraining" class="nav-link" data-scroll-target="#pretraining"><span class="header-section-number">2.3.4</span> Pretraining</a></li>
  <li><a href="#others-consideration" id="toc-others-consideration" class="nav-link" data-scroll-target="#others-consideration"><span class="header-section-number">2.3.5</span> Others Consideration</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">2.4</span> Exercises</a>
  <ul class="collapse">
  <li><a href="#question-1-choice-modeling-15-points" id="toc-question-1-choice-modeling-15-points" class="nav-link" data-scroll-target="#question-1-choice-modeling-15-points">Question 1: Choice Modeling (15 points)</a></li>
  <li><a href="#question-2-revealed-and-stated-preferences-20-points" id="toc-question-2-revealed-and-stated-preferences-20-points" class="nav-link" data-scroll-target="#question-2-revealed-and-stated-preferences-20-points">Question 2: Revealed and Stated Preferences (20 points)</a></li>
  <li><a href="#question-3-probabilistic-multi-modal-preferences-25-points" id="toc-question-3-probabilistic-multi-modal-preferences-25-points" class="nav-link" data-scroll-target="#question-3-probabilistic-multi-modal-preferences-25-points">Question 3: Probabilistic Multi-modal Preferences (25 points)</a></li>
  </ul></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/chap1.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
<script src="https://cdn.jsdelivr.net/npm/monaco-editor@0.46.0/min/vs/loader.js"></script>
<script type="module" id="qpyodide-monaco-editor-init">

  // Configure the Monaco Editor's loader
  require.config({
    paths: {
      'vs': 'https://cdn.jsdelivr.net/npm/monaco-editor@0.46.0/min/vs'
    }
  });
</script>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Models of Preferences and Decisions</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<iframe src="https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/" style="width:45%; height:225px;">
</iframe>
<iframe src="https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/" style="width:45%; height:225px;">
</iframe>
<p><a href="https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/" class="btn btn-outline-primary" role="button">Fullscreen Part 1</a> <a href="https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/" class="btn btn-outline-primary" role="button">Fullscreen Part 2</a></p>
<p>Human preference modeling aims to capture humans’ decision making processes in a probabilistic framework. Many problems would benefit from a quantitative perspective, enabling an understanding of how humans engage with the world. In this chapter, we will explore how one can model human preferences, including different formulations of such models, how one can optimize these models given data, and considerations one should understand to create such systems.</p>
<section id="sec-foundations" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="sec-foundations"><span class="header-section-number">2.1</span> The Construction of Preference</h2>
<section id="axiom-1-preference-models-model-choice" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="axiom-1-preference-models-model-choice"><span class="header-section-number">2.1.1</span> Axiom 1. Construction of Choices Set: Luce’s Choice Axiom (Luce, 1959)</h3>
<p>Preference models model the preferred choices amongst a set of items. Preference models must enumerate the set of all possible choices included in a human decision. As such, we must ensure that the choices we enumerate capture the entire domain (collectively exhaustive) but are distinct (mutually exclusive) choices. A discrete set of choices is a constraint we canonically impose to ensure we can tractably model preferences and aptly estimate the parameters of preference models. We assume that if a new item is added to the choice set, the relative probabilities of choosing between the original items remain unchanged. This is known as the Independence of Irrelevant Alternatives (IIA) property from Luce’s axiom of choices <span class="citation" data-cites="Luce1977">(<a href="#ref-Luce1977" role="doc-biblioref">Luce 1977</a>)</span>.</p>
</section>
<section id="axiom-2-preference-centers-around-reward" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="axiom-2-preference-centers-around-reward"><span class="header-section-number">2.1.2</span> Axiom 2. Preference Centers around Utility: Reciprocity (Block &amp; Marschak, 1960)</h3>
<p>Preference models are centered around the notion of reward, a scalar quantity representing the benefit or value an individual attains from selecting a given choice. We assume that the underlying reward mechanism of a human preference model captures the final decision output from a human. We use the notation <span class="math inline">\(u_{i,j}\)</span> as the reward of person <span class="math inline">\(i\)</span> choosing item <span class="math inline">\(j\)</span>. The reward is a random variable, decomposing into true reward <span class="math inline">\(u_{i,j}^*\)</span> and a random noise <span class="math inline">\(\epsilon_{i,j}\)</span>: <span class="math inline">\(u_{i,j} = u_{i,j}^* + \epsilon_{i,j}\)</span>. <span class="citation" data-cites="mcfadden_conditional_1974">McFadden (<a href="#ref-mcfadden_conditional_1974" role="doc-biblioref">1974</a>)</span> posits that reward can further be decomposed into user-specific reward <span class="math inline">\(\theta_i\)</span> and item-specific reward <span class="math inline">\(z_j\)</span>: <span class="math inline">\(u_{i,j}^* = \theta_i + z_j\)</span>. This decomposition indicates that for a single user, only the relative difference in reward matters to predict the choice among items, and the scale of rewards is important when comparing across users.</p>
</section>
<section id="axiom-3-preference-captures-decision-making" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="axiom-3-preference-captures-decision-making"><span class="header-section-number">2.1.3</span> Axiom 3. Preference captures decision-making: Wins as a Sufficient Statistic (Bühlmann &amp; Huber, 1963)</h3>
<p>Human preferences are classified into two categories: revealed preferences and stated preferences. Revealed preferences are those one can observe retroactively from existing data. The implicit decision-making knowledge can be captured via learnable parameters and their usage in models that represent relationships between input decision attributes that may have little interpretability but enable powerful models of human preference. Such data may be easier to acquire and can reflect real-world outcomes (since they are, at least theoretically, inherently based on human preferences). However, if we fail to capture sufficient context in such data, human preference models may not sufficiently capture human preferences. Stated preferences are those individuals explicitly indicate in potentially experimental conditions. The explicit knowledge may be leveraged by including inductive biases during modeling (for example, the context used in a model), which are reasonable assumptions for how a human would consider a set of items. This may include controlled experiments or studies. This may be harder to obtain and somewhat biased, as they can be hypothetical or only accurately reflect a piece of the overall context of a decision. However, they enable greater control of the decision-making process.</p>
</section>
<section id="human-rationality" class="level3" data-number="2.1.4">
<h3 data-number="2.1.4" class="anchored" data-anchor-id="human-rationality"><span class="header-section-number">2.1.4</span> Axiom 4. Rationality: The Transitivity of odds (Good, 1955)</h3>
<p>The preference model assumes that humans are rational. Perfect rationality posits that individuals make decisions that maximize their reward, assuming they have complete information and the cognitive ability to process this information to make optimal choices. Numerous studies have shown that this assumption frequently fails to describe actual human behavior. Bounded rationality acknowledges that individuals operate within the limits of their information and cognitive capabilities <span class="citation" data-cites="simon1972theories">(<a href="#ref-simon1972theories" role="doc-biblioref">Simon 1972</a>)</span>. Here, decisions are influenced by noise, resulting in probabilistic choice behavior: while individuals aim to maximize their reward, noise can lead to deviations from perfectly rational choices <span class="citation" data-cites="miljkovic2005rational">(<a href="#ref-miljkovic2005rational" role="doc-biblioref">Miljkovic 2005</a>)</span>. Instead of deterministic reward maximization, the decision maker will choose an item with probability proportional to the reward they receive for that item. This probabilistic model can be operationalized with Boltzmann distribution. Utility of person <span class="math inline">\(i\)</span> on item <span class="math inline">\(j\)</span> is computed by a function <span class="math inline">\(f_i: e_j \rightarrow \mathbb{R}\)</span>, where <span class="math inline">\(e_j \in \mathbb{R}^d\)</span> is an embedding of item <span class="math inline">\(j\)</span>. The probability of item <span class="math inline">\(j\)</span> being preferred by person <span class="math inline">\(i\)</span> over all other alternatives in the choice set <span class="math inline">\(\mathcal{C}\)</span> is</p>
<p><span class="math display">\[
p_{ij} =  p_i(j \succ j': j' \neq j \forall j' \in \mathcal{C}) = Z_i^{-1} \exp \circ f_i(e_j) \text{ where } Z_i = \sum_{j' \in \mathcal{C}} \exp \circ f_i(e_{j'})
\]</span></p>
<p>One can extend the above model in various ways. For example, the above model does not account for similar actions. Consider the following example when choosing a mode of transportation: car and train, with no particular preference for either choice. The preferred probability is 50% for either item. However, if we have 99 cars and one train in the choice set, we would have a 99% probability of choosing a car. To address this issue, various extensions have been proposed. For example, we can introduce a similarity metric to cluster items. We want a metric that acts more as a distance in the feature space with the following properties: Identity (an item is most similar to itself), symmetric (the similarity of item <span class="math inline">\(j\)</span> to <span class="math inline">\(j'\)</span> is the same as that of <span class="math inline">\(j'\)</span> to <span class="math inline">\(j\)</span>), and positive semidefinite (similarity metric is non-negative). Under this extension, the probablity of item <span class="math inline">\(j\)</span> being preferred over all other alternatives by person <span class="math inline">\(i\)</span> is <span class="math inline">\(p_{ij} / w_j, \text{ where } w_j = \sum_{j' \in \mathcal{C}} s(e_j, e_{j'})\)</span>. This de-weights similar items, which is the desired effect for human decision-making.</p>
</section>
</section>
<section id="preference-model" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="preference-model"><span class="header-section-number">2.2</span> Models of Preferences and Decisions</h2>
<p>Next, we explore ways humans can express their preferences, including accept-reject sampling, pairwise sampling, rank-order sampling, rating-scale sampling, best-worst scaling, and multiple-choice samples. We will understand the process of collecting data through simulation and, when appropriate, discuss the real-world application of these models. Each item <span class="math inline">\(i\)</span> is represented by a <span class="math inline">\(d=2\)</span> dimensional vector <span class="math inline">\(x^i\)</span>. There is only one user in the simulation, and they have a latent reward function <span class="math inline">\(f\)</span> that they use to compute the latent reward of an item from the features. Here, the latent reward function is the Ackley function .</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-1"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
</div>
<div id="27f89333" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="kw">def</span> ackley(X, a<span class="op">=</span><span class="dv">20</span>, b<span class="op">=</span><span class="fl">0.2</span>, c<span class="op">=</span><span class="dv">2</span><span class="op">*</span>np.pi):</span>
<span id="cb1-5"><a href="#cb1-5"></a>    <span class="co">"""</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co">    Compute the Ackley function.</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co">    Parameters:</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co">      X: A NumPy array of shape (n, d) where each row is a d-dimensional point.</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="co">      a, b, c: Parameters of the Ackley function.</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="co">    Returns:</span></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="co">      A NumPy array of function values.</span></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co">    """</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>    X <span class="op">=</span> np.atleast_2d(X)</span>
<span id="cb1-14"><a href="#cb1-14"></a>    d <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb1-15"><a href="#cb1-15"></a>    sum_sq <span class="op">=</span> np.<span class="bu">sum</span>(X <span class="op">**</span> <span class="dv">2</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-16"><a href="#cb1-16"></a>    term1 <span class="op">=</span> <span class="op">-</span>a <span class="op">*</span> np.exp(<span class="op">-</span>b <span class="op">*</span> np.sqrt(sum_sq <span class="op">/</span> d))</span>
<span id="cb1-17"><a href="#cb1-17"></a>    term2 <span class="op">=</span> <span class="op">-</span>np.exp(np.<span class="bu">sum</span>(np.cos(c <span class="op">*</span> X), axis<span class="op">=</span><span class="dv">1</span>) <span class="op">/</span> d)</span>
<span id="cb1-18"><a href="#cb1-18"></a>    <span class="cf">return</span> term1 <span class="op">+</span> term2 <span class="op">+</span> a <span class="op">+</span> np.e</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We next define a function to visualize the surface:</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-2"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
</div>
<div id="d8e95bd4" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> LinearSegmentedColormap</span>
<span id="cb2-3"><a href="#cb2-3"></a>ccmap <span class="op">=</span> LinearSegmentedColormap.from_list(<span class="st">"ackley"</span>, [<span class="st">"#f76a05"</span>, <span class="st">"#FFF2C9"</span>])</span>
<span id="cb2-4"><a href="#cb2-4"></a>plt.rcParams.update({</span>
<span id="cb2-5"><a href="#cb2-5"></a>    <span class="st">"font.size"</span>: <span class="dv">14</span>,</span>
<span id="cb2-6"><a href="#cb2-6"></a>    <span class="st">"axes.labelsize"</span>: <span class="dv">16</span>,</span>
<span id="cb2-7"><a href="#cb2-7"></a>    <span class="st">"xtick.labelsize"</span>: <span class="dv">14</span>,</span>
<span id="cb2-8"><a href="#cb2-8"></a>    <span class="st">"ytick.labelsize"</span>: <span class="dv">14</span>,</span>
<span id="cb2-9"><a href="#cb2-9"></a>    <span class="st">"legend.fontsize"</span>: <span class="dv">14</span>,</span>
<span id="cb2-10"><a href="#cb2-10"></a>    <span class="st">"axes.titlesize"</span>: <span class="dv">16</span>,</span>
<span id="cb2-11"><a href="#cb2-11"></a>})</span>
<span id="cb2-12"><a href="#cb2-12"></a>plt.rcParams[<span class="st">'text.usetex'</span>] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-13"><a href="#cb2-13"></a></span>
<span id="cb2-14"><a href="#cb2-14"></a><span class="kw">def</span> draw_surface():</span>
<span id="cb2-15"><a href="#cb2-15"></a>    inps <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb2-16"><a href="#cb2-16"></a>    X, Y <span class="op">=</span> np.meshgrid(inps, inps)</span>
<span id="cb2-17"><a href="#cb2-17"></a>    grid <span class="op">=</span> np.column_stack([X.ravel(), Y.ravel()])</span>
<span id="cb2-18"><a href="#cb2-18"></a>    Z <span class="op">=</span> ackley(grid).reshape(X.shape)</span>
<span id="cb2-19"><a href="#cb2-19"></a>    </span>
<span id="cb2-20"><a href="#cb2-20"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">5</span>))</span>
<span id="cb2-21"><a href="#cb2-21"></a>    contour <span class="op">=</span> plt.contourf(X, Y, Z, <span class="dv">50</span>, cmap<span class="op">=</span>ccmap)</span>
<span id="cb2-22"><a href="#cb2-22"></a>    plt.contour(X, Y, Z, levels<span class="op">=</span><span class="dv">15</span>, colors<span class="op">=</span><span class="st">'black'</span>, linewidths<span class="op">=</span><span class="fl">0.5</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb2-23"><a href="#cb2-23"></a>    plt.colorbar(contour, label<span class="op">=</span><span class="vs">r'$f(x)$'</span>, ticks<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">6</span>])</span>
<span id="cb2-24"><a href="#cb2-24"></a>    plt.xlim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-25"><a href="#cb2-25"></a>    plt.ylim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-26"><a href="#cb2-26"></a>    plt.xticks([<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>])</span>
<span id="cb2-27"><a href="#cb2-27"></a>    plt.yticks([<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>])</span>
<span id="cb2-28"><a href="#cb2-28"></a>    plt.xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb2-29"><a href="#cb2-29"></a>    plt.ylabel(<span class="vs">r'$x_2$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="item-wise-model" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="item-wise-model"><span class="header-section-number">2.2.1</span> Item-wise Model</h3>
<p>One method for data collection is accept-reject sampling, where the user considers one item at a time and decides if they like it. Below is an example survey using accept-reject sampling:</p>
<iframe src="https://app.opinionx.co/5f30e903-c7b2-42e3-821a-e65271144bd9" style="width:100%; height:450px;">
</iframe>
<p>We will use a simulation to familiarize ourselves with accept-reject sampling. On the surface below, blue and red points correspond to accept or reject points.</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-3"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
</div>
<div id="4baae0a2" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>d <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>n_items <span class="op">=</span> <span class="dv">800</span></span>
<span id="cb3-3"><a href="#cb3-3"></a>items <span class="op">=</span> np.random.randn(n_items, d)<span class="op">*</span><span class="fl">0.5</span> <span class="op">+</span> np.ones((n_items, d))<span class="op">*</span><span class="fl">0.5</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>rewards <span class="op">=</span> ackley(items)</span>
<span id="cb3-5"><a href="#cb3-5"></a>y <span class="op">=</span> (rewards <span class="op">&gt;</span> rewards.mean())</span>
<span id="cb3-6"><a href="#cb3-6"></a>draw_surface()</span>
<span id="cb3-7"><a href="#cb3-7"></a>plt.scatter(items[:, <span class="dv">0</span>], items[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">'coolwarm'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb3-8"><a href="#cb3-8"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chap1_files/figure-html/cell-4-output-1.png" width="519" height="445" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The binary choice model centers around one item. The model predicts, for that item, after observing user choices in the past, whether that item will be chosen. We use binary variable <span class="math inline">\(y \in \{0, 1\}\)</span> to represent whether the user will pick that choice in the next selection phase. We denote <span class="math inline">\(P = p(y = 1)\)</span>. We can formally model <span class="math inline">\(y\)</span> as a function of the reward of the positive choice: <span class="math inline">\(y = \mathbb{I}[U&gt;0]\)</span>. We explore two cases based on the noise distribution. <span class="math inline">\(\psi\)</span> is the logistic function or the standard normal cumulative distribution function if noise follows logistic distribution and the standard normal distribution, respectively: <span class="math display">\[
p(u_{i,j} &gt; 0) = p(u_{i,j}^* + \epsilon &gt; 0) = 1 - p( \epsilon &lt; -u_{i,j}^*) = \psi(u_{i,j}^*).
\]</span></p>
<p>A generalization of accept-reject sampling is rating-scale sampling. Rating-scale sampling, such as the Likert scale, is a method in which participants rate items on a fixed-point scale (e.g., 1 to 5, “Strongly Disagree” to “Strongly Agree”) to measure levels of preference towards items <span class="citation" data-cites="harpe2015">(<a href="#ref-harpe2015" role="doc-biblioref">Harpe 2015</a>)</span>. Participants can also mark a point on a continuous rating scale to indicate their preference or attitude. Commonly used in surveys, product reviews, and psychological assessments, this method provides a more nuanced measure than discrete scales. Rating-scale sampling is simple for participants to understand and use, provides rich data on the intensity of preferences, and is flexible enough for various measurements (e.g., agreement, satisfaction). However, rating-scale sampling methods also have limitations. Ratings can be influenced by personal biases and interpretations of scales, leading to subjectivity. There is a central tendency bias, where participants may avoid extreme ratings, resulting in clustering responses around the middle. Different participants might interpret scale points differently, and fixed-point scales may not capture the full nuance of participants’ preferences or attitudes.</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-4"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
</div>
<div id="d435d041" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> LinearSegmentedColormap</span>
<span id="cb4-2"><a href="#cb4-2"></a>likert_cmap <span class="op">=</span> LinearSegmentedColormap.from_list(<span class="st">"likert_scale"</span>, [<span class="st">"red"</span>, <span class="st">"blue"</span>], N<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb4-3"><a href="#cb4-3"></a>normalized <span class="op">=</span> (rewards <span class="op">-</span> rewards.<span class="bu">min</span>()) <span class="op">/</span> (rewards.<span class="bu">max</span>() <span class="op">-</span> rewards.<span class="bu">min</span>())</span>
<span id="cb4-4"><a href="#cb4-4"></a>ratings <span class="op">=</span> np.<span class="bu">round</span>(normalized <span class="op">*</span> <span class="dv">4</span>).squeeze()</span>
<span id="cb4-5"><a href="#cb4-5"></a></span>
<span id="cb4-6"><a href="#cb4-6"></a>draw_surface()</span>
<span id="cb4-7"><a href="#cb4-7"></a>scatter <span class="op">=</span> plt.scatter(items[:, <span class="dv">0</span>], items[:, <span class="dv">1</span>], c<span class="op">=</span>ratings, cmap<span class="op">=</span>likert_cmap, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb4-8"><a href="#cb4-8"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chap1_files/figure-html/cell-5-output-1.png" width="519" height="445" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Suppose we have a single example with attributes <span class="math inline">\(z_i\)</span> and wish to know which of <span class="math inline">\(J\)</span> rating scales an individual will choose from. We can define <span class="math inline">\(J - 1\)</span> parameters, which act as thresholds on the reward computed by <span class="math inline">\(u_i = u_{i,j}^*\)</span> to classify the predicted choice between these items. For example, if there are three predefined items, we can define parameters <span class="math inline">\(a, b \in \mathbb{R}\)</span> such that <span class="math display">\[
y_i =
\begin{cases}
    1 &amp; u &lt; a \\
    2 &amp; a \le u &lt; b \\
    3 &amp; \text{else}
\end{cases}
\]</span></p>
<p>By assuming the noise distribution to be either logistic or standard normal, we have <span class="math display">\[
\begin{split}
    p(y_i = 1) &amp; = p(u &lt; a) = p(u_{i,j}^* + \epsilon &lt; a) = \psi(a-u_{i,j}^*) \\
    p(y_i = 2) &amp; = p(a \le u &lt; b) = p(a - u_{i,j}^* \le \epsilon &lt; b - u_{i,j}^*) = \psi(b-u_{i,j}^*)  - \psi(u_{i,j}^*-a) \\
    p(y_i = 3) &amp; = p(u &gt; b) = p(u_{i,j}^* + \epsilon &gt; b ) = p( \epsilon &gt; b - u_{i,j}^*) = \psi(b-u_{i,j}^*)
\end{split}
\]</span></p>
<p>Having the model, we next explore the estimation of model parameters. A common approach for parameter estimation is maximum likelihood <span class="citation" data-cites="book_estimation_casella book_estimation_bock">(<a href="#ref-book_estimation_casella" role="doc-biblioref">Casella and Berger 1990</a>; <a href="#ref-book_estimation_bock" role="doc-biblioref">Bock et al. 2015</a>)</span>. The likelihood of a model is the probability of the observed data given the model parameters; intuitively, we wish to maximize this likelihood, as that would mean that our model associates observed human preferences with high probability. Assuming our data is independent and identically distributed (iid), the likelihood over the entire dataset is the joint probability of all observed data as defined by the binary choice model with logistic noise is</p>
<p><span class="math display">\[\mathcal{L}(z, Y; \beta) = \prod_{i = 1}^J p(y = y_i | z_i; \beta) = \prod_{i = 1}^J \frac{1}{1 + \exp^{-u_{i,j}^*}}\]</span></p>
<p>This objective can be optimized with a gradient-based method, such as gradient descent <span class="citation" data-cites="gradient_descent">(<a href="#ref-gradient_descent" role="doc-biblioref">Ruder 2016</a>)</span>. Gradient descent operates by computing the gradient of the objective with respect to the parameters of the model, which provides a signal of the direction in which the parameters must move to minimize the objective. Then, SGD makes an update step by subtracting this gradient from the parameters (most often with a scale factor called a learning rate) to move the parameters in a direction that minimizes the objective. In the case of logistic and Gaussian models, SGD may yield a challenging optimization problem as its stochasticity can lead to noisy updates, for example, if certain examples or batches of examples are biased. Mitigations include batched SGD, in which multiple samples are randomly sampled from the dataset at each iteration; learning rates, which reduce the impact of noisy gradient updates, and momentum and higher-order optimizers, which reduce noise by using moving averages of gradients or provide better estimates of the best direction in which to update the gradients.</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-5"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
</div>
<div id="d1e1a957" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="im">from</span> torch.distributions <span class="im">import</span> Bernoulli</span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb5-6"><a href="#cb5-6"></a></span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="co"># Set device</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb5-9"><a href="#cb5-9"></a></span>
<span id="cb5-10"><a href="#cb5-10"></a><span class="co"># Number of users and items</span></span>
<span id="cb5-11"><a href="#cb5-11"></a>num_users <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb5-12"><a href="#cb5-12"></a>num_items <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb5-13"><a href="#cb5-13"></a></span>
<span id="cb5-14"><a href="#cb5-14"></a><span class="co"># Generate user-specific and item-specific rewards</span></span>
<span id="cb5-15"><a href="#cb5-15"></a>theta <span class="op">=</span> torch.randn(num_users, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-16"><a href="#cb5-16"></a>z <span class="op">=</span> torch.randn(num_items, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-17"><a href="#cb5-17"></a></span>
<span id="cb5-18"><a href="#cb5-18"></a><span class="co"># Generate observed choices using logistic function</span></span>
<span id="cb5-19"><a href="#cb5-19"></a>probs <span class="op">=</span> torch.sigmoid(theta[:, <span class="va">None</span>] <span class="op">-</span> z[<span class="va">None</span>, :])</span>
<span id="cb5-20"><a href="#cb5-20"></a>data <span class="op">=</span> Bernoulli(probs<span class="op">=</span>probs).sample()</span>
<span id="cb5-21"><a href="#cb5-21"></a></span>
<span id="cb5-22"><a href="#cb5-22"></a><span class="co"># Mask out a fraction of the response matrix</span></span>
<span id="cb5-23"><a href="#cb5-23"></a>mask <span class="op">=</span> torch.rand_like(data) <span class="op">&gt;</span> <span class="fl">0.2</span>  <span class="co"># 80% observed, 20% missing</span></span>
<span id="cb5-24"><a href="#cb5-24"></a>data_masked <span class="op">=</span> data.clone()</span>
<span id="cb5-25"><a href="#cb5-25"></a>data_masked[<span class="op">~</span>mask] <span class="op">=</span> <span class="bu">float</span>(<span class="st">'nan'</span>)</span>
<span id="cb5-26"><a href="#cb5-26"></a></span>
<span id="cb5-27"><a href="#cb5-27"></a><span class="co"># Initialize parameters for EM algorithm</span></span>
<span id="cb5-28"><a href="#cb5-28"></a>theta_est <span class="op">=</span> torch.randn(num_users, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-29"><a href="#cb5-29"></a>z_est <span class="op">=</span> torch.randn(num_items, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-30"><a href="#cb5-30"></a></span>
<span id="cb5-31"><a href="#cb5-31"></a><span class="co"># Optimizer</span></span>
<span id="cb5-32"><a href="#cb5-32"></a>optimizer <span class="op">=</span> optim.LBFGS([theta_est, z_est], lr<span class="op">=</span><span class="fl">0.1</span>, max_iter<span class="op">=</span><span class="dv">20</span>, history_size<span class="op">=</span><span class="dv">10</span>, line_search_fn<span class="op">=</span><span class="st">"strong_wolfe"</span>)</span>
<span id="cb5-33"><a href="#cb5-33"></a></span>
<span id="cb5-34"><a href="#cb5-34"></a><span class="kw">def</span> closure():</span>
<span id="cb5-35"><a href="#cb5-35"></a>    optimizer.zero_grad()</span>
<span id="cb5-36"><a href="#cb5-36"></a>    probs_est <span class="op">=</span> torch.sigmoid(theta_est[:, <span class="va">None</span>] <span class="op">-</span> z_est[<span class="va">None</span>, :])</span>
<span id="cb5-37"><a href="#cb5-37"></a>    loss <span class="op">=</span> <span class="op">-</span>(Bernoulli(probs<span class="op">=</span>probs_est).log_prob(data) <span class="op">*</span> mask).mean()</span>
<span id="cb5-38"><a href="#cb5-38"></a>    loss.backward()</span>
<span id="cb5-39"><a href="#cb5-39"></a>    <span class="cf">return</span> loss</span>
<span id="cb5-40"><a href="#cb5-40"></a></span>
<span id="cb5-41"><a href="#cb5-41"></a><span class="co"># EM Algorithm</span></span>
<span id="cb5-42"><a href="#cb5-42"></a>pbar <span class="op">=</span> tqdm(<span class="bu">range</span>(<span class="dv">100</span>))</span>
<span id="cb5-43"><a href="#cb5-43"></a><span class="cf">for</span> iteration <span class="kw">in</span> pbar:</span>
<span id="cb5-44"><a href="#cb5-44"></a>    <span class="cf">if</span> iteration <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb5-45"><a href="#cb5-45"></a>        previous_theta <span class="op">=</span> theta_est.clone()</span>
<span id="cb5-46"><a href="#cb5-46"></a>        previous_z <span class="op">=</span> z_est.clone()</span>
<span id="cb5-47"><a href="#cb5-47"></a>        previous_loss <span class="op">=</span> loss.clone()</span>
<span id="cb5-48"><a href="#cb5-48"></a>    </span>
<span id="cb5-49"><a href="#cb5-49"></a>    loss <span class="op">=</span> optimizer.step(closure)</span>
<span id="cb5-50"><a href="#cb5-50"></a>    </span>
<span id="cb5-51"><a href="#cb5-51"></a>    <span class="cf">if</span> iteration <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb5-52"><a href="#cb5-52"></a>        d_loss <span class="op">=</span> (previous_loss <span class="op">-</span> loss).item()</span>
<span id="cb5-53"><a href="#cb5-53"></a>        d_theta <span class="op">=</span> torch.norm(previous_theta <span class="op">-</span> theta_est, p<span class="op">=</span><span class="dv">2</span>).item()</span>
<span id="cb5-54"><a href="#cb5-54"></a>        d_z <span class="op">=</span> torch.norm(previous_z <span class="op">-</span> z_est, p<span class="op">=</span><span class="dv">2</span>).item()</span>
<span id="cb5-55"><a href="#cb5-55"></a>        grad_norm <span class="op">=</span> torch.norm(optimizer.param_groups[<span class="dv">0</span>][<span class="st">"params"</span>][<span class="dv">0</span>].grad, p<span class="op">=</span><span class="dv">2</span>).item()</span>
<span id="cb5-56"><a href="#cb5-56"></a>        grad_norm <span class="op">+=</span> torch.norm(optimizer.param_groups[<span class="dv">0</span>][<span class="st">"params"</span>][<span class="dv">1</span>].grad, p<span class="op">=</span><span class="dv">2</span>).item()</span>
<span id="cb5-57"><a href="#cb5-57"></a>        pbar.set_postfix({<span class="st">"grad_norm"</span>: grad_norm, <span class="st">"d_theta"</span>: d_theta, <span class="st">"d_z"</span>: d_z, <span class="st">"d_loss"</span>: d_loss})</span>
<span id="cb5-58"><a href="#cb5-58"></a>        <span class="cf">if</span> d_loss <span class="op">&lt;</span> <span class="fl">1e-5</span> <span class="kw">and</span> d_theta <span class="op">&lt;</span> <span class="fl">1e-5</span> <span class="kw">and</span> d_z <span class="op">&lt;</span> <span class="fl">1e-5</span> <span class="kw">and</span> grad_norm <span class="op">&lt;</span> <span class="fl">1e-5</span>:</span>
<span id="cb5-59"><a href="#cb5-59"></a>            <span class="cf">break</span></span>
<span id="cb5-60"><a href="#cb5-60"></a></span>
<span id="cb5-61"><a href="#cb5-61"></a><span class="co"># Compute AUC ROC on observed and inferred data</span></span>
<span id="cb5-62"><a href="#cb5-62"></a><span class="im">from</span> torchmetrics <span class="im">import</span> AUROC</span>
<span id="cb5-63"><a href="#cb5-63"></a>auroc <span class="op">=</span> AUROC(task<span class="op">=</span><span class="st">"binary"</span>)</span>
<span id="cb5-64"><a href="#cb5-64"></a>probs_final <span class="op">=</span> torch.sigmoid(theta_est[:, <span class="va">None</span>] <span class="op">-</span> z_est[<span class="va">None</span>, :])</span>
<span id="cb5-65"><a href="#cb5-65"></a>train_probs <span class="op">=</span> probs_final[mask]</span>
<span id="cb5-66"><a href="#cb5-66"></a>test_probs <span class="op">=</span> probs_final[<span class="op">~</span>mask]</span>
<span id="cb5-67"><a href="#cb5-67"></a>train_labels <span class="op">=</span> data[mask]</span>
<span id="cb5-68"><a href="#cb5-68"></a>test_labels <span class="op">=</span> data[<span class="op">~</span>mask]</span>
<span id="cb5-69"><a href="#cb5-69"></a>auc_train <span class="op">=</span> auroc(train_probs, train_labels)</span>
<span id="cb5-70"><a href="#cb5-70"></a>auc_test <span class="op">=</span> auroc(test_probs, test_labels)</span>
<span id="cb5-71"><a href="#cb5-71"></a><span class="bu">print</span>(<span class="ss">f"train auc: </span><span class="sc">{</span>auc_train<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-72"><a href="#cb5-72"></a><span class="bu">print</span>(<span class="ss">f"test auc: </span><span class="sc">{</span>auc_test<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train auc: 0.7983196973800659
test auc: 0.7844991683959961</code></pre>
</div>
</div>
</section>
<section id="pairwise-model" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="pairwise-model"><span class="header-section-number">2.2.2</span> Pairwise Model</h3>
<p>In <em>pairwise sampling</em>, participants compare two items to determine which is preferred. One of the major advantages of this method is the low cognitive demand for raters. Its disadvantage is the limited amount of information content elicited by a sample. Below is a survey based on pairwise sampling:</p>
<iframe src="https://app.opinionx.co/6bef4ca1-82f5-4c1d-8c5a-2274509f22e2" style="width:100%; height:450px;">
</iframe>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-6"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
</div>
<div id="8ad40066" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>n_pairs <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>pair_indices <span class="op">=</span> np.random.randint(<span class="dv">0</span>, n_items, size<span class="op">=</span>(n_pairs, <span class="dv">2</span>))</span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="co"># Exclude pairs where both indices are the same</span></span>
<span id="cb7-4"><a href="#cb7-4"></a>mask <span class="op">=</span> pair_indices[:, <span class="dv">0</span>] <span class="op">!=</span> pair_indices[:, <span class="dv">1</span>]</span>
<span id="cb7-5"><a href="#cb7-5"></a>pair_indices <span class="op">=</span> pair_indices[mask]</span>
<span id="cb7-6"><a href="#cb7-6"></a></span>
<span id="cb7-7"><a href="#cb7-7"></a>scores <span class="op">=</span> np.zeros(n_items, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb7-8"><a href="#cb7-8"></a>wins <span class="op">=</span> rewards[pair_indices[:, <span class="dv">0</span>]] <span class="op">&gt;</span> rewards[pair_indices[:, <span class="dv">1</span>]]</span>
<span id="cb7-9"><a href="#cb7-9"></a></span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="co"># For pairs where the first item wins:</span></span>
<span id="cb7-11"><a href="#cb7-11"></a><span class="co">#   - Increase score for the first item by 1</span></span>
<span id="cb7-12"><a href="#cb7-12"></a><span class="co">#   - Decrease score for the second item by 1</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>np.add.at(scores, pair_indices[wins, <span class="dv">0</span>], <span class="dv">1</span>)</span>
<span id="cb7-14"><a href="#cb7-14"></a>np.add.at(scores, pair_indices[wins, <span class="dv">1</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb7-15"><a href="#cb7-15"></a></span>
<span id="cb7-16"><a href="#cb7-16"></a><span class="co"># For pairs where the second item wins or it's a tie:</span></span>
<span id="cb7-17"><a href="#cb7-17"></a><span class="co">#   - Decrease score for the first item by 1</span></span>
<span id="cb7-18"><a href="#cb7-18"></a><span class="co">#   - Increase score for the second item by 1</span></span>
<span id="cb7-19"><a href="#cb7-19"></a>np.add.at(scores, pair_indices[<span class="op">~</span>wins, <span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb7-20"><a href="#cb7-20"></a>np.add.at(scores, pair_indices[<span class="op">~</span>wins, <span class="dv">1</span>], <span class="dv">1</span>)</span>
<span id="cb7-21"><a href="#cb7-21"></a></span>
<span id="cb7-22"><a href="#cb7-22"></a><span class="co"># Determine preferred and non-preferred items based on scores</span></span>
<span id="cb7-23"><a href="#cb7-23"></a>preferred <span class="op">=</span> scores <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb7-24"><a href="#cb7-24"></a>non_preferred <span class="op">=</span> scores <span class="op">&lt;</span> <span class="dv">0</span></span>
<span id="cb7-25"><a href="#cb7-25"></a></span>
<span id="cb7-26"><a href="#cb7-26"></a>draw_surface()</span>
<span id="cb7-27"><a href="#cb7-27"></a>plt.scatter(items[preferred, <span class="dv">0</span>], items[preferred, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Preferred'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb7-28"><a href="#cb7-28"></a>plt.scatter(items[non_preferred, <span class="dv">0</span>], items[non_preferred, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'purple'</span>, label<span class="op">=</span><span class="st">'Non-preferred'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb7-29"><a href="#cb7-29"></a>plt.legend()</span>
<span id="cb7-30"><a href="#cb7-30"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chap1_files/figure-html/cell-7-output-1.png" width="519" height="445" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The Bradley-Terry model compares the reward of choice over all others <span class="citation" data-cites="bradley-terry-model">(<a href="#ref-bradley-terry-model" role="doc-biblioref">Bradley and Terry 1952</a>)</span> in the set of <span class="math inline">\(J\)</span> choices <span class="math inline">\(i \in \{1, 2, \dots, J\}\)</span>. Each choice can also have its unique random noise variable representing the unobserved factor. However, we can also choose to have all choices’ unobserved factors follow the same distribution (e.g., independent and identically distributed, IID). The noise is represented as an extreme value distribution, although we can choose alternatives such as a multivariate Gaussian distribution: <span class="math inline">\(\epsilon \sim \mathcal{N}(0, \Sigma)\)</span>. If <span class="math inline">\(\Sigma\)</span> is not a diagonal matrix, we effectively model correlations in the noise across choices, enabling us to avoid the IID assumption. In the case of the extreme value distribution, we model the probability of a user preferring choice <span class="math inline">\(i\)</span>, which we denote as <span class="math inline">\(P_i = Z^{-1}\exp(u_{i,j}^*)\)</span> where <span class="math inline">\(Z = \sum_{j = 1}^{J} \exp(u_{i,j}^*)\)</span>.</p>
<p>We can model an open-ended ranking of the available items with the Plackett-Luce model, in which we jointly model the full sequence of choice ordering <span class="citation" data-cites="plackett_luce">(<a href="#ref-plackett_luce" role="doc-biblioref">Plackett 1975</a>)</span>. The general form models the joint distribution as the product of conditional probabilities, where each is conditioned on the preceding ranking terms. Given an ordering of <span class="math inline">\(J\)</span> choices <span class="math inline">\(\{y_1, \dots, y_J\}\)</span>, we factorize the joint probability into conditionals. Each conditional follows the Bradley-Terry model: <span class="math display">\[
p(y_1, \dots, y_J) = p(y_1) p(y_2 | y_1) ... p(y_J | y_{1:{J - 1}}) = \prod_{i = 1}^J \frac{\exp(u_{i,j}^*)}{\sum_{j \ge i} \exp(u_{i,j}^*)}
\]</span></p>
<p>Pairwise sampling has proven useful in aligning large language models (LLM) with human preference. An LLM, such as GPT-4, Llama 3.2, and BERT, typically refers to a large and pre-trained neural network that serves as the basis for various downstream tasks. They are pre-trained on a massive corpus of text data, learning to understand language and context. They are capable of multiple language-related tasks such as text classification, language generation, and question answering. A LLM should be aligned to respond correctly based on human preferences. A promising approach is to train LLMs using reinforcement learning (RL) with the reward model (RM) learned from human preference data, providing a mechanism to score the quality of the generated text. This approach, known as RL from human feedback (RLHF), leverages human feedback to guide model training, allowing LLMs to better align with human expectations while continuously improving performance.</p>
<p>We discuss the reward model used in the Llama2 model. The Llama2 RM <span class="citation" data-cites="2307.09288">(<a href="#ref-2307.09288" role="doc-biblioref">Touvron et al. 2023</a>)</span> is initialized from the pretrained Llama2 LLM. In the LLM, the last layer is a mapping <span class="math inline">\(L: \mathbb{R}^D \rightarrow \mathbb{R}^V\)</span>, where <span class="math inline">\(D\)</span> is the embedding dimension from the transformer decoder stack and <span class="math inline">\(V\)</span> is the vocabulary size. To get the RM, we replace that last layer with a randomly initialized scalar head that maps <span class="math inline">\(L: \mathbb{R}^D \rightarrow \mathbb{R}^1\)</span>. It’s important to initialize the RM from the LLM it’s meant to evaluate. The RM will have the same “knowledge” as the LLM. This is particularly useful for evaluation objectives such as “Does the LLM know when it doesn’t know?”. However, in cases where the RM is simply evaluating helpfulness or factuality, it may be helpful to have the RM know more. In addition, the RM is on distribution for the LLM - it is initialized in a way where it semantically understands the LLM’s outputs. An RM is trained with paired preferences (prompt history, accepted response, rejected response). Prompt history is a multiturn history of user prompts and model generations; the accepted response is the preferred final model generation by an annotator, and the rejected response is the unpreferred response. The RM is trained with maximum likelihood under the Bradley-Terry model with an optional margin term m(r):</p>
<p><span class="math display">\[p(y_c \succ y_r | x) = \sigma(r_\theta(x,y_c) - r_\theta(x,y_r) - m(r))\]</span></p>
<p>The margin term increases the distance in scores specifically for preference pairs annotators rate as easier to separate. Margins were designed primarily based on the sigmoid function, which is used to normalize the raw reward model score flattens out beyond the range of <span class="math inline">\([-4, 4]\)</span>. Thus, the maximum possible margin is eight. A small regularization term is often added to center the score distribution on 0. We consider two variants of preference rating-based margin. When the preference rating-based margin is small, outcomes are rated as “Significantly Better” (1), “Better” (2 out of 3), and “Slightly Better” (1 out of 3), and “Negligibly Better or Unsure” (0 out of 3). In contrast, when the margin is large, outcomes are rated as “Significantly Better” (3), “Better” (2), and “Slightly Better” (1), and “Negligibly Better or Unsure” (0 out of 3).</p>
</section>
<section id="list-wise-model" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="list-wise-model"><span class="header-section-number">2.2.3</span> List-wise Model</h3>
<p><em>Multiple-choice sampling</em> involves participants selecting one item from a set of alternatives. Multiple-choice sampling is simple for participants to understand and reflect on realistic decision-making scenarios where individuals choose one item from many. It is beneficial in complex choice scenarios, such as modes of transportation, where choices are not independent <span class="citation" data-cites="bolt2009">(<a href="#ref-bolt2009" role="doc-biblioref">Bolt and Wollack 2009</a>)</span>. Multiple-choice sampling often relies on simplistic assumptions such as the independence of irrelevant alternatives (IIA), which may not always hold true. This method may also fail to capture the variation in preferences among different individuals, as it typically records only the most preferred choice without accounting for the relative importance of other items. In <em>rank-order sampling</em>, participants rank items from most to least preferred. Used in voting, market research, and psychology, it provides rich preference data but is more complex and cognitively demanding than pairwise comparisons, especially for large item sets. Participants may also rank inconsistently <span class="citation" data-cites="ragain2019">(<a href="#ref-ragain2019" role="doc-biblioref">Ragain and Ugander 2019</a>)</span>. <em>In Best-worst scaling</em> (BWS), participants are presented with items and asked to identify the most and least preferred items. The primary objective of BWS is to discern the relative importance or preference of items, making it widely applicable in various fields such as market research, health economics, and social sciences <span class="citation" data-cites="campbell2015">(<a href="#ref-campbell2015" role="doc-biblioref">Campbell and Erdem 2015</a>)</span>. BWS provides rich data on the relative importance of items, helps clarify preferences, reduces biases found in traditional rating scales, and results in rewards that are easy to interpret. However, BWS also has limitations, including potential scale interpretation differences among participants and design challenges to avoid biases, such as the order effect or the context in which items are presented.</p>
</section>
</section>
<section id="function-class" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="function-class"><span class="header-section-number">2.3</span> The Utility Function Class</h2>
<section id="parametric-and-nonparametric-function-class" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="parametric-and-nonparametric-function-class"><span class="header-section-number">2.3.1</span> Parametric and Nonparametric Function Class</h3>
<p>The reward of the item can take parametric form, such as <span class="math inline">\(z_j = f_{\theta}(x_j)\)</span>. It can also take the nonparametric form, which is commonly used in the ideal point model, where the reward of an item <span class="math inline">\(j\)</span> is calculated by the distance from the item to the human in some embedding space<span class="citation" data-cites="huber1976ideal">(<a href="#ref-huber1976ideal" role="doc-biblioref">Huber 1976</a>)</span>. Given vector representation <span class="math inline">\(e_i\)</span> of choice <span class="math inline">\(i\)</span> and a vector <span class="math inline">\(v_n\)</span> representing an individual <span class="math inline">\(n\)</span>, we can use a distance function <span class="math inline">\(K\)</span> to model a stochastic reward function with the unobserved factors following a specified distribution: <span class="math inline">\(u_{n, i} = K(e_i, v_n) + \epsilon_{n, i}\)</span>. The intuition is that vectors exist in a shared <span class="math inline">\(n\)</span>-dimensional space, and as such, we can use geometry to match choices whose representations are closest to that of a given individual <span class="citation" data-cites="ideal_point tatli2022distancepreferences">(<a href="#ref-ideal_point" role="doc-biblioref">Jamieson and Nowak 2011</a>; <a href="#ref-tatli2022distancepreferences" role="doc-biblioref">Tatli, Nowak, and Vinayak 2022</a>)</span> when equipped with a distance metric. Certain distance metrics, such as Euclidian distance or inner product, can easily be biased by the scale of vectors. A distance measure such as cosine similarity, which compensates for scale by normalizing the inner product of two vectors by the product of their magnitudes, can mitigate this bias yet may discard valuable information encoded by the length of the vectors. Beyond the distance metric alone, this model places a strong inductive bias that the individual and choice representations share a common embedding space. In some contexts, this can be a robust bias to add to the model <span class="citation" data-cites="idealpoints">(<a href="#ref-idealpoints" role="doc-biblioref">Greiner 2005</a>)</span>, but it is a key factor one must consider before employing such a model, and it is a key design choice for modeling.</p>
</section>
<section id="unimodal-and-multimodal-function-class" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="unimodal-and-multimodal-function-class"><span class="header-section-number">2.3.2</span> Unimodal and Multimodal Function Class</h3>
<p>So far, we have considered learning from data from one person with a particular set of preferences or a group with similar preferences, but this is not always the case. Consider a scenario where a user turns left at an intersection <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span>. What would they do if they saw a car speeding down the road approaching them? Following a timid driving pattern, some vehicles would stop to let the other car go, preventing a collision. Other vehicles would be more aggressive and try to make the turn before colliding with the oncoming vehicle. Given the data of one of these driving patterns, the model can make an appropriate decision. However, what if the model was given data from both aggressive and timid drivers and does not know which data corresponds to which type of driver? A naive preference learning approach would result in a model trying to find a policy close enough to both driving patterns. The group label is often unobserved because it is expensive to obtain or a data point cannot be cleanly separated into any group (e.g., a more timid driver can be aggressive when they are in a hurry).</p>
<p><span class="citation" data-cites="myers2022learning">Myers et al. (<a href="#ref-myers2022learning" role="doc-biblioref">2022</a>)</span> formulates this problem as learning a mixture of <span class="math inline">\(M\)</span> linear reward functions on the embedding space, where <span class="math inline">\(M\)</span> is given. The reward of item <span class="math inline">\(j\)</span> given by the expert <span class="math inline">\(i\)</span> is given by: <span class="math inline">\(f_i(e_j) = w^\top_i e_j,\)</span> where <span class="math inline">\(w_m\)</span> is a vector of parameters corresponding to the <span class="math inline">\(m\)</span>-th expert’s preferences. An unknown distribution over the reward parameters exists, and we can represent this distribution with convex mixing coefficients <span class="math inline">\(\alpha = [\alpha_1, ..., \alpha_M]\)</span>. Consider a robot that performs the following trajectories and asks a user to rank all the trajectories. The robot will be given back a set of trajectory rankings from M humans, and the objective is to learn the underlying reward function. Given the ranking <span class="math inline">\((j_1 \succ ... \succ j_K | m)\)</span> of expert <span class="math inline">\(m\)</span> and define <span class="math inline">\(\theta = \{w_{1:M}, \alpha_{1:M}\}\)</span>, the probability of item <span class="math inline">\(j\)</span> being preferred by <span class="math inline">\(m\)</span> over all other alternatives is</p>
<p><span class="math display">\[p(j_1 \succ ... \succ j_K | \theta) = \sum_{i = 1}^M \alpha_i \prod_{j = 1}^K  p_{ij}\]</span></p>
<p>Then the parameters posterior is <span class="math inline">\(p(\theta | Q_{1:T}, x_{1:T}) \propto p(\theta) \prod_t p(x_t | Q_{\leq t}, \theta) = p(\theta) \prod_t p(x_t | \theta, Q_t)\)</span>. The first proportionality is from the Bayes rule and the assumption that the queries at timestamp <span class="math inline">\(t\)</span> are conditionally independent of the parameters given history. This assumption is reasonable because the previous queries &amp; rankings ideally give all the information to inform the choice of the next set. The last proportionality term comes from the assumption that the ranked queries are conditionally independent given the parameters. The prior distribution is dependent on the use case. For example, in the user studies conducted by the authors to verify this method, they use a standard Gaussian for the reward weights and the mixing coefficients to be uniform on a <span class="math inline">\(M - 1\)</span> simplex to ensure that they add up to 1. Then, we can use maximum likelihood estimation to compute the parameters with the simplified posterior.</p>
<p>Another example setting multimodal preference is negotiations <span class="citation" data-cites="kwon2021targeted">(<a href="#ref-kwon2021targeted" role="doc-biblioref">Kwon et al. 2021</a>)</span>. Let’s say there are some shared items and two people with different utilities and desires for items, where each person only knows their utility. In a specific case of <span class="quarto-unresolved-ref">?fig-negotiation</span>, Bob as a proposing agent and Alice as a controlled agent who has many different ways of responding to Bob’s proposals. Different methods can be used to design Alice as an AI agent. The first idea is reinforcement learning, where multiple rounds of negotiations are done, the model simulates game theory and sees how Bob reacts. Authors of this setting <span class="citation" data-cites="kwon2021targeted">(<a href="#ref-kwon2021targeted" role="doc-biblioref">Kwon et al. 2021</a>)</span> show that over time the model learns to ask for the same thing over and over again, as Alice is not trained to be human-like or negotiable, and just tries to maximize Alice’s utility. The second approach is supervised learning, where the model can be trained on some dataset, learning the history of negotiations. This results in Alice being very agreeable, which demonstrates two polar results of the two approaches, and it would be ideal to find a middle ground and combine both of them. The authors proposed the Targeted acquisition approach, which is based on active learning ideas. The model asks diverse questions at different cases and stages of negotiations like humans, determining which questions are more valuable to be asked throughout learning. Such an approach ended up in more fair and optimal results than supervised or reinforcement learning <span class="citation" data-cites="kwon2021targeted">(<a href="#ref-kwon2021targeted" role="doc-biblioref">Kwon et al. 2021</a>)</span>.</p>
</section>
<section id="single-objective-and-multi-objective-utility" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="single-objective-and-multi-objective-utility"><span class="header-section-number">2.3.3</span> Single Objective and Multi-Objective Utility</h3>
<p>The industry has centered around optimizing for two primary reward signals: helpfulness and harmlessness (safety). There are also other axes, such as factuality, reasoning, tool use, code, and multilingualism, but these are out of scope for us. The Llama2 paper collected preference data from humans for each quality, with separate guidelines. This presents a challenge for co-optimizing the final LLM towards both goals. Two main approaches can be taken for RLHF in this context. Train a unified reward model that integrates both datasets or train two separate reward models, one for each quality, and optimize the LLM toward both. Option 1 is difficult because of the tension between helpfulness and harmlessness. They trade off against each other, confusing an RM trained in both. The chosen solution was item 2, where two RMs are used to train the LLM piecewise. The helpfulness RM is used as the primary optimization term, while the harmlessness RM acts as a penalty term, driving the behavior of the LLM away from unsafe territory only when the LLM veers beyond a certain threshold. This is formalized as follows, where <span class="math inline">\(R_s\)</span>, <span class="math inline">\(R_h\)</span>, and <span class="math inline">\(R_c\)</span> are the safety, helpfulness, and combined reward, respectively. <span class="math inline">\(g\)</span> and <span class="math inline">\(p\)</span> are the model generation and the user prompt:</p>
<p><span class="math display">\[
\begin{aligned}
    R_c(g \mid p) =
    \begin{cases}
        R_s(g \mid p) &amp; \text{if } \text{is\_safety}(p) \text{ or } R_s(g \mid p) &lt; 0.15 \\
        R_h(g \mid p) &amp; \text{otherwise}
    \end{cases}
\end{aligned}
\]</span></p>
</section>
<section id="pretraining" class="level3" data-number="2.3.4">
<h3 data-number="2.3.4" class="anchored" data-anchor-id="pretraining"><span class="header-section-number">2.3.4</span> Pretraining</h3>
<!--
In the context of robotics, a very compelling answer is the *cost of data collection*. In a hypothetical world where we have a vast number of expert demonstrations of robots accomplishing many diverse tasks, we don't necessarily need to worry about learning from trials or from humans. We could simply learn a competent imitation agent to perform any task. Natural Language Processing could be seen as living in this world because internet-scale data is available. Robots, however, are expensive, so people generally don't have access to them and, therefore cannot use them to produce information to imitate. Similarly, human time is expensive, so even for large organizations with access to many robots, it's still hard to collect a lot of expert demonstrations. The most extensive available collection of robotics datasets today is the Open X-Embodiment [@padalkar2023open], which consists of around 1M episodes from more than 300 different scenes. Even such large datasets are not enough to learn generally capable robotic policies from imitation learning alone.
-->
<p>RL often stumbles when it comes to devising reward functions aligning with human intentions. Preference-based RL aims to solve this by learning from human feedback, but this often demands a <em>highly impractical number of queries</em> or leads to oversimplified reward functions that don’t hold up in real-world tasks. As discussed in the previous section, one may apply meta-learning so that the RL agent can adapt to new tasks with fewer human queries to address the impractical requirement of human queries. <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> proposes pre-training models on previous tasks with the meta-learning method MAML <span class="citation" data-cites="finn2017model">(<a href="#ref-finn2017model" role="doc-biblioref">Finn, Abbeel, and Levine 2017</a>)</span>, and then the meta-trained model can adapt to new tasks with fewer queries. We consider settings where a state is denoted as <span class="math inline">\(s\in S\)</span>, and action is denoted as <span class="math inline">\(a\in A\)</span>, for state space <span class="math inline">\(S\)</span> and action space <span class="math inline">\(A\)</span>. The reward function <span class="math inline">\(r: S\times A \to \mathbb{R}\)</span> is unknown and needs to be learned from eliciting human preferences. There are multiple tasks, each with its own reward function and transition probabilities. The reward model is parameterized by <span class="math inline">\(\psi\)</span>. We denote <span class="math inline">\(\hat{r}_\psi(s, a)\)</span> to be a learned estimate of an unknown ground-truth reward function <span class="math inline">\(r(s, a)\)</span>, parameterized by <span class="math inline">\(\psi\)</span>. Accordingly, a reward model determines an RL policy <span class="math inline">\(\phi\)</span> by maximizing the accumulated rewards. The preferences is learned via pair. For each pre-training task, there is a dataset <span class="math inline">\(D\)</span> consists of binary preference between pair of trajectory. Bradley-Terry model is used to predict the preferred trajectory.</p>
<p>To efficiently approximate the reward function <span class="math inline">\(r_\text{new}\)</span> for a new task with minimal queries, <span class="citation" data-cites="hejna2023few">Hejna III and Sadigh (<a href="#ref-hejna2023few" role="doc-biblioref">2023</a>)</span> utilizes a pre-trained reward function <span class="math inline">\(\hat{r}_\psi\)</span> that can be quickly fine-tuned using just a few preference comparisons by leveraging the common structure across tasks by pre-training on data from prior tasks. Although any meta-learning method is compatible, <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> opts for Model Agnostic Meta-Learning (MAML) due to its simplicity. With the aforementioned pre-training with meta learning, the meta-learned reward model can then be used for few-shot preference-based RL during an online adaptation phase. Given a pre-trained reward model <span class="math inline">\(\psi\)</span>, the the active few-shot adaption iterates between finding informative pair of trajectory to query human and update reward model and corresponding policy with new data. Informative pair is selected using the disagreement of an ensemble of reward functions over the preference predictors. Specifically, comparisons that maximize <span class="math inline">\(\mathbb{V}(p(e_j \succ e_{j'}))\)</span> are selected each time feedback is collected.</p>
<p>The experiment tests the proposed method on the Meta-World benchmark <span class="citation" data-cites="yu2020meta">(<a href="#ref-yu2020meta" role="doc-biblioref">Yu et al. 2020</a>)</span>. Three baselines compared with the proposed method are (1) Soft-Actor Critic (SAC) trained from ground truth rewards, representing performance upper bound, PEBBLE <span class="citation" data-cites="lee2021pebble">(<a href="#ref-lee2021pebble" role="doc-biblioref">Lee, Smith, and Abbeel 2021</a>)</span>, which does not use information from prior tasks, and (3) Init, which initializes the reward model with the pretrained weights from meta learning but instead of adapting the reward model to the new task, it performs standard updates as in PEBBLE. The results show that the proposed method outperforms all of the baseline methods. There are still some drawbacks. For example, many of the queries the model picks to elicit human preference are almost identical. Moreover, despite the improved query complexity, an impractical number of queries still need to be made. In addition, it is mentioned in the paper that the proposed method may be even worse than training from scratch if the new task is too out-of-distribution. Designing a method that automatically balances between using the prior information or training from scratch is an important future direction.</p>
<p><span class="citation" data-cites="zhou2019watch">Zhou et al. (<a href="#ref-zhou2019watch" role="doc-biblioref">2019</a>)</span> studies a related problem by asking the question, “How can we efficiently learn both from expert demonstrations and from trials where we only get binary feedback from a human?” This paper seeks to learn new tasks with the following general problem setting: We only get one expert demonstration of the target task; after seeing the expert demonstration, robots try to solve the task 1 or more times; then the user (or some pre-defined reward function) annotates each trial as a success/failure; the agent learns from both the demos and the annotated trials to perform well on the target task. A task <span class="math inline">\(i\)</span> is described by the tuple <span class="math inline">\(\{S, A, r_i, P_i\}\)</span>. <span class="math inline">\(S\)</span> and <span class="math inline">\(A\)</span> represents all possible states and action, respectively. <span class="math inline">\(r_i\)</span> is the reward function <span class="math inline">\(r_i : S \times A \to \mathbb{R}\)</span>, and <span class="math inline">\(P_i\)</span> is the transition dynamics function. <span class="math inline">\(S\)</span> and <span class="math inline">\(A\)</span> are shared across tasks. Learning occurs in 3 phases. During the watch phase, we give the agent <span class="math inline">\(K=1\)</span> demonstrations of the target tasks and all demonstrations are successful. In the Try phase, we use the agent learned during the Watch phase to attempt the task for <span class="math inline">\(L\)</span> trials. After the agent completes the trials, humans (or pre-programmed reward functions) provide one binary reward for each trial, indicating whether the trial was successful. The expected output of this phase is <span class="math inline">\(L\)</span> trajectories and corresponding feedback. After completing the trials, the agent must learn from both the original expert demonstrations and the trials to solve the target task.</p>
<p>First, we are given a dataset of expert demonstrations containing multiple demos for each task and the dataset contains hundreds of tasks. Importantly, no online interaction is needed for training, and this method trains only with supervised learning. This section describes how this paper trains an agent from the given expert demonstrations, and how to incorporate the trials and human feedback into the loop. What we want to obtain out of the Watch phase is a policy conditioned on a set of expert demonstrations via meta-imitation learning. Given the demonstrations <span class="math inline">\(\{d_{i,k}\}\)</span> for task <span class="math inline">\(i\)</span>, we sample another different demonstration coming from the same task <span class="math inline">\(d_i^{\text{test}}\)</span>, where <span class="math inline">\(d_i^{\text{test}}\)</span> is an example of optimal behavior given the demonstrations. The policy is obtained by imitating actions taken on <span class="math inline">\(d_i^{\text{test}}\)</span> via maximum likelihood:</p>
<p><span class="math display">\[\mathcal{L}^\text{watch}(\theta, \mathcal{D}_i^*) = \mathbb{E}_{\{d_{i,k}\} \sim \mathcal{D}_i^*} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^*  \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \log \pi_\theta^{\text{watch}} (a_t | s_t, \{d_{i,k}\})\]</span></p>
<p>This corresponds to imitation learning by minimizing the negative log-likelihood of the test trajectory actions, conditioning the policy on the entire demo set. However, how is the conditioning on the demo set achieved? In addition to using features obtained from the images of the current state, the architecture uses features from frames sampled (in order) from the demonstration episodes, which are concatenated together. On the Try phase when the agent is given a set of demonstrations <span class="math inline">\(\{d_{i,k}\}\)</span>, we deploy the policy <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{d_{i,k}\})\)</span> to collect <span class="math inline">\(L\)</span> trials. There is no training involved in the Try phase; we simply condition the policy on the given demonstrations. During the Watch phase, the objective was to train a policy conditioned on demonstrations <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{d_{i,k}\})\)</span>. The authors of Watch, Try, Learn uses a similar strategy as the Watch phase for the Learn phase. We now want to train a policy that is conditioned on the demonstrations, as well as the trials and binary feedback. We want to learn <span class="math inline">\(\pi_\phi^{\text{watch}}(a | s, \{d_{i,k}\}, \{\mathbf{\tau}_{i, l}\})\)</span>. To train the policy, we again use meta-imitation learning, where we additionally sample yet another trajectory from the same task. Concretely, we train policy parameters <span class="math inline">\(\phi\)</span> to minimize the following loss: <span class="math display">\[\mathcal{L}^{\text{learn}}(\phi, \mathcal{D}_i, \mathcal{D}_i^*) = \mathbb{E}_{(\{d_{i,k}\}, \{\mathbf{\tau}_{i,l}\}) \sim \mathcal{D}_i} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^* \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[- \log \pi_\theta^{\text{learn}} (a_t | s_t, \{d_{i,k}\}, \{\tau_{i,l}\}) \big]\]</span></p>
<p>Three baselines are considered: (1) behavior cloning is simple imitation learning based on maximum log-likelihood training using data from all tasks, (2) meta-imitation learning corresponds to simply running the policy from the Watch step without using any trial data. We only condition on the set of expert demonstrations, but no online trials, and (3) behavior cloning + SAC pretrains a policy with behavior cloning on all data, and follow that with RL fine-tuning for the specific target task, using the maximum-entropy algorithm SAC <span class="citation" data-cites="haarnoja2018soft">(<a href="#ref-haarnoja2018soft" role="doc-biblioref">Haarnoja et al. 2018</a>)</span>. The proposed approach significantly outperforms baselines on every task family: it is far superior to behavior cloning and it significantly surpasses Meta-Imitation Learning on 3 out of 4 task families.</p>
</section>
<section id="others-consideration" class="level3" data-number="2.3.5">
<h3 data-number="2.3.5" class="anchored" data-anchor-id="others-consideration"><span class="header-section-number">2.3.5</span> Others Consideration</h3>
<p>One key challenge is managing the bias and variance trade-off. Bias refers to assumptions made during model design and training that can skew predictions. For example, in Ideal Point Models, we make the assumption that the representations we use for individuals and choices are aligned in the embedding space and that this representation is sufficient to capture human preferences using distance metrics. However, there are myriad cases in which this may break down, for example, if the two sets of vectors follow different distributions, each with their own unique biases. If the representations do not come from the same domain, one may have little visibility into how a distance metric computes the final reward value for a choice for a given individual. Some ways to mitigate bias in human preference models include increasing the number of parameters in a model (allowing for better learning of patterns in the data) or removing inductive biases based on our assumptions of the underlying data. On the other hand, variance refers to the model’s sensitivity to small changes in the input, which leads to significant changes in the output. This phenomenon is often termed ‘overfitting’ or ‘overparameterization.’ This behavior can occur in models that have many parameters and learn correlations in the data that do not contribute to learning human preferences but are artifacts of noise in the dataset that one should ultimately ignore. One can address variance in models by reducing the number of parameters or incorporating biases in the model based on factors we can assume about the data.</p>
<p>Another important consideration unique to human preference models is that we wish to model individual preferences, and we may choose to do so at arbitrary granularity. For example, we can fit models to a specific individual or even multiple models for an individual, each for different purposes or contexts. On the other end of the spectrum, we may create a model to capture human preferences across large populations or the world. Individual models may prove to be more powerful, as they do not need to generalize across multiple individuals and can dedicate all of their parameters to learning the preferences of a single user. In the context of human behavior, this can be a significant advantage as any two individuals can be arbitrarily different or even opposite in their preferences. On the other hand, models that fit only one person can tremendously overfit the training distribution and capture noise in the data, which is not truly representative of human preferences. On the end of the spectrum, models fit to the entire world may be inadequate to model human preferences for arbitrary individuals, especially those whose data it has not been fit to. As such, models may underfit the given training distribution. These models aim to generalize to many people but may fail to capture the nuances of individual preferences, especially for those whose data is not represented in the training set. As a result, they may not perform well for arbitrary individuals within the target population. Choosing the appropriate scope for a model is crucial. It must balance the trade-off between overfitting to noise in highly granular models and underfitting in broader models that may not capture individual nuances.</p>
<p>When training or using a reward model, LLM Distribution Shift is an important factor to consider. With each finetune of the LLM, the RM should be updated through a collection of fresh human preferences using generations from the new LLM. This ensures that the RM stays aligned with the current distribution of the LLM and avoids drifting off-distribution. In addition, RM and LLM are coupled: An RM is generally optimized to distinguish human preferences more efficiently within the specific distribution of the LLM to be optimized. However, this specialization poses a challenge: such an RM will underperform when dealing with generations not aligned with this specific LLM distribution, such as generations from a completely different LLM. Last but not least, training RMs can be unstable and prone to overfitting, especially with multiple training epochs. It’s generally advisable to limit the number of epochs during RM training to avoid this issue.</p>
</section>
</section>
<section id="exercises" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="exercises"><span class="header-section-number">2.4</span> Exercises</h2>
<section id="question-1-choice-modeling-15-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="question-1-choice-modeling-15-points">Question 1: Choice Modeling (15 points)</h3>
<p>We discussed discrete choice modeling in the context of reward being a linear function. Suppose we are deciding between <span class="math inline">\(N\)</span> choices and that the reward for each choice is given by <span class="math inline">\(U_i=\beta_i\mathbf{x}+\epsilon_i\)</span> for <span class="math inline">\(i=1, 2, \cdots, N\)</span>. We view <span class="math inline">\(\mathbf{x}\)</span> as the data point that is being conditioned on for deciding which choice to select, and <span class="math inline">\(\beta_i\)</span> as the weights driving the linear reward model. The noise <span class="math inline">\(\epsilon_i\)</span> is i.i.d. sampled from a type of extreme value distribution called the <em>Gumbel</em> distribution. The standard Gumbel distribution is given by the density function <span class="math inline">\(f(x)=e^{-(x+e^{-x})}\)</span> and cumulative distribution function <span class="math inline">\(F(x)=e^{-e^{-x}}.\)</span> Fix <span class="math inline">\(i\)</span>. Our objective is to calculate <span class="math inline">\(p(U_i\,\, \text{has max reward})\)</span>.</p>
<ol type="a">
<li><p><strong>(Written, 2 points)</strong>. Set <span class="math inline">\(U_i=t\)</span> and compute <span class="math inline">\(p(U_j&lt;t)\)</span> for <span class="math inline">\(j\neq i\)</span> in terms of <span class="math inline">\(F\)</span>. Use this probability to derive an integral for <span class="math inline">\(p(U_i\,\,  \text{has max reward})\)</span> over <span class="math inline">\(t\)</span> in terms of <span class="math inline">\(f\)</span> and <span class="math inline">\(F\)</span>. Example of solution environment.</p></li>
<li><p><strong>(Written, 4 points)</strong>. Compute the integral derived in part (a) with the appropriate <span class="math inline">\(u\)</span>-substitution. You should arrive at multi-class logistic regression!</p></li>
</ol>
<p>Next, you will implement logistic regression to predict preferred completions. We will use the preference dataset from <a href="https://huggingface.co/datasets/allenai/reward-bench">RewardBench</a>. Notice the provided <code>data/chosen_embeddings.pt</code> and <code>data/rejected_embeddings.pt</code> files. These files were constructed by feeding the prompt alongside the chosen/rejected responses through Llama3-8B-Instruct and selecting the last token’s final hidden embedding. Let <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> be two hidden embeddings with <span class="math inline">\(e_1\succ e_2\)</span>. We assume reward is a linear function of embedding <span class="math inline">\(u_j=w^\top e_j\)</span> and use the Bradley-Terry model to predict the preferred item. We can view maximum likelihood across the preference dataset with this model as logistic regression on <span class="math inline">\(e_1-e_2\)</span> and all labels being <span class="math inline">\(1\)</span>. Here, we are given a dataset <span class="math inline">\(X\)</span> with <span class="math inline">\(N\)</span> rows of datapoints and <span class="math inline">\(D\)</span> features per datapoint. The weights of the model are parametrized by <span class="math inline">\(w\)</span>, a <span class="math inline">\(d\)</span>-dimensional column vector. Given binary labels <span class="math inline">\(y\)</span> of shape <span class="math inline">\(N\)</span> by <span class="math inline">\(1\)</span>, the negative log likelihood function and the corresponding gradient is</p>
<p><span class="math display">\[p(y, X| w)=-\frac{1}{N}(y^\top \log(\sigma(X^\top w)) + (1-y)^\tau \log(1-\sigma(X^\top w))), \quad \nabla_w p(y, X | w)=\frac{1}{N}X^T(\sigma(X^\top w)-y),\]</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is the sigmoid function and is applied element-wise along with <span class="math inline">\(\log\)</span>. As usual, we use maximum likelihood to learn the parameter.</p>
<ol type="1">
<li><strong>(Coding, 5 points)</strong>. Implement the functions <code>train</code> and the <code>predict_probs</code> in <code>LogisticRegression</code> class. The starter code is provided below.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-7"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
</div>
<ol start="3" type="1">
<li><strong>(Written, 4 points)</strong>. Open the notebook <code>rewardbench_preferences.ipynb</code> and run all the cells. Make sure to tune the <code>learning_rate</code> and <code>num_iterations</code>. Report your final expected accuracy on the training and validation sets. How close are the two expected accuracies? You should be able to achieve <span class="math inline">\(\approx 90\%\)</span> expected accuracy on validation. You may add loss reporting to the <code>train</code> function to verify your model is improving over time.</li>
</ol>
</section>
<section id="question-2-revealed-and-stated-preferences-20-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="question-2-revealed-and-stated-preferences-20-points">Question 2: Revealed and Stated Preferences (20 points)</h3>
<p>Alice and Bob are running for president. For <span class="math inline">\(R\)</span> voters, we can access their revealed candidate preferences through some means (e.g., social media, blogs, event history). Assume there is an unknown probability <span class="math inline">\(z\)</span> of voting for Alice among the population. The aim of this question is to estimate <span class="math inline">\(z\)</span> through <em>maximum likelihood estimation</em> by also incorporating stated preferences. In this scenario, we collect stated preferences through surveys. When surveyed, voters tend to be more likely to vote for Alice with probability <span class="math inline">\(\frac{z+1}{2}\)</span> for reasons of “political correctness.”</p>
<ol type="a">
<li><p><strong>(Written, 5 points)</strong>. Suppose there are <span class="math inline">\(R_A\)</span> revealed preferences for Alice, <span class="math inline">\(R_B\)</span> revealed preferences for Bob, <span class="math inline">\(S_A\)</span> stated preferences for Alice, and <span class="math inline">\(S_B\)</span> stated preferences for Bob. Note <span class="math inline">\(R=R_A+R_B\)</span>. Compute the log-likelihood of observing such preferences in terms of <span class="math inline">\(z, R_A, R_B, S_A, S_B\)</span>.</p></li>
<li><p><strong>(Coding, 1 point)</strong>. Implement the short function <code>stated_prob</code> in the file <code>voting/simulation.py</code>.</p></li>
<li><p><strong>(Coding, 5 points)</strong>. Implement the class <code>VotingSimulation</code>.</p></li>
<li><p><strong>(Coding, 7 points)</strong>. Implement your derived expression from part (a) in the <code>log_likelihoods</code> function.</p></li>
<li><p><strong>(Written, 2 points)</strong>. Finally, implement the <code>average_mae_mle</code> method that will allow us to visualize the mean absolute error (MAE) of our maximum likelihood estimate <span class="math inline">\(\hat{z}\)</span> (i.e., <span class="math inline">\(|\hat{z}-z|\)</span>) as the number of voters surveyed increases. Open <code>voting/visualize_sim.ipynb</code> and run the cells to get a plot of MAE vs.&nbsp;voters surveyed averaged across <span class="math inline">\(100\)</span> simulations. Attach the plot to this question and briefly explain what you notice.</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-8"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
</div>
</section>
<section id="question-3-probabilistic-multi-modal-preferences-25-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="question-3-probabilistic-multi-modal-preferences-25-points">Question 3: Probabilistic Multi-modal Preferences (25 points)</h3>
<p>Suppose you are part of the ML team on the movie streaming site CardinalStreams. After taking CS329H, you collect a movie preferences dataset with <span class="math inline">\(30000\)</span> examples of the form <span class="math inline">\((m_1, m_2, \text{user id})\)</span> where <span class="math inline">\(m_1\)</span> and <span class="math inline">\(m_2\)</span> are movies with <span class="math inline">\(m_1\succ m_2\)</span>. The preferences come from <span class="math inline">\(600\)</span> distinct users with <span class="math inline">\(50\)</span> examples per user. Each movie has a <span class="math inline">\(10\)</span>-dimensional feature vector <span class="math inline">\(m\)</span>, and each user has a <span class="math inline">\(10\)</span>-dimensional weight vector <span class="math inline">\(u\)</span>. Given movie features <span class="math inline">\(m_1, m_2\)</span> and user weights <span class="math inline">\(u\)</span>, the user’s preference between the movies is given by a Bradley-Terry reward model: <span class="math display">\[P(m_1\succ m_2)=\frac{e^{u\cdot m_1}}{e^{u\cdot m_1} + e^{u\cdot m_2}}=\frac{1}{1+e^{u\cdot (m_2-m_1)}}=\sigma(u\cdot (m_1-m_2)).\]</span></p>
<p>You realize that trying to estimate the weights for each user with only <span class="math inline">\(50\)</span> examples will not work due to the lack of data. Instead, you choose to drop the user IDs column and shuffle the dataset in order to take a <em>multi-modal preferences</em> approach. For simplicity, you assume a model where a proportion <span class="math inline">\(p\)</span> of the users have weights <span class="math inline">\(w_1\)</span> and the other <span class="math inline">\(1-p\)</span> have weights <span class="math inline">\(w_2\)</span>. In this setting, each user belongs to one of two groups: users with weights <span class="math inline">\(w_1\)</span> are part of Group 1, and users with weights <span class="math inline">\(w_2\)</span> are part of Group 2.</p>
<ol type="a">
<li><p><strong>(Written, 3 points)</strong>. For a datapoint <span class="math inline">\((m_1, m_2)\)</span> with label <span class="math inline">\(m_1\succ m_2\)</span>, compute the data likelihood <span class="math inline">\(P(m_1\succ m_2 | p, w_1, w_2)\)</span> assuming <span class="math inline">\(p, w_1, w_2\)</span> are given.</p></li>
<li><p><strong>(Written, 3 points)</strong>. As a follow up, use the likelihood to simplify the posterior distribution of <span class="math inline">\(p, w_1, w_2\)</span> after updating on <span class="math inline">\((m_1, m_2)\)</span> leaving terms for the priors unchanged.</p></li>
<li><p><strong>(Written, 4 points)</strong>. Assume priors <span class="math inline">\(p\sim B(1, 1)\)</span>, <span class="math inline">\(w_1\sim\mathcal{N}(0, \mathbf{I})\)</span>, and <span class="math inline">\(w_2\sim\mathcal{N}(0, \mathbf{I})\)</span> where <span class="math inline">\(B\)</span> represents the Beta distribution and <span class="math inline">\(\mathcal{N}\)</span> represents the normal distribution. You will notice that the posterior from part (b) has no simple closed-form. As a result, we must resort to <em>Markov Chain Monte Carlo (MCMC)</em> approaches to sample from the posterior. These approaches allow sampling from highly complex distributions by constructing a Markov chain <span class="math inline">\(\{x_t\}_{t=1}^\infty\)</span> so that <span class="math inline">\(\lim_{t\to\infty}x_t\)</span> act as desired samples from the target distribution. You can think of a Markov chain as a sequence with the special property that <span class="math inline">\(x_{t+1}\)</span> only depends on <span class="math inline">\(x_t\)</span> for all <span class="math inline">\(t\ge 1\)</span>.</p></li>
</ol>
<p>The most basic version of MCMC is known as Metropolis-Hastings. Assume <span class="math inline">\(\pi\)</span> is the target distribution we wish to sample from where <span class="math inline">\(\pi(z)\)</span> represents the probability density at point <span class="math inline">\(z\)</span>. Metropolis-Hastings constructs the approximating Markov chain <span class="math inline">\(x_t\)</span> as follows: a proposal <span class="math inline">\(P\)</span> for <span class="math inline">\(x_{t+1}\)</span> is made via sampling from a chosen distribution <span class="math inline">\(Q(\,\cdot\,| x_t)\)</span> (e.g., adding Gaussian noise). The acceptance probability of the proposal is given by</p>
<p><span class="math display">\[A= \min \left( 1, \frac{\pi(P)Q(x_t | P)}{\pi(x_t)Q(P | x_t)} \right). \text{ That is } x_{t+1}=\begin{cases} P &amp; \text{with probability } A, \\ x_t &amp; \text{with probability } 1 - A. \end{cases}\]</span> To extract our samples from <span class="math inline">\(\pi\)</span>, we run the Markov chain for <span class="math inline">\(N\)</span> timesteps and disregard the first <span class="math inline">\(T&lt;N\)</span> timesteps in what is called the <em>burn-in or mixing time</em> (i.e., our final samples are <span class="math inline">\(x_{T+1}, x_{T+2},\cdots, x_{N}\)</span>). The mixing time is needed to ensure that the Markov chain elements are representative of the distribution <span class="math inline">\(\pi\)</span> – initial elements of the chain will not be a good approximation of <span class="math inline">\(\pi\)</span> and depend more on the choice of initialization <span class="math inline">\(x_1\)</span>. To build some intuition, suppose we have a biased coin that turns heads with probability <span class="math inline">\(p_{\text{heads}}\)</span>. We observe <span class="math inline">\(12\)</span> coin flips to have <span class="math inline">\(9\)</span> heads (H) and <span class="math inline">\(3\)</span> tails (T). If our prior for <span class="math inline">\(p_{\text{H}}\)</span> was <span class="math inline">\(B(1, 1)\)</span>, then our posterior will be <span class="math inline">\(B(1+9, 1+3)=B(10, 4)\)</span>. The Bayesian update is given by</p>
<p><span class="math display">\[p(p_{\text{H}}|9\text{H}, 3\text{T}) = \frac{p(9\text{H}, 3\text{T} | p_{\text{H}})B(1, 1)(p_{\text{H}})}{\int_0^1 P(9\text{H}, 3\text{T} | p_{\text{H}})B(1, 1)(p_{\text{H}}) dp_{\text{H}}} =\frac{p(9\text{H}, 3\text{T} | p_{\text{H}})}{\int_0^1 p(9\text{H}, 3\text{T} | p_{\text{H}})  dp_{\text{H}}}.\]</span></p>
<p><strong>Find the acceptance probablity</strong> <span class="math inline">\(A\)</span> in the setting of the biased coin assuming the proposal distribution <span class="math inline">\(Q(\cdot|x_t)=x_t+N(0,\sigma)\)</span> for given <span class="math inline">\(\sigma\)</span>. Notice that this choice of <span class="math inline">\(Q\)</span> is symmetric, i.e., <span class="math inline">\(Q(x_t|P)=Q(P|x_t)\)</span>. In addition, you will realize that is unnecessary to compute the normalizing constant of the Bayesian update (i.e., the integral in the denominator) which is why MCMC is commonly used to sample from posteriors!</p>
<ol start="4" type="a">
<li><strong>(Written + Coding, 6 points)</strong>. Implement Metropolis-Hastings to sample from the posterior distribution of the biased coin in <code>multimodal_preferences/biased_coin.py</code>. Attach a histogram of your MCMC samples overlayed on top of the true posterior <span class="math inline">\(B(10, 4)\)</span> by running <code>python biased_coin.py</code>.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-9"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
</div>
<ol start="5" type="a">
<li><strong>(Coding, 9 points)</strong>. Implement Metropolis-Hastings in the movie setting inside&nbsp;<code>multimodal_preferences/movie_metropolis.py</code>. The movie dataset we use for grading will not be provided. However, randomly constructed datasets can be used to test your implementation by running <code>python movie_metropolis.py</code>. You should be able to achieve a <span class="math inline">\(90\%\)</span> success rate with most <code>fraction_accepted</code> values above <span class="math inline">\(0.1\)</span>. Success is measured by thresholded closeness of predicted parameters to true parameters. You may notice occasional failures that occur due to lack of convergence which we will account for in grading.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-10"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
</div>


<!-- -->

</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-book_estimation_bock" class="csl-entry" role="listitem">
Bock, Hans Georg, Thomas Carraro, Willi Jäger, Stefan Körkel, Rolf Rannacher, and Johannes P. Schlöder. 2015. <em>Model Based Parameter Estimation: Theory and Applications</em>. Springer. <a href="https://api.semanticscholar.org/CorpusID:60333071">https://api.semanticscholar.org/CorpusID:60333071</a>.
</div>
<div id="ref-bolt2009" class="csl-entry" role="listitem">
Bolt, Daniel M., and James A. Wollack. 2009. <span>“Application of a Multidimensional Nested Logit Model to Multiple-Choice Test Items.”</span> <em>Journal of Educational Measurement</em> 46 (3): 181–98. <a href="https://doi.org/10.1111/j.1745-3984.2009.00081.x">https://doi.org/10.1111/j.1745-3984.2009.00081.x</a>.
</div>
<div id="ref-bradley-terry-model" class="csl-entry" role="listitem">
Bradley, Ralph Allan, and Milton E. Terry. 1952. <span>“Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons.”</span> <em>Biometrika</em> 39 (3/4): 324–45. <a href="http://www.jstor.org/stable/2334029">http://www.jstor.org/stable/2334029</a>.
</div>
<div id="ref-campbell2015" class="csl-entry" role="listitem">
Campbell, Danny, and Seda Erdem. 2015. <span>“Position Bias in Best-Worst Scaling Surveys: A Case Study on Trust in Institutions.”</span> <em>American Journal of Agricultural Economics</em> 97 (2): 526–45. <a href="https://doi.org/10.1093/ajae/aau112">https://doi.org/10.1093/ajae/aau112</a>.
</div>
<div id="ref-book_estimation_casella" class="csl-entry" role="listitem">
Casella, George, and Roger L. Berger. 1990. <em>Statistical Inference</em>. Springer. <a href="https://api.semanticscholar.org/CorpusID:125727004">https://api.semanticscholar.org/CorpusID:125727004</a>.
</div>
<div id="ref-finn2017model" class="csl-entry" role="listitem">
Finn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017. <span>“Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.”</span> In <em>International Conference on Machine Learning</em>, 1126–35. PMLR.
</div>
<div id="ref-idealpoints" class="csl-entry" role="listitem">
Greiner, James. 2005. <span>“Ideal Points.”</span> Harvard IQSS Blog. <a href="https://blogs.iq.harvard.edu/ideal_points_1">https://blogs.iq.harvard.edu/ideal_points_1</a>.
</div>
<div id="ref-haarnoja2018soft" class="csl-entry" role="listitem">
Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. <span>“Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.”</span> In <em>International Conference on Machine Learning</em>, 1861–70. PMLR.
</div>
<div id="ref-harpe2015" class="csl-entry" role="listitem">
Harpe, Spencer E. 2015. <span>“How to Analyze Likert and Other Rating Scale Data.”</span> <em>Currents in Pharmacy Teaching and Learning</em> 7 (5): 836–50. <a href="http://dx.doi.org/10.1016/j.cptl.2015.08.001">http://dx.doi.org/10.1016/j.cptl.2015.08.001</a>.
</div>
<div id="ref-hejna2023few" class="csl-entry" role="listitem">
Hejna III, Donald Joseph, and Dorsa Sadigh. 2023. <span>“Few-Shot Preference Learning for Human-in-the-Loop Rl.”</span> In <em>Conference on Robot Learning</em>, 2014–25. PMLR.
</div>
<div id="ref-huber1976ideal" class="csl-entry" role="listitem">
Huber, Joel. 1976. <span>“Ideal Point Models of Preference.”</span> In <em>Advances in Consumer Research</em>, 03:138–42. Association for Consumer Research.
</div>
<div id="ref-ideal_point" class="csl-entry" role="listitem">
Jamieson, Kevin G, and Robert Nowak. 2011. <span>“Active Ranking Using Pairwise Comparisons.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger. Vol. 24. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2011/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2011/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf</a>.
</div>
<div id="ref-kwon2021targeted" class="csl-entry" role="listitem">
Kwon, Minae, Siddharth Karamcheti, Mariano-Florentino Cuellar, and Dorsa Sadigh. 2021. <span>“Targeted Data Acquisition for Evolving Negotiation Agents.”</span> <a href="https://arxiv.org/abs/2106.07728">https://arxiv.org/abs/2106.07728</a>.
</div>
<div id="ref-lee2021pebble" class="csl-entry" role="listitem">
Lee, Kimin, Laura Smith, and Pieter Abbeel. 2021. <span>“Pebble: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-Training.”</span> <em>arXiv Preprint arXiv:2106.05091</em>.
</div>
<div id="ref-Luce1977" class="csl-entry" role="listitem">
Luce, R.Duncan. 1977. <span>“The Choice Axiom After Twenty Years.”</span> <em>Journal of Mathematical Psychology</em> 15 (3): 215–33. <a href="https://doi.org/10.1016/0022-2496(77)90032-3">https://doi.org/10.1016/0022-2496(77)90032-3</a>.
</div>
<div id="ref-mcfadden_conditional_1974" class="csl-entry" role="listitem">
McFadden, Daniel. 1974. <span>“Conditional Logit Analysis of Qualitative Choice Behavior.”</span> In <em>Frontiers in Econometrics</em>, edited by Paul Zarembka, 105–42. New York: Academic Press.
</div>
<div id="ref-miljkovic2005rational" class="csl-entry" role="listitem">
Miljkovic, Dragan. 2005. <span>“Rational Choice and Irrational Individuals or Simply an Irrational Theory: A Critical Review of the Hypothesis of Perfect Rationality.”</span> <em>The Journal of Socio-Economics</em> 34 (5): 621–34. <a href="https://doi.org/10.1016/j.socec.2003.12.031">https://doi.org/10.1016/j.socec.2003.12.031</a>.
</div>
<div id="ref-myers2022learning" class="csl-entry" role="listitem">
Myers, Vivek, Erdem Biyik, Nima Anari, and Dorsa Sadigh. 2022. <span>“Learning Multimodal Rewards from Rankings.”</span> In <em>Conference on Robot Learning</em>, 342–52. PMLR.
</div>
<div id="ref-myers2021learning" class="csl-entry" role="listitem">
Myers, Vivek, Erdem Bıyık, Nima Anari, and Dorsa Sadigh. 2021. <span>“Learning Multimodal Rewards from Rankings.”</span> <a href="https://arxiv.org/abs/2109.12750">https://arxiv.org/abs/2109.12750</a>.
</div>
<div id="ref-plackett_luce" class="csl-entry" role="listitem">
Plackett, R. L. 1975. <span>“The Analysis of Permutations.”</span> <em>Journal of the Royal Statistical Society. Series C (Applied Statistics)</em> 24 (2): 193–202. <a href="http://www.jstor.org/stable/2346567">http://www.jstor.org/stable/2346567</a>.
</div>
<div id="ref-ragain2019" class="csl-entry" role="listitem">
Ragain, Stephen, and Johan Ugander. 2019. <span>“Choosing to Rank.”</span> <em>arXiv Preprint arXiv:1809.05139</em>. <a href="https://arxiv.org/abs/1809.05139">https://arxiv.org/abs/1809.05139</a>.
</div>
<div id="ref-gradient_descent" class="csl-entry" role="listitem">
Ruder, Sebastian. 2016. <span>“An Overview of Gradient Descent Optimization Algorithms.”</span> <em>ArXiv</em> abs/1609.04747. <a href="https://api.semanticscholar.org/CorpusID:17485266">https://api.semanticscholar.org/CorpusID:17485266</a>.
</div>
<div id="ref-simon1972theories" class="csl-entry" role="listitem">
Simon, Herbert A. 1972. <span>“Theories of Bounded Rationality.”</span> In <em>Decision and Organization</em>, edited by C. B. McGuire and Roy Radner, 161–76. North-Holland Publishing Company.
</div>
<div id="ref-tatli2022distancepreferences" class="csl-entry" role="listitem">
Tatli, Gokcan, Rob Nowak, and Ramya Korlakai Vinayak. 2022. <span>“Learning Preference Distributions from Distance Measurements.”</span> In <em>2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)</em>, 1–8. <a href="https://doi.org/10.1109/Allerton49937.2022.9929404">https://doi.org/10.1109/Allerton49937.2022.9929404</a>.
</div>
<div id="ref-2307.09288" class="csl-entry" role="listitem">
Touvron, Hugo et al. 2023. <span>“Llama 2: Open Foundation and Fine-Tuned Chat Models.”</span> <a href="https://arxiv.org/abs/2307.09288">https://arxiv.org/abs/2307.09288</a>.
</div>
<div id="ref-yu2020meta" class="csl-entry" role="listitem">
Yu, Tianhe, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. 2020. <span>“Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning.”</span> In <em>Conference on Robot Learning</em>, 1094–1100. PMLR.
</div>
<div id="ref-zhou2019watch" class="csl-entry" role="listitem">
Zhou, Allan, Eric Jang, Daniel Kappler, Alex Herzog, Mohi Khansari, Paul Wohlhart, Yunfei Bai, Mrinal Kalakrishnan, Sergey Levine, and Chelsea Finn. 2019. <span>“Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards.”</span> In <em>International Conference on Learning Representations</em>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../index.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../src/chap2.html" class="pagination-link" aria-label="Active Learning of Preference Models">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Active Learning of Preference Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb8" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb8-1"><a href="#cb8-1"></a><span class="co">---</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="an">title:</span><span class="co"> Models of Preferences and Decisions</span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="an">format:</span><span class="co"> html</span></span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="an">filters:</span></span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="co">  - pyodide</span></span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="an">execute:</span></span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="co">  engine: pyodide</span></span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="co">  pyodide:</span></span>
<span id="cb8-9"><a href="#cb8-9"></a><span class="co">    auto: true</span></span>
<span id="cb8-10"><a href="#cb8-10"></a></span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="co">---</span></span>
<span id="cb8-12"><a href="#cb8-12"></a></span>
<span id="cb8-13"><a href="#cb8-13"></a>::: {.content-visible when-format="html"}</span>
<span id="cb8-14"><a href="#cb8-14"></a></span>
<span id="cb8-15"><a href="#cb8-15"></a>&lt;iframe</span>
<span id="cb8-16"><a href="#cb8-16"></a>  src="https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/"</span>
<span id="cb8-17"><a href="#cb8-17"></a>  style="width:45%; height:225px;"</span>
<span id="cb8-18"><a href="#cb8-18"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb8-19"><a href="#cb8-19"></a>&lt;iframe</span>
<span id="cb8-20"><a href="#cb8-20"></a>  src="https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/"</span>
<span id="cb8-21"><a href="#cb8-21"></a>  style="width:45%; height:225px;"</span>
<span id="cb8-22"><a href="#cb8-22"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb8-23"><a href="#cb8-23"></a><span class="co">[</span><span class="ot">Fullscreen Part 1</span><span class="co">](https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/)</span>{.btn .btn-outline-primary .btn role="button"}</span>
<span id="cb8-24"><a href="#cb8-24"></a><span class="co">[</span><span class="ot">Fullscreen Part 2</span><span class="co">](https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/)</span>{.btn .btn-outline-primary .btn role="button"}</span>
<span id="cb8-25"><a href="#cb8-25"></a></span>
<span id="cb8-26"><a href="#cb8-26"></a>:::</span>
<span id="cb8-27"><a href="#cb8-27"></a></span>
<span id="cb8-28"><a href="#cb8-28"></a>Human preference modeling aims to capture humans' decision making processes in a probabilistic framework. Many problems would benefit from a quantitative perspective, enabling an understanding of how humans engage with the world. In this chapter, we will explore how one can model human preferences, including different formulations of such models, how one can optimize these models given data, and considerations one should understand to create such systems.</span>
<span id="cb8-29"><a href="#cb8-29"></a></span>
<span id="cb8-30"><a href="#cb8-30"></a><span class="fu">## The Construction of Preference {#sec-foundations}</span></span>
<span id="cb8-31"><a href="#cb8-31"></a></span>
<span id="cb8-32"><a href="#cb8-32"></a><span class="fu">### Axiom 1. Construction of Choices Set: Luce’s Choice Axiom (Luce, 1959) {#axiom-1-preference-models-model-choice}</span></span>
<span id="cb8-33"><a href="#cb8-33"></a>Preference models model the preferred choices amongst a set of items. Preference models must enumerate the set of all possible choices included in a human decision. As such, we must ensure that the choices we enumerate capture the entire domain (collectively exhaustive) but are distinct (mutually exclusive) choices. A discrete set of choices is a constraint we canonically impose to ensure we can tractably model preferences and aptly estimate the parameters of preference models. We assume that if a new item is added to the choice set, the relative probabilities of choosing between the original items remain unchanged. This is known as the Independence of Irrelevant Alternatives (IIA) property from Luce's axiom of choices <span class="co">[</span><span class="ot">@Luce1977</span><span class="co">]</span>.</span>
<span id="cb8-34"><a href="#cb8-34"></a></span>
<span id="cb8-35"><a href="#cb8-35"></a><span class="fu">### Axiom 2. Preference Centers around Utility: Reciprocity (Block &amp; Marschak, 1960) {#axiom-2-preference-centers-around-reward}</span></span>
<span id="cb8-36"><a href="#cb8-36"></a>Preference models are centered around the notion of reward, a scalar quantity representing the benefit or value an individual attains from selecting a given choice. We assume that the underlying reward mechanism of a human preference model captures the final decision output from a human. We use the notation $u_{i,j}$ as the reward of person $i$ choosing item $j$. The reward is a random variable, decomposing into true reward $u_{i,j}^*$ and a random noise $\epsilon_{i,j}$: $u_{i,j} = u_{i,j}^* + \epsilon_{i,j}$. @mcfadden_conditional_1974 posits that reward can further be decomposed into user-specific reward $\theta_i$ and item-specific reward $z_j$: $u_{i,j}^* = \theta_i + z_j$. This decomposition indicates that for a single user, only the relative difference in reward matters to predict the choice among items, and the scale of rewards is important when comparing across users. </span>
<span id="cb8-37"><a href="#cb8-37"></a></span>
<span id="cb8-38"><a href="#cb8-38"></a><span class="fu">### Axiom 3. Preference captures decision-making: Wins as a Sufficient Statistic (Bühlmann &amp; Huber, 1963) {#axiom-3-preference-captures-decision-making}</span></span>
<span id="cb8-39"><a href="#cb8-39"></a>Human preferences are classified into two categories: revealed preferences and stated preferences. Revealed preferences are those one can observe retroactively from existing data. The implicit decision-making knowledge can be captured via learnable parameters and their usage in models that represent relationships between input decision attributes that may have little interpretability but enable powerful models of human preference. Such data may be easier to acquire and can reflect real-world outcomes (since they are, at least theoretically, inherently based on human preferences). However, if we fail to capture sufficient context in such data, human preference models may not sufficiently capture human preferences. Stated preferences are those individuals explicitly indicate in potentially experimental conditions. The explicit knowledge may be leveraged by including inductive biases during modeling (for example, the context used in a model), which are reasonable assumptions for how a human would consider a set of items. This may include controlled experiments or studies. This may be harder to obtain and somewhat biased, as they can be hypothetical or only accurately reflect a piece of the overall context of a decision. However, they enable greater control of the decision-making process.</span>
<span id="cb8-40"><a href="#cb8-40"></a></span>
<span id="cb8-41"><a href="#cb8-41"></a><span class="fu">### Axiom 4. Rationality: The Transitivity of odds (Good, 1955) {#human-rationality}</span></span>
<span id="cb8-42"><a href="#cb8-42"></a>The preference model assumes that humans are rational. Perfect rationality posits that individuals make decisions that maximize their reward, assuming they have complete information and the cognitive ability to process this information to make optimal choices. Numerous studies have shown that this assumption frequently fails to describe actual human behavior. Bounded rationality acknowledges that individuals operate within the limits of their information and cognitive capabilities <span class="co">[</span><span class="ot">@simon1972theories</span><span class="co">]</span>. Here, decisions are influenced by noise, resulting in probabilistic choice behavior: while individuals aim to maximize their reward, noise can lead to deviations from perfectly rational choices <span class="co">[</span><span class="ot">@miljkovic2005rational</span><span class="co">]</span>. Instead of deterministic reward maximization, the decision maker will choose an item with probability proportional to the reward they receive for that item. This probabilistic model can be operationalized with Boltzmann distribution. Utility of person $i$ on item $j$ is computed by a function $f_i: e_j \rightarrow \mathbb{R}$, where $e_j \in \mathbb{R}^d$ is an embedding of item $j$. The probability of item $j$ being preferred by person $i$ over all other alternatives in the choice set $\mathcal{C}$ is</span>
<span id="cb8-43"><a href="#cb8-43"></a></span>
<span id="cb8-44"><a href="#cb8-44"></a>$$</span>
<span id="cb8-45"><a href="#cb8-45"></a>p_{ij} =  p_i(j \succ j': j' \neq j \forall j' \in \mathcal{C}) = Z_i^{-1} \exp \circ f_i(e_j) \text{ where } Z_i = \sum_{j' \in \mathcal{C}} \exp \circ f_i(e_{j'})</span>
<span id="cb8-46"><a href="#cb8-46"></a>$$</span>
<span id="cb8-47"><a href="#cb8-47"></a></span>
<span id="cb8-48"><a href="#cb8-48"></a>One can extend the above model in various ways. For example, the above model does not account for similar actions. Consider the following example when choosing a mode of transportation: car and train, with no particular preference for either choice. The preferred probability is 50% for either item. However, if we have 99 cars and one train in the choice set, we would have a 99% probability of choosing a car. To address this issue, various extensions have been proposed. For example, we can introduce a similarity metric to cluster items. We want a metric that acts more as a distance in the feature space with the following properties: Identity (an item is most similar to itself), symmetric (the similarity of item $j$ to $j'$ is the same as that of $j'$ to $j$), and positive semidefinite (similarity metric is non-negative). Under this extension, the probablity of item $j$ being preferred over all other alternatives by person $i$ is $p_{ij} / w_j, \text{ where } w_j = \sum_{j' \in \mathcal{C}} s(e_j, e_{j'})$. This de-weights similar items, which is the desired effect for human decision-making. </span>
<span id="cb8-49"><a href="#cb8-49"></a></span>
<span id="cb8-50"><a href="#cb8-50"></a><span class="fu">## Models of Preferences and Decisions {#preference-model}</span></span>
<span id="cb8-51"><a href="#cb8-51"></a></span>
<span id="cb8-52"><a href="#cb8-52"></a>Next, we explore ways humans can express their preferences, including accept-reject sampling, pairwise sampling, rank-order sampling, rating-scale sampling, best-worst scaling, and multiple-choice samples. We will understand the process of collecting data through simulation and, when appropriate, discuss the real-world application of these models. Each item $i$ is represented by a $d=2$ dimensional vector $x^i$. There is only one user in the simulation, and they have a latent reward function $f$ that they use to compute the latent reward of an item from the features. Here, the latent reward function is the Ackley function \cite{ackley1987}.</span>
<span id="cb8-53"><a href="#cb8-53"></a></span>
<span id="cb8-54"><a href="#cb8-54"></a>::: {.callout-note title="code"}</span>
<span id="cb8-55"><a href="#cb8-55"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb8-56"><a href="#cb8-56"></a><span class="in">import numpy as np</span></span>
<span id="cb8-57"><a href="#cb8-57"></a><span class="in">np.random.seed(0)</span></span>
<span id="cb8-58"><a href="#cb8-58"></a></span>
<span id="cb8-59"><a href="#cb8-59"></a><span class="in">def ackley(X, a=20, b=0.2, c=2*np.pi):</span></span>
<span id="cb8-60"><a href="#cb8-60"></a><span class="in">    """</span></span>
<span id="cb8-61"><a href="#cb8-61"></a><span class="in">    Compute the Ackley function.</span></span>
<span id="cb8-62"><a href="#cb8-62"></a><span class="in">    Parameters:</span></span>
<span id="cb8-63"><a href="#cb8-63"></a><span class="in">      X: A NumPy array of shape (n, d) where each row is a d-dimensional point.</span></span>
<span id="cb8-64"><a href="#cb8-64"></a><span class="in">      a, b, c: Parameters of the Ackley function.</span></span>
<span id="cb8-65"><a href="#cb8-65"></a><span class="in">    Returns:</span></span>
<span id="cb8-66"><a href="#cb8-66"></a><span class="in">      A NumPy array of function values.</span></span>
<span id="cb8-67"><a href="#cb8-67"></a><span class="in">    """</span></span>
<span id="cb8-68"><a href="#cb8-68"></a><span class="in">    X = np.atleast_2d(X)</span></span>
<span id="cb8-69"><a href="#cb8-69"></a><span class="in">    d = X.shape[1]</span></span>
<span id="cb8-70"><a href="#cb8-70"></a><span class="in">    sum_sq = np.sum(X ** 2, axis=1)</span></span>
<span id="cb8-71"><a href="#cb8-71"></a><span class="in">    term1 = -a * np.exp(-b * np.sqrt(sum_sq / d))</span></span>
<span id="cb8-72"><a href="#cb8-72"></a><span class="in">    term2 = -np.exp(np.sum(np.cos(c * X), axis=1) / d)</span></span>
<span id="cb8-73"><a href="#cb8-73"></a><span class="in">    return term1 + term2 + a + np.e</span></span>
<span id="cb8-74"><a href="#cb8-74"></a><span class="in">```</span></span>
<span id="cb8-75"><a href="#cb8-75"></a>:::</span>
<span id="cb8-76"><a href="#cb8-76"></a></span>
<span id="cb8-79"><a href="#cb8-79"></a><span class="in">```{python}</span></span>
<span id="cb8-80"><a href="#cb8-80"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-81"><a href="#cb8-81"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb8-82"><a href="#cb8-82"></a></span>
<span id="cb8-83"><a href="#cb8-83"></a><span class="kw">def</span> ackley(X, a<span class="op">=</span><span class="dv">20</span>, b<span class="op">=</span><span class="fl">0.2</span>, c<span class="op">=</span><span class="dv">2</span><span class="op">*</span>np.pi):</span>
<span id="cb8-84"><a href="#cb8-84"></a>    <span class="co">"""</span></span>
<span id="cb8-85"><a href="#cb8-85"></a><span class="co">    Compute the Ackley function.</span></span>
<span id="cb8-86"><a href="#cb8-86"></a><span class="co">    Parameters:</span></span>
<span id="cb8-87"><a href="#cb8-87"></a><span class="co">      X: A NumPy array of shape (n, d) where each row is a d-dimensional point.</span></span>
<span id="cb8-88"><a href="#cb8-88"></a><span class="co">      a, b, c: Parameters of the Ackley function.</span></span>
<span id="cb8-89"><a href="#cb8-89"></a><span class="co">    Returns:</span></span>
<span id="cb8-90"><a href="#cb8-90"></a><span class="co">      A NumPy array of function values.</span></span>
<span id="cb8-91"><a href="#cb8-91"></a><span class="co">    """</span></span>
<span id="cb8-92"><a href="#cb8-92"></a>    X <span class="op">=</span> np.atleast_2d(X)</span>
<span id="cb8-93"><a href="#cb8-93"></a>    d <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb8-94"><a href="#cb8-94"></a>    sum_sq <span class="op">=</span> np.<span class="bu">sum</span>(X <span class="op">**</span> <span class="dv">2</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-95"><a href="#cb8-95"></a>    term1 <span class="op">=</span> <span class="op">-</span>a <span class="op">*</span> np.exp(<span class="op">-</span>b <span class="op">*</span> np.sqrt(sum_sq <span class="op">/</span> d))</span>
<span id="cb8-96"><a href="#cb8-96"></a>    term2 <span class="op">=</span> <span class="op">-</span>np.exp(np.<span class="bu">sum</span>(np.cos(c <span class="op">*</span> X), axis<span class="op">=</span><span class="dv">1</span>) <span class="op">/</span> d)</span>
<span id="cb8-97"><a href="#cb8-97"></a>    <span class="cf">return</span> term1 <span class="op">+</span> term2 <span class="op">+</span> a <span class="op">+</span> np.e</span>
<span id="cb8-98"><a href="#cb8-98"></a><span class="in">```</span></span>
<span id="cb8-99"><a href="#cb8-99"></a></span>
<span id="cb8-100"><a href="#cb8-100"></a>We next define a function to visualize the surface:</span>
<span id="cb8-101"><a href="#cb8-101"></a></span>
<span id="cb8-102"><a href="#cb8-102"></a>::: {.callout-note title="code"}</span>
<span id="cb8-103"><a href="#cb8-103"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb8-104"><a href="#cb8-104"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb8-105"><a href="#cb8-105"></a><span class="in">from matplotlib.colors import LinearSegmentedColormap</span></span>
<span id="cb8-106"><a href="#cb8-106"></a><span class="in">ccmap = LinearSegmentedColormap.from_list("ackley", ["#f76a05", "#FFF2C9"])</span></span>
<span id="cb8-107"><a href="#cb8-107"></a><span class="in">plt.rcParams.update({</span></span>
<span id="cb8-108"><a href="#cb8-108"></a><span class="in">    "font.size": 14,</span></span>
<span id="cb8-109"><a href="#cb8-109"></a><span class="in">    "axes.labelsize": 16,</span></span>
<span id="cb8-110"><a href="#cb8-110"></a><span class="in">    "xtick.labelsize": 14,</span></span>
<span id="cb8-111"><a href="#cb8-111"></a><span class="in">    "ytick.labelsize": 14,</span></span>
<span id="cb8-112"><a href="#cb8-112"></a><span class="in">    "legend.fontsize": 14,</span></span>
<span id="cb8-113"><a href="#cb8-113"></a><span class="in">    "axes.titlesize": 16,</span></span>
<span id="cb8-114"><a href="#cb8-114"></a><span class="in">})</span></span>
<span id="cb8-115"><a href="#cb8-115"></a></span>
<span id="cb8-116"><a href="#cb8-116"></a><span class="in">def draw_surface():</span></span>
<span id="cb8-117"><a href="#cb8-117"></a><span class="in">    inps = np.linspace(-2, 2, 100)</span></span>
<span id="cb8-118"><a href="#cb8-118"></a><span class="in">    X, Y = np.meshgrid(inps, inps)</span></span>
<span id="cb8-119"><a href="#cb8-119"></a><span class="in">    grid = np.column_stack([X.ravel(), Y.ravel()])</span></span>
<span id="cb8-120"><a href="#cb8-120"></a><span class="in">    Z = ackley(grid).reshape(X.shape)</span></span>
<span id="cb8-121"><a href="#cb8-121"></a><span class="in">    </span></span>
<span id="cb8-122"><a href="#cb8-122"></a><span class="in">    plt.figure(figsize=(6, 5))</span></span>
<span id="cb8-123"><a href="#cb8-123"></a><span class="in">    contour = plt.contourf(X, Y, Z, 50, cmap=ccmap)</span></span>
<span id="cb8-124"><a href="#cb8-124"></a><span class="in">    plt.contour(X, Y, Z, levels=15, colors='black', linewidths=0.5, alpha=0.6)</span></span>
<span id="cb8-125"><a href="#cb8-125"></a><span class="in">    plt.colorbar(contour, label=r'$f(x)$', ticks=[0, 3, 6])</span></span>
<span id="cb8-126"><a href="#cb8-126"></a><span class="in">    plt.xlim(-2, 2)</span></span>
<span id="cb8-127"><a href="#cb8-127"></a><span class="in">    plt.ylim(-2, 2)</span></span>
<span id="cb8-128"><a href="#cb8-128"></a><span class="in">    plt.xticks([-2, 0, 2])</span></span>
<span id="cb8-129"><a href="#cb8-129"></a><span class="in">    plt.yticks([-2, 0, 2])</span></span>
<span id="cb8-130"><a href="#cb8-130"></a><span class="in">    plt.xlabel(r'$x_1$')</span></span>
<span id="cb8-131"><a href="#cb8-131"></a><span class="in">    plt.ylabel(r'$x_2$')</span></span>
<span id="cb8-132"><a href="#cb8-132"></a><span class="in">```</span></span>
<span id="cb8-133"><a href="#cb8-133"></a>:::</span>
<span id="cb8-136"><a href="#cb8-136"></a><span class="in">```{python}</span></span>
<span id="cb8-137"><a href="#cb8-137"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-138"><a href="#cb8-138"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> LinearSegmentedColormap</span>
<span id="cb8-139"><a href="#cb8-139"></a>ccmap <span class="op">=</span> LinearSegmentedColormap.from_list(<span class="st">"ackley"</span>, [<span class="st">"#f76a05"</span>, <span class="st">"#FFF2C9"</span>])</span>
<span id="cb8-140"><a href="#cb8-140"></a>plt.rcParams.update({</span>
<span id="cb8-141"><a href="#cb8-141"></a>    <span class="st">"font.size"</span>: <span class="dv">14</span>,</span>
<span id="cb8-142"><a href="#cb8-142"></a>    <span class="st">"axes.labelsize"</span>: <span class="dv">16</span>,</span>
<span id="cb8-143"><a href="#cb8-143"></a>    <span class="st">"xtick.labelsize"</span>: <span class="dv">14</span>,</span>
<span id="cb8-144"><a href="#cb8-144"></a>    <span class="st">"ytick.labelsize"</span>: <span class="dv">14</span>,</span>
<span id="cb8-145"><a href="#cb8-145"></a>    <span class="st">"legend.fontsize"</span>: <span class="dv">14</span>,</span>
<span id="cb8-146"><a href="#cb8-146"></a>    <span class="st">"axes.titlesize"</span>: <span class="dv">16</span>,</span>
<span id="cb8-147"><a href="#cb8-147"></a>})</span>
<span id="cb8-148"><a href="#cb8-148"></a>plt.rcParams[<span class="st">'text.usetex'</span>] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb8-149"><a href="#cb8-149"></a></span>
<span id="cb8-150"><a href="#cb8-150"></a><span class="kw">def</span> draw_surface():</span>
<span id="cb8-151"><a href="#cb8-151"></a>    inps <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb8-152"><a href="#cb8-152"></a>    X, Y <span class="op">=</span> np.meshgrid(inps, inps)</span>
<span id="cb8-153"><a href="#cb8-153"></a>    grid <span class="op">=</span> np.column_stack([X.ravel(), Y.ravel()])</span>
<span id="cb8-154"><a href="#cb8-154"></a>    Z <span class="op">=</span> ackley(grid).reshape(X.shape)</span>
<span id="cb8-155"><a href="#cb8-155"></a>    </span>
<span id="cb8-156"><a href="#cb8-156"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">5</span>))</span>
<span id="cb8-157"><a href="#cb8-157"></a>    contour <span class="op">=</span> plt.contourf(X, Y, Z, <span class="dv">50</span>, cmap<span class="op">=</span>ccmap)</span>
<span id="cb8-158"><a href="#cb8-158"></a>    plt.contour(X, Y, Z, levels<span class="op">=</span><span class="dv">15</span>, colors<span class="op">=</span><span class="st">'black'</span>, linewidths<span class="op">=</span><span class="fl">0.5</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb8-159"><a href="#cb8-159"></a>    plt.colorbar(contour, label<span class="op">=</span><span class="vs">r'$f(x)$'</span>, ticks<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">6</span>])</span>
<span id="cb8-160"><a href="#cb8-160"></a>    plt.xlim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb8-161"><a href="#cb8-161"></a>    plt.ylim(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb8-162"><a href="#cb8-162"></a>    plt.xticks([<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>])</span>
<span id="cb8-163"><a href="#cb8-163"></a>    plt.yticks([<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>])</span>
<span id="cb8-164"><a href="#cb8-164"></a>    plt.xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb8-165"><a href="#cb8-165"></a>    plt.ylabel(<span class="vs">r'$x_2$'</span>)</span>
<span id="cb8-166"><a href="#cb8-166"></a><span class="in">```</span></span>
<span id="cb8-167"><a href="#cb8-167"></a></span>
<span id="cb8-168"><a href="#cb8-168"></a><span class="fu">### Item-wise Model {#item-wise-model}</span></span>
<span id="cb8-169"><a href="#cb8-169"></a>One method for data collection is accept-reject sampling, where the user considers one item at a time and decides if they like it. Below is an example survey using accept-reject sampling:</span>
<span id="cb8-170"><a href="#cb8-170"></a></span>
<span id="cb8-171"><a href="#cb8-171"></a>::: {.content-visible when-format="html"}</span>
<span id="cb8-172"><a href="#cb8-172"></a>&lt;iframe</span>
<span id="cb8-173"><a href="#cb8-173"></a>  src="https://app.opinionx.co/5f30e903-c7b2-42e3-821a-e65271144bd9"</span>
<span id="cb8-174"><a href="#cb8-174"></a>  style="width:100%; height:450px;"</span>
<span id="cb8-175"><a href="#cb8-175"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb8-176"><a href="#cb8-176"></a>:::</span>
<span id="cb8-177"><a href="#cb8-177"></a></span>
<span id="cb8-178"><a href="#cb8-178"></a>We will use a simulation to familiarize ourselves with accept-reject sampling. On the surface below, blue and red points correspond to accept or reject points.</span>
<span id="cb8-179"><a href="#cb8-179"></a></span>
<span id="cb8-180"><a href="#cb8-180"></a>::: {.callout-note title="code"}</span>
<span id="cb8-181"><a href="#cb8-181"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb8-182"><a href="#cb8-182"></a><span class="in">d = 2</span></span>
<span id="cb8-183"><a href="#cb8-183"></a><span class="in">n_items = 800</span></span>
<span id="cb8-184"><a href="#cb8-184"></a><span class="in">items = np.random.randn(n_items, d)*0.5 + np.ones((n_items, d))*0.5</span></span>
<span id="cb8-185"><a href="#cb8-185"></a><span class="in">rewards = ackley(items)</span></span>
<span id="cb8-186"><a href="#cb8-186"></a><span class="in">y = (rewards &gt; rewards.mean())</span></span>
<span id="cb8-187"><a href="#cb8-187"></a><span class="in">draw_surface()</span></span>
<span id="cb8-188"><a href="#cb8-188"></a><span class="in">plt.scatter(items[:, 0], items[:, 1], c=y, cmap='coolwarm', alpha=0.5)</span></span>
<span id="cb8-189"><a href="#cb8-189"></a><span class="in">plt.show()</span></span>
<span id="cb8-190"><a href="#cb8-190"></a><span class="in">```</span></span>
<span id="cb8-191"><a href="#cb8-191"></a>:::</span>
<span id="cb8-192"><a href="#cb8-192"></a></span>
<span id="cb8-195"><a href="#cb8-195"></a><span class="in">```{python}</span></span>
<span id="cb8-196"><a href="#cb8-196"></a>d <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb8-197"><a href="#cb8-197"></a>n_items <span class="op">=</span> <span class="dv">800</span></span>
<span id="cb8-198"><a href="#cb8-198"></a>items <span class="op">=</span> np.random.randn(n_items, d)<span class="op">*</span><span class="fl">0.5</span> <span class="op">+</span> np.ones((n_items, d))<span class="op">*</span><span class="fl">0.5</span></span>
<span id="cb8-199"><a href="#cb8-199"></a>rewards <span class="op">=</span> ackley(items)</span>
<span id="cb8-200"><a href="#cb8-200"></a>y <span class="op">=</span> (rewards <span class="op">&gt;</span> rewards.mean())</span>
<span id="cb8-201"><a href="#cb8-201"></a>draw_surface()</span>
<span id="cb8-202"><a href="#cb8-202"></a>plt.scatter(items[:, <span class="dv">0</span>], items[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">'coolwarm'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-203"><a href="#cb8-203"></a>plt.show()</span>
<span id="cb8-204"><a href="#cb8-204"></a><span class="in">```</span></span>
<span id="cb8-205"><a href="#cb8-205"></a></span>
<span id="cb8-206"><a href="#cb8-206"></a>The binary choice model centers around one item. The model predicts, for that item, after observing user choices in the past, whether that item will be chosen. We use binary variable $y \in <span class="sc">\{</span>0, 1<span class="sc">\}</span>$ to represent whether the user will pick that choice in the next selection phase. We denote $P = p(y = 1)$. We can formally model $y$ as a function of the reward of the positive choice: $y = \mathbb{I}<span class="co">[</span><span class="ot">U&gt;0</span><span class="co">]</span>$. We explore two cases based on the noise distribution. $\psi$ is the logistic function or the standard normal cumulative distribution function if noise follows logistic distribution and the standard normal distribution, respectively:</span>
<span id="cb8-207"><a href="#cb8-207"></a>$$</span>
<span id="cb8-208"><a href="#cb8-208"></a>p(u_{i,j} &gt; 0) = p(u_{i,j}^* + \epsilon &gt; 0) = 1 - p( \epsilon &lt; -u_{i,j}^*) = \psi(u_{i,j}^*).</span>
<span id="cb8-209"><a href="#cb8-209"></a>$$</span>
<span id="cb8-210"><a href="#cb8-210"></a></span>
<span id="cb8-211"><a href="#cb8-211"></a>A generalization of accept-reject sampling is rating-scale sampling. Rating-scale sampling, such as the Likert scale, is a method in which participants rate items on a fixed-point scale (e.g., 1 to 5, "Strongly Disagree" to "Strongly Agree") to measure levels of preference towards items <span class="co">[</span><span class="ot">@harpe2015</span><span class="co">]</span>. Participants can also mark a point on a continuous rating scale to indicate their preference or attitude. Commonly used in surveys, product reviews, and psychological assessments, this method provides a more nuanced measure than discrete scales. Rating-scale sampling is simple for participants to understand and use, provides rich data on the intensity of preferences, and is flexible enough for various measurements (e.g., agreement, satisfaction). However, rating-scale sampling methods also have limitations. Ratings can be influenced by personal biases and interpretations of scales, leading to subjectivity. There is a central tendency bias, where participants may avoid extreme ratings, resulting in clustering responses around the middle. Different participants might interpret scale points differently, and fixed-point scales may not capture the full nuance of participants' preferences or attitudes.</span>
<span id="cb8-212"><a href="#cb8-212"></a></span>
<span id="cb8-213"><a href="#cb8-213"></a>::: {.callout-note title="code"}</span>
<span id="cb8-214"><a href="#cb8-214"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb8-215"><a href="#cb8-215"></a><span class="in">from matplotlib.colors import LinearSegmentedColormap</span></span>
<span id="cb8-216"><a href="#cb8-216"></a><span class="in">likert_cmap = LinearSegmentedColormap.from_list("likert_scale", ["red", "blue"], N=5)</span></span>
<span id="cb8-217"><a href="#cb8-217"></a><span class="in">normalized = (rewards - rewards.min()) / (rewards.max() - rewards.min())</span></span>
<span id="cb8-218"><a href="#cb8-218"></a><span class="in">ratings = np.round(normalized * 4).squeeze()</span></span>
<span id="cb8-219"><a href="#cb8-219"></a></span>
<span id="cb8-220"><a href="#cb8-220"></a><span class="in">draw_surface()</span></span>
<span id="cb8-221"><a href="#cb8-221"></a><span class="in">scatter = plt.scatter(items[:, 0], items[:, 1], c=ratings, cmap=likert_cmap, alpha=0.5)</span></span>
<span id="cb8-222"><a href="#cb8-222"></a><span class="in">plt.show()</span></span>
<span id="cb8-223"><a href="#cb8-223"></a><span class="in">```</span></span>
<span id="cb8-224"><a href="#cb8-224"></a>:::</span>
<span id="cb8-225"><a href="#cb8-225"></a></span>
<span id="cb8-228"><a href="#cb8-228"></a><span class="in">```{python}</span></span>
<span id="cb8-229"><a href="#cb8-229"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> LinearSegmentedColormap</span>
<span id="cb8-230"><a href="#cb8-230"></a>likert_cmap <span class="op">=</span> LinearSegmentedColormap.from_list(<span class="st">"likert_scale"</span>, [<span class="st">"red"</span>, <span class="st">"blue"</span>], N<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb8-231"><a href="#cb8-231"></a>normalized <span class="op">=</span> (rewards <span class="op">-</span> rewards.<span class="bu">min</span>()) <span class="op">/</span> (rewards.<span class="bu">max</span>() <span class="op">-</span> rewards.<span class="bu">min</span>())</span>
<span id="cb8-232"><a href="#cb8-232"></a>ratings <span class="op">=</span> np.<span class="bu">round</span>(normalized <span class="op">*</span> <span class="dv">4</span>).squeeze()</span>
<span id="cb8-233"><a href="#cb8-233"></a></span>
<span id="cb8-234"><a href="#cb8-234"></a>draw_surface()</span>
<span id="cb8-235"><a href="#cb8-235"></a>scatter <span class="op">=</span> plt.scatter(items[:, <span class="dv">0</span>], items[:, <span class="dv">1</span>], c<span class="op">=</span>ratings, cmap<span class="op">=</span>likert_cmap, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-236"><a href="#cb8-236"></a>plt.show()</span>
<span id="cb8-237"><a href="#cb8-237"></a><span class="in">```</span></span>
<span id="cb8-238"><a href="#cb8-238"></a></span>
<span id="cb8-239"><a href="#cb8-239"></a>Suppose we have a single example with attributes $z_i$ and wish to know which of $J$ rating scales an individual will choose from. We can define $J - 1$ parameters, which act as thresholds on the reward computed by $u_i = u_{i,j}^*$ to classify the predicted choice between these items. For example, if there are three predefined items, we can define parameters $a, b \in \mathbb{R}$ such that</span>
<span id="cb8-240"><a href="#cb8-240"></a>$$</span>
<span id="cb8-241"><a href="#cb8-241"></a>y_i =</span>
<span id="cb8-242"><a href="#cb8-242"></a>\begin{cases} </span>
<span id="cb8-243"><a href="#cb8-243"></a>    1 &amp; u &lt; a <span class="sc">\\</span></span>
<span id="cb8-244"><a href="#cb8-244"></a>    2 &amp; a \le u &lt; b <span class="sc">\\</span></span>
<span id="cb8-245"><a href="#cb8-245"></a>    3 &amp; \text{else}</span>
<span id="cb8-246"><a href="#cb8-246"></a>\end{cases}</span>
<span id="cb8-247"><a href="#cb8-247"></a>$$</span>
<span id="cb8-248"><a href="#cb8-248"></a></span>
<span id="cb8-249"><a href="#cb8-249"></a>By assuming the noise distribution to be either logistic or standard normal, we have </span>
<span id="cb8-250"><a href="#cb8-250"></a>$$</span>
<span id="cb8-251"><a href="#cb8-251"></a>\begin{split}</span>
<span id="cb8-252"><a href="#cb8-252"></a>    p(y_i = 1) &amp; = p(u &lt; a) = p(u_{i,j}^* + \epsilon &lt; a) = \psi(a-u_{i,j}^*) <span class="sc">\\</span></span>
<span id="cb8-253"><a href="#cb8-253"></a>    p(y_i = 2) &amp; = p(a \le u &lt; b) = p(a - u_{i,j}^* \le \epsilon &lt; b - u_{i,j}^*) = \psi(b-u_{i,j}^*)  - \psi(u_{i,j}^*-a) <span class="sc">\\</span></span>
<span id="cb8-254"><a href="#cb8-254"></a>    p(y_i = 3) &amp; = p(u &gt; b) = p(u_{i,j}^* + \epsilon &gt; b ) = p( \epsilon &gt; b - u_{i,j}^*) = \psi(b-u_{i,j}^*)</span>
<span id="cb8-255"><a href="#cb8-255"></a>\end{split}</span>
<span id="cb8-256"><a href="#cb8-256"></a>$$</span>
<span id="cb8-257"><a href="#cb8-257"></a></span>
<span id="cb8-258"><a href="#cb8-258"></a>Having the model, we next explore the estimation of model parameters. A common approach for parameter estimation is maximum likelihood <span class="co">[</span><span class="ot">@book_estimation_casella; @book_estimation_bock</span><span class="co">]</span>. The likelihood of a model is the probability of the observed data given the model parameters; intuitively, we wish to maximize this likelihood, as that would mean that our model associates observed human preferences with high probability. Assuming our data is independent and identically distributed (iid), the likelihood over the entire dataset is the joint probability of all observed data as defined by the binary choice model with logistic noise is</span>
<span id="cb8-259"><a href="#cb8-259"></a></span>
<span id="cb8-260"><a href="#cb8-260"></a>$$\mathcal{L}(z, Y; \beta) = \prod_{i = 1}^J p(y = y_i | z_i; \beta) = \prod_{i = 1}^J \frac{1}{1 + \exp^{-u_{i,j}^*}}$$ </span>
<span id="cb8-261"><a href="#cb8-261"></a></span>
<span id="cb8-262"><a href="#cb8-262"></a>This objective can be optimized with a gradient-based method, such as gradient descent <span class="co">[</span><span class="ot">@gradient_descent</span><span class="co">]</span>. Gradient descent operates by computing the gradient of the objective with respect to the parameters of the model, which provides a signal of the direction in which the parameters must move to minimize the objective. Then, SGD makes an update step by subtracting this gradient from the parameters (most often with a scale factor called a learning rate) to move the parameters in a direction that minimizes the objective. In the case of logistic and Gaussian models, SGD may yield a challenging optimization problem as its stochasticity can lead to noisy updates, for example, if certain examples or batches of examples are biased. Mitigations include batched SGD, in which multiple samples are randomly sampled from the dataset at each iteration; learning rates, which reduce the impact of noisy gradient updates, and momentum and higher-order optimizers, which reduce noise by using moving averages of gradients or provide better estimates of the best direction in which to update the gradients.</span>
<span id="cb8-263"><a href="#cb8-263"></a></span>
<span id="cb8-264"><a href="#cb8-264"></a>::: {.callout-note title="code"}</span>
<span id="cb8-265"><a href="#cb8-265"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb8-266"><a href="#cb8-266"></a><span class="in">import numpy as np</span></span>
<span id="cb8-267"><a href="#cb8-267"></a><span class="in">from scipy.optimize import minimize</span></span>
<span id="cb8-268"><a href="#cb8-268"></a><span class="in">from sklearn.metrics import roc_auc_score</span></span>
<span id="cb8-269"><a href="#cb8-269"></a><span class="in">from tqdm import tqdm</span></span>
<span id="cb8-270"><a href="#cb8-270"></a></span>
<span id="cb8-271"><a href="#cb8-271"></a><span class="in"># Set random seed for reproducibility (optional)</span></span>
<span id="cb8-272"><a href="#cb8-272"></a><span class="in">np.random.seed(42)</span></span>
<span id="cb8-273"><a href="#cb8-273"></a></span>
<span id="cb8-274"><a href="#cb8-274"></a><span class="in"># Number of users and items</span></span>
<span id="cb8-275"><a href="#cb8-275"></a><span class="in">num_users = 50</span></span>
<span id="cb8-276"><a href="#cb8-276"></a><span class="in">num_items = 100</span></span>
<span id="cb8-277"><a href="#cb8-277"></a></span>
<span id="cb8-278"><a href="#cb8-278"></a><span class="in"># Generate user-specific and item-specific rewards</span></span>
<span id="cb8-279"><a href="#cb8-279"></a><span class="in">theta_true = np.random.randn(num_users)</span></span>
<span id="cb8-280"><a href="#cb8-280"></a><span class="in">z_true = np.random.randn(num_items)</span></span>
<span id="cb8-281"><a href="#cb8-281"></a></span>
<span id="cb8-282"><a href="#cb8-282"></a><span class="in"># Define the logistic (sigmoid) function</span></span>
<span id="cb8-283"><a href="#cb8-283"></a><span class="in">def sigmoid(x):</span></span>
<span id="cb8-284"><a href="#cb8-284"></a><span class="in">    return 1.0 / (1.0 + np.exp(-x))</span></span>
<span id="cb8-285"><a href="#cb8-285"></a></span>
<span id="cb8-286"><a href="#cb8-286"></a><span class="in"># Generate observed choices using the logistic function</span></span>
<span id="cb8-287"><a href="#cb8-287"></a><span class="in"># Compute probability matrix: shape (num_users, num_items)</span></span>
<span id="cb8-288"><a href="#cb8-288"></a><span class="in">probs = sigmoid(theta_true[:, None] - z_true[None, :])</span></span>
<span id="cb8-289"><a href="#cb8-289"></a><span class="in"># Sample binary responses (0 or 1) from a Bernoulli distribution</span></span>
<span id="cb8-290"><a href="#cb8-290"></a><span class="in">data = np.random.binomial(1, probs)</span></span>
<span id="cb8-291"><a href="#cb8-291"></a></span>
<span id="cb8-292"><a href="#cb8-292"></a><span class="in"># Mask out a fraction of the response matrix (80% observed, 20% missing)</span></span>
<span id="cb8-293"><a href="#cb8-293"></a><span class="in">mask = np.random.rand(num_users, num_items) &gt; 0.2  # boolean mask</span></span>
<span id="cb8-294"><a href="#cb8-294"></a><span class="in"># Create a version of the data with missing values (not needed for optimization, but for reference)</span></span>
<span id="cb8-295"><a href="#cb8-295"></a><span class="in">data_masked = data.copy().astype(float)</span></span>
<span id="cb8-296"><a href="#cb8-296"></a><span class="in">data_masked[~mask] = np.nan</span></span>
<span id="cb8-297"><a href="#cb8-297"></a></span>
<span id="cb8-298"><a href="#cb8-298"></a><span class="in"># Count of observed entries (used for averaging)</span></span>
<span id="cb8-299"><a href="#cb8-299"></a><span class="in">observed_count = np.sum(mask)</span></span>
<span id="cb8-300"><a href="#cb8-300"></a></span>
<span id="cb8-301"><a href="#cb8-301"></a><span class="in"># We will optimize over parameters theta and z.</span></span>
<span id="cb8-302"><a href="#cb8-302"></a><span class="in"># Initialize estimates (random starting points)</span></span>
<span id="cb8-303"><a href="#cb8-303"></a><span class="in">theta_init = np.random.randn(num_users)</span></span>
<span id="cb8-304"><a href="#cb8-304"></a><span class="in">z_init = np.random.randn(num_items)</span></span>
<span id="cb8-305"><a href="#cb8-305"></a></span>
<span id="cb8-306"><a href="#cb8-306"></a><span class="in"># Pack parameters into a single vector for the optimizer.</span></span>
<span id="cb8-307"><a href="#cb8-307"></a><span class="in"># First num_users elements are theta_est, next num_items are z_est.</span></span>
<span id="cb8-308"><a href="#cb8-308"></a><span class="in">params_init = np.concatenate([theta_init, z_init])</span></span>
<span id="cb8-309"><a href="#cb8-309"></a></span>
<span id="cb8-310"><a href="#cb8-310"></a><span class="in">def objective(params):</span></span>
<span id="cb8-311"><a href="#cb8-311"></a><span class="in">    """</span></span>
<span id="cb8-312"><a href="#cb8-312"></a><span class="in">    Computes the loss and gradient for the current parameters.</span></span>
<span id="cb8-313"><a href="#cb8-313"></a><span class="in">    Loss is defined as the negative log likelihood (averaged over observed entries).</span></span>
<span id="cb8-314"><a href="#cb8-314"></a><span class="in">    """</span></span>
<span id="cb8-315"><a href="#cb8-315"></a><span class="in">    # Unpack parameters</span></span>
<span id="cb8-316"><a href="#cb8-316"></a><span class="in">    theta = params[:num_users]</span></span>
<span id="cb8-317"><a href="#cb8-317"></a><span class="in">    z = params[num_users:]</span></span>
<span id="cb8-318"><a href="#cb8-318"></a><span class="in">    </span></span>
<span id="cb8-319"><a href="#cb8-319"></a><span class="in">    # Compute difference and estimated probabilities</span></span>
<span id="cb8-320"><a href="#cb8-320"></a><span class="in">    diff = theta[:, None] - z[None, :]  # shape: (num_users, num_items)</span></span>
<span id="cb8-321"><a href="#cb8-321"></a><span class="in">    sigma = sigmoid(diff)</span></span>
<span id="cb8-322"><a href="#cb8-322"></a><span class="in">    </span></span>
<span id="cb8-323"><a href="#cb8-323"></a><span class="in">    # To avoid log(0), clip probabilities a little bit</span></span>
<span id="cb8-324"><a href="#cb8-324"></a><span class="in">    eps = 1e-8</span></span>
<span id="cb8-325"><a href="#cb8-325"></a><span class="in">    sigma = np.clip(sigma, eps, 1 - eps)</span></span>
<span id="cb8-326"><a href="#cb8-326"></a><span class="in">    </span></span>
<span id="cb8-327"><a href="#cb8-327"></a><span class="in">    # Compute negative log likelihood only on observed entries</span></span>
<span id="cb8-328"><a href="#cb8-328"></a><span class="in">    # For each observed entry: if data == 1 then -log(sigma) else -log(1-sigma)</span></span>
<span id="cb8-329"><a href="#cb8-329"></a><span class="in">    log_likelihood = data * np.log(sigma) + (1 - data) * np.log(1 - sigma)</span></span>
<span id="cb8-330"><a href="#cb8-330"></a><span class="in">    loss = -np.sum(mask * log_likelihood) / observed_count</span></span>
<span id="cb8-331"><a href="#cb8-331"></a><span class="in">    </span></span>
<span id="cb8-332"><a href="#cb8-332"></a><span class="in">    # Compute gradient with respect to the difference x = theta_i - z_j</span></span>
<span id="cb8-333"><a href="#cb8-333"></a><span class="in">    # d(loss)/d(x) = sigma - data  (for observed entries, zero otherwise)</span></span>
<span id="cb8-334"><a href="#cb8-334"></a><span class="in">    diff_grad = (sigma - data) * mask  # shape: (num_users, num_items)</span></span>
<span id="cb8-335"><a href="#cb8-335"></a><span class="in">    </span></span>
<span id="cb8-336"><a href="#cb8-336"></a><span class="in">    # Gradients for theta: sum over items (axis 1)</span></span>
<span id="cb8-337"><a href="#cb8-337"></a><span class="in">    grad_theta = np.sum(diff_grad, axis=1) / observed_count</span></span>
<span id="cb8-338"><a href="#cb8-338"></a><span class="in">    # Gradients for z: negative sum over users (axis 0)</span></span>
<span id="cb8-339"><a href="#cb8-339"></a><span class="in">    grad_z = -np.sum(diff_grad, axis=0) / observed_count</span></span>
<span id="cb8-340"><a href="#cb8-340"></a><span class="in">    </span></span>
<span id="cb8-341"><a href="#cb8-341"></a><span class="in">    # Pack gradients back into a single vector</span></span>
<span id="cb8-342"><a href="#cb8-342"></a><span class="in">    grad = np.concatenate([grad_theta, grad_z])</span></span>
<span id="cb8-343"><a href="#cb8-343"></a><span class="in">    return loss, grad</span></span>
<span id="cb8-344"><a href="#cb8-344"></a></span>
<span id="cb8-345"><a href="#cb8-345"></a><span class="in"># Callback to track progress (optional)</span></span>
<span id="cb8-346"><a href="#cb8-346"></a><span class="in">iteration_progress = tqdm()</span></span>
<span id="cb8-347"><a href="#cb8-347"></a></span>
<span id="cb8-348"><a href="#cb8-348"></a><span class="in">def callback(xk):</span></span>
<span id="cb8-349"><a href="#cb8-349"></a><span class="in">    iteration_progress.update(1)</span></span>
<span id="cb8-350"><a href="#cb8-350"></a></span>
<span id="cb8-351"><a href="#cb8-351"></a><span class="in"># Optimize using L-BFGS-B</span></span>
<span id="cb8-352"><a href="#cb8-352"></a><span class="in">result = minimize(</span></span>
<span id="cb8-353"><a href="#cb8-353"></a><span class="in">    fun=lambda params: objective(params),</span></span>
<span id="cb8-354"><a href="#cb8-354"></a><span class="in">    x0=params_init,</span></span>
<span id="cb8-355"><a href="#cb8-355"></a><span class="in">    method="L-BFGS-B",</span></span>
<span id="cb8-356"><a href="#cb8-356"></a><span class="in">    jac=True,</span></span>
<span id="cb8-357"><a href="#cb8-357"></a><span class="in">    callback=callback,</span></span>
<span id="cb8-358"><a href="#cb8-358"></a><span class="in">    options={"maxiter": 100, "disp": True}</span></span>
<span id="cb8-359"><a href="#cb8-359"></a><span class="in">)</span></span>
<span id="cb8-360"><a href="#cb8-360"></a><span class="in">iteration_progress.close()</span></span>
<span id="cb8-361"><a href="#cb8-361"></a></span>
<span id="cb8-362"><a href="#cb8-362"></a><span class="in"># Extract the estimated parameters</span></span>
<span id="cb8-363"><a href="#cb8-363"></a><span class="in">theta_est = result.x[:num_users]</span></span>
<span id="cb8-364"><a href="#cb8-364"></a><span class="in">z_est = result.x[num_users:]</span></span>
<span id="cb8-365"><a href="#cb8-365"></a></span>
<span id="cb8-366"><a href="#cb8-366"></a><span class="in"># Compute final estimated probabilities</span></span>
<span id="cb8-367"><a href="#cb8-367"></a><span class="in">probs_final = sigmoid(theta_est[:, None] - z_est[None, :])</span></span>
<span id="cb8-368"><a href="#cb8-368"></a></span>
<span id="cb8-369"><a href="#cb8-369"></a><span class="in"># Compute AUC ROC on observed (training) and missing (test) entries</span></span>
<span id="cb8-370"><a href="#cb8-370"></a><span class="in">train_probs = probs_final[mask]</span></span>
<span id="cb8-371"><a href="#cb8-371"></a><span class="in">test_probs = probs_final[~mask]</span></span>
<span id="cb8-372"><a href="#cb8-372"></a><span class="in">train_labels = data[mask]</span></span>
<span id="cb8-373"><a href="#cb8-373"></a><span class="in">test_labels = data[~mask]</span></span>
<span id="cb8-374"><a href="#cb8-374"></a></span>
<span id="cb8-375"><a href="#cb8-375"></a><span class="in">auc_train = roc_auc_score(train_labels, train_probs)</span></span>
<span id="cb8-376"><a href="#cb8-376"></a><span class="in">auc_test = roc_auc_score(test_labels, test_probs)</span></span>
<span id="cb8-377"><a href="#cb8-377"></a></span>
<span id="cb8-378"><a href="#cb8-378"></a><span class="in">print(f"Train AUC: {auc_train:.4f}")</span></span>
<span id="cb8-379"><a href="#cb8-379"></a><span class="in">print(f"Test AUC: {auc_test:.4f}")</span></span>
<span id="cb8-380"><a href="#cb8-380"></a></span>
<span id="cb8-381"><a href="#cb8-381"></a><span class="in">```</span></span>
<span id="cb8-382"><a href="#cb8-382"></a>:::</span>
<span id="cb8-383"><a href="#cb8-383"></a></span>
<span id="cb8-386"><a href="#cb8-386"></a><span class="in">```{python}</span></span>
<span id="cb8-387"><a href="#cb8-387"></a><span class="im">import</span> torch</span>
<span id="cb8-388"><a href="#cb8-388"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb8-389"><a href="#cb8-389"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb8-390"><a href="#cb8-390"></a><span class="im">from</span> torch.distributions <span class="im">import</span> Bernoulli</span>
<span id="cb8-391"><a href="#cb8-391"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb8-392"><a href="#cb8-392"></a></span>
<span id="cb8-393"><a href="#cb8-393"></a><span class="co"># Set device</span></span>
<span id="cb8-394"><a href="#cb8-394"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb8-395"><a href="#cb8-395"></a></span>
<span id="cb8-396"><a href="#cb8-396"></a><span class="co"># Number of users and items</span></span>
<span id="cb8-397"><a href="#cb8-397"></a>num_users <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb8-398"><a href="#cb8-398"></a>num_items <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb8-399"><a href="#cb8-399"></a></span>
<span id="cb8-400"><a href="#cb8-400"></a><span class="co"># Generate user-specific and item-specific rewards</span></span>
<span id="cb8-401"><a href="#cb8-401"></a>theta <span class="op">=</span> torch.randn(num_users, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-402"><a href="#cb8-402"></a>z <span class="op">=</span> torch.randn(num_items, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-403"><a href="#cb8-403"></a></span>
<span id="cb8-404"><a href="#cb8-404"></a><span class="co"># Generate observed choices using logistic function</span></span>
<span id="cb8-405"><a href="#cb8-405"></a>probs <span class="op">=</span> torch.sigmoid(theta[:, <span class="va">None</span>] <span class="op">-</span> z[<span class="va">None</span>, :])</span>
<span id="cb8-406"><a href="#cb8-406"></a>data <span class="op">=</span> Bernoulli(probs<span class="op">=</span>probs).sample()</span>
<span id="cb8-407"><a href="#cb8-407"></a></span>
<span id="cb8-408"><a href="#cb8-408"></a><span class="co"># Mask out a fraction of the response matrix</span></span>
<span id="cb8-409"><a href="#cb8-409"></a>mask <span class="op">=</span> torch.rand_like(data) <span class="op">&gt;</span> <span class="fl">0.2</span>  <span class="co"># 80% observed, 20% missing</span></span>
<span id="cb8-410"><a href="#cb8-410"></a>data_masked <span class="op">=</span> data.clone()</span>
<span id="cb8-411"><a href="#cb8-411"></a>data_masked[<span class="op">~</span>mask] <span class="op">=</span> <span class="bu">float</span>(<span class="st">'nan'</span>)</span>
<span id="cb8-412"><a href="#cb8-412"></a></span>
<span id="cb8-413"><a href="#cb8-413"></a><span class="co"># Initialize parameters for EM algorithm</span></span>
<span id="cb8-414"><a href="#cb8-414"></a>theta_est <span class="op">=</span> torch.randn(num_users, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-415"><a href="#cb8-415"></a>z_est <span class="op">=</span> torch.randn(num_items, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-416"><a href="#cb8-416"></a></span>
<span id="cb8-417"><a href="#cb8-417"></a><span class="co"># Optimizer</span></span>
<span id="cb8-418"><a href="#cb8-418"></a>optimizer <span class="op">=</span> optim.LBFGS([theta_est, z_est], lr<span class="op">=</span><span class="fl">0.1</span>, max_iter<span class="op">=</span><span class="dv">20</span>, history_size<span class="op">=</span><span class="dv">10</span>, line_search_fn<span class="op">=</span><span class="st">"strong_wolfe"</span>)</span>
<span id="cb8-419"><a href="#cb8-419"></a></span>
<span id="cb8-420"><a href="#cb8-420"></a><span class="kw">def</span> closure():</span>
<span id="cb8-421"><a href="#cb8-421"></a>    optimizer.zero_grad()</span>
<span id="cb8-422"><a href="#cb8-422"></a>    probs_est <span class="op">=</span> torch.sigmoid(theta_est[:, <span class="va">None</span>] <span class="op">-</span> z_est[<span class="va">None</span>, :])</span>
<span id="cb8-423"><a href="#cb8-423"></a>    loss <span class="op">=</span> <span class="op">-</span>(Bernoulli(probs<span class="op">=</span>probs_est).log_prob(data) <span class="op">*</span> mask).mean()</span>
<span id="cb8-424"><a href="#cb8-424"></a>    loss.backward()</span>
<span id="cb8-425"><a href="#cb8-425"></a>    <span class="cf">return</span> loss</span>
<span id="cb8-426"><a href="#cb8-426"></a></span>
<span id="cb8-427"><a href="#cb8-427"></a><span class="co"># EM Algorithm</span></span>
<span id="cb8-428"><a href="#cb8-428"></a>pbar <span class="op">=</span> tqdm(<span class="bu">range</span>(<span class="dv">100</span>))</span>
<span id="cb8-429"><a href="#cb8-429"></a><span class="cf">for</span> iteration <span class="kw">in</span> pbar:</span>
<span id="cb8-430"><a href="#cb8-430"></a>    <span class="cf">if</span> iteration <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb8-431"><a href="#cb8-431"></a>        previous_theta <span class="op">=</span> theta_est.clone()</span>
<span id="cb8-432"><a href="#cb8-432"></a>        previous_z <span class="op">=</span> z_est.clone()</span>
<span id="cb8-433"><a href="#cb8-433"></a>        previous_loss <span class="op">=</span> loss.clone()</span>
<span id="cb8-434"><a href="#cb8-434"></a>    </span>
<span id="cb8-435"><a href="#cb8-435"></a>    loss <span class="op">=</span> optimizer.step(closure)</span>
<span id="cb8-436"><a href="#cb8-436"></a>    </span>
<span id="cb8-437"><a href="#cb8-437"></a>    <span class="cf">if</span> iteration <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb8-438"><a href="#cb8-438"></a>        d_loss <span class="op">=</span> (previous_loss <span class="op">-</span> loss).item()</span>
<span id="cb8-439"><a href="#cb8-439"></a>        d_theta <span class="op">=</span> torch.norm(previous_theta <span class="op">-</span> theta_est, p<span class="op">=</span><span class="dv">2</span>).item()</span>
<span id="cb8-440"><a href="#cb8-440"></a>        d_z <span class="op">=</span> torch.norm(previous_z <span class="op">-</span> z_est, p<span class="op">=</span><span class="dv">2</span>).item()</span>
<span id="cb8-441"><a href="#cb8-441"></a>        grad_norm <span class="op">=</span> torch.norm(optimizer.param_groups[<span class="dv">0</span>][<span class="st">"params"</span>][<span class="dv">0</span>].grad, p<span class="op">=</span><span class="dv">2</span>).item()</span>
<span id="cb8-442"><a href="#cb8-442"></a>        grad_norm <span class="op">+=</span> torch.norm(optimizer.param_groups[<span class="dv">0</span>][<span class="st">"params"</span>][<span class="dv">1</span>].grad, p<span class="op">=</span><span class="dv">2</span>).item()</span>
<span id="cb8-443"><a href="#cb8-443"></a>        pbar.set_postfix({<span class="st">"grad_norm"</span>: grad_norm, <span class="st">"d_theta"</span>: d_theta, <span class="st">"d_z"</span>: d_z, <span class="st">"d_loss"</span>: d_loss})</span>
<span id="cb8-444"><a href="#cb8-444"></a>        <span class="cf">if</span> d_loss <span class="op">&lt;</span> <span class="fl">1e-5</span> <span class="kw">and</span> d_theta <span class="op">&lt;</span> <span class="fl">1e-5</span> <span class="kw">and</span> d_z <span class="op">&lt;</span> <span class="fl">1e-5</span> <span class="kw">and</span> grad_norm <span class="op">&lt;</span> <span class="fl">1e-5</span>:</span>
<span id="cb8-445"><a href="#cb8-445"></a>            <span class="cf">break</span></span>
<span id="cb8-446"><a href="#cb8-446"></a></span>
<span id="cb8-447"><a href="#cb8-447"></a><span class="co"># Compute AUC ROC on observed and inferred data</span></span>
<span id="cb8-448"><a href="#cb8-448"></a><span class="im">from</span> torchmetrics <span class="im">import</span> AUROC</span>
<span id="cb8-449"><a href="#cb8-449"></a>auroc <span class="op">=</span> AUROC(task<span class="op">=</span><span class="st">"binary"</span>)</span>
<span id="cb8-450"><a href="#cb8-450"></a>probs_final <span class="op">=</span> torch.sigmoid(theta_est[:, <span class="va">None</span>] <span class="op">-</span> z_est[<span class="va">None</span>, :])</span>
<span id="cb8-451"><a href="#cb8-451"></a>train_probs <span class="op">=</span> probs_final[mask]</span>
<span id="cb8-452"><a href="#cb8-452"></a>test_probs <span class="op">=</span> probs_final[<span class="op">~</span>mask]</span>
<span id="cb8-453"><a href="#cb8-453"></a>train_labels <span class="op">=</span> data[mask]</span>
<span id="cb8-454"><a href="#cb8-454"></a>test_labels <span class="op">=</span> data[<span class="op">~</span>mask]</span>
<span id="cb8-455"><a href="#cb8-455"></a>auc_train <span class="op">=</span> auroc(train_probs, train_labels)</span>
<span id="cb8-456"><a href="#cb8-456"></a>auc_test <span class="op">=</span> auroc(test_probs, test_labels)</span>
<span id="cb8-457"><a href="#cb8-457"></a><span class="bu">print</span>(<span class="ss">f"train auc: </span><span class="sc">{</span>auc_train<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-458"><a href="#cb8-458"></a><span class="bu">print</span>(<span class="ss">f"test auc: </span><span class="sc">{</span>auc_test<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-459"><a href="#cb8-459"></a><span class="in">```</span></span>
<span id="cb8-460"><a href="#cb8-460"></a></span>
<span id="cb8-461"><a href="#cb8-461"></a><span class="fu">### Pairwise Model {#pairwise-model}</span></span>
<span id="cb8-462"><a href="#cb8-462"></a></span>
<span id="cb8-463"><a href="#cb8-463"></a>In *pairwise sampling*, participants compare two items to determine which is preferred. One of the major advantages of this method is the low cognitive demand for raters. Its disadvantage is the limited amount of information content elicited by a sample. Below is a survey based on pairwise sampling:</span>
<span id="cb8-464"><a href="#cb8-464"></a></span>
<span id="cb8-465"><a href="#cb8-465"></a>::: {.content-visible when-format="html"}</span>
<span id="cb8-466"><a href="#cb8-466"></a>&lt;iframe</span>
<span id="cb8-467"><a href="#cb8-467"></a>  src="https://app.opinionx.co/6bef4ca1-82f5-4c1d-8c5a-2274509f22e2"</span>
<span id="cb8-468"><a href="#cb8-468"></a>  style="width:100%; height:450px;"</span>
<span id="cb8-469"><a href="#cb8-469"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb8-470"><a href="#cb8-470"></a>:::</span>
<span id="cb8-471"><a href="#cb8-471"></a></span>
<span id="cb8-472"><a href="#cb8-472"></a>::: {.callout-note title="code"}</span>
<span id="cb8-473"><a href="#cb8-473"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb8-474"><a href="#cb8-474"></a><span class="in">n_pairs = 10000</span></span>
<span id="cb8-475"><a href="#cb8-475"></a><span class="in">pair_indices = np.random.randint(0, n_items, size=(n_pairs, 2))</span></span>
<span id="cb8-476"><a href="#cb8-476"></a><span class="in"># Exclude pairs where both indices are the same</span></span>
<span id="cb8-477"><a href="#cb8-477"></a><span class="in">mask = pair_indices[:, 0] != pair_indices[:, 1]</span></span>
<span id="cb8-478"><a href="#cb8-478"></a><span class="in">pair_indices = pair_indices[mask]</span></span>
<span id="cb8-479"><a href="#cb8-479"></a></span>
<span id="cb8-480"><a href="#cb8-480"></a><span class="in">scores = np.zeros(n_items, dtype=int)</span></span>
<span id="cb8-481"><a href="#cb8-481"></a><span class="in">wins = rewards[pair_indices[:, 0]] &gt; rewards[pair_indices[:, 1]]</span></span>
<span id="cb8-482"><a href="#cb8-482"></a></span>
<span id="cb8-483"><a href="#cb8-483"></a><span class="in"># For pairs where the first item wins:</span></span>
<span id="cb8-484"><a href="#cb8-484"></a><span class="in">#   - Increase score for the first item by 1</span></span>
<span id="cb8-485"><a href="#cb8-485"></a><span class="in">#   - Decrease score for the second item by 1</span></span>
<span id="cb8-486"><a href="#cb8-486"></a><span class="in">np.add.at(scores, pair_indices[wins, 0], 1)</span></span>
<span id="cb8-487"><a href="#cb8-487"></a><span class="in">np.add.at(scores, pair_indices[wins, 1], -1)</span></span>
<span id="cb8-488"><a href="#cb8-488"></a></span>
<span id="cb8-489"><a href="#cb8-489"></a><span class="in"># For pairs where the second item wins or it's a tie:</span></span>
<span id="cb8-490"><a href="#cb8-490"></a><span class="in">#   - Decrease score for the first item by 1</span></span>
<span id="cb8-491"><a href="#cb8-491"></a><span class="in">#   - Increase score for the second item by 1</span></span>
<span id="cb8-492"><a href="#cb8-492"></a><span class="in">np.add.at(scores, pair_indices[~wins, 0], -1)</span></span>
<span id="cb8-493"><a href="#cb8-493"></a><span class="in">np.add.at(scores, pair_indices[~wins, 1], 1)</span></span>
<span id="cb8-494"><a href="#cb8-494"></a></span>
<span id="cb8-495"><a href="#cb8-495"></a><span class="in"># Determine preferred and non-preferred items based on scores</span></span>
<span id="cb8-496"><a href="#cb8-496"></a><span class="in">preferred = scores &gt; 0</span></span>
<span id="cb8-497"><a href="#cb8-497"></a><span class="in">non_preferred = scores &lt; 0</span></span>
<span id="cb8-498"><a href="#cb8-498"></a></span>
<span id="cb8-499"><a href="#cb8-499"></a><span class="in">draw_surface()</span></span>
<span id="cb8-500"><a href="#cb8-500"></a><span class="in">plt.scatter(items[preferred, 0], items[preferred, 1], c='blue', label='Preferred', alpha=0.5)</span></span>
<span id="cb8-501"><a href="#cb8-501"></a><span class="in">plt.scatter(items[non_preferred, 0], items[non_preferred, 1], c='purple', label='Non-preferred', alpha=0.5)</span></span>
<span id="cb8-502"><a href="#cb8-502"></a><span class="in">plt.legend()</span></span>
<span id="cb8-503"><a href="#cb8-503"></a><span class="in">plt.show()</span></span>
<span id="cb8-504"><a href="#cb8-504"></a><span class="in">```</span></span>
<span id="cb8-505"><a href="#cb8-505"></a>:::</span>
<span id="cb8-506"><a href="#cb8-506"></a></span>
<span id="cb8-509"><a href="#cb8-509"></a><span class="in">```{python}</span></span>
<span id="cb8-510"><a href="#cb8-510"></a>n_pairs <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb8-511"><a href="#cb8-511"></a>pair_indices <span class="op">=</span> np.random.randint(<span class="dv">0</span>, n_items, size<span class="op">=</span>(n_pairs, <span class="dv">2</span>))</span>
<span id="cb8-512"><a href="#cb8-512"></a><span class="co"># Exclude pairs where both indices are the same</span></span>
<span id="cb8-513"><a href="#cb8-513"></a>mask <span class="op">=</span> pair_indices[:, <span class="dv">0</span>] <span class="op">!=</span> pair_indices[:, <span class="dv">1</span>]</span>
<span id="cb8-514"><a href="#cb8-514"></a>pair_indices <span class="op">=</span> pair_indices[mask]</span>
<span id="cb8-515"><a href="#cb8-515"></a></span>
<span id="cb8-516"><a href="#cb8-516"></a>scores <span class="op">=</span> np.zeros(n_items, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb8-517"><a href="#cb8-517"></a>wins <span class="op">=</span> rewards[pair_indices[:, <span class="dv">0</span>]] <span class="op">&gt;</span> rewards[pair_indices[:, <span class="dv">1</span>]]</span>
<span id="cb8-518"><a href="#cb8-518"></a></span>
<span id="cb8-519"><a href="#cb8-519"></a><span class="co"># For pairs where the first item wins:</span></span>
<span id="cb8-520"><a href="#cb8-520"></a><span class="co">#   - Increase score for the first item by 1</span></span>
<span id="cb8-521"><a href="#cb8-521"></a><span class="co">#   - Decrease score for the second item by 1</span></span>
<span id="cb8-522"><a href="#cb8-522"></a>np.add.at(scores, pair_indices[wins, <span class="dv">0</span>], <span class="dv">1</span>)</span>
<span id="cb8-523"><a href="#cb8-523"></a>np.add.at(scores, pair_indices[wins, <span class="dv">1</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-524"><a href="#cb8-524"></a></span>
<span id="cb8-525"><a href="#cb8-525"></a><span class="co"># For pairs where the second item wins or it's a tie:</span></span>
<span id="cb8-526"><a href="#cb8-526"></a><span class="co">#   - Decrease score for the first item by 1</span></span>
<span id="cb8-527"><a href="#cb8-527"></a><span class="co">#   - Increase score for the second item by 1</span></span>
<span id="cb8-528"><a href="#cb8-528"></a>np.add.at(scores, pair_indices[<span class="op">~</span>wins, <span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-529"><a href="#cb8-529"></a>np.add.at(scores, pair_indices[<span class="op">~</span>wins, <span class="dv">1</span>], <span class="dv">1</span>)</span>
<span id="cb8-530"><a href="#cb8-530"></a></span>
<span id="cb8-531"><a href="#cb8-531"></a><span class="co"># Determine preferred and non-preferred items based on scores</span></span>
<span id="cb8-532"><a href="#cb8-532"></a>preferred <span class="op">=</span> scores <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb8-533"><a href="#cb8-533"></a>non_preferred <span class="op">=</span> scores <span class="op">&lt;</span> <span class="dv">0</span></span>
<span id="cb8-534"><a href="#cb8-534"></a></span>
<span id="cb8-535"><a href="#cb8-535"></a>draw_surface()</span>
<span id="cb8-536"><a href="#cb8-536"></a>plt.scatter(items[preferred, <span class="dv">0</span>], items[preferred, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Preferred'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-537"><a href="#cb8-537"></a>plt.scatter(items[non_preferred, <span class="dv">0</span>], items[non_preferred, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'purple'</span>, label<span class="op">=</span><span class="st">'Non-preferred'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-538"><a href="#cb8-538"></a>plt.legend()</span>
<span id="cb8-539"><a href="#cb8-539"></a>plt.show()</span>
<span id="cb8-540"><a href="#cb8-540"></a><span class="in">```</span></span>
<span id="cb8-541"><a href="#cb8-541"></a></span>
<span id="cb8-542"><a href="#cb8-542"></a>The Bradley-Terry model compares the reward of choice over all others <span class="co">[</span><span class="ot">@bradley-terry-model</span><span class="co">]</span> in the set of $J$ choices $i \in <span class="sc">\{</span>1, 2, \dots, J<span class="sc">\}</span>$. Each choice can also have its unique random noise variable representing the unobserved factor. However, we can also choose to have all choices' unobserved factors follow the same distribution (e.g., independent and identically distributed, IID). The noise is represented as an extreme value distribution, although we can choose alternatives such as a multivariate Gaussian distribution: $\epsilon \sim \mathcal{N}(0, \Sigma)$. If $\Sigma$ is not a diagonal matrix, we effectively model correlations in the noise across choices, enabling us to avoid the IID assumption. In the case of the extreme value distribution, we model the probability of a user preferring choice $i$, which we denote as $P_i = Z^{-1}\exp(u_{i,j}^*)$ where $Z = \sum_{j = 1}^{J} \exp(u_{i,j}^*)$.</span>
<span id="cb8-543"><a href="#cb8-543"></a></span>
<span id="cb8-544"><a href="#cb8-544"></a>We can model an open-ended ranking of the available items with the Plackett-Luce model, in which we jointly model the full sequence of choice ordering <span class="co">[</span><span class="ot">@plackett_luce</span><span class="co">]</span>. The general form models the joint distribution as the product of conditional probabilities, where each is conditioned on the preceding ranking terms. Given an ordering of $J$ choices $<span class="sc">\{</span>y_1, \dots, y_J<span class="sc">\}</span>$, we factorize the joint probability into conditionals. Each conditional follows the Bradley-Terry model:</span>
<span id="cb8-545"><a href="#cb8-545"></a>$$</span>
<span id="cb8-546"><a href="#cb8-546"></a>p(y_1, \dots, y_J) = p(y_1) p(y_2 | y_1) ... p(y_J | y_{1:{J - 1}}) = \prod_{i = 1}^J \frac{\exp(u_{i,j}^*)}{\sum_{j \ge i} \exp(u_{i,j}^*)}</span>
<span id="cb8-547"><a href="#cb8-547"></a>$$</span>
<span id="cb8-548"><a href="#cb8-548"></a></span>
<span id="cb8-549"><a href="#cb8-549"></a>Pairwise sampling has proven useful in aligning large language models (LLM) with human preference. An LLM, such as GPT-4, Llama 3.2, and BERT, typically refers to a large and pre-trained neural network that serves as the basis for various downstream tasks. They are pre-trained on a massive corpus of text data, learning to understand language and context. They are capable of multiple language-related tasks such as text classification, language generation, and question answering. A LLM should be aligned to respond correctly based on human preferences. A promising approach is to train LLMs using reinforcement learning (RL) with the reward model (RM) learned from human preference data, providing a mechanism to score the quality of the generated text. This approach, known as RL from human feedback (RLHF), leverages human feedback to guide model training, allowing LLMs to better align with human expectations while continuously improving performance.</span>
<span id="cb8-550"><a href="#cb8-550"></a></span>
<span id="cb8-551"><a href="#cb8-551"></a>We discuss the reward model used in the Llama2 model. The Llama2 RM <span class="co">[</span><span class="ot">@2307.09288</span><span class="co">]</span> is initialized from the pretrained Llama2 LLM. In the LLM, the last layer is a mapping $L: \mathbb{R}^D \rightarrow \mathbb{R}^V$, where $D$ is the embedding dimension from the transformer decoder stack and $V$ is the vocabulary size. To get the RM, we replace that last layer with a randomly initialized scalar head that maps $L: \mathbb{R}^D \rightarrow \mathbb{R}^1$. It's important to initialize the RM from the LLM it's meant to evaluate. The RM will have the same "knowledge" as the LLM. This is particularly useful for evaluation objectives such as "Does the LLM know when it doesn't know?". However, in cases where the RM is simply evaluating helpfulness or factuality, it may be helpful to have the RM know more. In addition, the RM is on distribution for the LLM - it is initialized in a way where it semantically understands the LLM's outputs. An RM is trained with paired preferences (prompt history, accepted response, rejected response). Prompt history is a multiturn history of user prompts and model generations; the accepted response is the preferred final model generation by an annotator, and the rejected response is the unpreferred response. The RM is trained with maximum likelihood under the Bradley-Terry model with an optional margin term m(r):</span>
<span id="cb8-552"><a href="#cb8-552"></a></span>
<span id="cb8-553"><a href="#cb8-553"></a>$$p(y_c \succ y_r | x) = \sigma(r_\theta(x,y_c) - r_\theta(x,y_r) - m(r))$$ </span>
<span id="cb8-554"><a href="#cb8-554"></a></span>
<span id="cb8-555"><a href="#cb8-555"></a>The margin term increases the distance in scores specifically for preference pairs annotators rate as easier to separate. Margins were designed primarily based on the sigmoid function, which is used to normalize the raw reward model score flattens out beyond the range of $<span class="co">[</span><span class="ot">-4, 4</span><span class="co">]</span>$. Thus, the maximum possible margin is eight. A small regularization term is often added to center the score distribution on 0. We consider two variants of preference rating-based margin. When the preference rating-based margin is small, outcomes are rated as "Significantly Better" (1), "Better" (2 out of 3), and "Slightly Better" (1 out of 3), and "Negligibly Better or Unsure" (0 out of 3). In contrast, when the margin is large, outcomes are rated as "Significantly Better" (3), "Better" (2), and "Slightly Better" (1), and "Negligibly Better or Unsure" (0 out of 3).</span>
<span id="cb8-556"><a href="#cb8-556"></a></span>
<span id="cb8-557"><a href="#cb8-557"></a><span class="fu">### List-wise Model {#list-wise-model}</span></span>
<span id="cb8-558"><a href="#cb8-558"></a>*Multiple-choice sampling* involves participants selecting one item from a set of alternatives. Multiple-choice sampling is simple for participants to understand and reflect on realistic decision-making scenarios where individuals choose one item from many. It is beneficial in complex choice scenarios, such as modes of transportation, where choices are not independent [@bolt2009]. Multiple-choice sampling often relies on simplistic assumptions such as the independence of irrelevant alternatives (IIA), which may not always hold true. This method may also fail to capture the variation in preferences among different individuals, as it typically records only the most preferred choice without accounting for the relative importance of other items. In *rank-order sampling*, participants rank items from most to least preferred. Used in voting, market research, and psychology, it provides rich preference data but is more complex and cognitively demanding than pairwise comparisons, especially for large item sets. Participants may also rank inconsistently [@ragain2019]. *In Best-worst scaling* (BWS), participants are presented with items and asked to identify the most and least preferred items. The primary objective of BWS is to discern the relative importance or preference of items, making it widely applicable in various fields such as market research, health economics, and social sciences <span class="co">[</span><span class="ot">@campbell2015</span><span class="co">]</span>. BWS provides rich data on the relative importance of items, helps clarify preferences, reduces biases found in traditional rating scales, and results in rewards that are easy to interpret. However, BWS also has limitations, including potential scale interpretation differences among participants and design challenges to avoid biases, such as the order effect or the context in which items are presented.</span>
<span id="cb8-559"><a href="#cb8-559"></a></span>
<span id="cb8-560"><a href="#cb8-560"></a><span class="fu">## The Utility Function Class {#function-class}</span></span>
<span id="cb8-561"><a href="#cb8-561"></a></span>
<span id="cb8-562"><a href="#cb8-562"></a><span class="fu">### Parametric and Nonparametric Function Class</span></span>
<span id="cb8-563"><a href="#cb8-563"></a>The reward of the item can take parametric form, such as $z_j = f_{\theta}(x_j)$. It can also take the nonparametric form, which is commonly used in the ideal point model, where the reward of an item $j$ is calculated by the distance from the item to the human in some embedding space<span class="co">[</span><span class="ot">@huber1976ideal</span><span class="co">]</span>. Given vector representation $e_i$ of choice $i$ and a vector $v_n$ representing an individual $n$, we can use a distance function $K$ to model a stochastic reward function with the unobserved factors following a specified distribution: $u_{n, i} = K(e_i, v_n) + \epsilon_{n, i}$. The intuition is that vectors exist in a shared $n$-dimensional space, and as such, we can use geometry to match choices whose representations are closest to that of a given individual <span class="co">[</span><span class="ot">@ideal_point; @tatli2022distancepreferences</span><span class="co">]</span> when equipped with a distance metric. Certain distance metrics, such as Euclidian distance or inner product, can easily be biased by the scale of vectors. A distance measure such as cosine similarity, which compensates for scale by normalizing the inner product of two vectors by the product of their magnitudes, can mitigate this bias yet may discard valuable information encoded by the length of the vectors. Beyond the distance metric alone, this model places a strong inductive bias that the individual and choice representations share a common embedding space. In some contexts, this can be a robust bias to add to the model <span class="co">[</span><span class="ot">@idealpoints</span><span class="co">]</span>, but it is a key factor one must consider before employing such a model, and it is a key design choice for modeling.</span>
<span id="cb8-564"><a href="#cb8-564"></a></span>
<span id="cb8-565"><a href="#cb8-565"></a><span class="fu">### Unimodal and Multimodal Function Class</span></span>
<span id="cb8-566"><a href="#cb8-566"></a>So far, we have considered learning from data from one person with a particular set of preferences or a group with similar preferences, but this is not always the case. Consider a scenario where a user turns left at an intersection <span class="co">[</span><span class="ot">@myers2021learning</span><span class="co">]</span>. What would they do if they saw a car speeding down the road approaching them? Following a timid driving pattern, some vehicles would stop to let the other car go, preventing a collision. Other vehicles would be more aggressive and try to make the turn before colliding with the oncoming vehicle. Given the data of one of these driving patterns, the model can make an appropriate decision. However, what if the model was given data from both aggressive and timid drivers and does not know which data corresponds to which type of driver? A naive preference learning approach would result in a model trying to find a policy close enough to both driving patterns. The group label is often unobserved because it is expensive to obtain or a data point cannot be cleanly separated into any group (e.g., a more timid driver can be aggressive when they are in a hurry).</span>
<span id="cb8-567"><a href="#cb8-567"></a></span>
<span id="cb8-568"><a href="#cb8-568"></a>@myers2022learning formulates this problem as learning a mixture of $M$ linear reward functions on the embedding space, where $M$ is given. The reward of item $j$ given by the expert $i$ is given by: $f_i(e_j) = w^\top_i e_j,$ where $w_m$ is a vector of parameters corresponding to the $m$-th expert's preferences. An unknown distribution over the reward parameters exists, and we can represent this distribution with convex mixing coefficients $\alpha = <span class="co">[</span><span class="ot">\alpha_1, ..., \alpha_M</span><span class="co">]</span>$. Consider a robot that performs the following trajectories and asks a user to rank all the trajectories. The robot will be given back a set of trajectory rankings from M humans, and the objective is to learn the underlying reward function. Given the ranking $(j_1 \succ ... \succ j_K | m)$ of expert $m$ and define $\theta = <span class="sc">\{</span>w_{1:M}, \alpha_{1:M}<span class="sc">\}</span>$, the probability of item $j$ being preferred by $m$ over all other alternatives is </span>
<span id="cb8-569"><a href="#cb8-569"></a></span>
<span id="cb8-570"><a href="#cb8-570"></a>$$p(j_1 \succ ... \succ j_K | \theta) = \sum_{i = 1}^M \alpha_i \prod_{j = 1}^K  p_{ij}$$</span>
<span id="cb8-571"><a href="#cb8-571"></a></span>
<span id="cb8-572"><a href="#cb8-572"></a>Then the parameters posterior is $p(\theta | Q_{1:T}, x_{1:T}) \propto p(\theta) \prod_t p(x_t | Q_{\leq t}, \theta) = p(\theta) \prod_t p(x_t | \theta, Q_t)$. The first proportionality is from the Bayes rule and the assumption that the queries at timestamp $t$ are conditionally independent of the parameters given history. This assumption is reasonable because the previous queries &amp; rankings ideally give all the information to inform the choice of the next set. The last proportionality term comes from the assumption that the ranked queries are conditionally independent given the parameters. The prior distribution is dependent on the use case. For example, in the user studies conducted by the authors to verify this method, they use a standard Gaussian for the reward weights and the mixing coefficients to be uniform on a $M - 1$ simplex to ensure that they add up to 1. Then, we can use maximum likelihood estimation to compute the parameters with the simplified posterior.</span>
<span id="cb8-573"><a href="#cb8-573"></a></span>
<span id="cb8-574"><a href="#cb8-574"></a>Another example setting multimodal preference is negotiations <span class="co">[</span><span class="ot">@kwon2021targeted</span><span class="co">]</span>. Let's say there are some shared items and two people with different utilities and desires for items, where each person only knows their utility. In a specific case of @fig-negotiation, Bob as a proposing agent and Alice as a controlled agent who has many different ways of responding to Bob's proposals. Different methods can be used to design Alice as an AI agent. The first idea is reinforcement learning, where multiple rounds of negotiations are done, the model simulates game theory and sees how Bob reacts. Authors of this setting <span class="co">[</span><span class="ot">@kwon2021targeted</span><span class="co">]</span> show that over time the model learns to ask for the same thing over and over again, as Alice is not trained to be human-like or negotiable, and just tries to maximize Alice's utility. The second approach is supervised learning, where the model can be trained on some dataset, learning the history of negotiations. This results in Alice being very agreeable, which demonstrates two polar results of the two approaches, and it would be ideal to find a middle ground and combine both of them. The authors proposed the Targeted acquisition approach, which is based on active learning ideas. The model asks diverse questions at different cases and stages of negotiations like humans, determining which questions are more valuable to be asked throughout learning. Such an approach ended up in more fair and optimal results than supervised or reinforcement learning <span class="co">[</span><span class="ot">@kwon2021targeted</span><span class="co">]</span>.</span>
<span id="cb8-575"><a href="#cb8-575"></a></span>
<span id="cb8-576"><a href="#cb8-576"></a><span class="fu">### Single Objective and Multi-Objective Utility</span></span>
<span id="cb8-577"><a href="#cb8-577"></a>The industry has centered around optimizing for two primary reward signals: helpfulness and harmlessness (safety). There are also other axes, such as factuality, reasoning, tool use, code, and multilingualism, but these are out of scope for us. The Llama2 paper collected preference data from humans for each quality, with separate guidelines. This presents a challenge for co-optimizing the final LLM towards both goals. Two main approaches can be taken for RLHF in this context. Train a unified reward model that integrates both datasets or train two separate reward models, one for each quality, and optimize the LLM toward both. Option 1 is difficult because of the tension between helpfulness and harmlessness. They trade off against each other, confusing an RM trained in both. The chosen solution was item 2, where two RMs are used to train the LLM piecewise. The helpfulness RM is used as the primary optimization term, while the harmlessness RM acts as a penalty term, driving the behavior of the LLM away from unsafe territory only when the LLM veers beyond a certain threshold. This is formalized as follows, where $R_s$, $R_h$, and $R_c$ are the safety, helpfulness, and combined reward, respectively. $g$ and $p$ are the model generation and the user prompt:</span>
<span id="cb8-578"><a href="#cb8-578"></a></span>
<span id="cb8-579"><a href="#cb8-579"></a>$$</span>
<span id="cb8-580"><a href="#cb8-580"></a>\begin{aligned}</span>
<span id="cb8-581"><a href="#cb8-581"></a>    R_c(g \mid p) =</span>
<span id="cb8-582"><a href="#cb8-582"></a>    \begin{cases}</span>
<span id="cb8-583"><a href="#cb8-583"></a>        R_s(g \mid p) &amp; \text{if } \text{is<span class="sc">\_</span>safety}(p) \text{ or } R_s(g \mid p) &lt; 0.15 <span class="sc">\\</span></span>
<span id="cb8-584"><a href="#cb8-584"></a>        R_h(g \mid p) &amp; \text{otherwise}</span>
<span id="cb8-585"><a href="#cb8-585"></a>    \end{cases}</span>
<span id="cb8-586"><a href="#cb8-586"></a>\end{aligned}</span>
<span id="cb8-587"><a href="#cb8-587"></a>$$</span>
<span id="cb8-588"><a href="#cb8-588"></a></span>
<span id="cb8-589"><a href="#cb8-589"></a><span class="fu">### Pretraining</span></span>
<span id="cb8-590"><a href="#cb8-590"></a><span class="co">&lt;!--</span></span>
<span id="cb8-591"><a href="#cb8-591"></a><span class="co">In the context of robotics, a very compelling answer is the *cost of data collection*. In a hypothetical world where we have a vast number of expert demonstrations of robots accomplishing many diverse tasks, we don't necessarily need to worry about learning from trials or from humans. We could simply learn a competent imitation agent to perform any task. Natural Language Processing could be seen as living in this world because internet-scale data is available. Robots, however, are expensive, so people generally don't have access to them and, therefore cannot use them to produce information to imitate. Similarly, human time is expensive, so even for large organizations with access to many robots, it's still hard to collect a lot of expert demonstrations. The most extensive available collection of robotics datasets today is the Open X-Embodiment [@padalkar2023open], which consists of around 1M episodes from more than 300 different scenes. Even such large datasets are not enough to learn generally capable robotic policies from imitation learning alone.</span></span>
<span id="cb8-592"><a href="#cb8-592"></a><span class="co">--&gt;</span></span>
<span id="cb8-593"><a href="#cb8-593"></a></span>
<span id="cb8-594"><a href="#cb8-594"></a>RL often stumbles when it comes to devising reward functions aligning with human intentions. Preference-based RL aims to solve this by learning from human feedback, but this often demands a *highly impractical number of queries* or leads to oversimplified reward functions that don't hold up in real-world tasks. As discussed in the previous section, one may apply meta-learning so that the RL agent can adapt to new tasks with fewer human queries to address the impractical requirement of human queries. <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span> proposes pre-training models on previous tasks with the meta-learning method MAML <span class="co">[</span><span class="ot">@finn2017model</span><span class="co">]</span>, and then the meta-trained model can adapt to new tasks with fewer queries. We consider settings where a state is denoted as $s\in S$, and action is denoted as $a\in A$, for state space $S$ and action space $A$. The reward function $r: S\times A \to \mathbb{R}$ is unknown and needs to be learned from eliciting human preferences. There are multiple tasks, each with its own reward function and transition probabilities. The reward model is parameterized by $\psi$. We denote $\hat{r}_\psi(s, a)$ to be a learned estimate of an unknown ground-truth reward function $r(s, a)$, parameterized by $\psi$. Accordingly, a reward model determines an RL policy $\phi$ by maximizing the accumulated rewards. The preferences is learned via pair. For each pre-training task, there is a dataset $D$ consists of binary preference between pair of trajectory. Bradley-Terry model is used to predict the preferred trajectory. </span>
<span id="cb8-595"><a href="#cb8-595"></a></span>
<span id="cb8-596"><a href="#cb8-596"></a>To efficiently approximate the reward function $r_\text{new}$ for a new task with minimal queries, @hejna2023few utilizes a pre-trained reward function $\hat{r}_\psi$ that can be quickly fine-tuned using just a few preference comparisons by leveraging the common structure across tasks by pre-training on data from prior tasks. Although any meta-learning method is compatible, [@hejna2023few] opts for Model Agnostic Meta-Learning (MAML) due to its simplicity. With the aforementioned pre-training with meta learning, the meta-learned reward model can then be used for few-shot preference-based RL during an online adaptation phase. Given a pre-trained reward model $\psi$, the the active few-shot adaption iterates between finding informative pair of trajectory to query human and update reward model and corresponding policy with new data. Informative pair is selected using the disagreement of an ensemble of reward functions over the preference predictors. Specifically, comparisons that maximize $\mathbb{V}(p(e_j \succ e_{j'}))$ are selected each time feedback is collected.</span>
<span id="cb8-597"><a href="#cb8-597"></a></span>
<span id="cb8-598"><a href="#cb8-598"></a>The experiment tests the proposed method on the Meta-World benchmark <span class="co">[</span><span class="ot">@yu2020meta</span><span class="co">]</span>. Three baselines compared with the proposed method are (1) Soft-Actor Critic (SAC) trained from ground truth rewards, representing performance upper bound, PEBBLE <span class="co">[</span><span class="ot">@lee2021pebble</span><span class="co">]</span>, which does not use information from prior tasks, and (3) Init, which initializes the reward model with the pretrained weights from meta learning but instead of adapting the reward model to the new task, it performs standard updates as in PEBBLE. The results show that the proposed method outperforms all of the baseline methods. There are still some drawbacks. For example, many of the queries the model picks to elicit human preference are almost identical. Moreover, despite the improved query complexity, an impractical number of queries still need to be made. In addition, it is mentioned in the paper that the proposed method may be even worse than training from scratch if the new task is too out-of-distribution. Designing a method that automatically balances between using the prior information or training from scratch is an important future direction.</span>
<span id="cb8-599"><a href="#cb8-599"></a></span>
<span id="cb8-600"><a href="#cb8-600"></a>@zhou2019watch studies a related problem by asking the question, "How can we efficiently learn both from expert demonstrations and from trials where we only get binary feedback from a human?" This paper seeks to learn new tasks with the following general problem setting: We only get one expert demonstration of the target task; after seeing the expert demonstration, robots try to solve the task 1 or more times; then the user (or some pre-defined reward function) annotates each trial as a success/failure; the agent learns from both the demos and the annotated trials to perform well on the target task. A task $i$ is described by the tuple $<span class="sc">\{</span>S, A, r_i, P_i<span class="sc">\}</span>$. $S$ and $A$ represents all possible states and action, respectively. $r_i$ is the reward function $r_i : S \times A \to \mathbb{R}$, and $P_i$ is the transition dynamics function. $S$ and $A$ are shared across tasks. Learning occurs in 3 phases. During the watch phase, we give the agent $K=1$ demonstrations of the target tasks and all demonstrations are successful. In the Try phase, we use the agent learned during the Watch phase to attempt the task for $L$ trials. After the agent completes the trials, humans (or pre-programmed reward functions) provide one binary reward for each trial, indicating whether the trial was successful. The expected output of this phase is $L$ trajectories and corresponding feedback. After completing the trials, the agent must learn from both the original expert demonstrations and the trials to solve the target task.</span>
<span id="cb8-601"><a href="#cb8-601"></a></span>
<span id="cb8-602"><a href="#cb8-602"></a>First, we are given a dataset of expert demonstrations containing multiple demos for each task and the dataset contains hundreds of tasks. Importantly, no online interaction is needed for training, and this method trains only with supervised learning. This section describes how this paper trains an agent from the given expert demonstrations, and how to incorporate the trials and human feedback into the loop. What we want to obtain out of the Watch phase is a policy conditioned on a set of expert demonstrations via meta-imitation learning. Given the demonstrations $<span class="sc">\{</span>d_{i,k}<span class="sc">\}</span>$ for task $i$, we sample another different demonstration coming from the same task $d_i^{\text{test}}$, where $d_i^{\text{test}}$ is an example of optimal behavior given the demonstrations. The policy is obtained by imitating actions taken on $d_i^{\text{test}}$ via maximum likelihood:</span>
<span id="cb8-603"><a href="#cb8-603"></a></span>
<span id="cb8-604"><a href="#cb8-604"></a>$$\mathcal{L}^\text{watch}(\theta, \mathcal{D}_i^*) = \mathbb{E}_{\{d_{i,k}\} \sim \mathcal{D}_i^*} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^*  \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \log \pi_\theta^{\text{watch}} (a_t | s_t, \{d_{i,k}<span class="sc">\}</span>)$$</span>
<span id="cb8-605"><a href="#cb8-605"></a></span>
<span id="cb8-606"><a href="#cb8-606"></a>This corresponds to imitation learning by minimizing the negative log-likelihood of the test trajectory actions, conditioning the policy on the entire demo set. However, how is the conditioning on the demo set achieved? In addition to using features obtained from the images of the current state, the architecture uses features from frames sampled (in order) from the demonstration episodes, which are concatenated together. On the Try phase when the agent is given a set of demonstrations $<span class="sc">\{</span>d_{i,k}<span class="sc">\}</span>$, we deploy the policy $\pi_\theta^{\text{watch}}(a | s, <span class="sc">\{</span>d_{i,k}<span class="sc">\}</span>)$ to collect $L$ trials. There is no training involved in the Try phase; we simply condition the policy on the given demonstrations. During the Watch phase, the objective was to train a policy conditioned on demonstrations $\pi_\theta^{\text{watch}}(a | s, <span class="sc">\{</span>d_{i,k}<span class="sc">\}</span>)$. The authors of Watch, Try, Learn uses a similar strategy as the Watch phase for the Learn phase. We now want to train a policy that is conditioned on the demonstrations, as well as the trials and binary feedback. We want to learn $\pi_\phi^{\text{watch}}(a | s, <span class="sc">\{</span>d_{i,k}<span class="sc">\}</span>, <span class="sc">\{</span>\mathbf{\tau}_{i, l}<span class="sc">\}</span>)$. To train the policy, we again use meta-imitation learning, where we additionally sample yet another trajectory from the same task. Concretely, we train policy parameters $\phi$ to minimize the following loss:</span>
<span id="cb8-607"><a href="#cb8-607"></a>$$\mathcal{L}^{\text{learn}}(\phi, \mathcal{D}_i, \mathcal{D}_i^*) = \mathbb{E}_{(\{d_{i,k}\}, \{\mathbf{\tau}_{i,l}\}) \sim \mathcal{D}_i} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^* \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[- \log \pi_\theta^{\text{learn}} (a_t | s_t, \{d_{i,k}\}, \{\tau_{i,l}<span class="sc">\}</span>) \big]$$</span>
<span id="cb8-608"><a href="#cb8-608"></a></span>
<span id="cb8-609"><a href="#cb8-609"></a>Three baselines are considered: (1) behavior cloning is simple imitation learning based on maximum log-likelihood training using data from all tasks, (2) meta-imitation learning corresponds to simply running the policy from the Watch step without using any trial data. We only condition on the set of expert demonstrations, but no online trials, and (3) behavior cloning + SAC pretrains a policy with behavior cloning on all data, and follow that with RL fine-tuning for the specific target task, using the maximum-entropy algorithm SAC <span class="co">[</span><span class="ot">@haarnoja2018soft</span><span class="co">]</span>. The proposed approach significantly outperforms baselines on every task family: it is far superior to behavior cloning and it significantly surpasses Meta-Imitation Learning on 3 out of 4 task families.</span>
<span id="cb8-610"><a href="#cb8-610"></a></span>
<span id="cb8-611"><a href="#cb8-611"></a><span class="fu">### Others Consideration</span></span>
<span id="cb8-612"><a href="#cb8-612"></a>One key challenge is managing the bias and variance trade-off. Bias refers to assumptions made during model design and training that can skew predictions. For example, in Ideal Point Models, we make the assumption that the representations we use for individuals and choices are aligned in the embedding space and that this representation is sufficient to capture human preferences using distance metrics. However, there are myriad cases in which this may break down, for example, if the two sets of vectors follow different distributions, each with their own unique biases. If the representations do not come from the same domain, one may have little visibility into how a distance metric computes the final reward value for a choice for a given individual. Some ways to mitigate bias in human preference models include increasing the number of parameters in a model (allowing for better learning of patterns in the data) or removing inductive biases based on our assumptions of the underlying data. On the other hand, variance refers to the model's sensitivity to small changes in the input, which leads to significant changes in the output. This phenomenon is often termed 'overfitting' or 'overparameterization.' This behavior can occur in models that have many parameters and learn correlations in the data that do not contribute to learning human preferences but are artifacts of noise in the dataset that one should ultimately ignore. One can address variance in models by reducing the number of parameters or incorporating biases in the model based on factors we can assume about the data.</span>
<span id="cb8-613"><a href="#cb8-613"></a></span>
<span id="cb8-614"><a href="#cb8-614"></a>Another important consideration unique to human preference models is that we wish to model individual preferences, and we may choose to do so at arbitrary granularity. For example, we can fit models to a specific individual or even multiple models for an individual, each for different purposes or contexts. On the other end of the spectrum, we may create a model to capture human preferences across large populations or the world. Individual models may prove to be more powerful, as they do not need to generalize across multiple individuals and can dedicate all of their parameters to learning the preferences of a single user. In the context of human behavior, this can be a significant advantage as any two individuals can be arbitrarily different or even opposite in their preferences. On the other hand, models that fit only one person can tremendously overfit the training distribution and capture noise in the data, which is not truly representative of human preferences. On the end of the spectrum, models fit to the entire world may be inadequate to model human preferences for arbitrary individuals, especially those whose data it has not been fit to. As such, models may underfit the given training distribution. These models aim to generalize to many people but may fail to capture the nuances of individual preferences, especially for those whose data is not represented in the training set. As a result, they may not perform well for arbitrary individuals within the target population. Choosing the appropriate scope for a model is crucial. It must balance the trade-off between overfitting to noise in highly granular models and underfitting in broader models that may not capture individual nuances.</span>
<span id="cb8-615"><a href="#cb8-615"></a></span>
<span id="cb8-616"><a href="#cb8-616"></a>When training or using a reward model, LLM Distribution Shift is an important factor to consider. With each finetune of the LLM, the RM should be updated through a collection of fresh human preferences using generations from the new LLM. This ensures that the RM stays aligned with the current distribution of the LLM and avoids drifting off-distribution. In addition, RM and LLM are coupled: An RM is generally optimized to distinguish human preferences more efficiently within the specific distribution of the LLM to be optimized. However, this specialization poses a challenge: such an RM will underperform when dealing with generations not aligned with this specific LLM distribution, such as generations from a completely different LLM. Last but not least, training RMs can be unstable and prone to overfitting, especially with multiple training epochs. It's generally advisable to limit the number of epochs during RM training to avoid this issue.</span>
<span id="cb8-617"><a href="#cb8-617"></a></span>
<span id="cb8-618"><a href="#cb8-618"></a><span class="fu">## Exercises</span></span>
<span id="cb8-619"><a href="#cb8-619"></a><span class="fu">### Question 1: Choice Modeling (15 points) {#question-1-choice-modeling-15-points .unnumbered}</span></span>
<span id="cb8-620"><a href="#cb8-620"></a></span>
<span id="cb8-621"><a href="#cb8-621"></a>We discussed discrete choice modeling in the context of reward being a linear function. Suppose we are deciding between $N$ choices and that the reward for each choice is given by $U_i=\beta_i\mathbf{x}+\epsilon_i$ for $i=1, 2, \cdots, N$. We view $\mathbf{x}$ as the data point that is being conditioned on for deciding which choice to select, and $\beta_i$ as the weights driving the linear reward model. The noise $\epsilon_i$ is i.i.d. sampled from a type of extreme value distribution called the *Gumbel* distribution. The standard Gumbel distribution is given by the density function $f(x)=e^{-(x+e^{-x})}$ and cumulative distribution function $F(x)=e^{-e^{-x}}.$ Fix $i$. Our objective is to calculate $p(U_i\,\, \text{has max reward})$.</span>
<span id="cb8-622"><a href="#cb8-622"></a></span>
<span id="cb8-623"><a href="#cb8-623"></a>(a) **(Written, 2 points)**. Set $U_i=t$ and compute $p(U_j&lt;t)$ for $j\neq i$ in terms of $F$. Use this probability to derive an integral for $p(U_i\,\,  \text{has max reward})$ over $t$ in terms of $f$ and $F$. Example of solution environment.</span>
<span id="cb8-624"><a href="#cb8-624"></a></span>
<span id="cb8-625"><a href="#cb8-625"></a>(b) **(Written, 4 points)**. Compute the integral derived in part (a) with the appropriate $u$-substitution. You should arrive at multi-class logistic regression!</span>
<span id="cb8-626"><a href="#cb8-626"></a></span>
<span id="cb8-627"><a href="#cb8-627"></a>Next, you will implement logistic regression to predict preferred completions. We will use the preference dataset from <span class="co">[</span><span class="ot">RewardBench</span><span class="co">](&lt;https://huggingface.co/datasets/allenai/reward-bench&gt;)</span>. Notice the provided <span class="in">`data/chosen_embeddings.pt`</span> and <span class="in">`data/rejected_embeddings.pt`</span> files. These files were constructed by feeding the prompt alongside the chosen/rejected responses through Llama3-8B-Instruct and selecting the last token's final hidden embedding. Let $e_1$ and $e_2$ be two hidden embeddings with $e_1\succ e_2$. We assume reward is a linear function of embedding $u_j=w^\top e_j$ and use the Bradley-Terry model to predict the preferred item. We can view maximum likelihood across the preference dataset with this model as logistic regression on $e_1-e_2$ and all labels being $1$. Here, we are given a dataset $X$ with $N$ rows of datapoints and $D$ features per datapoint. The weights of the model are parametrized by $w$, a $d$-dimensional column vector. Given binary labels $y$ of shape $N$ by $1$, the negative log likelihood function and the corresponding gradient is </span>
<span id="cb8-628"><a href="#cb8-628"></a></span>
<span id="cb8-629"><a href="#cb8-629"></a>$$p(y, X| w)=-\frac{1}{N}(y^\top \log(\sigma(X^\top w)) + (1-y)^\tau \log(1-\sigma(X^\top w))), \quad \nabla_w p(y, X | w)=\frac{1}{N}X^T(\sigma(X^\top w)-y),$$</span>
<span id="cb8-630"><a href="#cb8-630"></a></span>
<span id="cb8-631"><a href="#cb8-631"></a>where $\sigma$ is the sigmoid function and is applied element-wise along with $\log$. As usual, we use maximum likelihood to learn the parameter.</span>
<span id="cb8-632"><a href="#cb8-632"></a></span>
<span id="cb8-633"><a href="#cb8-633"></a><span class="ss">1.  </span>**(Coding, 5 points)**. Implement the functions <span class="in">`train`</span> and the <span class="in">`predict_probs`</span> in <span class="in">`LogisticRegression`</span> class. The starter code is provided below.</span>
<span id="cb8-634"><a href="#cb8-634"></a></span>
<span id="cb8-635"><a href="#cb8-635"></a>::: {.callout-note title="code"}</span>
<span id="cb8-636"><a href="#cb8-636"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb8-637"><a href="#cb8-637"></a><span class="in">from sklearn.model_selection import train_test_split</span></span>
<span id="cb8-638"><a href="#cb8-638"></a><span class="in">import torch</span></span>
<span id="cb8-639"><a href="#cb8-639"></a></span>
<span id="cb8-640"><a href="#cb8-640"></a><span class="in">class LogisticRegression:</span></span>
<span id="cb8-641"><a href="#cb8-641"></a><span class="in">    def __init__(self):</span></span>
<span id="cb8-642"><a href="#cb8-642"></a><span class="in">        self.weights = None  # Initialized during training</span></span>
<span id="cb8-643"><a href="#cb8-643"></a></span>
<span id="cb8-644"><a href="#cb8-644"></a><span class="in">    def train(self, X, y, learning_rate, num_iterations):</span></span>
<span id="cb8-645"><a href="#cb8-645"></a><span class="in">        """</span></span>
<span id="cb8-646"><a href="#cb8-646"></a><span class="in">        Train the logistic regression model using full batch gradient-based optimization.</span></span>
<span id="cb8-647"><a href="#cb8-647"></a></span>
<span id="cb8-648"><a href="#cb8-648"></a><span class="in">        Parameters:</span></span>
<span id="cb8-649"><a href="#cb8-649"></a><span class="in">        - X (torch.Tensor): Training data of shape (n_samples, n_features).</span></span>
<span id="cb8-650"><a href="#cb8-650"></a><span class="in">        - y (torch.Tensor): Target labels of shape (n_samples,).</span></span>
<span id="cb8-651"><a href="#cb8-651"></a><span class="in">        """</span></span>
<span id="cb8-652"><a href="#cb8-652"></a><span class="in">        n_samples, n_features = X.shape</span></span>
<span id="cb8-653"><a href="#cb8-653"></a></span>
<span id="cb8-654"><a href="#cb8-654"></a><span class="in">        # Initialize weights without the bias term</span></span>
<span id="cb8-655"><a href="#cb8-655"></a><span class="in">        self.weights = torch.zeros(n_features)</span></span>
<span id="cb8-656"><a href="#cb8-656"></a></span>
<span id="cb8-657"><a href="#cb8-657"></a><span class="in">        for i in range(num_iterations):</span></span>
<span id="cb8-658"><a href="#cb8-658"></a><span class="in">            # YOUR CODE HERE (~4-5 lines)</span></span>
<span id="cb8-659"><a href="#cb8-659"></a><span class="in">                pass</span></span>
<span id="cb8-660"><a href="#cb8-660"></a><span class="in">            # END OF YOUR CODE</span></span>
<span id="cb8-661"><a href="#cb8-661"></a></span>
<span id="cb8-662"><a href="#cb8-662"></a><span class="in">    def predict_probs(self, X):</span></span>
<span id="cb8-663"><a href="#cb8-663"></a><span class="in">        """</span></span>
<span id="cb8-664"><a href="#cb8-664"></a><span class="in">        Predict probabilities for samples in X.</span></span>
<span id="cb8-665"><a href="#cb8-665"></a></span>
<span id="cb8-666"><a href="#cb8-666"></a><span class="in">        Parameters:</span></span>
<span id="cb8-667"><a href="#cb8-667"></a><span class="in">        - X (torch.Tensor): Input data of shape (n_samples, n_features).</span></span>
<span id="cb8-668"><a href="#cb8-668"></a></span>
<span id="cb8-669"><a href="#cb8-669"></a><span class="in">        Returns:</span></span>
<span id="cb8-670"><a href="#cb8-670"></a><span class="in">        - y_probs (torch.Tensor): Predicted probabilities.</span></span>
<span id="cb8-671"><a href="#cb8-671"></a><span class="in">        """</span></span>
<span id="cb8-672"><a href="#cb8-672"></a><span class="in">        y_probs = None</span></span>
<span id="cb8-673"><a href="#cb8-673"></a></span>
<span id="cb8-674"><a href="#cb8-674"></a><span class="in">        # YOUR CODE HERE (~2-3 lines)</span></span>
<span id="cb8-675"><a href="#cb8-675"></a><span class="in">        pass</span></span>
<span id="cb8-676"><a href="#cb8-676"></a><span class="in">        # END OF YOUR CODE</span></span>
<span id="cb8-677"><a href="#cb8-677"></a></span>
<span id="cb8-678"><a href="#cb8-678"></a><span class="in">        return y_probs</span></span>
<span id="cb8-679"><a href="#cb8-679"></a></span>
<span id="cb8-680"><a href="#cb8-680"></a></span>
<span id="cb8-681"><a href="#cb8-681"></a><span class="in">if __name__ == "__main__":</span></span>
<span id="cb8-682"><a href="#cb8-682"></a><span class="in">    # Load in Llama3 embeddings of prompt + completions on RewardBench</span></span>
<span id="cb8-683"><a href="#cb8-683"></a><span class="in">    chosen_embeddings = torch.load('data/chosen_embeddings.pt')</span></span>
<span id="cb8-684"><a href="#cb8-684"></a><span class="in">    rejected_embeddings = torch.load('data/rejected_embeddings.pt')</span></span>
<span id="cb8-685"><a href="#cb8-685"></a></span>
<span id="cb8-686"><a href="#cb8-686"></a><span class="in">    # Subtract the embeddings according to the Bradley-Terry reward model setup presented in the problem </span></span>
<span id="cb8-687"><a href="#cb8-687"></a><span class="in">    X = (chosen_embeddings - rejected_embeddings).to(torch.float)</span></span>
<span id="cb8-688"><a href="#cb8-688"></a><span class="in">    y = torch.ones(X.shape[0])</span></span>
<span id="cb8-689"><a href="#cb8-689"></a><span class="in">    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)  </span></span>
<span id="cb8-690"><a href="#cb8-690"></a></span>
<span id="cb8-691"><a href="#cb8-691"></a><span class="in">    # Tune the learning_rate and num_iterations</span></span>
<span id="cb8-692"><a href="#cb8-692"></a><span class="in">    learning_rate = None</span></span>
<span id="cb8-693"><a href="#cb8-693"></a><span class="in">    num_iterations = None</span></span>
<span id="cb8-694"><a href="#cb8-694"></a><span class="in">    model = LogisticRegression()</span></span>
<span id="cb8-695"><a href="#cb8-695"></a><span class="in">    model.train(X_train, y_train, learning_rate=learning_rate, num_iterations=num_iterations)</span></span>
<span id="cb8-696"><a href="#cb8-696"></a><span class="in">    print(f"Expected Train Accuracy: {model.predict_probs(X_train).mean()}")</span></span>
<span id="cb8-697"><a href="#cb8-697"></a><span class="in">    print(f"Expected Validation Accuracy: {del.predict_probs(X_val).mean()}") # Should reach at least 90%</span></span>
<span id="cb8-698"><a href="#cb8-698"></a><span class="in">```</span></span>
<span id="cb8-699"><a href="#cb8-699"></a>:::</span>
<span id="cb8-700"><a href="#cb8-700"></a></span>
<span id="cb8-701"><a href="#cb8-701"></a><span class="ss">3.  </span>**(Written, 4 points)**. Open the notebook <span class="in">`rewardbench_preferences.ipynb`</span> and run all the cells. Make sure to tune the <span class="in">`learning_rate`</span> and <span class="in">`num_iterations`</span>. Report your final expected accuracy on the training and validation sets. How close are the two expected accuracies? You should be able to achieve $\approx 90\%$ expected accuracy on validation. You may add loss reporting to the <span class="in">`train`</span> function to verify your model is improving over time.</span>
<span id="cb8-702"><a href="#cb8-702"></a></span>
<span id="cb8-703"><a href="#cb8-703"></a><span class="fu">### Question 2: Revealed and Stated Preferences (20 points) {#question-2-revealed-and-stated-preferences-20-points .unnumbered}</span></span>
<span id="cb8-704"><a href="#cb8-704"></a></span>
<span id="cb8-705"><a href="#cb8-705"></a>Alice and Bob are running for president. For $R$ voters, we can access their revealed candidate preferences through some means (e.g., social media, blogs, event history). Assume there is an unknown probability $z$ of voting for Alice among the population. The aim of this question is to estimate $z$ through *maximum likelihood estimation* by also incorporating stated preferences. In this scenario, we collect stated preferences through surveys. When surveyed, voters tend to be more likely to vote for Alice with probability $\frac{z+1}{2}$ for reasons of "political correctness."</span>
<span id="cb8-706"><a href="#cb8-706"></a></span>
<span id="cb8-707"><a href="#cb8-707"></a>(a) **(Written, 5 points)**. Suppose there are $R_A$ revealed preferences for Alice, $R_B$ revealed preferences for Bob, $S_A$ stated preferences for Alice, and $S_B$ stated preferences for Bob. Note $R=R_A+R_B$. Compute the log-likelihood of observing such preferences in terms of $z, R_A, R_B, S_A, S_B$.</span>
<span id="cb8-708"><a href="#cb8-708"></a></span>
<span id="cb8-709"><a href="#cb8-709"></a>(b) **(Coding, 1 point)**. Implement the short function <span class="in">`stated_prob`</span> in the file <span class="in">`voting/simulation.py`</span>.</span>
<span id="cb8-710"><a href="#cb8-710"></a></span>
<span id="cb8-711"><a href="#cb8-711"></a>(c) **(Coding, 5 points)**. Implement the class <span class="in">`VotingSimulation`</span>.</span>
<span id="cb8-712"><a href="#cb8-712"></a></span>
<span id="cb8-713"><a href="#cb8-713"></a>(d) **(Coding, 7 points)**. Implement your derived expression from part (a) in the <span class="in">`log_likelihoods`</span> function.</span>
<span id="cb8-714"><a href="#cb8-714"></a></span>
<span id="cb8-715"><a href="#cb8-715"></a>(e) **(Written, 2 points)**. Finally, implement the <span class="in">`average_mae_mle`</span> method that will allow us to visualize the mean absolute error (MAE) of our maximum likelihood estimate $\hat{z}$ (i.e., $|\hat{z}-z|$) as the number of voters surveyed increases. Open <span class="in">`voting/visualize_sim.ipynb`</span> and run the cells to get a plot of MAE vs. voters surveyed averaged across $100$ simulations. Attach the plot to this question and briefly explain what you notice.</span>
<span id="cb8-716"><a href="#cb8-716"></a></span>
<span id="cb8-717"><a href="#cb8-717"></a>::: {.callout-note title="code"}</span>
<span id="cb8-718"><a href="#cb8-718"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb8-719"><a href="#cb8-719"></a><span class="in">import torch</span></span>
<span id="cb8-720"><a href="#cb8-720"></a><span class="in">import random</span></span>
<span id="cb8-721"><a href="#cb8-721"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb8-722"><a href="#cb8-722"></a><span class="in">from tqdm import tqdm</span></span>
<span id="cb8-723"><a href="#cb8-723"></a><span class="in">random.seed(42)</span></span>
<span id="cb8-724"><a href="#cb8-724"></a><span class="in">torch.manual_seed(42)</span></span>
<span id="cb8-725"><a href="#cb8-725"></a></span>
<span id="cb8-726"><a href="#cb8-726"></a><span class="in">def stated_prob(z_values):</span></span>
<span id="cb8-727"><a href="#cb8-727"></a><span class="in">    """</span></span>
<span id="cb8-728"><a href="#cb8-728"></a><span class="in">    Computes the probability of stated preferences based on z values.</span></span>
<span id="cb8-729"><a href="#cb8-729"></a><span class="in">    </span></span>
<span id="cb8-730"><a href="#cb8-730"></a><span class="in">    Args:</span></span>
<span id="cb8-731"><a href="#cb8-731"></a><span class="in">        z_values (torch.Tensor): The z value(s), where z represents the true probability of voting for Alice.</span></span>
<span id="cb8-732"><a href="#cb8-732"></a></span>
<span id="cb8-733"><a href="#cb8-733"></a><span class="in">    Returns:</span></span>
<span id="cb8-734"><a href="#cb8-734"></a><span class="in">        torch.Tensor: Probability for stated preferences, derived from z values.</span></span>
<span id="cb8-735"><a href="#cb8-735"></a><span class="in">    """</span></span>
<span id="cb8-736"><a href="#cb8-736"></a><span class="in">    # YOUR CODE HERE (~1 line)</span></span>
<span id="cb8-737"><a href="#cb8-737"></a><span class="in">    # END OF YOUR CODE</span></span>
<span id="cb8-738"><a href="#cb8-738"></a></span>
<span id="cb8-739"><a href="#cb8-739"></a><span class="in">class VotingSimulation:</span></span>
<span id="cb8-740"><a href="#cb8-740"></a><span class="in">    """</span></span>
<span id="cb8-741"><a href="#cb8-741"></a><span class="in">    A class to simulate the voting process where revealed and stated preferences are generated.</span></span>
<span id="cb8-742"><a href="#cb8-742"></a><span class="in">    </span></span>
<span id="cb8-743"><a href="#cb8-743"></a><span class="in">    Attributes:</span></span>
<span id="cb8-744"><a href="#cb8-744"></a><span class="in">        R (int): Number of revealed preferences.</span></span>
<span id="cb8-745"><a href="#cb8-745"></a><span class="in">        z (float): The true probability of voting for Alice.</span></span>
<span id="cb8-746"><a href="#cb8-746"></a><span class="in">        revealed_preferences (torch.Tensor): Simulated revealed preferences of R voters using Bernoulli distribution.</span></span>
<span id="cb8-747"><a href="#cb8-747"></a><span class="in">                                             Takes on 1 for Alice, and 0 for Bob.</span></span>
<span id="cb8-748"><a href="#cb8-748"></a><span class="in">        stated_preferences (torch.Tensor): Simulated stated preferences, initialized as an empty tensor.</span></span>
<span id="cb8-749"><a href="#cb8-749"></a><span class="in">                                           Takes on 1 for Alice, and 0 for Bob.</span></span>
<span id="cb8-750"><a href="#cb8-750"></a><span class="in">    """</span></span>
<span id="cb8-751"><a href="#cb8-751"></a><span class="in">    def __init__(self, R, z):</span></span>
<span id="cb8-752"><a href="#cb8-752"></a><span class="in">        self.R = R</span></span>
<span id="cb8-753"><a href="#cb8-753"></a><span class="in">        self.z = z</span></span>
<span id="cb8-754"><a href="#cb8-754"></a><span class="in">        self.revealed_preferences = None # YOUR CODE HERE (~1 line)</span></span>
<span id="cb8-755"><a href="#cb8-755"></a><span class="in">        self.stated_preferences = torch.tensor([])</span></span>
<span id="cb8-756"><a href="#cb8-756"></a></span>
<span id="cb8-757"><a href="#cb8-757"></a><span class="in">    def add_survey(self):</span></span>
<span id="cb8-758"><a href="#cb8-758"></a><span class="in">        """</span></span>
<span id="cb8-759"><a href="#cb8-759"></a><span class="in">        Simulates an additional stated preference based on stated_prob and adds it to the list.</span></span>
<span id="cb8-760"><a href="#cb8-760"></a><span class="in">        This updates the self.stated_preferences tensor by concatenating on a new simulated survey result.</span></span>
<span id="cb8-761"><a href="#cb8-761"></a><span class="in">        """</span></span>
<span id="cb8-762"><a href="#cb8-762"></a><span class="in">        # YOUR CODE HERE (~3 lines)</span></span>
<span id="cb8-763"><a href="#cb8-763"></a><span class="in">        # END OF YOUR CODE</span></span>
<span id="cb8-764"><a href="#cb8-764"></a></span>
<span id="cb8-765"><a href="#cb8-765"></a><span class="in">def log_likelihoods(revealed_preferences, stated_preferences, z_values):</span></span>
<span id="cb8-766"><a href="#cb8-766"></a><span class="in">    """</span></span>
<span id="cb8-767"><a href="#cb8-767"></a><span class="in">    Computes the log likelihoods across both revealed and stated preferences.</span></span>
<span id="cb8-768"><a href="#cb8-768"></a><span class="in">    Use your answer in part (a) to help.</span></span>
<span id="cb8-769"><a href="#cb8-769"></a><span class="in">    </span></span>
<span id="cb8-770"><a href="#cb8-770"></a><span class="in">    Args:</span></span>
<span id="cb8-771"><a href="#cb8-771"></a><span class="in">        revealed_preferences (torch.Tensor): Tensor containing revealed preferences (0 or 1).</span></span>
<span id="cb8-772"><a href="#cb8-772"></a><span class="in">        stated_preferences (torch.Tensor): Tensor containing stated preferences (0 or 1).</span></span>
<span id="cb8-773"><a href="#cb8-773"></a><span class="in">        z_values (torch.Tensor): Tensor of underlying z values to calculate likelihood for.</span></span>
<span id="cb8-774"><a href="#cb8-774"></a></span>
<span id="cb8-775"><a href="#cb8-775"></a><span class="in">    Returns:</span></span>
<span id="cb8-776"><a href="#cb8-776"></a><span class="in">        torch.Tensor: Log likelihood for each z value.</span></span>
<span id="cb8-777"><a href="#cb8-777"></a><span class="in">    """</span></span>
<span id="cb8-778"><a href="#cb8-778"></a><span class="in">    # YOUR CODE HERE (~10-16 lines)</span></span>
<span id="cb8-779"><a href="#cb8-779"></a><span class="in">    pass</span></span>
<span id="cb8-780"><a href="#cb8-780"></a><span class="in">    # END OF YOUR CODE </span></span>
<span id="cb8-781"><a href="#cb8-781"></a></span>
<span id="cb8-782"><a href="#cb8-782"></a><span class="in">def average_mae_mle(R, z, survey_count, num_sims, z_sweep):</span></span>
<span id="cb8-783"><a href="#cb8-783"></a><span class="in">    """</span></span>
<span id="cb8-784"><a href="#cb8-784"></a><span class="in">    Runs multiple simulations to compute the average mean absolute error (MAE) of Maximum Likelihood Estimation (MLE) </span></span>
<span id="cb8-785"><a href="#cb8-785"></a><span class="in">    for z after increasing number of surveys.</span></span>
<span id="cb8-786"><a href="#cb8-786"></a><span class="in">    </span></span>
<span id="cb8-787"><a href="#cb8-787"></a><span class="in">    Args:</span></span>
<span id="cb8-788"><a href="#cb8-788"></a><span class="in">        R (int): Number of revealed preferences.</span></span>
<span id="cb8-789"><a href="#cb8-789"></a><span class="in">        z (float): The true probability of voting for Alice.</span></span>
<span id="cb8-790"><a href="#cb8-790"></a><span class="in">        survey_count (int): Number of additional surveys to perform.</span></span>
<span id="cb8-791"><a href="#cb8-791"></a><span class="in">        num_sims (int): Number of simulation runs to average over.</span></span>
<span id="cb8-792"><a href="#cb8-792"></a><span class="in">        z_sweep (torch.Tensor): Range of z values to consider for maximum likelihood estimation.</span></span>
<span id="cb8-793"><a href="#cb8-793"></a></span>
<span id="cb8-794"><a href="#cb8-794"></a><span class="in">    Returns:</span></span>
<span id="cb8-795"><a href="#cb8-795"></a><span class="in">        torch.Tensor: Tensor of mean absolute errors averaged over simulations.</span></span>
<span id="cb8-796"><a href="#cb8-796"></a><span class="in">                      Should have shape (survey_count, )</span></span>
<span id="cb8-797"><a href="#cb8-797"></a><span class="in">    """</span></span>
<span id="cb8-798"><a href="#cb8-798"></a><span class="in">    all_errors = []</span></span>
<span id="cb8-799"><a href="#cb8-799"></a><span class="in">    for _ in tqdm(range(num_sims)):</span></span>
<span id="cb8-800"><a href="#cb8-800"></a><span class="in">        errors = []</span></span>
<span id="cb8-801"><a href="#cb8-801"></a><span class="in">        vote_simulator = VotingSimulation(R=R, z=z)</span></span>
<span id="cb8-802"><a href="#cb8-802"></a></span>
<span id="cb8-803"><a href="#cb8-803"></a><span class="in">        for _ in range(survey_count):</span></span>
<span id="cb8-804"><a href="#cb8-804"></a><span class="in">            revealed_preferences = vote_simulator.revealed_preferences</span></span>
<span id="cb8-805"><a href="#cb8-805"></a><span class="in">            stated_preferences = vote_simulator.stated_preferences</span></span>
<span id="cb8-806"><a href="#cb8-806"></a></span>
<span id="cb8-807"><a href="#cb8-807"></a><span class="in">            # YOUR CODE HERE (~6-8 lines)</span></span>
<span id="cb8-808"><a href="#cb8-808"></a><span class="in">            pass # Compute log_likelihoods across z_sweep. Argmax to find MLE for z. </span></span>
<span id="cb8-809"><a href="#cb8-809"></a><span class="in">                 # Append the absolute error to errors and add a survey to the simulator.</span></span>
<span id="cb8-810"><a href="#cb8-810"></a><span class="in">            # END OF YOUR CODE</span></span>
<span id="cb8-811"><a href="#cb8-811"></a></span>
<span id="cb8-812"><a href="#cb8-812"></a><span class="in">        errors_tensor = torch.stack(errors) </span></span>
<span id="cb8-813"><a href="#cb8-813"></a><span class="in">        all_errors.append(errors_tensor)</span></span>
<span id="cb8-814"><a href="#cb8-814"></a></span>
<span id="cb8-815"><a href="#cb8-815"></a><span class="in">    # Calculate the average error across simulations </span></span>
<span id="cb8-816"><a href="#cb8-816"></a><span class="in">    mean_errors = torch.stack(all_errors).mean(dim=0)</span></span>
<span id="cb8-817"><a href="#cb8-817"></a><span class="in">    return mean_errors</span></span>
<span id="cb8-818"><a href="#cb8-818"></a></span>
<span id="cb8-819"><a href="#cb8-819"></a><span class="in">if __name__ == "__main__":</span></span>
<span id="cb8-820"><a href="#cb8-820"></a><span class="in">    # DO NOT CHANGE!</span></span>
<span id="cb8-821"><a href="#cb8-821"></a><span class="in">    max_surveys = 2000</span></span>
<span id="cb8-822"><a href="#cb8-822"></a><span class="in">    z = 0.5</span></span>
<span id="cb8-823"><a href="#cb8-823"></a><span class="in">    R = 10</span></span>
<span id="cb8-824"><a href="#cb8-824"></a><span class="in">    num_sims = 100</span></span>
<span id="cb8-825"><a href="#cb8-825"></a><span class="in">    z_sweep = torch.linspace(0.01, 0.99, 981)</span></span>
<span id="cb8-826"><a href="#cb8-826"></a></span>
<span id="cb8-827"><a href="#cb8-827"></a><span class="in">    # Compute and plot the errors. Attach this plot to part (d).</span></span>
<span id="cb8-828"><a href="#cb8-828"></a><span class="in">    mean_errors = average_mae_mle(R, z, max_surveys, num_sims, z_sweep)</span></span>
<span id="cb8-829"><a href="#cb8-829"></a><span class="in">    plt.plot(mean_errors)</span></span>
<span id="cb8-830"><a href="#cb8-830"></a></span>
<span id="cb8-831"><a href="#cb8-831"></a><span class="in">    plt.xlabel('Surveys Conducted')</span></span>
<span id="cb8-832"><a href="#cb8-832"></a><span class="in">    plt.ylabel('Average Error')</span></span>
<span id="cb8-833"><a href="#cb8-833"></a><span class="in">    plt.title(f'MLE MAE Error (z={z}, {num_sims} simulations)')</span></span>
<span id="cb8-834"><a href="#cb8-834"></a><span class="in">    plt.show()</span></span>
<span id="cb8-835"><a href="#cb8-835"></a><span class="in">```</span></span>
<span id="cb8-836"><a href="#cb8-836"></a>:::</span>
<span id="cb8-837"><a href="#cb8-837"></a></span>
<span id="cb8-838"><a href="#cb8-838"></a><span class="fu">### Question 3: Probabilistic Multi-modal Preferences (25 points) {#question-3-probabilistic-multi-modal-preferences-25-points .unnumbered}</span></span>
<span id="cb8-839"><a href="#cb8-839"></a></span>
<span id="cb8-840"><a href="#cb8-840"></a>Suppose you are part of the ML team on the movie streaming site CardinalStreams. After taking CS329H, you collect a movie preferences dataset with $30000$ examples of the form $(m_1, m_2, \text{user id})$ where $m_1$ and $m_2$ are movies with $m_1\succ m_2$. The preferences come from $600$ distinct users with $50$ examples per user. Each movie has a $10$-dimensional feature vector $m$, and each user has a $10$-dimensional weight vector $u$. Given movie features $m_1, m_2$ and user weights $u$, the user's preference between the movies is given by a Bradley-Terry reward model:</span>
<span id="cb8-841"><a href="#cb8-841"></a>$$P(m_1\succ m_2)=\frac{e^{u\cdot m_1}}{e^{u\cdot m_1} + e^{u\cdot m_2}}=\frac{1}{1+e^{u\cdot (m_2-m_1)}}=\sigma(u\cdot (m_1-m_2)).$$</span>
<span id="cb8-842"><a href="#cb8-842"></a></span>
<span id="cb8-843"><a href="#cb8-843"></a>You realize that trying to estimate the weights for each user with only $50$ examples will not work due to the lack of data. Instead, you choose to drop the user IDs column and shuffle the dataset in order to take a *multi-modal preferences* approach. For simplicity, you assume a model where a proportion $p$ of the users have weights $w_1$ and the other $1-p$ have weights $w_2$. In this setting, each user belongs to one of two groups: users with weights $w_1$ are part of Group 1, and users with weights $w_2$ are part of Group 2.</span>
<span id="cb8-844"><a href="#cb8-844"></a></span>
<span id="cb8-845"><a href="#cb8-845"></a>(a) **(Written, 3 points)**. For a datapoint $(m_1, m_2)$ with label $m_1\succ m_2$, compute the data likelihood $P(m_1\succ m_2 | p, w_1, w_2)$ assuming $p, w_1, w_2$ are given.</span>
<span id="cb8-846"><a href="#cb8-846"></a></span>
<span id="cb8-847"><a href="#cb8-847"></a>(b) **(Written, 3 points)**. As a follow up, use the likelihood to simplify the posterior distribution of $p, w_1, w_2$ after updating on $(m_1, m_2)$ leaving terms for the priors unchanged.</span>
<span id="cb8-848"><a href="#cb8-848"></a></span>
<span id="cb8-849"><a href="#cb8-849"></a>(c) **(Written, 4 points)**. Assume priors $p\sim B(1, 1)$, $w_1\sim\mathcal{N}(0, \mathbf{I})$, and $w_2\sim\mathcal{N}(0, \mathbf{I})$ where $B$ represents the Beta distribution and $\mathcal{N}$ represents the normal distribution. You will notice that the posterior from part (b) has no simple closed-form. As a result, we must resort to *Markov Chain Monte Carlo (MCMC)* approaches to sample from the posterior. These approaches allow sampling from highly complex distributions by constructing a Markov chain $<span class="sc">\{</span>x_t<span class="sc">\}</span>_{t=1}^\infty$ so that $\lim_{t\to\infty}x_t$ act as desired samples from the target distribution. You can think of a Markov chain as a sequence with the special property that $x_{t+1}$ only depends on $x_t$ for all $t\ge 1$.</span>
<span id="cb8-850"><a href="#cb8-850"></a></span>
<span id="cb8-851"><a href="#cb8-851"></a>The most basic version of MCMC is known as Metropolis-Hastings. Assume $\pi$ is the target distribution we wish to sample from where $\pi(z)$ represents the probability density at point $z$. Metropolis-Hastings constructs the approximating Markov chain $x_t$ as follows: a proposal $P$ for $x_{t+1}$ is made via sampling from a chosen distribution $Q(\,\cdot\,| x_t)$ (e.g., adding Gaussian noise). The acceptance probability of the proposal is given by</span>
<span id="cb8-852"><a href="#cb8-852"></a></span>
<span id="cb8-853"><a href="#cb8-853"></a>$$A= \min \left( 1, \frac{\pi(P)Q(x_t | P)}{\pi(x_t)Q(P | x_t)} \right). \text{ That is } x_{t+1}=\begin{cases} P &amp; \text{with probability } A, <span class="sc">\\</span> x_t &amp; \text{with probability } 1 - A. \end{cases}$$ To extract our samples from $\pi$, we run the Markov chain for $N$ timesteps and disregard the first $T&lt;N$ timesteps in what is called the *burn-in or mixing time* (i.e., our final samples are $x_{T+1}, x_{T+2},\cdots, x_{N}$). The mixing time is needed to ensure that the Markov chain elements are representative of the distribution $\pi$ -- initial elements of the chain will not be a good approximation of $\pi$ and depend more on the choice of initialization $x_1$. To build some intuition, suppose we have a biased coin that turns heads with probability $p_{\text{heads}}$. We observe $12$ coin flips to have $9$ heads (H) and $3$ tails (T). If our prior for $p_{\text{H}}$ was $B(1, 1)$, then our posterior will be $B(1+9, 1+3)=B(10, 4)$. The Bayesian update is given by</span>
<span id="cb8-854"><a href="#cb8-854"></a></span>
<span id="cb8-855"><a href="#cb8-855"></a>$$p(p_{\text{H}}|9\text{H}, 3\text{T}) = \frac{p(9\text{H}, 3\text{T} | p_{\text{H}})B(1, 1)(p_{\text{H}})}{\int_0^1 P(9\text{H}, 3\text{T} | p_{\text{H}})B(1, 1)(p_{\text{H}}) dp_{\text{H}}} =\frac{p(9\text{H}, 3\text{T} | p_{\text{H}})}{\int_0^1 p(9\text{H}, 3\text{T} | p_{\text{H}})  dp_{\text{H}}}.$$</span>
<span id="cb8-856"><a href="#cb8-856"></a></span>
<span id="cb8-857"><a href="#cb8-857"></a>**Find the acceptance probablity** $A$ in the setting of the biased coin assuming the proposal distribution $Q(\cdot|x_t)=x_t+N(0,\sigma)$ for given $\sigma$. Notice that this choice of $Q$ is symmetric, i.e., $Q(x_t|P)=Q(P|x_t)$. In addition, you will realize that is unnecessary to compute the normalizing constant of the Bayesian update (i.e., the integral in the denominator) which is why MCMC is commonly used to sample from posteriors!</span>
<span id="cb8-858"><a href="#cb8-858"></a></span>
<span id="cb8-859"><a href="#cb8-859"></a>(d) **(Written + Coding, 6 points)**. Implement Metropolis-Hastings to sample from the posterior distribution of the biased coin in <span class="in">`multimodal_preferences/biased_coin.py`</span>. Attach a histogram of your MCMC samples overlayed on top of the true posterior $B(10, 4)$ by running <span class="in">`python biased_coin.py`</span>.</span>
<span id="cb8-860"><a href="#cb8-860"></a></span>
<span id="cb8-861"><a href="#cb8-861"></a>::: {.callout-note title="code"}</span>
<span id="cb8-862"><a href="#cb8-862"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb8-863"><a href="#cb8-863"></a><span class="in">import numpy as np</span></span>
<span id="cb8-864"><a href="#cb8-864"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb8-865"><a href="#cb8-865"></a><span class="in">from scipy.stats import beta</span></span>
<span id="cb8-866"><a href="#cb8-866"></a></span>
<span id="cb8-867"><a href="#cb8-867"></a><span class="in">def likelihood(p: float) -&gt; float:</span></span>
<span id="cb8-868"><a href="#cb8-868"></a><span class="in">    """</span></span>
<span id="cb8-869"><a href="#cb8-869"></a><span class="in">    Computes the likelihood of 9 heads and 3 tails assuming p_heads is p.</span></span>
<span id="cb8-870"><a href="#cb8-870"></a></span>
<span id="cb8-871"><a href="#cb8-871"></a><span class="in">    Args:</span></span>
<span id="cb8-872"><a href="#cb8-872"></a><span class="in">    p (float): A value between 0 and 1 representing the probability of heads.</span></span>
<span id="cb8-873"><a href="#cb8-873"></a></span>
<span id="cb8-874"><a href="#cb8-874"></a><span class="in">    Returns:</span></span>
<span id="cb8-875"><a href="#cb8-875"></a><span class="in">    float: The likelihood value at p_heads=p. Return 0 if p is outside the range [0, 1].</span></span>
<span id="cb8-876"><a href="#cb8-876"></a><span class="in">    """</span></span>
<span id="cb8-877"><a href="#cb8-877"></a><span class="in">    # YOUR CODE HERE (~1-3 lines)</span></span>
<span id="cb8-878"><a href="#cb8-878"></a><span class="in">    pass</span></span>
<span id="cb8-879"><a href="#cb8-879"></a><span class="in">    # END OF YOUR CODE</span></span>
<span id="cb8-880"><a href="#cb8-880"></a></span>
<span id="cb8-881"><a href="#cb8-881"></a></span>
<span id="cb8-882"><a href="#cb8-882"></a><span class="in">def propose(x_current: float, sigma: float) -&gt; float:</span></span>
<span id="cb8-883"><a href="#cb8-883"></a><span class="in">    """</span></span>
<span id="cb8-884"><a href="#cb8-884"></a><span class="in">    Proposes a new sample from the proposal distribution Q.</span></span>
<span id="cb8-885"><a href="#cb8-885"></a><span class="in">    Here, Q is a normal distribution centered at x_current with standard deviation sigma.</span></span>
<span id="cb8-886"><a href="#cb8-886"></a></span>
<span id="cb8-887"><a href="#cb8-887"></a><span class="in">    Args:</span></span>
<span id="cb8-888"><a href="#cb8-888"></a><span class="in">    x_current (float): The current value in the Markov chain.</span></span>
<span id="cb8-889"><a href="#cb8-889"></a><span class="in">    sigma (float): Standard deviation of the normal proposal distribution.</span></span>
<span id="cb8-890"><a href="#cb8-890"></a></span>
<span id="cb8-891"><a href="#cb8-891"></a><span class="in">    Returns:</span></span>
<span id="cb8-892"><a href="#cb8-892"></a><span class="in">    float: The proposed new sample.</span></span>
<span id="cb8-893"><a href="#cb8-893"></a><span class="in">    """</span></span>
<span id="cb8-894"><a href="#cb8-894"></a><span class="in">    # YOUR CODE HERE (~1-3 lines)</span></span>
<span id="cb8-895"><a href="#cb8-895"></a><span class="in">    pass</span></span>
<span id="cb8-896"><a href="#cb8-896"></a><span class="in">    # END OF YOUR CODE</span></span>
<span id="cb8-897"><a href="#cb8-897"></a></span>
<span id="cb8-898"><a href="#cb8-898"></a></span>
<span id="cb8-899"><a href="#cb8-899"></a><span class="in">def acceptance_probability(x_current: float, x_proposed: float) -&gt; float:</span></span>
<span id="cb8-900"><a href="#cb8-900"></a><span class="in">    """</span></span>
<span id="cb8-901"><a href="#cb8-901"></a><span class="in">    Computes the acceptance probability A for the proposed sample.</span></span>
<span id="cb8-902"><a href="#cb8-902"></a><span class="in">    Since the proposal distribution is symmetric, Q cancels out.</span></span>
<span id="cb8-903"><a href="#cb8-903"></a></span>
<span id="cb8-904"><a href="#cb8-904"></a><span class="in">    Args:</span></span>
<span id="cb8-905"><a href="#cb8-905"></a><span class="in">    x_current (float): The current value in the Markov chain.</span></span>
<span id="cb8-906"><a href="#cb8-906"></a><span class="in">    x_proposed (float): The proposed new value.</span></span>
<span id="cb8-907"><a href="#cb8-907"></a></span>
<span id="cb8-908"><a href="#cb8-908"></a><span class="in">    Returns:</span></span>
<span id="cb8-909"><a href="#cb8-909"></a><span class="in">    float: The acceptance probability</span></span>
<span id="cb8-910"><a href="#cb8-910"></a><span class="in">    """</span></span>
<span id="cb8-911"><a href="#cb8-911"></a><span class="in">    # YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb8-912"><a href="#cb8-912"></a><span class="in">    pass</span></span>
<span id="cb8-913"><a href="#cb8-913"></a><span class="in">    # END OF YOUR CODE</span></span>
<span id="cb8-914"><a href="#cb8-914"></a></span>
<span id="cb8-915"><a href="#cb8-915"></a></span>
<span id="cb8-916"><a href="#cb8-916"></a><span class="in">def metropolis_hastings(N: int, T: int, x_init: float, sigma: float) -&gt; np.ndarray:</span></span>
<span id="cb8-917"><a href="#cb8-917"></a><span class="in">    """</span></span>
<span id="cb8-918"><a href="#cb8-918"></a><span class="in">    Runs the Metropolis-Hastings algorithm to sample from a posterior distribution.</span></span>
<span id="cb8-919"><a href="#cb8-919"></a></span>
<span id="cb8-920"><a href="#cb8-920"></a><span class="in">    Args:</span></span>
<span id="cb8-921"><a href="#cb8-921"></a><span class="in">    N (int): Total number of iterations.</span></span>
<span id="cb8-922"><a href="#cb8-922"></a><span class="in">    T (int): Burn-in period (number of initial samples to discard).</span></span>
<span id="cb8-923"><a href="#cb8-923"></a><span class="in">    x_init (float): Initial value of the chain.</span></span>
<span id="cb8-924"><a href="#cb8-924"></a><span class="in">    sigma (float): Standard deviation of the proposal distribution.</span></span>
<span id="cb8-925"><a href="#cb8-925"></a></span>
<span id="cb8-926"><a href="#cb8-926"></a><span class="in">    Returns:</span></span>
<span id="cb8-927"><a href="#cb8-927"></a><span class="in">    list: Samples collected after the burn-in period.</span></span>
<span id="cb8-928"><a href="#cb8-928"></a><span class="in">    """</span></span>
<span id="cb8-929"><a href="#cb8-929"></a><span class="in">    samples = []</span></span>
<span id="cb8-930"><a href="#cb8-930"></a><span class="in">    x_current = x_init</span></span>
<span id="cb8-931"><a href="#cb8-931"></a></span>
<span id="cb8-932"><a href="#cb8-932"></a><span class="in">    for t in range(N):</span></span>
<span id="cb8-933"><a href="#cb8-933"></a><span class="in">        # YOUR CODE HERE (~7-10 lines)</span></span>
<span id="cb8-934"><a href="#cb8-934"></a><span class="in">        # Use the propose and acceptance_probability functions to get x_{t+1} and store it in samples after the burn-in period T</span></span>
<span id="cb8-935"><a href="#cb8-935"></a><span class="in">        pass</span></span>
<span id="cb8-936"><a href="#cb8-936"></a><span class="in">        # END OF YOUR CODE</span></span>
<span id="cb8-937"><a href="#cb8-937"></a></span>
<span id="cb8-938"><a href="#cb8-938"></a><span class="in">    return samples</span></span>
<span id="cb8-939"><a href="#cb8-939"></a></span>
<span id="cb8-940"><a href="#cb8-940"></a></span>
<span id="cb8-941"><a href="#cb8-941"></a><span class="in">def plot_results(samples: np.ndarray) -&gt; None:</span></span>
<span id="cb8-942"><a href="#cb8-942"></a><span class="in">    """</span></span>
<span id="cb8-943"><a href="#cb8-943"></a><span class="in">    Plots the histogram of MCMC samples along with the true Beta(10, 4) PDF.</span></span>
<span id="cb8-944"><a href="#cb8-944"></a></span>
<span id="cb8-945"><a href="#cb8-945"></a><span class="in">    Args:</span></span>
<span id="cb8-946"><a href="#cb8-946"></a><span class="in">    samples (np.ndarray): Array of samples collected from the Metropolis-Hastings algorithm.</span></span>
<span id="cb8-947"><a href="#cb8-947"></a></span>
<span id="cb8-948"><a href="#cb8-948"></a><span class="in">    Returns:</span></span>
<span id="cb8-949"><a href="#cb8-949"></a><span class="in">    None</span></span>
<span id="cb8-950"><a href="#cb8-950"></a><span class="in">    """</span></span>
<span id="cb8-951"><a href="#cb8-951"></a><span class="in">    # Histogram of the samples from the Metropolis-Hastings algorithm</span></span>
<span id="cb8-952"><a href="#cb8-952"></a><span class="in">    plt.hist(samples, bins=50, density=True, alpha=0.5, label="MCMC Samples")</span></span>
<span id="cb8-953"><a href="#cb8-953"></a></span>
<span id="cb8-954"><a href="#cb8-954"></a><span class="in">    # True Beta(10, 4) distribution for comparison</span></span>
<span id="cb8-955"><a href="#cb8-955"></a><span class="in">    p = np.linspace(0, 1, 1000)</span></span>
<span id="cb8-956"><a href="#cb8-956"></a><span class="in">    beta_pdf = beta.pdf(p, 10, 4)</span></span>
<span id="cb8-957"><a href="#cb8-957"></a><span class="in">    plt.plot(p, beta_pdf, "r-", label="Beta(10, 4) PDF")</span></span>
<span id="cb8-958"><a href="#cb8-958"></a></span>
<span id="cb8-959"><a href="#cb8-959"></a><span class="in">    plt.xlabel("p_heads")</span></span>
<span id="cb8-960"><a href="#cb8-960"></a><span class="in">    plt.ylabel("Density")</span></span>
<span id="cb8-961"><a href="#cb8-961"></a><span class="in">    plt.title("Metropolis-Hastings Sampling of Biased Coin Posterior")</span></span>
<span id="cb8-962"><a href="#cb8-962"></a><span class="in">    plt.legend()</span></span>
<span id="cb8-963"><a href="#cb8-963"></a><span class="in">    plt.show()</span></span>
<span id="cb8-964"><a href="#cb8-964"></a></span>
<span id="cb8-965"><a href="#cb8-965"></a></span>
<span id="cb8-966"><a href="#cb8-966"></a><span class="in">if __name__ == "__main__":</span></span>
<span id="cb8-967"><a href="#cb8-967"></a><span class="in">    # MCMC Parameters (DO NOT CHANGE!)</span></span>
<span id="cb8-968"><a href="#cb8-968"></a><span class="in">    N = 50000  # Total number of iterations</span></span>
<span id="cb8-969"><a href="#cb8-969"></a><span class="in">    T = 10000  # Burn-in period to discard</span></span>
<span id="cb8-970"><a href="#cb8-970"></a><span class="in">    x_init = 0.5  # Initial guess for p_heads</span></span>
<span id="cb8-971"><a href="#cb8-971"></a><span class="in">    sigma = 0.1  # Standard deviation of the proposal distribution</span></span>
<span id="cb8-972"><a href="#cb8-972"></a></span>
<span id="cb8-973"><a href="#cb8-973"></a><span class="in">    # Run Metropolis-Hastings and plot the results</span></span>
<span id="cb8-974"><a href="#cb8-974"></a><span class="in">    samples = metropolis_hastings(N, T, x_init, sigma)</span></span>
<span id="cb8-975"><a href="#cb8-975"></a><span class="in">    plot_results(samples)</span></span>
<span id="cb8-976"><a href="#cb8-976"></a><span class="in">```</span></span>
<span id="cb8-977"><a href="#cb8-977"></a>:::</span>
<span id="cb8-978"><a href="#cb8-978"></a></span>
<span id="cb8-979"><a href="#cb8-979"></a>(e) **(Coding, 9 points)**. Implement Metropolis-Hastings in the movie setting inside\ <span class="in">`multimodal_preferences/movie_metropolis.py`</span>. The movie dataset we use for grading will not be provided. However, randomly constructed datasets can be used to test your implementation by running <span class="in">`python movie_metropolis.py`</span>. You should be able to achieve a $90\%$ success rate with most <span class="in">`fraction_accepted`</span> values above $0.1$. Success is measured by thresholded closeness of predicted parameters to true parameters. You may notice occasional failures that occur due to lack of convergence which we will account for in grading.</span>
<span id="cb8-980"><a href="#cb8-980"></a></span>
<span id="cb8-981"><a href="#cb8-981"></a>::: {.callout-note title="code"}</span>
<span id="cb8-982"><a href="#cb8-982"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb8-983"><a href="#cb8-983"></a><span class="in">import torch</span></span>
<span id="cb8-984"><a href="#cb8-984"></a><span class="in">import torch.distributions as dist</span></span>
<span id="cb8-985"><a href="#cb8-985"></a><span class="in">import math</span></span>
<span id="cb8-986"><a href="#cb8-986"></a><span class="in">from tqdm import tqdm</span></span>
<span id="cb8-987"><a href="#cb8-987"></a><span class="in">from typing import Tuple</span></span>
<span id="cb8-988"><a href="#cb8-988"></a></span>
<span id="cb8-989"><a href="#cb8-989"></a><span class="in">def make_data(</span></span>
<span id="cb8-990"><a href="#cb8-990"></a><span class="in">    true_p: torch.Tensor, true_weights_1: torch.Tensor, true_weights_2: torch.Tensor, num_movies: int, feature_dim: int</span></span>
<span id="cb8-991"><a href="#cb8-991"></a><span class="in">) -&gt; Tuple[torch.Tensor, torch.Tensor]:</span></span>
<span id="cb8-992"><a href="#cb8-992"></a><span class="in">    """</span></span>
<span id="cb8-993"><a href="#cb8-993"></a><span class="in">    Generates a synthetic movie dataset according to the CardinalStreams model.</span></span>
<span id="cb8-994"><a href="#cb8-994"></a></span>
<span id="cb8-995"><a href="#cb8-995"></a><span class="in">    Args:</span></span>
<span id="cb8-996"><a href="#cb8-996"></a><span class="in">        true_p (torch.Tensor): Probability of coming from Group 1.</span></span>
<span id="cb8-997"><a href="#cb8-997"></a><span class="in">        true_weights_1 (torch.Tensor): Weights for Group 1.</span></span>
<span id="cb8-998"><a href="#cb8-998"></a><span class="in">        true_weights_2 (torch.Tensor): Weights for Group 2.</span></span>
<span id="cb8-999"><a href="#cb8-999"></a></span>
<span id="cb8-1000"><a href="#cb8-1000"></a><span class="in">    Returns:</span></span>
<span id="cb8-1001"><a href="#cb8-1001"></a><span class="in">        Tuple[torch.Tensor, torch.Tensor]: A tuple containing the dataset and labels.</span></span>
<span id="cb8-1002"><a href="#cb8-1002"></a><span class="in">    """</span></span>
<span id="cb8-1003"><a href="#cb8-1003"></a><span class="in">    # Create movie features</span></span>
<span id="cb8-1004"><a href="#cb8-1004"></a><span class="in">    first_movie_features = torch.randn((num_movies, feature_dim))</span></span>
<span id="cb8-1005"><a href="#cb8-1005"></a><span class="in">    second_movie_features = torch.randn((num_movies, feature_dim))</span></span>
<span id="cb8-1006"><a href="#cb8-1006"></a></span>
<span id="cb8-1007"><a href="#cb8-1007"></a><span class="in">    # Only care about difference of features for Bradley-Terry</span></span>
<span id="cb8-1008"><a href="#cb8-1008"></a><span class="in">    dataset = first_movie_features - second_movie_features</span></span>
<span id="cb8-1009"><a href="#cb8-1009"></a></span>
<span id="cb8-1010"><a href="#cb8-1010"></a><span class="in">    # Get probabilities that first movie is preferred assuming Group 1 or Group 2</span></span>
<span id="cb8-1011"><a href="#cb8-1011"></a><span class="in">    weight_1_probs = torch.sigmoid(dataset @ true_weights_1)</span></span>
<span id="cb8-1012"><a href="#cb8-1012"></a><span class="in">    weight_2_probs = torch.sigmoid(dataset @ true_weights_2)</span></span>
<span id="cb8-1013"><a href="#cb8-1013"></a></span>
<span id="cb8-1014"><a href="#cb8-1014"></a><span class="in">    # Probability that first movie is preferred overall can be viewed as sum of conditioning on Group 1 and Group 2</span></span>
<span id="cb8-1015"><a href="#cb8-1015"></a><span class="in">    first_movie_preferred_probs = (</span></span>
<span id="cb8-1016"><a href="#cb8-1016"></a><span class="in">        true_p * weight_1_probs + (1 - true_p) * weight_2_probs</span></span>
<span id="cb8-1017"><a href="#cb8-1017"></a><span class="in">    )</span></span>
<span id="cb8-1018"><a href="#cb8-1018"></a><span class="in">    labels = dist.Bernoulli(first_movie_preferred_probs).sample()</span></span>
<span id="cb8-1019"><a href="#cb8-1019"></a><span class="in">    return dataset, labels</span></span>
<span id="cb8-1020"><a href="#cb8-1020"></a></span>
<span id="cb8-1021"><a href="#cb8-1021"></a></span>
<span id="cb8-1022"><a href="#cb8-1022"></a><span class="in">def compute_likelihoods(</span></span>
<span id="cb8-1023"><a href="#cb8-1023"></a><span class="in">    dataset: torch.Tensor,</span></span>
<span id="cb8-1024"><a href="#cb8-1024"></a><span class="in">    labels: torch.Tensor,</span></span>
<span id="cb8-1025"><a href="#cb8-1025"></a><span class="in">    p: torch.Tensor,</span></span>
<span id="cb8-1026"><a href="#cb8-1026"></a><span class="in">    w_1: torch.Tensor,</span></span>
<span id="cb8-1027"><a href="#cb8-1027"></a><span class="in">    w_2: torch.Tensor,</span></span>
<span id="cb8-1028"><a href="#cb8-1028"></a><span class="in">) -&gt; torch.Tensor:</span></span>
<span id="cb8-1029"><a href="#cb8-1029"></a><span class="in">    """</span></span>
<span id="cb8-1030"><a href="#cb8-1030"></a><span class="in">    Computes the likelihood of each datapoint. Use your calculation from part (a) to help.</span></span>
<span id="cb8-1031"><a href="#cb8-1031"></a></span>
<span id="cb8-1032"><a href="#cb8-1032"></a><span class="in">    Args:</span></span>
<span id="cb8-1033"><a href="#cb8-1033"></a><span class="in">        dataset (torch.Tensor): The dataset of differences between movie features.</span></span>
<span id="cb8-1034"><a href="#cb8-1034"></a><span class="in">        labels (torch.Tensor): The labels where 1 indicates the first movie is preferred, and 0 indicates preference of the second movie.</span></span>
<span id="cb8-1035"><a href="#cb8-1035"></a><span class="in">        p (torch.Tensor): The probability of coming from Group 1.</span></span>
<span id="cb8-1036"><a href="#cb8-1036"></a><span class="in">        w_1 (torch.Tensor): Weights for Group 1.</span></span>
<span id="cb8-1037"><a href="#cb8-1037"></a><span class="in">        w_2 (torch.Tensor): Weights for Group 2.</span></span>
<span id="cb8-1038"><a href="#cb8-1038"></a></span>
<span id="cb8-1039"><a href="#cb8-1039"></a><span class="in">    Returns:</span></span>
<span id="cb8-1040"><a href="#cb8-1040"></a><span class="in">        torch.Tensor: The likelihoods for each datapoint. Should have shape (dataset.shape[0], )</span></span>
<span id="cb8-1041"><a href="#cb8-1041"></a><span class="in">    """</span></span>
<span id="cb8-1042"><a href="#cb8-1042"></a><span class="in">    # YOUR CODE HERE (~6-8 lines)</span></span>
<span id="cb8-1043"><a href="#cb8-1043"></a><span class="in">    pass</span></span>
<span id="cb8-1044"><a href="#cb8-1044"></a><span class="in">    # END OF YOUR CODE</span></span>
<span id="cb8-1045"><a href="#cb8-1045"></a></span>
<span id="cb8-1046"><a href="#cb8-1046"></a><span class="in">def compute_prior_density(</span></span>
<span id="cb8-1047"><a href="#cb8-1047"></a><span class="in">    p: torch.Tensor, w_1: torch.Tensor, w_2: torch.Tensor</span></span>
<span id="cb8-1048"><a href="#cb8-1048"></a><span class="in">) -&gt; torch.Tensor:</span></span>
<span id="cb8-1049"><a href="#cb8-1049"></a><span class="in">    """</span></span>
<span id="cb8-1050"><a href="#cb8-1050"></a><span class="in">    Computes the prior density of the parameters.</span></span>
<span id="cb8-1051"><a href="#cb8-1051"></a></span>
<span id="cb8-1052"><a href="#cb8-1052"></a><span class="in">    Args:</span></span>
<span id="cb8-1053"><a href="#cb8-1053"></a><span class="in">        p (torch.Tensor): The probability of preferring model 1.</span></span>
<span id="cb8-1054"><a href="#cb8-1054"></a><span class="in">        w_1 (torch.Tensor): Weights for model 1.</span></span>
<span id="cb8-1055"><a href="#cb8-1055"></a><span class="in">        w_2 (torch.Tensor): Weights for model 2.</span></span>
<span id="cb8-1056"><a href="#cb8-1056"></a></span>
<span id="cb8-1057"><a href="#cb8-1057"></a><span class="in">    Returns:</span></span>
<span id="cb8-1058"><a href="#cb8-1058"></a><span class="in">        torch.Tensor: The prior densities of p, w_1, and w_2.</span></span>
<span id="cb8-1059"><a href="#cb8-1059"></a><span class="in">    """</span></span>
<span id="cb8-1060"><a href="#cb8-1060"></a><span class="in">    # Adjusts p to stay in the range [0.3, 0.7] to prevent multiple equilibria issues at p=0 and p=1</span></span>
<span id="cb8-1061"><a href="#cb8-1061"></a><span class="in">    p_prob = torch.tensor([2.5]) if 0.3 &lt;= p &lt;= 0.7 else torch.tensor([0.0])</span></span>
<span id="cb8-1062"><a href="#cb8-1062"></a></span>
<span id="cb8-1063"><a href="#cb8-1063"></a><span class="in">    def normal_pdf(x: torch.Tensor) -&gt; torch.Tensor:</span></span>
<span id="cb8-1064"><a href="#cb8-1064"></a><span class="in">        """Computes the PDF of the standard normal distribution at x."""</span></span>
<span id="cb8-1065"><a href="#cb8-1065"></a><span class="in">        return (1.0 / torch.sqrt(torch.tensor(2 * math.pi))) * torch.exp(-0.5 * x**2)</span></span>
<span id="cb8-1066"><a href="#cb8-1066"></a></span>
<span id="cb8-1067"><a href="#cb8-1067"></a><span class="in">    weights_1_prob = normal_pdf(w_1)</span></span>
<span id="cb8-1068"><a href="#cb8-1068"></a><span class="in">    weights_2_prob = normal_pdf(w_2)</span></span>
<span id="cb8-1069"><a href="#cb8-1069"></a></span>
<span id="cb8-1070"><a href="#cb8-1070"></a><span class="in">    # Concatenate the densities</span></span>
<span id="cb8-1071"><a href="#cb8-1071"></a><span class="in">    concatenated_prob = torch.cat([p_prob, weights_1_prob, weights_2_prob])</span></span>
<span id="cb8-1072"><a href="#cb8-1072"></a><span class="in">    return concatenated_prob</span></span>
<span id="cb8-1073"><a href="#cb8-1073"></a></span>
<span id="cb8-1074"><a href="#cb8-1074"></a></span>
<span id="cb8-1075"><a href="#cb8-1075"></a><span class="in">def metropolis_hastings(</span></span>
<span id="cb8-1076"><a href="#cb8-1076"></a><span class="in">    dataset: torch.Tensor,</span></span>
<span id="cb8-1077"><a href="#cb8-1077"></a><span class="in">    labels: torch.Tensor,</span></span>
<span id="cb8-1078"><a href="#cb8-1078"></a><span class="in">    sigma: float = 0.01,</span></span>
<span id="cb8-1079"><a href="#cb8-1079"></a><span class="in">    num_iters: int = 30000,</span></span>
<span id="cb8-1080"><a href="#cb8-1080"></a><span class="in">    burn_in: int = 20000,</span></span>
<span id="cb8-1081"><a href="#cb8-1081"></a><span class="in">) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]:</span></span>
<span id="cb8-1082"><a href="#cb8-1082"></a><span class="in">    """</span></span>
<span id="cb8-1083"><a href="#cb8-1083"></a><span class="in">    Performs the Metropolis-Hastings algorithm to sample from the posterior distribution.</span></span>
<span id="cb8-1084"><a href="#cb8-1084"></a><span class="in">    DO NOT CHANGE THE DEFAULT VALUES!</span></span>
<span id="cb8-1085"><a href="#cb8-1085"></a></span>
<span id="cb8-1086"><a href="#cb8-1086"></a><span class="in">    Args:</span></span>
<span id="cb8-1087"><a href="#cb8-1087"></a><span class="in">        dataset (torch.Tensor): The dataset of differences between movie features.</span></span>
<span id="cb8-1088"><a href="#cb8-1088"></a><span class="in">        labels (torch.Tensor): The labels indicating which movie is preferred.</span></span>
<span id="cb8-1089"><a href="#cb8-1089"></a><span class="in">        sigma (float, optional): Standard deviation for proposal distribution.</span></span>
<span id="cb8-1090"><a href="#cb8-1090"></a><span class="in">            Defaults to 0.01.</span></span>
<span id="cb8-1091"><a href="#cb8-1091"></a><span class="in">        num_iters (int, optional): Total number of iterations. Defaults to 30000.</span></span>
<span id="cb8-1092"><a href="#cb8-1092"></a><span class="in">        burn_in (int, optional): Number of iterations to discard as burn-in.</span></span>
<span id="cb8-1093"><a href="#cb8-1093"></a><span class="in">            Defaults to 20000.</span></span>
<span id="cb8-1094"><a href="#cb8-1094"></a></span>
<span id="cb8-1095"><a href="#cb8-1095"></a><span class="in">    Returns:</span></span>
<span id="cb8-1096"><a href="#cb8-1096"></a><span class="in">        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]: Samples of p,</span></span>
<span id="cb8-1097"><a href="#cb8-1097"></a><span class="in">        w_1, w_2, and the fraction of accepted proposals.</span></span>
<span id="cb8-1098"><a href="#cb8-1098"></a><span class="in">    """</span></span>
<span id="cb8-1099"><a href="#cb8-1099"></a><span class="in">    feature_dim = dataset.shape[1]</span></span>
<span id="cb8-1100"><a href="#cb8-1100"></a></span>
<span id="cb8-1101"><a href="#cb8-1101"></a><span class="in">    # Initialize random starting parameters by sampling priors</span></span>
<span id="cb8-1102"><a href="#cb8-1102"></a><span class="in">    curr_p = 0.3 + 0.4 * torch.rand(1)</span></span>
<span id="cb8-1103"><a href="#cb8-1103"></a><span class="in">    curr_w_1 = torch.randn(feature_dim)</span></span>
<span id="cb8-1104"><a href="#cb8-1104"></a><span class="in">    curr_w_2 = torch.randn(feature_dim)</span></span>
<span id="cb8-1105"><a href="#cb8-1105"></a></span>
<span id="cb8-1106"><a href="#cb8-1106"></a><span class="in">    # Keep track of samples and total number of accepted proposals</span></span>
<span id="cb8-1107"><a href="#cb8-1107"></a><span class="in">    p_samples = []</span></span>
<span id="cb8-1108"><a href="#cb8-1108"></a><span class="in">    w_1_samples = []</span></span>
<span id="cb8-1109"><a href="#cb8-1109"></a><span class="in">    w_2_samples = []</span></span>
<span id="cb8-1110"><a href="#cb8-1110"></a><span class="in">    accept_count = 0 </span></span>
<span id="cb8-1111"><a href="#cb8-1111"></a></span>
<span id="cb8-1112"><a href="#cb8-1112"></a><span class="in">    for T in tqdm(range(num_iters)):</span></span>
<span id="cb8-1113"><a href="#cb8-1113"></a><span class="in">        # YOUR CODE HERE (~3 lines)</span></span>
<span id="cb8-1114"><a href="#cb8-1114"></a><span class="in">        pass # Sample proposals for p, w_1, w_2</span></span>
<span id="cb8-1115"><a href="#cb8-1115"></a><span class="in">        # END OF YOUR CODE</span></span>
<span id="cb8-1116"><a href="#cb8-1116"></a></span>
<span id="cb8-1117"><a href="#cb8-1117"></a><span class="in">        # YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb8-1118"><a href="#cb8-1118"></a><span class="in">        pass # Compute likehoods and prior densities on both the proposed and current samples</span></span>
<span id="cb8-1119"><a href="#cb8-1119"></a><span class="in">        # END OF YOUR CODE</span></span>
<span id="cb8-1120"><a href="#cb8-1120"></a></span>
<span id="cb8-1121"><a href="#cb8-1121"></a><span class="in">        # YOUR CODE HERE (~2-4 lines)</span></span>
<span id="cb8-1122"><a href="#cb8-1122"></a><span class="in">        pass # Obtain the ratios of the likelihoods and prior densities between the proposed and current samples </span></span>
<span id="cb8-1123"><a href="#cb8-1123"></a><span class="in">        # END OF YOUR CODE </span></span>
<span id="cb8-1124"><a href="#cb8-1124"></a></span>
<span id="cb8-1125"><a href="#cb8-1125"></a><span class="in">        # YOUR CODE HERE (~1-2 lines)</span></span>
<span id="cb8-1126"><a href="#cb8-1126"></a><span class="in">        pass # Multiply all ratios (both likelihoods and prior densities) and use this to calculate the acceptance probability of the proposal</span></span>
<span id="cb8-1127"><a href="#cb8-1127"></a><span class="in">        # END OF YOUR CODE</span></span>
<span id="cb8-1128"><a href="#cb8-1128"></a></span>
<span id="cb8-1129"><a href="#cb8-1129"></a><span class="in">        # YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb8-1130"><a href="#cb8-1130"></a><span class="in">        pass # Sample randomness to determine whether the proposal should be accepted to update curr_p, curr_w_1, curr_w_2, and accept_count</span></span>
<span id="cb8-1131"><a href="#cb8-1131"></a><span class="in">        # END OF YOUR CODE </span></span>
<span id="cb8-1132"><a href="#cb8-1132"></a></span>
<span id="cb8-1133"><a href="#cb8-1133"></a><span class="in">        # YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb8-1134"><a href="#cb8-1134"></a><span class="in">        pass # Update p_samples, w_1_samples, w_2_samples if we have passed the burn in period T</span></span>
<span id="cb8-1135"><a href="#cb8-1135"></a><span class="in">        # END OF YOUR CODE </span></span>
<span id="cb8-1136"><a href="#cb8-1136"></a></span>
<span id="cb8-1137"><a href="#cb8-1137"></a><span class="in">    fraction_accepted = accept_count / num_iters</span></span>
<span id="cb8-1138"><a href="#cb8-1138"></a><span class="in">    print(f"Fraction of accepted proposals: {fraction_accepted}")</span></span>
<span id="cb8-1139"><a href="#cb8-1139"></a><span class="in">    return (</span></span>
<span id="cb8-1140"><a href="#cb8-1140"></a><span class="in">        torch.stack(p_samples),</span></span>
<span id="cb8-1141"><a href="#cb8-1141"></a><span class="in">        torch.stack(w_1_samples),</span></span>
<span id="cb8-1142"><a href="#cb8-1142"></a><span class="in">        torch.stack(w_2_samples),</span></span>
<span id="cb8-1143"><a href="#cb8-1143"></a><span class="in">        fraction_accepted,</span></span>
<span id="cb8-1144"><a href="#cb8-1144"></a><span class="in">    )</span></span>
<span id="cb8-1145"><a href="#cb8-1145"></a></span>
<span id="cb8-1146"><a href="#cb8-1146"></a></span>
<span id="cb8-1147"><a href="#cb8-1147"></a><span class="in">def evaluate_metropolis(num_sims: int, num_movies: int, feature_dim: int) -&gt; None:</span></span>
<span id="cb8-1148"><a href="#cb8-1148"></a><span class="in">    """</span></span>
<span id="cb8-1149"><a href="#cb8-1149"></a><span class="in">    Runs the Metropolis-Hastings algorithm N times and compare estimated parameters</span></span>
<span id="cb8-1150"><a href="#cb8-1150"></a><span class="in">    with true parameters to obtain success rate. You should attain a success rate of around 90%. </span></span>
<span id="cb8-1151"><a href="#cb8-1151"></a></span>
<span id="cb8-1152"><a href="#cb8-1152"></a><span class="in">    Note that there are two successful equilibria to converge to. They are true_weights_1 and true_weights_2 with probabilities</span></span>
<span id="cb8-1153"><a href="#cb8-1153"></a><span class="in">    p and 1-p in addition to true_weights_2 and true_weights_1 with probabilities 1-p and p. This is why even though it may appear your</span></span>
<span id="cb8-1154"><a href="#cb8-1154"></a><span class="in">    predicted parameters don't match the true parameters, they are in fact equivalent. </span></span>
<span id="cb8-1155"><a href="#cb8-1155"></a></span>
<span id="cb8-1156"><a href="#cb8-1156"></a><span class="in">    Args:</span></span>
<span id="cb8-1157"><a href="#cb8-1157"></a><span class="in">        num_sims (int): Number of simulations to run.</span></span>
<span id="cb8-1158"><a href="#cb8-1158"></a></span>
<span id="cb8-1159"><a href="#cb8-1159"></a><span class="in">    Returns:</span></span>
<span id="cb8-1160"><a href="#cb8-1160"></a><span class="in">        None</span></span>
<span id="cb8-1161"><a href="#cb8-1161"></a><span class="in">    """</span></span>
<span id="cb8-1162"><a href="#cb8-1162"></a><span class="in">    </span></span>
<span id="cb8-1163"><a href="#cb8-1163"></a><span class="in">    success_count = 0</span></span>
<span id="cb8-1164"><a href="#cb8-1164"></a><span class="in">    for _ in range(num_sims):</span></span>
<span id="cb8-1165"><a href="#cb8-1165"></a><span class="in">        # Sample random ground truth parameters</span></span>
<span id="cb8-1166"><a href="#cb8-1166"></a><span class="in">        true_p = 0.3 + 0.4 * torch.rand(1)</span></span>
<span id="cb8-1167"><a href="#cb8-1167"></a><span class="in">        true_weights_1 = torch.randn(feature_dim)</span></span>
<span id="cb8-1168"><a href="#cb8-1168"></a><span class="in">        true_weights_2 = torch.randn(feature_dim)</span></span>
<span id="cb8-1169"><a href="#cb8-1169"></a></span>
<span id="cb8-1170"><a href="#cb8-1170"></a><span class="in">        print("\n---- MCMC Simulation ----")</span></span>
<span id="cb8-1171"><a href="#cb8-1171"></a><span class="in">        print("True parameters:", true_p, true_weights_1, true_weights_2)</span></span>
<span id="cb8-1172"><a href="#cb8-1172"></a></span>
<span id="cb8-1173"><a href="#cb8-1173"></a><span class="in">        dataset, labels = make_data(true_p, true_weights_1, true_weights_2, num_movies, feature_dim)</span></span>
<span id="cb8-1174"><a href="#cb8-1174"></a><span class="in">        p_samples, w_1_samples, w_2_samples, _ = metropolis_hastings(dataset, labels)</span></span>
<span id="cb8-1175"><a href="#cb8-1175"></a></span>
<span id="cb8-1176"><a href="#cb8-1176"></a><span class="in">        p_pred = p_samples.mean(dim=0)</span></span>
<span id="cb8-1177"><a href="#cb8-1177"></a><span class="in">        w_1_pred = w_1_samples.mean(dim=0)</span></span>
<span id="cb8-1178"><a href="#cb8-1178"></a><span class="in">        w_2_pred = w_2_samples.mean(dim=0)</span></span>
<span id="cb8-1179"><a href="#cb8-1179"></a></span>
<span id="cb8-1180"><a href="#cb8-1180"></a><span class="in">        print("Predicted parameters:", p_pred, w_1_pred, w_2_pred)</span></span>
<span id="cb8-1181"><a href="#cb8-1181"></a></span>
<span id="cb8-1182"><a href="#cb8-1182"></a><span class="in">        # Do casework on two equilibria cases to check for success</span></span>
<span id="cb8-1183"><a href="#cb8-1183"></a><span class="in">        p_diff_case_1 = torch.abs(p_pred - true_p)</span></span>
<span id="cb8-1184"><a href="#cb8-1184"></a><span class="in">        p_diff_case_2 = torch.abs(p_pred - (1 - true_p))</span></span>
<span id="cb8-1185"><a href="#cb8-1185"></a></span>
<span id="cb8-1186"><a href="#cb8-1186"></a><span class="in">        w_1_diff_case_1 = torch.max(torch.abs(w_1_pred - true_weights_1))</span></span>
<span id="cb8-1187"><a href="#cb8-1187"></a><span class="in">        w_1_diff_case_2 = torch.max(torch.abs(w_1_pred - true_weights_2))</span></span>
<span id="cb8-1188"><a href="#cb8-1188"></a></span>
<span id="cb8-1189"><a href="#cb8-1189"></a><span class="in">        w_2_diff_case_1 = torch.max(torch.abs(w_2_pred - true_weights_2))</span></span>
<span id="cb8-1190"><a href="#cb8-1190"></a><span class="in">        w_2_diff_case_2 = torch.max(torch.abs(w_2_pred - true_weights_1))</span></span>
<span id="cb8-1191"><a href="#cb8-1191"></a></span>
<span id="cb8-1192"><a href="#cb8-1192"></a><span class="in">        pass_case_1 = (</span></span>
<span id="cb8-1193"><a href="#cb8-1193"></a><span class="in">            p_diff_case_1 &lt; 0.1 and w_1_diff_case_1 &lt; 0.5 and w_2_diff_case_1 &lt; 0.5</span></span>
<span id="cb8-1194"><a href="#cb8-1194"></a><span class="in">        )</span></span>
<span id="cb8-1195"><a href="#cb8-1195"></a><span class="in">        pass_case_2 = (</span></span>
<span id="cb8-1196"><a href="#cb8-1196"></a><span class="in">            p_diff_case_2 &lt; 0.1 and w_1_diff_case_2 &lt; 0.5 and w_2_diff_case_2 &lt; 0.5</span></span>
<span id="cb8-1197"><a href="#cb8-1197"></a><span class="in">        )</span></span>
<span id="cb8-1198"><a href="#cb8-1198"></a><span class="in">        passes = pass_case_1 or pass_case_2</span></span>
<span id="cb8-1199"><a href="#cb8-1199"></a></span>
<span id="cb8-1200"><a href="#cb8-1200"></a><span class="in">        print(f'Result: {"Success" if passes else "FAILED"}')</span></span>
<span id="cb8-1201"><a href="#cb8-1201"></a><span class="in">        if passes:</span></span>
<span id="cb8-1202"><a href="#cb8-1202"></a><span class="in">            success_count += 1</span></span>
<span id="cb8-1203"><a href="#cb8-1203"></a><span class="in">    print(f'Success rate: {success_count / num_sims}')</span></span>
<span id="cb8-1204"><a href="#cb8-1204"></a></span>
<span id="cb8-1205"><a href="#cb8-1205"></a></span>
<span id="cb8-1206"><a href="#cb8-1206"></a><span class="in">if __name__ == "__main__":</span></span>
<span id="cb8-1207"><a href="#cb8-1207"></a><span class="in">    evaluate_metropolis(num_sims=10, num_movies=30000, feature_dim=10)</span></span>
<span id="cb8-1208"><a href="#cb8-1208"></a><span class="in">```</span></span>
<span id="cb8-1209"><a href="#cb8-1209"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
    <footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/chap1.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer><script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  
<script type="module">
/**
 * Factory function to create different types of cells based on options.
 * @param {Object} cellData - JSON object containing code, id, and options.
 * @returns {BaseCell} Instance of the appropriate cell class.
 */
globalThis.qpyodideCreateCell = function(cellData) {
    switch (cellData.options.context) {
        case 'interactive':
            return new InteractiveCell(cellData);
        case 'output':
            return new OutputCell(cellData);
        case 'setup':
            return new SetupCell(cellData);
        default:
            return new InteractiveCell(cellData);
            // throw new Error('Invalid cell type specified in options.');
    }
}  

/**
 * CellContainer class for managing a collection of cells.
 * @class
 */
class CellContainer {
    /**
     * Constructor for CellContainer.
     * Initializes an empty array to store cells.
     * @constructor
     */
    constructor() {
        this.cells = [];
    }

    /**
     * Add a cell to the container.
     * @param {BaseCell} cell - Instance of a cell (BaseCell or its subclasses).
     */
    addCell(cell) {
        this.cells.push(cell);
    }

    /**
     * Execute all cells in the container.
     */
    async executeAllCells() {
        for (const cell of this.cells) {
            await cell.executeCode();
        }
    }

    /**
     * Execute all cells in the container.
     */
    async autoRunExecuteAllCells() {
        for (const cell of this.cells) {
            await cell.autoRunExecuteCode();
        }
    }
}
  

/**
 * BaseCell class for handling code execution using Pyodide.
 * @class
 */
class BaseCell {
    /**
     * Constructor for BaseCell.
     * @constructor
     * @param {Object} cellData - JSON object containing code, id, and options.
     */
    constructor(cellData) {
        this.code = cellData.code;
        this.id = cellData.id;
        this.options = cellData.options;
        this.insertionLocation = document.getElementById(`qpyodide-insertion-location-${this.id}`);
        this.executionLock = false;
    }

    cellOptions() {
        // Subclass this? 
        console.log(this.options);
        return this.options;
    }

    /**
     * Execute the Python code using Pyodide.
     * @returns {*} Result of the code execution.
     */
    async executeCode() {
        // Execute code using Pyodide
        const result = getPyodide().runPython(this.code);
        return result;
    }
};

/**
 * InteractiveCell class for creating editable code editor with Monaco Editor.
 * @class
 * @extends BaseCell
 */
class InteractiveCell extends BaseCell {

    /**
     * Constructor for InteractiveCell.
     * @constructor
     * @param {Object} cellData - JSON object containing code, id, and options.
     */
    constructor(cellData) {
        super(cellData);
        this.editor = null;
        this.setupElement();
        this.setupMonacoEditor();
    }

    /**
     * Set up the interactive cell elements
     */
    setupElement() {

        // Create main div element
        var mainDiv = document.createElement('div');
        mainDiv.id = `qpyodide-interactive-area-${this.id}`;
        mainDiv.className = `qpyodide-interactive-area`;
        if (this.options.classes) {
            mainDiv.className += " " + this.options.classes
        }

        // Add a unique cell identifier that users can customize
        if (this.options.label) {
            mainDiv.setAttribute('data-id', this.options.label);
        }

        // Create toolbar div
        var toolbarDiv = document.createElement('div');
        toolbarDiv.className = 'qpyodide-editor-toolbar';
        toolbarDiv.id = `qpyodide-editor-toolbar-${this.id}`;

        // Create a div to hold the left buttons
        var leftButtonsDiv = document.createElement('div');
        leftButtonsDiv.className = 'qpyodide-editor-toolbar-left-buttons';

        // Create a div to hold the right buttons
        var rightButtonsDiv = document.createElement('div');
        rightButtonsDiv.className = 'qpyodide-editor-toolbar-right-buttons';

        // Create Run Code button
        var runCodeButton = document.createElement('button');
        runCodeButton.className = 'btn btn-default qpyodide-button qpyodide-button-run';
        runCodeButton.disabled = true;
        runCodeButton.type = 'button';
        runCodeButton.id = `qpyodide-button-run-${this.id}`;
        runCodeButton.textContent = '🟡 Loading Pyodide...';
        runCodeButton.title = `Run code (Shift + Enter)`;

        // Append buttons to the leftButtonsDiv
        leftButtonsDiv.appendChild(runCodeButton);

        // Create Reset button
        var resetButton = document.createElement('button');
        resetButton.className = 'btn btn-light btn-xs qpyodide-button qpyodide-button-reset';
        resetButton.type = 'button';
        resetButton.id = `qpyodide-button-reset-${this.id}`;
        resetButton.title = 'Start over';
        resetButton.innerHTML = '<i class="fa-solid fa-arrows-rotate"></i>';

        // Create Copy button
        var copyButton = document.createElement('button');
        copyButton.className = 'btn btn-light btn-xs qpyodide-button qpyodide-button-copy';
        copyButton.type = 'button';
        copyButton.id = `qpyodide-button-copy-${this.id}`;
        copyButton.title = 'Copy code';
        copyButton.innerHTML = '<i class="fa-regular fa-copy"></i>';

        // Append buttons to the rightButtonsDiv
        rightButtonsDiv.appendChild(resetButton);
        rightButtonsDiv.appendChild(copyButton);

        // Create console area div
        var consoleAreaDiv = document.createElement('div');
        consoleAreaDiv.id = `qpyodide-console-area-${this.id}`;
        consoleAreaDiv.className = 'qpyodide-console-area';

        // Create editor div
        var editorDiv = document.createElement('div');
        editorDiv.id = `qpyodide-editor-${this.id}`;
        editorDiv.className = 'qpyodide-editor';

        // Create output code area div
        var outputCodeAreaDiv = document.createElement('div');
        outputCodeAreaDiv.id = `qpyodide-output-code-area-${this.id}`;
        outputCodeAreaDiv.className = 'qpyodide-output-code-area';
        outputCodeAreaDiv.setAttribute('aria-live', 'assertive');

        // Create pre element inside output code area
        var preElement = document.createElement('pre');
        preElement.style.visibility = 'hidden';
        outputCodeAreaDiv.appendChild(preElement);

        // Create output graph area div
        var outputGraphAreaDiv = document.createElement('div');
        outputGraphAreaDiv.id = `qpyodide-output-graph-area-${this.id}`;
        outputGraphAreaDiv.className = 'qpyodide-output-graph-area';

        // Append buttons to the toolbar
        toolbarDiv.appendChild(leftButtonsDiv);
        toolbarDiv.appendChild(rightButtonsDiv);

        // Append all elements to the main div
        mainDiv.appendChild(toolbarDiv);
        consoleAreaDiv.appendChild(editorDiv);
        consoleAreaDiv.appendChild(outputCodeAreaDiv);
        mainDiv.appendChild(consoleAreaDiv);
        mainDiv.appendChild(outputGraphAreaDiv);

        // Insert the dynamically generated object at the document location.
        this.insertionLocation.appendChild(mainDiv);
    }

    /**
     * Set up Monaco Editor for code editing.
     */
    setupMonacoEditor() {

        // Retrieve the previously created document elements
        this.runButton = document.getElementById(`qpyodide-button-run-${this.id}`);
        this.resetButton = document.getElementById(`qpyodide-button-reset-${this.id}`);
        this.copyButton = document.getElementById(`qpyodide-button-copy-${this.id}`);
        this.editorDiv = document.getElementById(`qpyodide-editor-${this.id}`);
        this.outputCodeDiv = document.getElementById(`qpyodide-output-code-area-${this.id}`);
        this.outputGraphDiv = document.getElementById(`qpyodide-output-graph-area-${this.id}`);
        
        // Store reference to the object
        var thiz = this;

        // Load the Monaco Editor and create an instance
        require(['vs/editor/editor.main'], function () {
            thiz.editor = monaco.editor.create(
                thiz.editorDiv, {
                    value: thiz.code,
                    language: 'python',
                    theme: 'vs-light',
                    automaticLayout: true,           // Works wonderfully with RevealJS
                    scrollBeyondLastLine: false,
                    minimap: {
                        enabled: false
                    },
                    fontSize: '17.5pt',              // Bootstrap is 1 rem
                    renderLineHighlight: "none",     // Disable current line highlighting
                    hideCursorInOverviewRuler: true,  // Remove cursor indictor in right hand side scroll bar
                    readOnly: thiz.options['read-only'] ?? false
                }
            );
        
            // Store the official counter ID to be used in keyboard shortcuts
            thiz.editor.__qpyodideCounter = thiz.id;
        
            // Store the official div container ID
            thiz.editor.__qpyodideEditorId = `qpyodide-editor-${thiz.id}`;
        
            // Store the initial code value and options
            thiz.editor.__qpyodideinitialCode = thiz.code;
            thiz.editor.__qpyodideOptions = thiz.options;
        
            // Set at the model level the preferred end of line (EOL) character to LF.
            // This prevent `\r\n` from being given to the Pyodide engine if the user is on Windows.
            // See details in: https://github.com/coatless/quarto-Pyodide/issues/94
            // Associated error text: 
            // Error: <text>:1:7 unexpected input
        
            // Retrieve the underlying model
            const model = thiz.editor.getModel();
            // Set EOL for the model
            model.setEOL(monaco.editor.EndOfLineSequence.LF);
        
            // Dynamically modify the height of the editor window if new lines are added.
            let ignoreEvent = false;
            const updateHeight = () => {
            const contentHeight = thiz.editor.getContentHeight();
            // We're avoiding a width change
            //editorDiv.style.width = `${width}px`;
            thiz.editorDiv.style.height = `${contentHeight}px`;
                try {
                    ignoreEvent = true;
            
                    // The key to resizing is this call
                    thiz.editor.layout();
                } finally {
                    ignoreEvent = false;
                }
            };
        
            // Helper function to check if selected text is empty
            function isEmptyCodeText(selectedCodeText) {
                return (selectedCodeText === null || selectedCodeText === undefined || selectedCodeText === "");
            }
        
            // Registry of keyboard shortcuts that should be re-added to each editor window
            // when focus changes.
            const addPyodideKeyboardShortCutCommands = () => {
            // Add a keydown event listener for Shift+Enter to run all code in cell
            thiz.editor.addCommand(monaco.KeyMod.Shift | monaco.KeyCode.Enter, () => {
                // Retrieve all text inside the editor
                thiz.runCode(thiz.editor.getValue());
            });
        
            // Add a keydown event listener for CMD/Ctrl+Enter to run selected code
            thiz.editor.addCommand(monaco.KeyMod.CtrlCmd | monaco.KeyCode.Enter, () => {
                    // Get the selected text from the editor
                    const selectedText = thiz.editor.getModel().getValueInRange(thiz.editor.getSelection());
                    // Check if no code is selected
                    if (isEmptyCodeText(selectedText)) {
                        // Obtain the current cursor position
                        let currentPosition = thiz.editor.getPosition();
                        // Retrieve the current line content
                        let currentLine = thiz.editor.getModel().getLineContent(currentPosition.lineNumber);
                
                        // Propose a new position to move the cursor to
                        let newPosition = new monaco.Position(currentPosition.lineNumber + 1, 1);
                
                        // Check if the new position is beyond the last line of the editor
                        if (newPosition.lineNumber > thiz.editor.getModel().getLineCount()) {
                            // Add a new line at the end of the editor
                            thiz.editor.executeEdits("addNewLine", [{
                            range: new monaco.Range(newPosition.lineNumber, 1, newPosition.lineNumber, 1),
                            text: "\n", 
                            forceMoveMarkers: true,
                            }]);
                        }
                        
                        // Run the entire line of code.
                        thiz.runCode(currentLine);
                
                        // Move cursor to new position
                        thiz.editor.setPosition(newPosition);
                    } else {
                        // Code to run when Ctrl+Enter is pressed with selected code
                        thiz.runCode(selectedText);
                    }
                });
            }
        
            // Register an on focus event handler for when a code cell is selected to update
            // what keyboard shortcut commands should work.
            // This is a workaround to fix a regression that happened with multiple
            // editor windows since Monaco 0.32.0 
            // https://github.com/microsoft/monaco-editor/issues/2947
            thiz.editor.onDidFocusEditorText(addPyodideKeyboardShortCutCommands);
        
            // Register an on change event for when new code is added to the editor window
            thiz.editor.onDidContentSizeChange(updateHeight);
        
            // Manually re-update height to account for the content we inserted into the call
            updateHeight();
                
        });

        
        // Add a click event listener to the run button
        thiz.runButton.onclick = function () {
            thiz.runCode(
                thiz.editor.getValue()
            );
        };
        
        // Add a click event listener to the reset button
        thiz.copyButton.onclick = function () {
            // Retrieve current code data
            const data = thiz.editor.getValue();
            
            // Write code data onto the clipboard.
            navigator.clipboard.writeText(data || "");
        };
        
        // Add a click event listener to the copy button
        thiz.resetButton.onclick = function () {
            thiz.editor.setValue(thiz.editor.__qpyodideinitialCode);
        };
    }

    disableInteractiveCells() {
        // Enable locking of execution for the cell
        this.executionLock = true;

        // Disallowing execution of other code cells
        document.querySelectorAll(".qpyodide-button-run").forEach((btn) => {
            btn.disabled = true;
        });
    }

    enableInteractiveCells() {
        // Remove locking of execution for the cell
        this.executionLock = false;

        // All execution of other code cells
        document.querySelectorAll(".qpyodide-button-run").forEach((btn) => {
            btn.disabled = false;
        });
    }

    /**
     * Execute the Python code inside the editor.
     */
    async runCode(code) {
        
        // Check if we have an execution lock
        if (this.executeLock) return; 
        
        this.disableInteractiveCells();

        // Force wait procedure
        await mainPyodide;

        // Clear the output stock
        qpyodideResetOutputArray();

        // Generate a new canvas element, avoid attaching until the end
        let graphFigure = document.createElement("figure");
        document.pyodideMplTarget = graphFigure;

        console.log("Running code!");
        // Obtain results from the base class
        try {
            // Always check to see if the user adds new packages
            await mainPyodide.loadPackagesFromImports(code);

            // Process result
            const output = await mainPyodide.runPythonAsync(code);

            // Add output
            qpyodideAddToOutputArray(output, "stdout");
        } catch (err) {
            // Add error message
            qpyodideAddToOutputArray(err, "stderr");
            // TODO: There has to be a way to remove the Pyodide portion of the errors... 
        }

        const result = qpyodideRetrieveOutput();

        // Nullify the output area of content
        this.outputCodeDiv.innerHTML = "";
        this.outputGraphDiv.innerHTML = "";        

        // Design an output object for messages
        const pre = document.createElement("pre");
        if (/\S/.test(result)) {
            // Display results as HTML elements to retain output styling
            const div = document.createElement("div");
            div.innerHTML = result;
            pre.appendChild(div);
        } else {
            // If nothing is present, hide the element.
            pre.style.visibility = "hidden";
        }

        // Add output under interactive div
        this.outputCodeDiv.appendChild(pre);

        // Place the graphics onto the page
        if (graphFigure) {

            if (this.options['fig-cap']) {
                // Create figcaption element
                const figcaptionElement = document.createElement('figcaption');
                figcaptionElement.innerText = this.options['fig-cap'];
                // Append figcaption to figure
                graphFigure.appendChild(figcaptionElement);    
            }

            this.outputGraphDiv.appendChild(graphFigure);
        }

        // Re-enable execution
        this.enableInteractiveCells();
    }
};

/**
 * OutputCell class for customizing and displaying output.
 * @class
 * @extends BaseCell
 */
class OutputCell extends BaseCell {
    /**
     * Constructor for OutputCell.
     * @constructor
     * @param {Object} cellData - JSON object containing code, id, and options.
     */
    constructor(cellData) {
      super(cellData);
    }
  
    /**
     * Display customized output on the page.
     * @param {*} output - Result to be displayed.
     */
    displayOutput(output) {
        const results = this.executeCode();
        return results;
    }
  }

/**
 * SetupCell class for suppressed output.
 * @class
 * @extends BaseCell
 */
class SetupCell extends BaseCell {
    /**
     * Constructor for SetupCell.
     * @constructor
     * @param {Object} cellData - JSON object containing code, id, and options.
     */
    constructor(cellData) {
        super(cellData);
    }

    /**
     * Execute the Python code without displaying the results.
     */
    runSetupCode() {
        // Execute code without displaying output
        this.executeCode();
    }
};
</script>
<script type="module">
// Handle cell initialization initialization
qpyodideCellDetails.map(
    (entry) => {
      // Handle the creation of the element
      qpyodideCreateCell(entry);
    }
  );
</script>




</body></html>