<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Models of Preferences and Decisions – Machine Learning from Human Preferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../src/003-measure.html" rel="next">
<link href="../index.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-07ba0ad10f5680c660e360ac31d2f3b6.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8b864f0777c60eecff11d75b6b2e1175.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-fa3d1c749edcb96cd5cb7d620f3e5237.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-6f24586c8b15e78d85e3983c622e3e8a.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../src/002-reward_model.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Models of Preferences and Decisions</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning from Human Preferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sangttruong/mlhp" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Machine-Learning-from-Human-Preferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/002-reward_model.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Models of Preferences and Decisions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/003-measure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/004-optim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model-Free Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/005-align.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/006-conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>license.qmd</p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">2.1</span> Introduction</a></li>
  <li><a href="#sec-foundations" id="toc-sec-foundations" class="nav-link" data-scroll-target="#sec-foundations"><span class="header-section-number">2.2</span> Foundations of Preference Models</a>
  <ul class="collapse">
  <li><a href="#axiom-1-preference-models-model-choice" id="toc-axiom-1-preference-models-model-choice" class="nav-link" data-scroll-target="#axiom-1-preference-models-model-choice">Axiom 1: Construction of Choices Set</a></li>
  <li><a href="#axiom-3-preference-centers-around-utility" id="toc-axiom-3-preference-centers-around-utility" class="nav-link" data-scroll-target="#axiom-3-preference-centers-around-utility">Axiom 2: Preference Centers around Utility</a></li>
  <li><a href="#human-rationality" id="toc-human-rationality" class="nav-link" data-scroll-target="#human-rationality">Axiom 3: Rationality</a></li>
  <li><a href="#axiom-2-preference-captures-decision-making" id="toc-axiom-2-preference-captures-decision-making" class="nav-link" data-scroll-target="#axiom-2-preference-captures-decision-making">Axiom 4: Preference captures decision-making</a></li>
  </ul></li>
  <li><a href="#sec-collect" id="toc-sec-collect" class="nav-link" data-scroll-target="#sec-collect"><span class="header-section-number">2.3</span> Methods for Collecting Preference Data</a></li>
  <li><a href="#sec-models" id="toc-sec-models" class="nav-link" data-scroll-target="#sec-models"><span class="header-section-number">2.4</span> Models of Choices</a></li>
  <li><a href="#choices-aggregation" id="toc-choices-aggregation" class="nav-link" data-scroll-target="#choices-aggregation"><span class="header-section-number">2.5</span> Choices Aggregation</a></li>
  <li><a href="#sec-learning" id="toc-sec-learning" class="nav-link" data-scroll-target="#sec-learning"><span class="header-section-number">2.6</span> Parameterization and Learning of Utility Functions</a>
  <ul class="collapse">
  <li><a href="#reward-learning-with-large-language-models" id="toc-reward-learning-with-large-language-models" class="nav-link" data-scroll-target="#reward-learning-with-large-language-models"><span class="header-section-number">2.6.1</span> Reward Learning with Large Language Models</a></li>
  <li><a href="#reward-learning-in-robotics" id="toc-reward-learning-in-robotics" class="nav-link" data-scroll-target="#reward-learning-in-robotics"><span class="header-section-number">2.6.2</span> Reward Learning in Robotics</a></li>
  <li><a href="#reward-learning-with-meta-learning" id="toc-reward-learning-with-meta-learning" class="nav-link" data-scroll-target="#reward-learning-with-meta-learning"><span class="header-section-number">2.6.3</span> Reward Learning with Meta Learning</a></li>
  <li><a href="#direct-preference-optimization" id="toc-direct-preference-optimization" class="nav-link" data-scroll-target="#direct-preference-optimization"><span class="header-section-number">2.6.4</span> Direct Preference Optimization</a></li>
  <li><a href="#model-design-consideration" id="toc-model-design-consideration" class="nav-link" data-scroll-target="#model-design-consideration"><span class="header-section-number">2.6.5</span> Model Design Consideration</a></li>
  </ul></li>
  <li><a href="#multimodal-preferences" id="toc-multimodal-preferences" class="nav-link" data-scroll-target="#multimodal-preferences"><span class="header-section-number">2.7</span> Multimodal Preferences</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">2.8</span> Exercises</a>
  <ul class="collapse">
  <li><a href="#question-1-choice-modeling-15-points" id="toc-question-1-choice-modeling-15-points" class="nav-link" data-scroll-target="#question-1-choice-modeling-15-points">Question 1: Choice Modeling (15 points)</a></li>
  <li><a href="#question-2-revealed-and-stated-preferences-20-points" id="toc-question-2-revealed-and-stated-preferences-20-points" class="nav-link" data-scroll-target="#question-2-revealed-and-stated-preferences-20-points">Question 2: Revealed and Stated Preferences (20 points)</a></li>
  <li><a href="#question-3-probabilistic-multi-modal-preferences-25-points" id="toc-question-3-probabilistic-multi-modal-preferences-25-points" class="nav-link" data-scroll-target="#question-3-probabilistic-multi-modal-preferences-25-points">Question 3: Probabilistic Multi-modal Preferences (25 points)</a></li>
  <li><a href="#question-4-direct-preference-optimization-40-points" id="toc-question-4-direct-preference-optimization-40-points" class="nav-link" data-scroll-target="#question-4-direct-preference-optimization-40-points">Question 4: Direct Preference Optimization (40 points)</a></li>
  </ul></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/002-reward_model.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="ch-human-decision-making-choice-models" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Models of Preferences and Decisions</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<iframe src="https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/" style="width:45%; height:225px;">
</iframe>
<iframe src="https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/" style="width:45%; height:225px;">
</iframe>
<p><a href="https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/" class="btn btn-outline-primary" role="button">Fullscreen Part 1</a> <a href="https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/" class="btn btn-outline-primary" role="button">Fullscreen Part 2</a></p>
<section id="introduction" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">2.1</span> Introduction</h2>
<p>Human preference modeling aims to capture humans’ decision making processes in a probabilistic framework. Many problems would benefit from a quantitative perspective, enabling an understanding of how humans engage with the world. In this chapter, we will explore how one can model human preferences, including different formulations of such models, how one can optimize these models given data, and considerations one should understand to create such systems. We describe these assumptions in <a href="#sec-foundations" class="quarto-xref"><span>Section 2.2</span></a>.</p>
</section>
<section id="sec-foundations" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-foundations"><span class="header-section-number">2.2</span> Foundations of Preference Models</h2>
<!--
An alternative framework we will explore is ranking, in which we can model an ordering of given choices from most to least desirable. It is possible that there is an infinite set of options; in this case, our model will have to reason about a discretized set of options and may fail to capture the full space of possibilities a human would choose from in the real world.
-->
<section id="axiom-1-preference-models-model-choice" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="axiom-1-preference-models-model-choice">Axiom 1: Construction of Choices Set</h3>
<p>Human preference models model the preferred choices amongst a set of options. For example, this could be modeling which meal from a set of options a person will most likely choose. Preference models must enumerate the set of all possible choices included in a human decision. As such, we must ensure that the choices we enumerate capture the entire domain (collectively exhaustive) but are distinct (mutually exclusive) choices. A discrete set of choices is a constraint we canonically impose to ensure we can tractably model preferences and aptly estimate the parameters of preference models. We assume that if a new option is added to the choice set, the relative probabilities of choosing between the original options remain unchanged. This is known as Independence of Irrelevant Alternatives (IIA) property from Luce’s axiom of choices <span class="citation" data-cites="Luce1977">(<a href="#ref-Luce1977" role="doc-biblioref">Luce 1977</a>)</span>.</p>
</section>
<section id="axiom-3-preference-centers-around-utility" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="axiom-3-preference-centers-around-utility">Axiom 2: Preference Centers around Utility</h3>
<p>Human preference models are centered around the notion of utility, a scalar quantity representing the benefit or value an individual attains from selecting a given choice. We assume that the underlying utility mechanism of a human preference model captures the final decision output from a human. We use the notation <span class="math inline">\(u_{i,j}\)</span> as the utility of person <span class="math inline">\(i\)</span> choosing item <span class="math inline">\(j\)</span>. The utility is a random variable, decomposing into true utility <span class="math inline">\(u_{i,j}^*\)</span> and a random noise <span class="math inline">\(\epsilon_{i,j}\)</span>: <span class="math inline">\(u_{i,j} = u_{i,j}^* + \epsilon_{i,j}\)</span>. Only the relative difference in utility matters to predict the choice among options. As such, the scale of the utilities is irrelevant within a given set of human preference data. The scale of utilities is important when comparing across datasets or across different humans; since utility may be defined differently in various datasets or different human. A common practice to address this consideration is to standardize the utilities in each dataset based on its variance in the observed data.</p>
</section>
<section id="human-rationality" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="human-rationality">Axiom 3: Rationality</h3>
<p>Modeling decision-making must also take into account rationality. Rationality assumption provides a framework for predicting and modeling human behavior by outlining the principles that guide decision-making processes <span class="citation" data-cites="keisler2003common">(<a href="#ref-keisler2003common" role="doc-biblioref">Keisler and Lee 2003</a>)</span>. By incorporating different types of rationality, researchers can create more accurate and realistic models that reflect the complexities of human decision-making <span class="citation" data-cites="miljkovic2005rational simon1972theories">(<a href="#ref-miljkovic2005rational" role="doc-biblioref">Miljkovic 2005</a>; <a href="#ref-simon1972theories" role="doc-biblioref">Simon 1972</a>)</span>. Perfect rationality posits that individuals make decisions that maximize their utility, assuming they have complete information and the cognitive ability to process this information to make optimal choices <span class="citation" data-cites="miljkovic2005rational">(<a href="#ref-miljkovic2005rational" role="doc-biblioref">Miljkovic 2005</a>)</span>. Numerous studies have shown that this assumption frequently fails to describe actual human behavior, as individuals do not always act in ways that maximize their utility due to various constraints and biases <span class="citation" data-cites="miljkovic2005rational">(<a href="#ref-miljkovic2005rational" role="doc-biblioref">Miljkovic 2005</a>)</span>. Bounded rationality acknowledges that individuals operate within the limits of their information and cognitive capabilities. Decisions are made using heuristics rather than through exhaustive analysis, reflecting the practical constraints of real-world decision-making <span class="citation" data-cites="simon1972theories">(<a href="#ref-simon1972theories" role="doc-biblioref">Simon 1972</a>)</span>. Bounded rationality acknowledges that decisions are influenced by noise, resulting in probabilistic choice behavior: while individuals aim to maximize their utility, random factors can lead to deviations from perfectly rational choices <span class="citation" data-cites="miljkovic2005rational">(<a href="#ref-miljkovic2005rational" role="doc-biblioref">Miljkovic 2005</a>)</span>.</p>
<p>Bounded rationality can be operationalized through Boltzmann rationalit. It addresses the likelihood of a human selecting an option <span class="math inline">\(o\)</span> from a set <span class="math inline">\(O\)</span>. Desirability is represented by a value function <span class="math inline">\(v : O \rightarrow \mathbb{R}^+\)</span>, with the selection probability calculated as <span class="math inline">\(P(o) = \frac{v(o)}{\sum_{o' \in O} v(o')}\)</span>. Assuming there is an underlying reward for each option <span class="math inline">\(R(o) \in \mathbb{R}\)</span> such that <span class="math inline">\(v(o) = e^{R(o)}\)</span>, we get <span class="math inline">\(P(o) = \frac{e^{R(o)}}{\sum_{\bar{o} \in \mathcal{O}} e^{R(\bar{o})}}\)</span>. Essentially, “A human will act out a trajectory with a probability proportional to the exponentiated return they receive for the trajectory.” When choices involve trajectories <span class="math inline">\(\xi \in \Xi\)</span> (sequences of actions), the reward <span class="math inline">\(R\)</span> is typically a function of a feature vector <span class="math inline">\(\phi : \Xi \rightarrow \mathbb{R}^k\)</span>, and the probability density is given by <span class="math inline">\(p(\xi) = \frac{e^{R(\phi(\xi))}}{\int_{\Xi} e^{R(\phi(\bar{\xi}))} d\bar{\xi}}\)</span>.</p>
<p>Boltzmann rationality has the “duplicates problem,” where there is no concept of similar actions (e.g., choosing between using a car or a train for transportation, with no particular preference). The probability of making the decision is 50% for either option. However, if we now have 100 cars, under Boltzmann, we would have a 99% probability of choosing a car, which is unrealistic. To address this issue, various extensions have been proposed. One such extension is the attribute rule, which interprets options as bundles of attributes. In this rule, attributes <span class="math inline">\(X\)</span> are associated with options, and they have desirability values <span class="math inline">\(w(x)\)</span>. An attribute intensity function <span class="math inline">\(s(x, o)\)</span> indicates the degree to which an attribute is expressed in an option. The probability of choosing option <span class="math inline">\(o\)</span> is</p>
<p><span class="math display">\[P(o) = \sum_{x \in \mathcal{X}_o} \frac{w(x)}{\sum_{\bar{x} \in \mathcal{X}_o} w(\bar{x})} \cdot \frac{s(x, o)}{\sum_{\tilde{o} \in \mathcal{O}} s(x, \bar{o})}\]</span></p>
<p>This equation describes a two-step process where an attribute <span class="math inline">\(x \in X_O\)</span> is first chosen according to a Boltzmann-like rule and then an option <span class="math inline">\(o \in O\)</span> with that attribute is selected using another Boltzmann-like rule. This approach handles duplicates gracefully by effectively creating a two-layer hierarchy in choosing an option. Boltzmann rationality finds practical applications in various fields, particularly in reinforcement learning, where it models decision-making in uncertain environments. It also applies to trajectory selection, where the probability of a sequence of actions (trajectory) is proportional to the exponential return. These applications enhance the accuracy of models that interact with or predict human behavior, making Boltzmann Rationality a vital component of the models of interaction.</p>
<p>We next explore a case study to deepen our understanding of rationality: Limiting Errors due to Similar Selection (LESS) <span class="citation" data-cites="2001.04465">(<a href="#ref-2001.04465" role="doc-biblioref">Bobu et al. 2020</a>)</span>. LESS takes inspiration from the attribute rule and extends it to continuous trajectories <span class="citation" data-cites="2001.04465">(<a href="#ref-2001.04465" role="doc-biblioref">Bobu et al. 2020</a>)</span>. The key insight is that instead of creating “attributes”, which group together similar discrete options, it introduces a similarity metric on the space of continuous actions, thereby creating similar groupings on trajectories. The LESS similarity metric could be defined in trajectory space, where the trajectory is some theoretical notion of all states and actions one passes through over time. However, it is instead defined on the measured feature vector <span class="math inline">\(\phi(\xi)\)</span> associated with the agent’s trajectory <span class="math inline">\(\xi\)</span>. In practice, one can never measure the exact trajectory with perfect fidelity. The feature vector will almost necessarily map in a one-to-many fashion with trajectories. Formally, let <span class="math inline">\(\phi \in \Phi\)</span> be the set of all possible feature vectors <span class="math inline">\(\xi \in \Xi\)</span> the set of all trajectories. The set of feature vectors belonging to a set of trajectories <span class="math inline">\(\Xi' \subseteq \Xi\)</span> is <span class="math inline">\(\Phi_{\Xi'}\)</span>. We begin with equation (4) and substitute our similarity metric on feature vectors of trajectories.</p>
<p><span class="math display">\[\begin{aligned}
    P(\xi) = \frac{e^{R(\phi(\xi))}}{\sum_{\bar{\phi} \in \Phi_{\Xi}} e^{R(\hat{\phi})}} \cdot \frac{s(\phi(\xi), \bar{\xi})}{\sum_{\hat{\xi} \in \Xi} s(\phi(\xi), \bar{\xi})}
\end{aligned}\]</span></p>
<p>The probability of choosing trajectory <span class="math inline">\(\xi\)</span> is proportional to the exponentiated reward for the agent’s measured trajectory <span class="math inline">\(\phi(\xi)\)</span>, normalized by the sum of all rewards over all possible measured trajectories. The second half of the product is a normalization factor based on how similar the current trajectory is to other trajectories in feature space. We can define the similarity function as an indicator function, where <span class="math inline">\(s(x, \xi) = 1\)</span> only if <span class="math inline">\(x = \phi(\xi)\)</span>. That means that multiple trajectories with the same feature vector will effectively be considered a single option. Thus, we achieve the “bundling” of trajectories, in the same way that the attribute rule bundled options under different attributes.</p>
<p>However, setting the similarity metric as an indicator function isn’t sufficiently flexible. We want a proper metric that acts more as a continuous distance over the feature space. We instead define <span class="math inline">\(s\)</span> to be a soft similarity metric <span class="math inline">\(s : \Phi \times \Xi \rightarrow \mathbb{R}^+\)</span> with the following properties:</p>
<ol type="1">
<li><p><span class="math inline">\(s(\phi(\xi), \xi) = \max_{x \in \phi, \bar{\xi} \in \Xi} s(x, \hat{\xi}) \forall (\xi \in \Xi)\)</span></p></li>
<li><p>Symmetric: <span class="math inline">\(s(\phi(\xi), \bar{\xi}) = s(\phi(\bar{\xi}), \xi)\)</span></p></li>
<li><p>Positive Semidefinite: <span class="math inline">\(s(x, \xi) \geq 0\)</span></p></li>
</ol>
<p>Using this redefined similarity metric <span class="math inline">\(s\)</span>, we extend (5) to be a probability density on the continuous trajectory space <span class="math inline">\(\mathcal{E}\)</span>, as in (3).</p>
<p><span class="math display">\[p(\hat{\xi}) = \frac{\frac{e^{R(\phi(\xi))}}{\int_{\Xi} s(\phi(\xi), \bar{\xi}) d\bar{\xi}}}{\int_{\Xi}\frac{e^{R(\phi(\hat{\xi}))}}{\int_{\Xi} s(\phi(\hat{\xi}), \bar{\xi}) d\bar{\xi}}d\hat{\xi}} \propto \frac{e^{R(\phi(\hat{\xi}))}}{\int_{\Xi} s(\phi(\xi), \bar{\xi}) d\bar{\xi}}\]</span></p>
<p>Under this formulation, the likelihood of selecting a trajectory is inversely proportional to its feature-space similarity with other trajectories. This de-weights similar trajectories, which is the desired effect for our LESS model of human decision-making. This means, though, that the “trajectory bundle” of similar trajectories still has a reasonable probability of being chosen.</p>
</section>
<section id="axiom-2-preference-captures-decision-making" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="axiom-2-preference-captures-decision-making">Axiom 4: Preference captures decision-making</h3>
<p>Human preferences are classified into two categories: revealed preferences and stated preferences.</p>
<p>Revealed preferences are those one can observe retroactively from existing data. The implicit decision-making knowledge can be captured via learnable parameters and their usage in models which represent relationships between input decision attributes that may have little human interpretability, but enable powerful models of human preference. For health coaching, we may have information about which foods an individual has chosen previously in different contexts, allowing us to build a model from their decisions. Such data may be easier to acquire and can reflect real-world outcomes (since they are, at least theoretically, inherently based on human preferences). However, if we fail to capture sufficient context in such data, human preference models may not sufficiently capture human preferences.</p>
<p>Stated preferences are those individuals explicitly indicate in potentially experimental conditions. The explicit knowledge may be leveraged by including inductive biases during modeling (for example, the context used in a model) which are reasonable assumptions for how a human would consider a set of options.This may include controlled experiments or studies. This may be harder to obtain and somewhat biased, as they can be hypothetical or only accurately reflect a piece of the overall context of a decision. However, they enable greater control of the decision-making process.</p>
</section>
</section>
<section id="sec-collect" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-collect"><span class="header-section-number">2.3</span> Methods for Collecting Preference Data</h2>
<p>Next, we explore various mechanisms by which humans can express their preferences, including pairwise sampling, rank-order sampling, rating-scale sampling, best-worst scaling, and multiple-choice samples. In <em>pairwise sampling</em>, participants compare two options to determine which is preferred. One of the major advantage of this method is low cognitive demand for rater. Its disavantage is the limited amount of information content elicited by a sample. Next, we will see that we can trading cognitive demand for rater to elicit more nuance preference information. For example, <em>Rank-order sampling</em> captures human preferences by having participants rank items from most to least preferred. Used in voting, market research, and psychology, it provides rich preference data but is more complex and cognitively demanding than pairwise comparisons, especially for large item sets. Participants may also rank inconsistently <span class="citation" data-cites="ragain2019">(<a href="#ref-ragain2019" role="doc-biblioref">Ragain and Ugander 2019</a>)</span>.</p>
<p><em>Rating-scale sampling</em>, such as the Likert scale, is a method in which participants rate items on a fixed-point scale (e.g., 1 to 5, “Strongly Disagree” to “Strongly Agree”) to measure levels of preference towards items <span class="citation" data-cites="harpe2015">(<a href="#ref-harpe2015" role="doc-biblioref">Harpe 2015</a>)</span>. Participants can also mark a point on a continuous rating scale to indicate their preference or attitude. Commonly used in surveys, product reviews, and psychological assessments, this method provides a more nuanced measure than discrete scales. Rating-scale sampling is simple for participants to understand and use, provides rich data on the intensity of preferences, and is flexible enough for various measurements (e.g., agreement, satisfaction) <span class="citation" data-cites="harpe2015">(<a href="#ref-harpe2015" role="doc-biblioref">Harpe 2015</a>)</span>. However, rating-scale sampling methods also have limitations. Ratings can be influenced by personal biases and interpretations of scales, leading to subjectivity. There is a central tendency bias, where participants may avoid extreme ratings, resulting in clustering responses around the middle. Different participants might interpret scale points differently, and fixed-point scales may not capture the full nuance of participants’ preferences or attitudes <span class="citation" data-cites="harpe2015">(<a href="#ref-harpe2015" role="doc-biblioref">Harpe 2015</a>)</span>.</p>
<p><em>In Best-worst scaling</em> (BWS), participants are presented with items and asked to identify the most and least preferred options. The primary objective of BWS is to discern the relative importance or preference of items, making it widely applicable in various fields such as market research, health economics, and social sciences <span class="citation" data-cites="campbell2015">(<a href="#ref-campbell2015" role="doc-biblioref">Campbell and Erdem 2015</a>)</span>. BWS provides rich data on the relative importance of items, helps clarify preferences, reduces biases found in traditional rating scales, and results in utility scores that are easy to interpret. However, BWS also has limitations, including potential scale interpretation differences among participants, and design challenges to avoid biases, such as the order effect or the context in which items are presented.</p>
<p><em>Multiple-choice sampling</em> involve participants selecting one option from a set of alternatives. Multiple-choice sampling is simple for participants to understand and reflect on realistic decision-making scenarios where individuals choose one option from many. It is beneficial in complex choice scenarios, such as modes of transportation, where choices are not independent <span class="citation" data-cites="bolt2009">(<a href="#ref-bolt2009" role="doc-biblioref">Bolt and Wollack 2009</a>)</span>. Multiple-choice sampling often relies on simplistic assumptions such as the independence of irrelevant alternatives (IIA), which may not always hold true. This method may also fail to capture the variation in preferences among different individuals, as it typically records only the most preferred choice without accounting for the relative importance of other options.</p>
</section>
<section id="sec-models" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-models"><span class="header-section-number">2.4</span> Models of Choices</h2>
<section id="binary-choice-model" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="binary-choice-model">Binary Choice Model</h4>
<p>Binary choice model is centered around one item. The model predicts, for that option, after observing user choices in the past, whether that option will be chosen or not. We use binary variable <span class="math inline">\(y \in \{0, 1\}\)</span> to represent whether that choice will be picked by the user in the next phase of selection. We denote <span class="math inline">\(P = \mathbb{P}(y = 1)\)</span>. We can formally model <span class="math inline">\(y\)</span> as a function of the utility of the positive choice: <span class="math inline">\(y = \mathbb{I}[U&gt;0]\)</span>. We explore two cases based on the noise distribution. <span class="math inline">\(\psi\)</span> is the logistic function or the standard normal cummulative distribution function if noise follows logistic distribution and the standard normal distribution, repsectively: <span class="math display">\[
\mathbb{P}(u_{i,j} &gt; 0) = \mathbb{P}(u_{i,j}^* + \epsilon &gt; 0) = 1 - \mathbb{P}( \epsilon &lt; -u_{i,j}^*) = \psi(u_{i,j}^*).
\]</span></p>
</section>
<section id="bradley-terry-model" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bradley-terry-model">Bradley-Terry Model</h4>
<p>The Bradley-Terry model compares the utility of choice over all others <span class="citation" data-cites="bradley-terry-model">(<a href="#ref-bradley-terry-model" role="doc-biblioref">Bradley and Terry 1952b</a>)</span> in the set of <span class="math inline">\(J\)</span> choices <span class="math inline">\(i \in \{1, 2, \dots, J\}\)</span>. Each choice can also have its unique random noise variable representing the unobserved factor, although we can also choose to have all choices’ unobserved factors follow the same distribution (e.g.&nbsp;independent and identically distributed, IID). The noise is represented as an extreme value distribution, although we can choose alternatives such as a multivariate Gaussian distribution: <span class="math inline">\(\epsilon \sim \mathcal{N}(0, \Sigma)\)</span>. If <span class="math inline">\(\Sigma\)</span> is not a diagonal matrix, we effectively model correlations in the noise across choices, enabling us to avoid the IID assumption. In the case of the extreme value distribution, we model the probability of a user preferring choice <span class="math inline">\(i\)</span>, which we denote as <span class="math inline">\(P_i = Z^{-1}\exp(u_{i,j}^*)\)</span> where <span class="math inline">\(Z = \sum_{j = 1}^{J} \exp(u_{i,j}^*)\)</span>.</p>
</section>
<section id="ordered-preferences-model" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="ordered-preferences-model">Ordered Preferences Model</h4>
<p>Previous models do not leverage information about ordering of the available options a human can choose from: all choices were treated as independent by the model. The model aims to capture how an individual chooses between them. However, in many cases, we may introduce an inductive bias based on information about the options. For example, in a study for stated preferences, a user may be able to choose from intricately dependent options such as very poor, poor, fair, good, and great. In this case, it can be useful to include this bias in our model to represent a human’s decision-making process better. Instead of comparing choices against alternatives, we can focus on a single example and use additional parameters to define classification criteria based on the utility determined by the model. Let us suppose we have a single example with attributes <span class="math inline">\(z_i\)</span>, and wish to know which of <span class="math inline">\(J\)</span> predefined options an individual will choose from. We can define <span class="math inline">\(J - 1\)</span> parameters, which act as thresholds on the utility computed by <span class="math inline">\(u_i = u_{i,j}^*\)</span> to classify the predicted choice between these options. For example, if there are 3 predefined options, we can define parameters <span class="math inline">\(a, b \in \mathbb{R}\)</span> such that</p>
<p><span class="math display">\[
y_i =
\begin{cases}
    1 &amp; u &lt; a \\
    2 &amp; a \le u &lt; b \\
    3 &amp; \text{else}
\end{cases}
\]</span></p>
<p>By assuming the noise distribution to be either logistic or standard normal, we have <span class="math display">\[
\begin{split}
    \mathbb{P}(y_i = 1) &amp; = \mathbb{P}(u &lt; a) = \mathbb{P}(u_{i,j}^* + \epsilon &lt; a) = \psi(a-u_{i,j}^*) \\
    \mathbb{P}(y_i = 2) &amp; = \mathbb{P}(a \le u &lt; b) = \mathbb{P}(a - u_{i,j}^* \le \epsilon &lt; b - u_{i,j}^*) = \psi(b-u_{i,j}^*)  - \psi(u_{i,j}^*-a) \\
    \mathbb{P}(y_i = 3) &amp; = \mathbb{P}(u &gt; b) = \mathbb{P}(u_{i,j}^* + \epsilon &gt; b ) = \mathbb{P}( \epsilon &gt; b - u_{i,j}^*) = \psi(b-u_{i,j}^*)
\end{split}
\]</span></p>
</section>
<section id="plackett-luce-model" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="plackett-luce-model">Plackett-Luce Model</h4>
<p>We can model an open-ended ranking of the available options with the Plackett-Luce model, in which we jointly model the full sequence of choice ordering <span class="citation" data-cites="plackett_luce">(<a href="#ref-plackett_luce" role="doc-biblioref">Plackett 1975</a>)</span>. The general form models the joint distribution as the product of conditional probabilities, where each is conditioned on the preceding ranking terms. Given an ordering of <span class="math inline">\(J\)</span> choices <span class="math inline">\(\{y_1, \dots, y_J\}\)</span>, we factorize the joint probability into conditionals. Each conditional follows the Bradley-Terry model: <span class="math display">\[
\mathbb{P}(y_1, \dots, y_J) = \mathbb{P}(y_1) \cdot \mathbb{P}(y_2 | y_1) \cdot \dots \cdot \mathbb{P}(y_J | y_1, y_2, \dots y_{J - 1}) = \prod_{i = 1}^J \frac{\exp(u_{i,j}^*)}{\sum_{j \ge i} \exp(u_{i,j}^*)}
\]</span></p>
</section>
<section id="ideal-point-model" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="ideal-point-model">Ideal Point Model</h4>
<p>The ideal point model uses distance functions to compute utility for individual-choice pairs <span class="citation" data-cites="huber1976ideal">(<a href="#ref-huber1976ideal" role="doc-biblioref">Huber 1976</a>)</span>. Given vector representation <span class="math inline">\(z_i\)</span> of choice <span class="math inline">\(i\)</span> and a vector <span class="math inline">\(v_n\)</span> representing an individual <span class="math inline">\(n\)</span>, we can use a distance function to model a stochastic utility function with the unobserved factors following a specified distribution: <span class="math inline">\(u_{n, i} = \texttt{dist}(z_i, v_n) + \epsilon_{n, i}\)</span>. We assume human preferences follow the choice with maximum utility: <span class="math inline">\(y_{n, i} = \mathbb{I}[u_{n, i} &gt; u_{n, j} \ \forall i \ne j]\)</span>. The intuition is that vectors exist in a shared <span class="math inline">\(n\)</span>-dimensional space, and as such we can use geometry to match choices whose representations are closest to that of a given individual. This model can often result in faster learning compared to non-geometric approaches <span class="citation" data-cites="ideal_point tatli2022distancepreferences">(<a href="#ref-ideal_point" role="doc-biblioref">Jamieson and Nowak 2011</a>; <a href="#ref-tatli2022distancepreferences" role="doc-biblioref">Tatli, Nowak, and Vinayak 2022</a>)</span> when equipped with a distance metric. Certain distance metrics, such as Euclidian distance or inner product, can easily be biased by the scale of vectors. A distance measure such as cosine similarity, which compensates for scale by normalizing the inner product of two vectors by the product of their magnitudes, can mitigate this bias yet may discard valuable information encoded by the length of the vectors. Beyond the distance metric alone, this model places a strong inductive bias that the individual and choice representations all share a common embedding space. In some contexts, this can be a robust bias to add to the model <span class="citation" data-cites="idealpoints">(<a href="#ref-idealpoints" role="doc-biblioref">Greiner 2005</a>)</span>, but it is a key factor one must take into account before employing such a model, and is a key design choice for modeling.</p>
</section>
</section>
<section id="choices-aggregation" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="choices-aggregation"><span class="header-section-number">2.5</span> Choices Aggregation</h2>
<p>Game theory provides a mathematical framework for analyzing strategic interactions among rational agents. These models help in understanding and predicting human behavior by considering multiple criteria and the associated trade-offs. They enhance the understanding of preferences across multiple criteria and allow for richer and more accurate feedback through structured comparisons. Game-theory framings capture the complexity of preferences and interactions in decision-making processes <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>The most popular form of preference elicitation involves pairwise comparisons. Users are asked to choose between two options, such as product A or product B. This method is used in various applications like search engines, recommender systems, and interactive robotics. Key concepts include the Von Neumann Winner and the Blackwell Winner. The Von Neumann Winner refers to a distribution over objects that beats or ties every other object in the collection under the expected utility assumption. The Blackwell Winner generalizes the Von Neumann Winner for multi-criteria problems using a target set for acceptable payoff vectors <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>Game-theory framings provide a framework for preference learning along multiple criteria. These models use tools from vector-valued payoffs in game theory, with Blackwell’s approach being a key concept. This approach allows for a more comprehensive understanding of preferences by considering multiple criteria simultaneously <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>In game-theory framings, pairwise preferences are modeled as random variables. Comparisons between objects along different criteria are captured in a preference tensor <span class="math inline">\(P\)</span>. This tensor models the probability that one object is preferred over another along a specific criterion, allowing for a detailed understanding of preferences across multiple dimensions <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>The preference tensor <span class="math inline">\(P\)</span> captures object comparisons along different criteria. It is defined as: <span class="math display">\[P(i_1, i_2; j) = P(i_1 \succ i_2 \text{ along criterion } j)\]</span> where <span class="math inline">\(P(i_2, i_1; j) = 1 - P(i_1, i_2; j)\)</span>. These values are aggregated to form an overall preference matrix <span class="math inline">\(P_{ov}\)</span> <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>The Blackwell Winner is defined using a target set <span class="math inline">\(S\)</span> of acceptable score vectors. The goal is to find a distribution <span class="math inline">\(\pi^*\)</span> such that <span class="math inline">\(P(\pi^*, \pi) \in S\)</span> for all <span class="math inline">\(\pi\)</span>. This method minimizes the maximum distance to the target set, providing a robust solution to multi-criteria preference problems <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>The optimization problem for finding the Blackwell Winner is defined as: <span class="math display">\[\pi(P, S, \|\cdot\|) = \arg \min_{\pi \in \Delta_d} \left[ \max_{\pi' \in \Delta_d} \rho(P(\pi, \pi'), S) \right]\]</span> where <span class="math inline">\(\rho(u, v) = \|u - v\|\)</span>. This measures the distance to the target set, ensuring that the selected distribution is as close as possible to the ideal preference vector <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
</section>
<section id="sec-learning" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="sec-learning"><span class="header-section-number">2.6</span> Parameterization and Learning of Utility Functions</h2>
<p>The attributes representing a choice <span class="math inline">\(z_i\)</span> are crucial in defining the human preference model, as they provide the context for capturing human behavior when choice <span class="math inline">\(i\)</span> is made.</p>
<p>With an understanding of the various techniques we can use to model human preferences, we can now create robust models which utilize context attributes about the options an individual has in front of them and model their choices. However, these models on their own are powerless; their parameters are initialized randomly and we must fit the models to the actual human choice data!</p>
<p>Each of the models we have studied contain distinct parameters which aim to capture human preferences; for example <span class="math inline">\(\beta\)</span> is a parameter vector containing variables which represent a linear function to compute utility given a choice’s attributes. We can also choose to represent stochastic utility functions or embedding functions for Ideal Point Models as neural networks. But how can we compute the optimal values of these parameters?</p>
<p>In this section, we give the reader an overview of the different methods available to tune human preference model parameters using given data. We refer the reader to <span class="citation" data-cites="book_estimation_casella book_estimation_bock">(<a href="#ref-book_estimation_casella" role="doc-biblioref">Casella and Berger 1990</a>; <a href="#ref-book_estimation_bock" role="doc-biblioref">Bock et al. 2015</a>)</span> for first-principle derivations of these methods and a deeper dive into their theoretical properties (convergence, generalization, data-hungriness, etc.).</p>
<p>A common and powerful approach for computing the parameters of a model is maximum likelihood estimation <span class="citation" data-cites="book_estimation_casella book_estimation_bock">(<a href="#ref-book_estimation_casella" role="doc-biblioref">Casella and Berger 1990</a>; <a href="#ref-book_estimation_bock" role="doc-biblioref">Bock et al. 2015</a>)</span>. The likelihood of a model is the probability of the observed data given the model parameters; intuitively we wish to maximize this likelihood, as that would mean that our model associates observed human preferences in the data with high probability. We can formally define the likelihood for a model with parameters <span class="math inline">\(\beta\)</span> and a given data point <span class="math inline">\((z_i, y_i)\)</span> as: <span class="math display">\[\mathcal{L}(z_i, y_i; \beta) = \mathbb{P}(y = y_i | z_i; \beta)\]</span></p>
<p>Assuming our data is independent and identically distributed (iid), the likelihood over the entire dataset is the joint probability of all observed data as defined by the model: <span class="math display">\[\mathcal{L}(z, Y; \beta) = \prod_{i = 1}^J \mathbb{P}(y = y_i | z_i; \beta)\]</span></p>
<p>In our very first example of binary choice with logistic noise, this was simply the model’s probability of the observed preference value: <span class="math display">\[\mathcal{L}(z_i, y_i; \beta) = \frac{1}{1 + \exp^{-u_{i,j}^*}}\]</span></p>
<p>In the same case with noise following a standard normal distribution, this took the form: <span class="math display">\[\mathcal{L}(z_i, y_i; \beta) = \Phi(u_{i,j}^*)\]</span></p>
<p>Fortunately, in these cases, there are straightforward methods for parameter estimation: logistic regression and probit regression (binary or multinomial, depending on the model), respectively. We can use ordinal regression to estimate the model’s parameters for our ordered preference model.</p>
<p>Generally, the objective function commonly found in parameter learning can be optimized with stochastic gradient descent (SGD) <span class="citation" data-cites="gradient_descent">(<a href="#ref-gradient_descent" role="doc-biblioref">Ruder 2016</a>)</span>. We can define an objective function as the likelihood to maximize this objective. Since SGD minimizes a given objective, we must negate the likelihood, which ensures that a converged solution maximizes the likelihood. SGD operates by computing the gradient of the objective with respect to the parameters of the model, which provides a signal of the direction in which the parameters must move to <em>maximize</em> the objective. Then, SGD makes an update step by subtracting this gradient from the parameters (most often with a scale factor called a <em>learning rate</em>), to move the parameters in a direction which <em>minimizes</em> the objective. When the objective is the negative likelihood (or sometimes negative log-likelihood for convenience or tractability), the result is an increase in the overall likelihood.</p>
<p>In the case of logistic and Gaussian models, SGD may yield a challenging optimization problem as its stochasticity can lead to noisy updates, for example, if certain examples or batches of examples are biased. Mitigations include batched SGD, in which multiple samples are randomly sampled from the dataset at each iteration, learning rates, which reduce the impact of noisy gradient updates, and momentum and higher-order optimizers which reduce noise by using movering averages of gradients or provide better estimates of the best direction in which to update the gradients. Some models, such as those that use neural networks, may, in fact, be intractable to estimate without a method such as SGD (or its momentum-based derivatives). For example, neural networks with many layers, non-linearities, and parameters can only be efficiently computed with gradient-based methods.</p>
<section id="reward-learning-with-large-language-models" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="reward-learning-with-large-language-models"><span class="header-section-number">2.6.1</span> Reward Learning with Large Language Models</h3>
<p>Taking a step away from explicitly modeling human bias and preference, we consider applying a deep learning approach to state-of-the-art language models. We begin by introducing the concepts of <em>foundation models</em> and <em>alignment</em>. A foundation model <span class="citation" data-cites="Liang2021">(<a href="#ref-Liang2021" role="doc-biblioref">Bommasani et al. 2021</a>)</span> in machine learning typically refers to a large and pre-trained neural network model that serves as the basis for various downstream tasks. In natural language processing, models like GPT-3, Llama, and BERT are considered foundation models. They are pre-trained on a massive corpus of text data, learning to understand language and context, and are capable of various language-related tasks such as text classification, language generation, and question answering. Foundation models are important because they alleviate the need to train massive neural networks from scratch, a compute and data expensive endeavor. However, a raw foundation model, trained on a pretraining objective such as a language modeling objective, is not useful on its own. It must be aligned to respond correctly based on human preferences.</p>
<p>In short, alignment for foundation models is the process by which model behavior is aligned with human values, ethics, and societal norms. Large Language Models (LLMs) are a foundation model for natural language processing. They are trained using a next-word prediction objective, allowing them to generate coherent language. A simple way to align a Large Language Model is to train it to follow instructions in a supervised way, using instruction-response pairs curated by hand. However, this limits the upper limit of LLM performance to the performance of the annotators’ writing abilities. This type of annotation is also expensive.</p>
<p>An alternative, more promising approach is to train LLMs using reinforcement learning, potentially enabling them to surpass human-level performance. The main challenge with this method lies in defining an explicit reward function for generating free-form text. To address this, a reward model (RM) can be trained based on human preferences, providing a mechanism to score the quality of the generated text. This approach, known as Reinforcement Learning from Human Feedback (RLHF), leverages human feedback to guide model training, allowing LLMs to better align with human expectations while continuously improving performance.</p>
<div id="fig-rm-arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rm-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/arch.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rm-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: Overall architecture of a reward model based on LLM
</figcaption>
</figure>
</div>
<p>The Llama2 reward model <span class="citation" data-cites="2307.09288">(<a href="#ref-2307.09288" role="doc-biblioref">Touvron et al. 2023</a>)</span> is initialized from the pretrained Llama2 LLM. In the LLM, the last layer is a mapping <span class="math inline">\(L: \mathbb{R}^D \rightarrow \mathbb{R}^V\)</span>, where <span class="math inline">\(D\)</span> is the embedding dimension from the transformer decoder stack and <span class="math inline">\(V\)</span> is the vocabulary size. To get the RM, we replace that last layer with a randomly initialized scalar head that maps <span class="math inline">\(L: \mathbb{R}^D \rightarrow \mathbb{R}^1\)</span>. It’s important to initialize the RM from the LLM it’s meant to evaluate. This is because:</p>
<ol type="1">
<li><p>The RM will have the same “knowledge” as the LLM. This is particularly useful if evaluating things like “does the LLM know when it doesn’t know?”. However, in cases where the RM is simply evaluating helpfulness or factuality, it may be useful to have the RM know more.</p></li>
<li><p>The RM is on distribution for the LLM - it is initialized in a way where it semantically understands the LLM’s outputs.</p></li>
</ol>
<p>An RM is trained with paired preferences, following the format: <span class="math display">\[\begin{aligned}
    \langle prompt\_history, response\_accepted, response\_rejected \rangle
\end{aligned}\]</span> Prompt_history is a multiturn history of user prompts and model generations, response_accepted is the preferred final model generation by an annotator, and response_rejected is the unpreferred response. The RM is trained with a binary ranking loss with an optional margin term m(r), shown in equation (7). There is also often a small regularization term added to center the score distribution on 0. <span class="math display">\[\mathcal{L}_{\text{ranking}} = -\log(\sigma(r_\theta(x,y_c) - r_\theta(x,y_r) - m(r)))\]</span> The margin term increases the distance in scores specifically for preference pairs annotators rate as easier to separate.</p>
<div id="tbl-margin_nums" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-margin_nums-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2.1: Two variants of preference rating based margin with different magnitude.
</figcaption>
<div aria-describedby="tbl-margin_nums-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;">Significantly</td>
<td style="text-align: center;">Better</td>
<td style="text-align: center;">Slightly</td>
<td style="text-align: center;">Negligibly</td>
</tr>
<tr class="even">
<td></td>
<td style="text-align: center;">Better</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Better</td>
<td style="text-align: center;">Better / Unsure</td>
</tr>
<tr class="odd">
<td>Margin Small</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2/3</td>
<td style="text-align: center;">1/3</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td>Margin Large</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="fig-margin-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-margin-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/margin-2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-margin-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: Reward model score distribution shift caused by incorporating preference rating based margin in ranking loss. With the margin term, we observe a binary split pattern in reward distribution, especially for a larger margin.
</figcaption>
</figure>
</div>
<p>It may seem confusing how the margins were chosen. It’s primarily because the sigmoid function, which is used to normalize the raw reward model score, flattens out beyond the range of <span class="math inline">\([-4, 4]\)</span>. Thus, the maximum possible margin is eight.</p>
<p>When training or using a reward model, watching for the following is important:</p>
<ol type="1">
<li><p><strong>LLM Distribution Shift</strong>: With each finetune of the LLM, the RM should be updated through a collection of fresh human preferences using generations from the new LLM. This ensures that the RM stays aligned with the current distribution of the LLM and avoids drifting off-distribution.</p></li>
<li><p><strong>RM and LLM are coupled</strong>: An RM is generally optimized to distinguish human preferences more efficiently within the specific distribution of the LLM to be optimized. However, this specialization poses a challenge: such an RM will underperform when dealing with generations not aligned with this specific LLM distribution, such as generations from a completely different LLM.</p></li>
<li><p><strong>Training Sensitivities of RMs</strong>: Training RMs can be unstable and prone to overfitting, especially with multiple training epochs. It’s generally advisable to limit the number of epochs during RM training to avoid this issue.</p></li>
</ol>
<p>The industry has centered around optimizing for two primary qualities in LLMs: helpfulness and harmlessness (safety). There are also other axes such as factuality, reasoning, tool use, code, multilingual, and more, but these are out of scope for us. In the Llama2 paper, preference data was collected from humans for each quality, with separate guidelines. This presents a challenge for co-optimizing the final LLM towards both goals.</p>
<p>Two main approaches can be taken for Reinforcement Learning from Human Feedback (RLHF) in this context:</p>
<ol type="1">
<li><p>Train a unified reward model that integrates both datasets.</p></li>
<li><p>Train two separate reward models, one for each quality, and optimize the LLM toward both.</p></li>
</ol>
<p>Option 1 is difficult because of the tension between helpfulness and harmlessness. They trade off against each other, confusing an RM trained on both. The chosen solution was option 2, where two RMs are used to train the LLM in a piecewise fashion. The helpfulness RM is used as the primary optimization term, while the harmlessness RM acts as a penalty term, driving the behavior of the LLM away from unsafe territory only when the LLM veers beyond a certain threshold. This is formalized as follows, where <span class="math inline">\(R_s\)</span>, <span class="math inline">\(R_h\)</span>, and <span class="math inline">\(R_c\)</span> are the safety, helpfulness, and combined reward, respectively. <span class="math inline">\(g\)</span> and <span class="math inline">\(p\)</span> are the model generation and the user prompt: <span class="math display">\[\begin{aligned}
    R_c(g \mid p) =
    \begin{cases}
        R_s(g \mid p) &amp; \text{if } \text{is\_safety}(p) \text{ or } R_s(g \mid p) &lt; 0.15 \\
        R_h(g \mid p) &amp; \text{otherwise}
    \end{cases}
\end{aligned}\]</span></p>
<p>There are several open issues with reward models alluded to in the paper. For example, how best to collect human feedback? Training annotators and making sure they do the correct thing is hard. What should the guidelines be? Another question is whether RMs can be made robust to adversarial prompts. Last but not least, do RMs have well-calibrated scores? This matters for RLHF - pure preference accuracy isn’t enough.</p>
</section>
<section id="reward-learning-in-robotics" class="level3" data-number="2.6.2">
<h3 data-number="2.6.2" class="anchored" data-anchor-id="reward-learning-in-robotics"><span class="header-section-number">2.6.2</span> Reward Learning in Robotics</h3>
<p>To help set up our basic reward learning problem, consider a user and a robot. The user’s preferences or goals can be represented by an internal reward function, R(<span class="math inline">\(\xi\)</span>), which the robot needs to learn. Since the reward function isn’t explicit, there are a variety of ways that the robot can learn this reward function, which we will discuss in the next section. An example method of learning a reward function from human data is using pairwise comparison. Consider the robot example from section one, but now, the robot shows the human two possible trajectories <span class="math inline">\(\xi_A\)</span> and <span class="math inline">\(\xi_B\)</span> as depicted in the diagram below.</p>
<div id="fig-reward-robot-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-reward-robot-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/robots.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-reward-robot-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: Two different trajectories taken by a robot to prompt user ranking.
</figcaption>
</figure>
</div>
<p>The user is show both the trajectories above and asked to rank which one is better. Based on iterations of multiple trajectories and ranking, the robot is able to learn the user’s internal reward function. There quite a lot of ways that models can learn a reward function from human data. Here’s a list <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span> of some of them:</p>
<ol type="1">
<li><p>Pairwise comparison: This is the method that we saw illustrated in the previous example. The robot is able to learn based on a comparison ranking provided by the user.</p></li>
<li><p>Expert demonstrations: Experts perform the task and the robot learns the optimal reward function from these demonstrations.</p></li>
<li><p>Sub-optimal demonstrations: The robot is provided with demonstrations that are not quite as good as the expert demonstrations but it is still able to learn a noisy reward function from the demonstrations.</p></li>
<li><p>Physical Corrections: While the robot is performing the task, at each point in its trajectory (or at an arbitrary point in its trajectory) its arm is corrected to a more suitable position. Based on these corrections, the robot is able to learn the reward function.</p></li>
<li><p>Ranking: This method is similar to pairwise comparison but involves more trajectories than 2. All the trajectories may have subtle differences from each other, but these differences help provide insight to the model.</p></li>
<li><p>Trajectory Assessment: Given a single trajectory, the user rates how close it is to optimal, typically using a ranking scale.</p>
<p>Each of these methods allows the robot to refine its understanding of the user’s reward function, but their effectiveness can vary depending on the application. For instance, expert demonstrations tend to produce more reliable results but may not always be feasible in everyday tasks. Pairwise comparison and ranking methods offer more flexibility but might require a higher number of iterations.</p></li>
</ol>
</section>
<section id="reward-learning-with-meta-learning" class="level3" data-number="2.6.3">
<h3 data-number="2.6.3" class="anchored" data-anchor-id="reward-learning-with-meta-learning"><span class="header-section-number">2.6.3</span> Reward Learning with Meta Learning</h3>
<p>Learning a reward function from human preferences is an intricate and complicated task. At its core, this task is about designing algorithms that can capture what humans value based on their elicited preferences. However, due to the nuanced and multifaceted nature of human desires, learning reward functions from human can be a difficult task. Therefore, meta-learning rewards may be considered to facilitate the reward learning processes. Meta-learning, often referred to as “learning to learn,” aims to design models that can adapt to new tasks with minimal additional efforts. We discuss paper <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> in <a href="#sec-few-shot" class="quarto-xref"><span>Section 2.6.3.1</span></a> showing how meta-learning can be leveraged for few-shot preference learning, where a system can quickly adapt to a new task after only a few queries to pairwise preferences from human.</p>
<p>Moving beyond the concept of learning from pairwise preferences, in <a href="#sec-watch" class="quarto-xref"><span>Section 2.6.3.2</span></a> we discuss a different approach where meta-learning intersects with both demonstrations and rewards <span class="citation" data-cites="zhou2019watch">(<a href="#ref-zhou2019watch" role="doc-biblioref">Zhou et al. 2019</a>)</span>. This paper considers the use of both demonstrations and rewards elicited from human that guide the learning process.</p>
<p>In the regular learning setting, a model is fitted to a dataset with certain learning algorithm. The learning algorithm, for example, can be the minimization of a loss function. To formulate the “regular” learning procedure, let’s denote the training dataset as <span class="math inline">\(D\)</span>, and the test dataset as <span class="math inline">\(S\)</span>. Given a model parameterized by <span class="math inline">\(\theta\)</span>; training loss function <span class="math inline">\(L(\theta, D)\)</span>; and test loss function <span class="math inline">\(L(\theta, S)\)</span>, we can formulate a process of “regular” machine learning process as <span class="math display">\[\begin{aligned}
    \theta^\star = \arg\min_\theta\quad L(\theta, D).
\end{aligned}\]</span> Note that the minimization of the training loss function is essentially <em>one</em> possible learning algorithm. For example, instead of minimizing the loss function, one may do gradient descent with model regularization on the loss function, where the final solution may not be the one that actually minimizes the loss function. As a result, we may want to be more general and more abstract for the moment, and denote the learning algorithm as <span class="math inline">\(\mathcal{A}\)</span>. Thus, we can write <span class="math display">\[\begin{aligned}
    \theta^\star = \mathcal{A}(D),
\end{aligned}\]</span> i.e., the learning algorithm <span class="math inline">\(\mathcal{A}\)</span> takes in a training dataset and outputs a model parameter <span class="math inline">\(\theta^\star\)</span>. Then, the performance of the model is evaluated by the test loss <span class="math inline">\(L(\mathcal{A}(D), S)\)</span>. As we can see, in the regime of “regular” learning, the learning algorithm <span class="math inline">\(\mathcal{A}\)</span> is pre-defined and fixed.</p>
<p>Meta-learning, or learning-to-learn, essentially asks the question of whether one can <em>learn</em> the learning algorithm <span class="math inline">\(\mathcal{A}\)</span> from prior tasks, such that the modal can adapt to a new task more quickly/proficiently. For example, different human languages share similar ideas, and therefore a human expert who has learned many languages should be able to learn a new language easier than an average person. In other words, the human expert should have learned how to learn new languages more quickly based on their past experiences on learning languages.</p>
<p>To mathematically formulate meta-learning, we consider a family of learning algorithms <span class="math inline">\(\mathcal{A}_\omega\)</span> parameterized by <span class="math inline">\(\omega\)</span>. The “prior” tasks are represented by a set of meta-training datasets <span class="math inline">\(\{(D_i, S_i)\}_{i=1}^N\)</span> consists of <span class="math inline">\(N\)</span> pairs of training dataset <span class="math inline">\(D_i\)</span> and test dataset <span class="math inline">\(S_i\)</span>. As we noted before, a learning algorithm <span class="math inline">\(\mathcal{A}_\omega\)</span> takes in a training dataset, and outputs a model, i.e., <span class="math display">\[\begin{aligned}
    \forall i: \quad \theta^\star_i=\mathcal{A}_\omega(D_i).
\end{aligned}\]</span></p>
<p>Therefore, the <strong>meta-learning objective</strong> is <span class="math display">\[\begin{aligned}
    \min_\omega \quad \sum_{i}\ L(\mathcal{A}_\omega(D_i), S_i).
\end{aligned}\]</span> The above optimization problem gives a solution <span class="math inline">\(\omega^\star\)</span> which we use as the meta-parameter. Then, when a new task comes with a new training dataset <span class="math inline">\(D_{new}\)</span>, we can simply apply <span class="math inline">\(\theta^\star_{new}=\mathcal{A}_{\omega^\star}(D_{new})\)</span> to obtain the adapted model <span class="math inline">\(\theta^\star_{new}\)</span>. Note that we usually assume the meta-training datasets <span class="math inline">\(D_i, S_i\)</span> and the new dataset <span class="math inline">\(D_{new}\)</span> share the same underlying structure, or they come from the same distribution of datasets.</p>
<p>One of the most popular meta-learning method is Model-Agnosic Meta-Learning (MAML) <span class="citation" data-cites="finn2017model">(<a href="#ref-finn2017model" role="doc-biblioref">Finn, Abbeel, and Levine 2017</a>)</span>. In MAML, the meta-parameter <span class="math inline">\(\omega\)</span> shares the same space as the model parameter <span class="math inline">\(\theta\)</span>. At its core, in MAML the learning algorithm is defined to be <span class="math display">\[\begin{aligned}
    \mathcal{A}_\omega(D_i)=\omega-\alpha \nabla_\omega L(\omega, D_i),
\end{aligned}\]</span> where <span class="math inline">\(\alpha\)</span> is the step size. As we can see, in fact <span class="math inline">\(\omega\)</span> is defined as the initialization of fine-tuning <span class="math inline">\(\theta\)</span>. With a good <span class="math inline">\(\omega\)</span> learned, the model can adapt to a new task very quickly. In general, meta-learning can be summarized as follows: Given data from prior tasks, learn to solve a new task more quickly/proficiently. Given the general nature of meta-learning, one may be curious about whether preference learning can be benefited from meta-learning, which we discuss in the following section.</p>
<section id="sec-few-shot" class="level4" data-number="2.6.3.1">
<h4 data-number="2.6.3.1" class="anchored" data-anchor-id="sec-few-shot"><span class="header-section-number">2.6.3.1</span> Few-Shot Preference Learning for Reinforcement Learning</h4>
<p>Reinforcement learning (RL) in robotics often stumbles when it comes to devising reward functions aligning with human intentions. Preference-based RL algorithms aim to solve this by learning from human feedback, but this often demands a <em>highly impractical number of queries</em> or leads to oversimplified reward functions that don’t hold up in real-world tasks.</p>
<p>To address the impractical requirement of human queries, as we discussed in the previous section, one may apply meta-learning so that the RL agent can adapt to new tasks with fewer human queries. <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> proposes to pre-training models on previous tasks with the meta-learning method MAML <span class="citation" data-cites="finn2017model">(<a href="#ref-finn2017model" role="doc-biblioref">Finn, Abbeel, and Levine 2017</a>)</span>, and then the meta-trained model can adapt to new tasks with fewer queries.</p>
<p>We consider Reinforcement Learning (RL) settings where a state is denoted as <span class="math inline">\(s\in S\)</span>, and action is denoted as <span class="math inline">\(a\in A\)</span>, for state space <span class="math inline">\(S\)</span> and action space <span class="math inline">\(A\)</span>. The reward function <span class="math inline">\(r:S\times A \to \mathbb{R}\)</span> is unknown and need to be learned from eliciting human preferences. There are multiple tasks, where each task has its own reward function and transition probabilities. The reward model is parameterized by <span class="math inline">\(\psi\)</span>. We denote <span class="math inline">\(\hat{r}_\psi(s,a)\)</span> to be a learned estimate of an unknown ground-truth reward function <span class="math inline">\(r(s,a)\)</span>, parameterized by <span class="math inline">\(\psi\)</span>. Accordingly, a reward model determines a RL policy <span class="math inline">\(\phi\)</span> by maximizing the accumulated rewards. The preferences is learned via pairwise comparison of trajectory segments <span class="math display">\[\begin{aligned}
    \sigma = (s_t, a_t, s_{t+1}, a_{t+1}, ..., s_{t+k-1}, s_{t+k-1})
\end{aligned}\]</span> of <span class="math inline">\(k\)</span> states and actions.</p>
<p>For each pre-training task, there is a dataset <span class="math inline">\(D\)</span> consists of labeled queries <span class="math inline">\((\sigma_1, \sigma_2, y)\)</span> where <span class="math inline">\(y\in \{0, 1\}\)</span> is the label representing which trajectory is preferred. Therefore, a loss function <span class="math inline">\(L(\psi, D)\)</span> captures how well the reward model characterizes the preferences in dataset <span class="math inline">\(D\)</span>. In <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> they the preference predictor over segments using the Bradley-Terry model of paired comparisons <span class="citation" data-cites="bradley1952rank">(<a href="#ref-bradley1952rank" role="doc-biblioref">Bradley and Terry 1952a</a>)</span>, i.e., <span class="math display">\[\begin{aligned}
    P[\sigma_1 \succ \sigma_2 ] = \frac{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1})}{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1}) + \exp \sum_t \hat{r}_\psi(s_t^{2}, a_t^{2})}.
\end{aligned}\]</span> Then, the loss function is essentially a binary cross-entropy which the reward model <span class="math inline">\(\psi\)</span> aims to minimize, i.e., <span class="math display">\[\begin{aligned}
    {L}(\psi,  {D}) = - \mathbb{E}_{(\sigma^1, \sigma^2, y) \sim {D}} \left[ y(1) \log (P[\sigma_1 \succ \sigma_2 ]) + y(2)\log(1 - P[\sigma_1 \succ \sigma_2 ]) \right].
\end{aligned}\]</span></p>
<section id="method-component-1-pre-training-with-meta-learning" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="method-component-1-pre-training-with-meta-learning">Method Component 1: Pre-Training with Meta Learning</h5>
<p>To efficiently approximate the reward function <span class="math inline">\(r_\text{new}\)</span> for a new task with minimal queries, as described in <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span>, we aim to utilize a pre-trained reward function <span class="math inline">\(\hat{r}_\psi\)</span> that can be quickly fine-tuned using just a few preference comparisons. By pre-training on data from prior tasks, we can leverage the common structure across tasks to speed up the adaptation process. Although any meta-learning method is compatible, <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> opt for Model Agnostic Meta-Learning (MAML) due to its simplicity. Therefore, the pre-training update for the reward model <span class="math inline">\(\psi\)</span> is <span class="math display">\[\begin{aligned}
    \psi \xleftarrow{} \psi - \beta \nabla_\psi \sum_{i = 1}^N {L} (\psi - \alpha \nabla_\psi {L}(\psi, {D}_i), {D}_i),
\end{aligned}\]</span> where <span class="math inline">\(\alpha, \beta\)</span> are the inner and outer learning rate, respectively. We note that data <span class="math inline">\(\{D_i\}_i\)</span> of labeled preferences queries for prior tasks can come from offline datasets, simulated policies, or actual humans.</p>
</section>
<section id="method-component-2-few-shot-adaptation" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="method-component-2-few-shot-adaptation">Method Component 2: Few-Shot Adaptation</h5>
<p>With the aforementioned pre-training with meta learning, the meta-learned reward model can then be used for few-shot preference based RL during an online adaptation phase. The core procedure of the few-shot adaption is descibed as below</p>
<ol type="1">
<li><p>Given a pre-trained reward model <span class="math inline">\(\psi\)</span></p></li>
<li><p>For time step <span class="math inline">\(t=1, 2, \dots\)</span></p>
<ol type="1">
<li><p>Find pairs of trajectories <span class="math inline">\((\sigma_1, \sigma_2)\)</span> with preference uncertainty based on <span class="math inline">\(\psi\)</span>.</p></li>
<li><p>Query human preference <span class="math inline">\(y\)</span> and forms a new dataset <span class="math inline">\(D_{new}\)</span></p></li>
<li><p>Update the reward model by <span class="math inline">\(\psi'\leftarrow \psi - \alpha \nabla_\psi L(\psi, D_{new})\)</span></p></li>
<li><p>Update the policy with the new reward model <span class="math inline">\(\psi'\)</span></p></li>
</ol></li>
</ol>
<p>As mentioned in <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span>, uncertain queries are selected using the disagreement of an ensemble of reward functions over the preference predictors. Specifically, comparisons that maximize <span class="math inline">\(\texttt{std}(P[\sigma_1 \succ \sigma_2])\)</span> are selected each time feedback is collected.</p>
<p>The whole pipeline of the method is outlined in <a href="#fig-few-1" class="quarto-xref">Figure&nbsp;<span>2.4</span></a>.</p>
<div id="fig-few-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-few-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/overview-few.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-few-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.4: An overview of the proposed method in <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span>. <strong>Pre-training (left):</strong> In the pre-training phase, trajectory segment comparisons are generated using data from previously learned tasks. Then, they are used to train a reward model. <strong>Online-Adaptation (Right)</strong>: After pre-training the reward model, it is adapted to new data from human feedback. The adapted reward model is then used to train a policy for a new task in a closed loop manner.
</figcaption>
</figure>
</div>
<p>We present one set of experiment from the paper, as it illustrates the effectiveness of the proposed method in a straightforward way. The experiment test the propoesed method on the Meta-World benchmark <span class="citation" data-cites="yu2020meta">(<a href="#ref-yu2020meta" role="doc-biblioref">Yu et al. 2020</a>)</span>. Three baselines are compared with the proposed method:</p>
<ol type="1">
<li><p>SAC: The Soft-Actor Critic RL algorithm trained from ground truth rewards. This represents the standard best possible method given the ground-truth reward.</p></li>
<li><p>PEBBLE: The PEBBLE algorithm <span class="citation" data-cites="lee2021pebble">(<a href="#ref-lee2021pebble" role="doc-biblioref">Lee, Smith, and Abbeel 2021</a>)</span>. It does not use information from pripor tasks.</p></li>
<li><p>Init: This method initialize the reward model with the pretained weights from meta learning. However, instead of adapting the reward model to the new task, it performs standard updates as in PEBBLE.</p></li>
</ol>
<p>The results are shown in <a href="#fig-few-exp" class="quarto-xref">Figure&nbsp;<span>2.5</span></a>, where we can see that the proposed methord outperforms all of the baselines.</p>
<div id="fig-few-exp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-few-exp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/few-exp.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-few-exp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.5: Results on MetaWorld tasks. The title of each subplot indicates the task and number of artificial feedback queries used in training. Results for each method are shown across five seeds.
</figcaption>
</figure>
</div>
<p>This paper <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> shows that meta reward learning indeed reduce the number of queries of human preferences. However, as mentioned in the paper, there are still some drawbacks, as shown in the following.</p>
<p>Many of the queries the model pick for human preference elicitation are actually almost identical to human. After all, the model would pick the most uncertain pair of trajectories for human preference queries, and similar trajectories are for sure having high uncertainty in their preference. This suggest the need of new ways for designing the query selection strategy.</p>
<p>Moreover, despite the improved query complexity, it still needs an impractical amount of queries. As shown in <a href="#fig-few-exp" class="quarto-xref">Figure&nbsp;<span>2.5</span></a>, the “sweep into” task still needs 2500 human queries for it to work properly, which is still not ideal for what we want them to be.</p>
<p>In addition, it is mentioned in the paper that the proposed method may be even worse than training from scratch, if the new task is too out-of-distribution. Certainly, since meta-learning assumes in-distribution tasks, we cannot expect the proposed method to be good for out-of-distribution task. It is thus an interesting future direction to investigate whether one can design a method that automatically balance between using the prior information or training from scratch.</p>
</section>
</section>
<section id="sec-watch" class="level4" data-number="2.6.3.2">
<h4 data-number="2.6.3.2" class="anchored" data-anchor-id="sec-watch"><span class="header-section-number">2.6.3.2</span> Watch Try Learn</h4>
<p>Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards <span class="citation" data-cites="zhou2019watch">(<a href="#ref-zhou2019watch" role="doc-biblioref">Zhou et al. 2019</a>)</span> asks the question “How can we efficiently learn both from expert demonstrations and from trials where we only get <strong>binary</strong> feedback from a human". Why do we care about this question? In the context of robotics, a very compelling answer is the <em>cost of data-collection</em>. In a hypothetical world in which we have a vast number of <strong>expert demonstrations</strong> of robots accomplishing a large number of diverse tasks, we don’t necessarily need to worry about learning from trials or from humans. We could simply learn a very capable imitation agent to perform any task. Natural Language Processing could be seen as living in this world, because internet-scale data is available. <strong>Robots, however, are expensive</strong>, so people generally don’t have access to them, and therefore cannot use them to produce information to imitate. Similarly, <strong>human time is expensive</strong>, so even for large organizations that do have access to a lot of robots, it’s still hard to collect a lot of expert demonstrations.</p>
<p>The largest available collection of robotics datasets today is Open X-Embodiment (<span class="citation" data-cites="padalkar2023open">(<a href="#ref-padalkar2023open" role="doc-biblioref">Padalkar et al. 2023</a>)</span>), which consists of around 1M episodes from more than 300 different scenes. Even such large datastes are not enough to learn generally-capable robotic policies from imitation learning alone.</p>
<div id="fig-open-x-embodiment" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-open-x-embodiment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/open_x_embodiment.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-open-x-embodiment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.6: Visualization of the Open X-Embodiment dataset collection. Even this large-scale dataset for robot learning is not yet enough to learn generally-capable robotic policies.
</figcaption>
</figure>
</div>
<p><strong>Main insight:</strong> binary feedback is much cheaper to obtain than expert demonstrations! Instead of hiring people to act as robot operators to tell the robot exactly what to do, if there was a way of having many robots trying things in parallel, we can have humans watch videos of what the robots did and then give a success classification of whether the robot accomplished the goal. This is a much cheaper form of human supervision because the human labels don’t necessarily need to be given in real time, so one human labeler can label many trajectories in parallel, and the human doesn’t need to be a skilled robot operator.</p>
<p>Concretely, this paper seeks to learn new tasks with the following general problem setting:</p>
<ol type="1">
<li><p>We only get 1 expert demonstration of the target task</p></li>
<li><p>After seeing the expert demonstration, we have robots try to solve the task 1 or more times.</p></li>
<li><p>The user (or some pre-defined reward function) annotates each trial as success/failure.</p></li>
<li><p>The agent learns from both the demos and the annotated trials to perform well on the target task.</p></li>
</ol>
<p>Note that this work falls under the <strong>meta-learning</strong> umbrella, because we are learning an algorithm for quickly learning new tasks given new observations (demos, trials, and success labels.)</p>
<p>The <strong>main contribution</strong> of this paper is a meta-learning algorithm for incorporating demonstrations and binary feedback from trials to solve new tasks.</p>
<p>Meta-Learning deals with efficient learning of new tasks. In the context of robotics or reinforcement learning in general, <strong>how do we define tasks</strong>? We will use the Markov decision process (<strong>MDP</strong>) formalism. A task <span class="math inline">\(T_i\)</span> is described with the tuple <span class="math inline">\(\{S, A, r_i, P_i\}\)</span>.</p>
<ol type="1">
<li><p><span class="math inline">\(S\)</span> represents the <em>state-space</em> of the task, or all possible states the agent could find itself in. This work uses image-observations, so <span class="math inline">\(S\)</span> is the space of all possible RGB images.</p></li>
<li><p><span class="math inline">\(A\)</span> is the action space, meaning the set of all possible actions the agent could take. In robotics there are many ways of representing action spaces, and this work considers end-effector positions, rotations, and opening.</p></li>
<li><p><span class="math inline">\(r_i\)</span> is the reward function for the task, with function signature <span class="math inline">\(r_i : S \times A \to \mathbb{R}\)</span>. This work assumes all reward functions are binary.</p></li>
<li><p><span class="math inline">\(P_i\)</span> is the transition dynamics function. It’s a function that maps state-action pairs to probability distributions over next states.</p></li>
</ol>
<p>Notice that <span class="math inline">\(S\)</span> and <span class="math inline">\(A\)</span> are shared across tasks. Transition dynamics functions are normally also shared between tasks because they represent the laws of physics. However, this work considers environments with different objects, so they don’t share the dynamics function. Given this definition for tasks, they assume that the tasks from the data that they get come from some unknown task-generating distribution <span class="math inline">\(p(T)\)</span>.</p>
<p>Let’s give a more precise definition of the problem statement considered by <strong>Watch, Try, Learn</strong>. As the paper name suggests, there are 3 phases for the problem statement.</p>
<p><strong>Watch:</strong> During the <em>watch</em> phase, we give the agent <span class="math inline">\(K\)</span> demonstrations of the target tasks. This paper considers the case where <span class="math inline">\(K\)</span> always equals 1, and all demonstrations are successful. That is, each demonstration consists of a trajectory <span class="math inline">\(\{(s_0, a_0), \ldots, (s_H, a_H)\}\)</span> where <span class="math inline">\(H\)</span> is the task horizon, and the final state is always successful, that is <span class="math inline">\(r_i(s_H, a_H) = 1, r_i(s_j, a_j) = 0\)</span> for every <span class="math inline">\(j \neq H\)</span>.</p>
<p>Importantly, these demonstrations alone might not be sufficient for <strong>full task specification</strong>. As an example, consider a demonstration in which an apple is moved to the right, next to a pan. Seeing this demonstration alone, the task could be always moving the apple to the right, or it could be always moving the apple next to the pan, irrespective of where the pan is. The expected output after the Watch phase is a policy capable of gathering information about a task, given demonstrations.</p>
<p><strong>Try:</strong> In the Try phase, we use the agent learned during the Watch phase to attempt the task for <span class="math inline">\(L\)</span> trials. As specified earlier, this paper considers the casae where <span class="math inline">\(L\)</span> always equals 1. After the agent completes the trials, humans (or pre-programmed reward functions) provide one binary reward for each trial, indicating whether the trial was successful. The expected output of this phase is <span class="math inline">\(L\)</span> trajectories and corresponding feedback that hopefully <em>disambiguate</em> the task.</p>
<p><strong>Learn:</strong> After completing the trials, the agent must learn from both the original expert demonstrations and the trials, and become capable of solving the target task.</p>
<p><strong>Given Data:</strong> To train agents that can Watch, Try, and Learn, we are given a dataset of expert demonstrations containing multiple demos for each task, and the dataset contains hundreds of tasks. Importantly, <strong>no online interaction</strong> is needed for training, and this method trains only with <strong>supervised learning</strong> and no reinforcement learning.</p>
<p>This section describes exactly how this paper trains an agent from the given expert demonstrations, and how to incorporate the trials and human feedback into the loop.</p>
<p><strong>Training to Watch:</strong> We now describe the algorithm to obtain an agent conditioned on the given expert demonstration. In particular, what we want to obtain out of the Watch phase is a policy conditioned on a set of expert demonstrations. Formally, we want to obtain <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{d_{i,k}\})\)</span>.</p>
<p>The way we can obtain this policy is through <strong>meta-imitation learning</strong>. Given the demonstrations <span class="math inline">\(\{\textbf{d}_{i,k}\}\)</span> for task <span class="math inline">\(i\)</span>, we sample another <em>different</em> demonstration coming from the same task <span class="math inline">\(\textbf{d}_i^{\text{test}}\)</span>. The key insight here is that <span class="math inline">\(\textbf{d}_i^{\text{test}}\)</span> is an example of <strong>optimal behavior</strong> given the demonstrations. Therefore, to obtain <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{d_{i,k}\})\)</span>, we simply regress the policy to imitate actions taken on <span class="math inline">\(\textbf{d}_i^{\text{test}}\)</span>. Concretely, we train policy parameters <span class="math inline">\(\theta\)</span> to minimize the following loss:</p>
<p><span class="math inline">\(\mathcal{L}^\text{watch}(\theta, \mathcal{D}_i^*) = \mathbb{E}_{\{d_{i,k}\} \sim \mathcal{D}_i^*} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^*  \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[
- \log \pi_\theta^{\text{watch}} (a_t | s_t, \{d_{i,k}\}) \big]\)</span></p>
<p>This corresponds to doing imitation learning by minimizing the negative log-likelihood of the test trajectory actions, conditioning the policy on the entire demo set. However, how is the conditioning on the demo set achieved?</p>
<div id="fig-watch-try-learn-arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-watch-try-learn-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/watch-try-learn-architecture.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-watch-try-learn-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.7: Vision-based policy architecture that conditions on a set of demonstrations.
</figcaption>
</figure>
</div>
<p><a href="#fig-watch-try-learn-arch" class="quarto-xref">Figure&nbsp;<span>2.7</span></a> visualizes how Watch Try Learn deals with conditioning on demonstrations. In addition to using features obtained from the images of the current state, the architecture uses features from frames sampled (in order) from the demonstration episodes, which are concatenated together.</p>
<p><strong>Trying:</strong> On the <strong>Try</strong> phase, when the agent is given a set of demonstrations <span class="math inline">\(\{\textbf{d}_{i,k}\}\)</span>, we deploy the policy <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{\textbf{d}_{i,k}\})\)</span> to collect <span class="math inline">\(L\)</span> trials. There is no training involved in the Try phase, we simply condition the policy on the given demonstrations</p>
<p><strong>Training to Learn:</strong> During the Watch phase the objective was to train a policy conditioned on demonstrations <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{\textbf{d}_{i,k}\})\)</span>. The authors of Watch, Try, Learn use a similar strategy as the Watch phase for the Learn phase. We now want to train a policy that is conditioned on the demonstrations, as well as the trials and binary feedback. That is, we want to learn <span class="math inline">\(\pi_\phi^{\text{watch}}(a | s, \{\textbf{d}_{i,k}\}, \{\mathbf{\tau}_{i, l}\})\)</span>. To train the policy, we again use meta-imitation learning where we additionally sample yet another trajectory from the same task. Concretely, we train policy parameters <span class="math inline">\(\phi\)</span> to minimize the following loss:</p>
<p><span class="math inline">\(\mathcal{L}^{\text{learn}}(\phi, \mathcal{D}_i, \mathcal{D}_i^*) = \mathbb{E}_{(\{d_{i,k}\}, \{\mathbf{\tau}_{i,l}\}) \sim \mathcal{D}_i} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^* \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[
- \log \pi_\theta^{\text{learn}} (a_t | s_t, \{d_{i,k}\}, \{\tau_{i,l}\}) \big]\)</span></p>
<p>The conditioning on both the demo episodes and the trial episodes is achieved in the exact same way as in the Watch phase, and is visualized in <a href="#fig-watch-try-learn-arch" class="quarto-xref">Figure&nbsp;<span>2.7</span></a>. The architecture is simply adjusted to be able to take in more images fro mthe trial episodes.</p>
<p>In this section, we describe the evaluation suite for the paper, including the simulation benchmark used, the baselines considered, and the results.</p>
<p><strong>Gripper environment setup:</strong></p>
<div id="fig-envs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-envs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/watch-try-learn-envs.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-envs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.8: Visualization of different tasks from the simulated benchmark for Watch Try Learn.
</figcaption>
</figure>
</div>
<p><a href="#fig-envs" class="quarto-xref">Figure&nbsp;<span>2.8</span></a> illustrates the different task families considered in the simulated Gripper environment. Button Pressing, Grasping, Pushing, and Pick and Place. For each task family, the environment supports hundreds of different tasks by changing the objects in the scene and the objectives (e.g.&nbsp;which object to pick and where to place). For each task in each task family, a handful of expert demonstrations are given in a demonstrations dataset. As mentioned previously, the environment gives the agent image observations, and take in actions as end-effector (gripper) positions, angles, and opening.</p>
<p><strong>Baselines:</strong> The following three baselines are considered:</p>
<ol type="1">
<li><p><strong>Behavior Cloning</strong>: simple imitation learning based on maximum log-likelihood training using data from all tasks.</p></li>
<li><p><strong>Meta-imitation learning</strong>: This baseline corresponds to simply running the policy from the Watch step, without using any trial data. That is, we only condition on the set of expert demonstrations, but no online trials.</p></li>
<li><p><strong>Behavior Cloning + SAC</strong>: Pre-train a policy with Behavior Cloning on all data, and follow that with Reinforcement Learning fine-tuning for the specific target task, using the maximum-entropy algorithm SAC (<span class="citation" data-cites="haarnoja2018soft">(<a href="#ref-haarnoja2018soft" role="doc-biblioref">Haarnoja et al. 2018</a>)</span>).</p></li>
</ol>
<div id="fig-watch-try-learn-results" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-watch-try-learn-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/watch-try-learn-results.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-watch-try-learn-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.9: Results for Watch Try Learn on the gripper control environment, and comparisons with baselines.
</figcaption>
</figure>
</div>
<div id="tbl-watch-try-learn-table" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-watch-try-learn-table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2.2: Average success rates over all tasks.
</figcaption>
<div aria-describedby="tbl-watch-try-learn-table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"><strong>METHOD</strong></th>
<th style="text-align: center;"><strong>SUCCESS RATE</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">BC</td>
<td style="text-align: center;">.09 <span class="math inline">\(\pm\)</span> .01</td>
</tr>
<tr class="even">
<td style="text-align: left;">MIL</td>
<td style="text-align: center;">.30 <span class="math inline">\(\pm\)</span> .02</td>
</tr>
<tr class="odd">
<td style="text-align: left;">WTL, 1 TRIAL (OURS)</td>
<td style="text-align: center;">.42 <span class="math inline">\(\pm\)</span> .02</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>RL FINE-TUNING WITH SAC</strong></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">BC + SAC, 1500 TRIALS</td>
<td style="text-align: center;">.11 <span class="math inline">\(\pm\)</span> .07</td>
</tr>
<tr class="even">
<td style="text-align: left;">BC + SAC, 2000 TRIALS</td>
<td style="text-align: center;">.29 <span class="math inline">\(\pm\)</span> .10</td>
</tr>
<tr class="odd">
<td style="text-align: left;">BC + SAC, 2500 TRIALS</td>
<td style="text-align: center;">.39 <span class="math inline">\(\pm\)</span> .11</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#fig-watch-try-learn-results" class="quarto-xref">Figure&nbsp;<span>2.9</span></a> shows average success rates for Watch Try Learn compared to baselines. Watch Try Learn significantly outperforms baselines on every task family. In particular, it is far superior to Behavior Cloning, which is a very weak baseline, and it significantly surpasses Meta-Imitation Learning on 3 out of 4 task families. <a href="#tbl-watch-try-learn-table" class="quarto-xref">Table&nbsp;<span>2.2</span></a> includes comparison with BC fine-tuned with Reinforcement Learning. Even after 2500 online trials, SAC is not able to obtain the success rate that Watch Try Learn achieves after only 1 trial. Overall, Watch Try Learn exhibits very significant performance gains over prior methods.</p>
</section>
</section>
<section id="direct-preference-optimization" class="level3" data-number="2.6.4">
<h3 data-number="2.6.4" class="anchored" data-anchor-id="direct-preference-optimization"><span class="header-section-number">2.6.4</span> Direct Preference Optimization</h3>
<p>A modern method for estimating the parameters of a human preference model is direct preference optimization <span class="citation" data-cites="rafailov2023direct">(<a href="#ref-rafailov2023direct" role="doc-biblioref">Rafailov et al. 2023</a>)</span>, which is used in the context of aligning language models to human preferences. A recent approach <span class="citation" data-cites="christiano2023deep">(<a href="#ref-christiano2023deep" role="doc-biblioref">Christiano et al. 2023</a>)</span> first trains a reward model that captures human preferences and then uses proximal policy optimization to train a language model-based policy to reflect those learned preferences. Direct Preference Optimization (DPO), on the other hand, removes the need for a reward model by directly using the model likelihood of two outcomes (a preferred or highly-ranked sequence and an unpreferred or low-ranked sequence) to capture the preference represented in the data. DPO provides a simpler framework than its reinforcement learning approach and results in comparable performance with improved stability. Furthermore, it obviates the need to train a reward model, instead using a language model policy and human preference dataset to align the policy directly to human preferences.</p>
</section>
<section id="model-design-consideration" class="level3" data-number="2.6.5">
<h3 data-number="2.6.5" class="anchored" data-anchor-id="model-design-consideration"><span class="header-section-number">2.6.5</span> Model Design Consideration</h3>
<p>When designing models and learning their parameters, one must account for important tradeoffs when designing and optimizing a model to learn human preferences.</p>
<p><strong>Bias vs.&nbsp;Variance Trade-off.</strong> In modeling human preferences, we aim to ensure that predicted utilities accurately reflect overall human preferences. One key challenge is managing the bias and variance trade-off.</p>
<p>Bias refers to assumptions made during model design and training that can skew predictions. For example, in Ideal Point Models, we make the assumption that the representations we use for individuals and choices are aligned in the embedding space, and that this representation is sufficient to capture human preferences using distance metrics. However, there are myriad cases in which this may break down, for example if the two sets of vectors follow different distributions each with their own unique biases. If the representations do not come from the same domain, one may have little visibility into how a distance metric computes the final utility value for a choice for a given individual. Some ways to mitigate bias in human preference models include increasing the number of parameters in a model (allowing for better learning of patterns in the data) or removing inductive biases based on our assumptions of the underlying data.</p>
<p>On the other hand, variance refers to the model’s sensitivity to small changes in the input, which leads to significant changes in the outp ut. This phenomenon is often termed ‘overfitting’ or ‘overparameterization.’ This behavior can occur in models that have many parameters, and learn correlations in the data that do not contribute to learning human preferences, but are artifacts of noise in the dataset that one should ultimately ignore. One can address variance in models by reducing the number of parameters or incorporating biases in the model based on factors we can assume about the data.</p>
<p><strong>Model Scope.</strong> One important consideration unique to human preference models is that we wish to model individual preferences, and we may choose to do so at arbitrary granularity. For example, we can fit models to a specific individual or even multiple models for an individual, each for different purposes or contexts. On the other end of the spectrum, we may create a model to capture human preferences across large populations or the world.</p>
<p>Individual models may certainly prove to be more powerful, as they do not need to generalize across multiple individuals and can dedicate all of their parameters to learning the preferences of a single user. In the context of human behavior, this can be a significant advantage as any two individuals can be arbitrarily different or even opposite in their preferences. On the other hand, models fit only one person can tremendously overfit to the training distribution and capture noise in the data, which is not truly representative of human preferences.</p>
<p>On the end of the spectrum, models fit to the entire world may be inadequate to model human preferences for arbitrary individuals, especially those whose data it has not been fit to. As such, models may underfit the given training distribution. These models aim to generalize to many people but may fail to capture the nuances of individual preferences, especially for those whose data is not represented in the training set. As a result, they may not perform well for arbitrary individuals within the target population</p>
<p>Choosing the appropriate scope for a model is crucial. ne must balance the trade-off between overfitting to noise in highly granular models and underfitting in broader models that may not capture individual nuances.</p>
</section>
</section>
<section id="multimodal-preferences" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="multimodal-preferences"><span class="header-section-number">2.7</span> Multimodal Preferences</h2>
<p>One of the core assumptions about learning a reward function is that it is unimodal, meaning that it consists of data from one person with a certain set of preferences or a group of people with similar preferences. However, the model of unimodality often oversimplifies human preferences and their often conflicting nature. To accurately capture all the nuances of human preference, we examine a multi-modal distribution with some baseline assumptions. Consider a scenario where we, as regular drivers, make a left-hand turn at an intersection <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span>. What would we do if we saw a car speeding down the road approaching us? The figure below describes some options. Following a timid driving pattern, some vehicles would stop to let the other car go, preventing a collision. Other vehicles would be more aggressive and try to make the turn before colliding with the oncoming vehicle. Given the data of one of these driving patterns, our model (our autonomous vehicle) can make an appropriate decision. However, what if our model was given data from both aggressive and timid drivers, and we don’t know which data corresponds to which type of driver? If we applied standard learning based on comparison techniques, we see, as illustrated by the figure below, that the car would have an accident trying to find a policy close enough to both driving patterns.</p>
<div id="fig-driving-patt" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-driving-patt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/driving-patt.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-driving-patt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.10: <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span> shows the possibilities of 2 different driving patterns when a car is taking a left-hand turn at an intersection and sees another car approaching head-on.
</figcaption>
</figure>
</div>
<div id="fig-driving-coll" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-driving-coll-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/driving-coll.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-driving-coll-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.11: The figure <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span> depicts the resultant collision when we try to find a policy close enough to both the driving patterns.
</figcaption>
</figure>
</div>
<p>As illustrated by the driving example, we see that multi-modality for our reward function is extremely important and, in some cases, if it is not considered, can lead to fatal decisions <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span>. But why can’t we label the groups, which would be the timid and aggressive drivers in the driving case, and then learn separate reward functions for each driver? The first problem with this approach is that it is inefficient and time-consuming to separate the data into groups because we would have to cluster and label the data. Secondly, it would not be accurate just to split the data because a more timid driver can be aggressive when they are in a hurry.</p>
<p>To formulate this problem of learning reward functions and mixing coefficients from ranking queries in a fully observable deterministic dynamical system, we begin by describing the system as a trajectory <span class="math inline">\(\xi = (s_0, a_0, ..., s_T, a_T)\)</span>, where the sequence of states and actions represents the system’s evolution over time. Assume there are <span class="math inline">\(M\)</span> different reward functions, each representing an expert’s preferences. Using the linearity assumption in reward learning, we model each expert’s reward function as a linear combination of features in a known, fixed feature space <span class="math inline">\(\phi(\xi)\)</span>. The reward for the <span class="math inline">\(m\)</span>-th expert is given by: <span class="math display">\[R_m(\xi) = \omega^T_m \phi(\xi),\]</span> where <span class="math inline">\(\omega_m\)</span> is a vector of parameters corresponding to the <span class="math inline">\(m\)</span>-th expert’s preferences. There exists an unknown distribution over the reward parameters and we can represent this distribution with mixing coefficients <span class="math inline">\(\alpha_m\)</span> such that <span class="math inline">\(\sum_M^{m = 1} \alpha_m = 1\)</span>. Our goal is to learn reward functions and mixing coefficients using ranking queries.</p>
<p>To define our problem, let’s consider a robot who performs the following trajectories and asks a user to rank all the trajectories.</p>
<div id="fig-robot-traj" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-robot-traj-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/robot-traj.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-robot-traj-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.12: The figure <span class="citation" data-cites="myers2022learning">(<a href="#ref-myers2022learning" role="doc-biblioref">Myers et al. 2022</a>)</span> depicts a few different trajectories for an example multi-modal ranking scenario.
</figcaption>
</figure>
</div>
<p>The robot will be given back a set of trajectory rankings, coming from M humans and the objective is to learn the underlying reward function. We can represent the response of the ranking query as <span class="math inline">\(x = (\xi_{a_1},\ ...\ ,\xi_{a_K})\)</span> where <span class="math inline">\(a_1\)</span> is the index of the expert’s top choice, <span class="math inline">\(a_2\)</span> is the index of the expert’s second choice, ... and so on. With the response <span class="math inline">\(x\)</span>, we generate a probability distribution with the softmax rule <span class="citation" data-cites="myers2022learning">(<a href="#ref-myers2022learning" role="doc-biblioref">Myers et al. 2022</a>)</span>: <span class="math inline">\(Pr(x_1 = \xi_{a_1} | R = R_m) = \frac{e^R_m(\xi_{a_1})}{\sum_{j=1}^Ke^R_m(\xi_{a_j})}\)</span>. where <span class="math inline">\(R_m(\xi_{a_i})\)</span> denotes the reward assigned by the <span class="math inline">\(m\)</span>-th expert to trajectory <span class="math inline">\(\xi_{a_i}\)</span>. Then, we randomly sample our probability distribution to pick our top choice. From the remaining trajectories, we noisily choose from our distribution to rank our second-best option. We repeat this process until we have ranked all our trajectories. This follows what is known as the Plackett-Luce Ranking Model.</p>
<p>Given knowledge of the true reward function weights <span class="math inline">\(\omega_m\)</span> and mixing coefficients <span class="math inline">\(\alpha_m\)</span>, we have the following joint mass over observations x from a query Q: <span class="math inline">\(Pr(x\ |\ Q) = \sum_{m = 1}^M \alpha_m\prod_{i = 1}^K\frac{e^{\omega_m^T \Phi(\xi_{a_i})}}{\sum_{j = i}^K e^{\omega_m^T \Phi(\xi_{a_j})}}\)</span>.</p>
<p>With the above formulation of the joint mass distribution over observation and queries, we can now formulate an objective. Specifically, it is to present users with the best set of queries that learn reward weights, <span class="math inline">\(\omega\)</span>, and mixing coefficient, <span class="math inline">\(\alpha\)</span>, based upon user rankings of preferred query responses. By learning these parameters, we can have an accurate estimation of the joint mass distribution of the observations.</p>
<p>To learn these parameters, we use a Bayesian learning framework. The goal will be to learn the reward weights, <span class="math inline">\(\omega_m\)</span>, and all mixing coefficients <span class="math inline">\(\alpha_m\)</span>. Thus, define the parameters to be <span class="math inline">\(\theta = \{\omega, \alpha\}\)</span>. We start by simplifying the posterior over the parameters.</p>
<p><span class="math display">\[\begin{aligned}
\Pr(\Theta | Q^{(1)}, x^{(1)}, Q^{(2)}, x^{(2)}, \ldots) &amp; \propto \Pr(\Theta) \Pr(Q^{(1)} | x^{(1)}, Q^{(2)}, x^{(2)}, \ldots | \Theta) \\
&amp; = \Pr(\Theta) \prod_t \Pr(x^{(t)} | Q^{(t)}, \Theta, Q^{(1)}, x^{(1)}, \ldots, Q^{(t-1)}, x^{(t-1)}) \\
&amp; \propto \Pr(\Theta) \prod_t \Pr(x^{(t)} | \Theta, Q^{(t)})
\end{aligned}\]</span></p>
<p>Note that the first proportionality term is directly from Bayes rule (removing normalization constant). The first equation comes directly from the assumption that the queries at timestamp <span class="math inline">\(t\)</span> are conditionally independent of the parameters given previous queries &amp; rankings. This assumption is reasonable because the previous queries &amp; rankings ideally give all the information to inform the choice of the next set of. The last proportionality term comes from the assumption that the ranked queries are conditionally independent given the parameters</p>
<p>The prior distribution is dependent on use case. For example, in the user studies conducted by the authors to verify this method, they use a standard Gaussian for the reward weights and the mixing coefficients to be uniform on a <span class="math inline">\(M - 1\)</span> simplex to ensure that they add up to 1. Then we can use maximum likelihood estimation to compute the parameters with the simplified posterior.</p>
</section>
<section id="exercises" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="exercises"><span class="header-section-number">2.8</span> Exercises</h2>
<section id="question-1-choice-modeling-15-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="question-1-choice-modeling-15-points">Question 1: Choice Modeling (15 points)</h3>
<p>In Chapter 2, we discussed discrete choice modeling in the context of utility being a linear function. Suppose we are deciding between <span class="math inline">\(N\)</span> choices and that the utility of each choice is given by <span class="math inline">\(U_i=\beta_i\mathbf{x}+\epsilon_i\)</span> for <span class="math inline">\(i=1, 2, \cdots, N\)</span>. We view <span class="math inline">\(\mathbf{x}\)</span> as the data point that is being conditioned on for deciding which choice to select, and <span class="math inline">\(\beta_i\)</span> as the weights driving the linear utility model. The noise <span class="math inline">\(\epsilon_i\)</span> is i.i.d. sampled from a type of extreme value distribution called the <em>Gumbel</em> distribution. The standard Gumbel distribution is given by the density function <span class="math inline">\(f(x)=e^{-(x+e^{-x})}\)</span> and cumulative distribution function <span class="math inline">\(F(x)=e^{-e^{-x}}.\)</span> Fix <span class="math inline">\(i\)</span>. Our objective is to calculate <span class="math inline">\(\Pr(U_i\,\, \text{has max utility})\)</span>.</p>
<ol type="a">
<li><p><strong>(Written, 2 points)</strong>. To start, set <span class="math inline">\(U_i=t\)</span> and compute <span class="math inline">\(\Pr(U_j&lt;t)\)</span> for <span class="math inline">\(j\neq i\)</span> in terms of <span class="math inline">\(F\)</span>. Use this probability to derive an integral for <span class="math inline">\(\Pr(U_i\,\,  \text{has max utility})\)</span> over <span class="math inline">\(t\)</span> in terms of <span class="math inline">\(f\)</span> and <span class="math inline">\(F\)</span>.</p>
<p>Example of solution environment.</p></li>
<li><p><strong>(Written, 4 points)</strong>. Compute the integral derived in part (a) with the appropriate <span class="math inline">\(u\)</span>-substitution. Show your work. You should arrive at multi-class logistic regression in the end!</p></li>
</ol>
<p>Next, you will implement logistic regression to predict preferred prompt completions. We will use the preference dataset from <a href="https://huggingface.co/datasets/allenai/reward-bench">RewardBench</a>. Notice the provided <code>data/chosen_embeddings.pt</code> and <code>data/rejected_embeddings.pt</code> files. These files were constructed by feeding the prompt alongside the chosen/rejected responses through Llama3-8B-Instruct and selecting the last token’s final hidden embedding. Let <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> be two hidden embeddings with <span class="math inline">\(e_1\succ e_2\)</span>. We assume weights <span class="math inline">\(w\)</span> exist for which the Bradley-Terry reward of an embedding <span class="math inline">\(e\)</span> can be modeled as <span class="math inline">\(r=w\cdot e\)</span>. In this setting, the probability of <span class="math inline">\(e_1\succ e_2\)</span> is <span class="math display">\[\frac{e^{w\cdot e_1}}{e^{w\cdot e_1}+e^{w\cdot e_2}}=\frac{1}{1+e^{w\cdot(e_2-e_1)}}=\sigma(w\cdot(e_1-e_2)).\]</span> Hence, we can view maximum likelihood across the preference dataset with this model as logistic regression on <span class="math inline">\(e_1-e_2\)</span> without a bias term and all labels being <span class="math inline">\(1\)</span>.</p>
<p>In biasless logistic regression, we are given a dataset <span class="math inline">\(X\)</span> with <span class="math inline">\(N\)</span> rows of datapoints and <span class="math inline">\(D\)</span> features per datapoint. The weights of the model are parametrized by <span class="math inline">\(\theta\)</span>, a <span class="math inline">\(D\)</span>-dimensional column vector. Given binary labels <span class="math inline">\(y\)</span> of shape <span class="math inline">\(N\)</span> by <span class="math inline">\(1\)</span>, the binary cross-entropy loss is <span class="math display">\[J(\theta)=-\frac{1}{N}(y^T\log(\sigma(X\theta)) + (1-y)^T\log(1-\sigma(X\theta)))\]</span> where <span class="math inline">\(\sigma\)</span> is the sigmoid function and is applied element-wise along with <span class="math inline">\(\log\)</span>. The gradient of loss is <span class="math display">\[\nabla_\theta J(\theta)=\frac{1}{N}X^T(\sigma(X\theta)-y).\]</span></p>
<ol type="1">
<li><p><strong>(Coding, 3 points)</strong>. Open the file <code>logistic_regression/logistic_regression.py</code>. Implement the function <code>train</code> in the biasless case.</p></li>
<li><p><strong>(Coding, 2 points)</strong>. Implement the function <code>predict_probs</code>.</p></li>
<li><p><strong>(Written, 4 points)</strong>. Open the notebook <code>rewardbench_preferences.ipynb</code> and run all the cells. Make sure to tune the <code>learning_rate</code> and <code>num_iterations</code>. Report your final expected accuracy on the training and validation sets. How close are the two expected accuracies? You should be able to achieve <span class="math inline">\(\approx 90\%\)</span> expected accuracy on validation. You may add loss reporting to the <code>train</code> function to verify your model is improving over time.</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="ebcec3be" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="kw">class</span> LogisticRegression:</span>
<span id="cb1-5"><a href="#cb1-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-6"><a href="#cb1-6"></a>        <span class="va">self</span>.weights <span class="op">=</span> <span class="va">None</span>  <span class="co"># Initialized during training</span></span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a>    <span class="kw">def</span> train(<span class="va">self</span>, X, y, learning_rate, num_iterations):</span>
<span id="cb1-9"><a href="#cb1-9"></a>        <span class="co">"""</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="co">        Train the logistic regression model using gradient descent (no bias).</span></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="co">        Each gradient update should be with respect to the entire dataset X.</span></span>
<span id="cb1-12"><a href="#cb1-12"></a></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="co">        Parameters:</span></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="co">        - X (torch.Tensor): Training data of shape (n_samples, n_features).</span></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="co">        - y (torch.Tensor): Target labels of shape (n_samples,).</span></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="co">        """</span></span>
<span id="cb1-17"><a href="#cb1-17"></a>        n_samples, n_features <span class="op">=</span> X.shape</span>
<span id="cb1-18"><a href="#cb1-18"></a></span>
<span id="cb1-19"><a href="#cb1-19"></a>        <span class="co"># Initialize weights without the bias term</span></span>
<span id="cb1-20"><a href="#cb1-20"></a>        <span class="va">self</span>.weights <span class="op">=</span> torch.zeros(n_features)</span>
<span id="cb1-21"><a href="#cb1-21"></a></span>
<span id="cb1-22"><a href="#cb1-22"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb1-23"><a href="#cb1-23"></a>            <span class="co"># YOUR CODE HERE (~4-5 lines)</span></span>
<span id="cb1-24"><a href="#cb1-24"></a>                <span class="cf">pass</span></span>
<span id="cb1-25"><a href="#cb1-25"></a>            <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb1-26"><a href="#cb1-26"></a></span>
<span id="cb1-27"><a href="#cb1-27"></a>    <span class="kw">def</span> predict_probs(<span class="va">self</span>, X):</span>
<span id="cb1-28"><a href="#cb1-28"></a>        <span class="co">"""</span></span>
<span id="cb1-29"><a href="#cb1-29"></a><span class="co">        Predict probabilities for samples in X (no bias).</span></span>
<span id="cb1-30"><a href="#cb1-30"></a></span>
<span id="cb1-31"><a href="#cb1-31"></a><span class="co">        Parameters:</span></span>
<span id="cb1-32"><a href="#cb1-32"></a><span class="co">        - X (torch.Tensor): Input data of shape (n_samples, n_features).</span></span>
<span id="cb1-33"><a href="#cb1-33"></a></span>
<span id="cb1-34"><a href="#cb1-34"></a><span class="co">        Returns:</span></span>
<span id="cb1-35"><a href="#cb1-35"></a><span class="co">        - y_probs (torch.Tensor): Predicted probabilities.</span></span>
<span id="cb1-36"><a href="#cb1-36"></a><span class="co">        """</span></span>
<span id="cb1-37"><a href="#cb1-37"></a>        y_probs <span class="op">=</span> <span class="va">None</span></span>
<span id="cb1-38"><a href="#cb1-38"></a></span>
<span id="cb1-39"><a href="#cb1-39"></a>        <span class="co"># YOUR CODE HERE (~2-3 lines)</span></span>
<span id="cb1-40"><a href="#cb1-40"></a>        <span class="cf">pass</span></span>
<span id="cb1-41"><a href="#cb1-41"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb1-42"><a href="#cb1-42"></a></span>
<span id="cb1-43"><a href="#cb1-43"></a>        <span class="cf">return</span> y_probs</span>
<span id="cb1-44"><a href="#cb1-44"></a></span>
<span id="cb1-45"><a href="#cb1-45"></a></span>
<span id="cb1-46"><a href="#cb1-46"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb1-47"><a href="#cb1-47"></a>    <span class="co"># %%</span></span>
<span id="cb1-48"><a href="#cb1-48"></a>    <span class="co"># Load in Llama3 embeddings of prompt + completions on RewardBench</span></span>
<span id="cb1-49"><a href="#cb1-49"></a>    chosen_embeddings <span class="op">=</span> torch.load(<span class="st">'data/chosen_embeddings.pt'</span>)</span>
<span id="cb1-50"><a href="#cb1-50"></a>    rejected_embeddings <span class="op">=</span> torch.load(<span class="st">'data/rejected_embeddings.pt'</span>)</span>
<span id="cb1-51"><a href="#cb1-51"></a></span>
<span id="cb1-52"><a href="#cb1-52"></a>    <span class="co"># Subtract the embeddings according to the Bradley-Terry reward model setup presented in the problem </span></span>
<span id="cb1-53"><a href="#cb1-53"></a>    X <span class="op">=</span> (chosen_embeddings <span class="op">-</span> rejected_embeddings).to(torch.<span class="bu">float</span>)</span>
<span id="cb1-54"><a href="#cb1-54"></a>    y <span class="op">=</span> torch.ones(X.shape[<span class="dv">0</span>])</span>
<span id="cb1-55"><a href="#cb1-55"></a></span>
<span id="cb1-56"><a href="#cb1-56"></a>    <span class="co"># Split dataset 80/20 into training and validation sets</span></span>
<span id="cb1-57"><a href="#cb1-57"></a>    X_train, X_val, y_train, y_val <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)  </span>
<span id="cb1-58"><a href="#cb1-58"></a></span>
<span id="cb1-59"><a href="#cb1-59"></a>    <span class="bu">print</span>(<span class="st">"Training set size:"</span>, X_train.shape)</span>
<span id="cb1-60"><a href="#cb1-60"></a>    <span class="bu">print</span>(<span class="st">"Validation set size:"</span>, X_val.shape)</span>
<span id="cb1-61"><a href="#cb1-61"></a></span>
<span id="cb1-62"><a href="#cb1-62"></a>    model <span class="op">=</span> LogisticRegression()</span>
<span id="cb1-63"><a href="#cb1-63"></a></span>
<span id="cb1-64"><a href="#cb1-64"></a>    <span class="co"># Tune the learning_rate and num_iterations until you achieve expected validation accuracy of at least 90%</span></span>
<span id="cb1-65"><a href="#cb1-65"></a>    learning_rate <span class="op">=</span> <span class="va">None</span></span>
<span id="cb1-66"><a href="#cb1-66"></a>    num_iterations <span class="op">=</span> <span class="va">None</span></span>
<span id="cb1-67"><a href="#cb1-67"></a></span>
<span id="cb1-68"><a href="#cb1-68"></a>    model.train(X_train, y_train, learning_rate<span class="op">=</span>learning_rate, num_iterations<span class="op">=</span>num_iterations)</span>
<span id="cb1-69"><a href="#cb1-69"></a></span>
<span id="cb1-70"><a href="#cb1-70"></a>    y_train_probs <span class="op">=</span> model.predict_probs(X_train)</span>
<span id="cb1-71"><a href="#cb1-71"></a>    <span class="bu">print</span>(<span class="ss">f"Expected Train Accuracy: </span><span class="sc">{</span>y_train_probs<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-72"><a href="#cb1-72"></a></span>
<span id="cb1-73"><a href="#cb1-73"></a>    y_val_probs <span class="op">=</span> model.predict_probs(X_val)</span>
<span id="cb1-74"><a href="#cb1-74"></a>    <span class="bu">print</span>(<span class="ss">f"Expected Validation Accuracy: </span><span class="sc">{</span>y_val_probs<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>) <span class="co"># Should reach at least 90%</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="question-2-revealed-and-stated-preferences-20-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="question-2-revealed-and-stated-preferences-20-points">Question 2: Revealed and Stated Preferences (20 points)</h3>
<p>Alice and Bob are running for president. For <span class="math inline">\(R\)</span> voters, we have access to their revealed candidate preferences through some means (e.g., social media, blogs, event history). Assume there is an underlying probability <span class="math inline">\(z\)</span> of voting for Alice among the population that is unknown. The aim of this question is to estimate <span class="math inline">\(z\)</span> through <em>maximum likelihood estimation</em> by also incorporating stated preferences. In this scenario, we collect stated preferences through surveys. When surveyed, voters tend to be more likely to vote for Alice with probability <span class="math inline">\(\frac{z+1}{2}\)</span> for reasons of “political correctness.”</p>
<ol type="a">
<li><p><strong>(Written, 5 points)</strong>. Suppose there are <span class="math inline">\(R_A\)</span> revealed preferences for Alice, <span class="math inline">\(R_B\)</span> revealed preferences for Bob, <span class="math inline">\(S_A\)</span> stated preferences for Alice, and <span class="math inline">\(S_B\)</span> stated preferences for Bob. Note <span class="math inline">\(R=R_A+R_B\)</span>. Compute the log-likelihood of observing such preferences in terms of <span class="math inline">\(z, R_A, R_B, S_A, S_B\)</span>.</p></li>
<li><p><strong>(Coding, 1 point)</strong>. Implement the short function <code>stated_prob</code> in the file <code>voting/simulation.py</code>.</p></li>
<li><p><strong>(Coding, 5 points)</strong>. Implement the class <code>VotingSimulation</code>.</p></li>
<li><p><strong>(Coding, 7 points)</strong>. Implement your derived expression from part (a) in the <code>log_likelihoods</code> function.</p></li>
<li><p><strong>(Written, 2 points)</strong>. Finally, implement the <code>average_mae_mle</code> method that will allow us to visualize the mean absolute error (MAE) of our maximum likelihood estimate <span class="math inline">\(\hat{z}\)</span> (i.e., <span class="math inline">\(|\hat{z}-z|\)</span>) as the number of voters surveyed increases. Open <code>voting/visualize_sim.ipynb</code> and run the cells to get a plot of MAE vs.&nbsp;voters surveyed averaged across <span class="math inline">\(100\)</span> simulations. Attach the plot to this question and briefly explain what you notice.</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="723ec23d" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> random</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb2-5"><a href="#cb2-5"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb2-6"><a href="#cb2-6"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb2-7"><a href="#cb2-7"></a></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="kw">def</span> stated_prob(z_values):</span>
<span id="cb2-9"><a href="#cb2-9"></a>    <span class="co">"""</span></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="co">    Computes the probability of stated preferences based on z values.</span></span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="co">    </span></span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="co">    Args:</span></span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="co">        z_values (torch.Tensor): The z value(s), where z represents the true probability of voting for Alice.</span></span>
<span id="cb2-14"><a href="#cb2-14"></a></span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="co">    Returns:</span></span>
<span id="cb2-16"><a href="#cb2-16"></a><span class="co">        torch.Tensor: Probability for stated preferences, derived from z values.</span></span>
<span id="cb2-17"><a href="#cb2-17"></a><span class="co">    """</span></span>
<span id="cb2-18"><a href="#cb2-18"></a>    <span class="co"># YOUR CODE HERE (~1 line)</span></span>
<span id="cb2-19"><a href="#cb2-19"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb2-20"><a href="#cb2-20"></a></span>
<span id="cb2-21"><a href="#cb2-21"></a><span class="kw">class</span> VotingSimulation:</span>
<span id="cb2-22"><a href="#cb2-22"></a>    <span class="co">"""</span></span>
<span id="cb2-23"><a href="#cb2-23"></a><span class="co">    A class to simulate the voting process where revealed and stated preferences are generated.</span></span>
<span id="cb2-24"><a href="#cb2-24"></a><span class="co">    </span></span>
<span id="cb2-25"><a href="#cb2-25"></a><span class="co">    Attributes:</span></span>
<span id="cb2-26"><a href="#cb2-26"></a><span class="co">        R (int): Number of revealed preferences.</span></span>
<span id="cb2-27"><a href="#cb2-27"></a><span class="co">        z (float): The true probability of voting for Alice.</span></span>
<span id="cb2-28"><a href="#cb2-28"></a><span class="co">        revealed_preferences (torch.Tensor): Simulated revealed preferences of R voters using Bernoulli distribution.</span></span>
<span id="cb2-29"><a href="#cb2-29"></a><span class="co">                                             Takes on 1 for Alice, and 0 for Bob.</span></span>
<span id="cb2-30"><a href="#cb2-30"></a><span class="co">        stated_preferences (torch.Tensor): Simulated stated preferences, initialized as an empty tensor.</span></span>
<span id="cb2-31"><a href="#cb2-31"></a><span class="co">                                           Takes on 1 for Alice, and 0 for Bob.</span></span>
<span id="cb2-32"><a href="#cb2-32"></a><span class="co">    """</span></span>
<span id="cb2-33"><a href="#cb2-33"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, R, z):</span>
<span id="cb2-34"><a href="#cb2-34"></a>        <span class="va">self</span>.R <span class="op">=</span> R</span>
<span id="cb2-35"><a href="#cb2-35"></a>        <span class="va">self</span>.z <span class="op">=</span> z</span>
<span id="cb2-36"><a href="#cb2-36"></a>        <span class="va">self</span>.revealed_preferences <span class="op">=</span> <span class="va">None</span> <span class="co"># YOUR CODE HERE (~1 line)</span></span>
<span id="cb2-37"><a href="#cb2-37"></a>        <span class="va">self</span>.stated_preferences <span class="op">=</span> torch.tensor([])</span>
<span id="cb2-38"><a href="#cb2-38"></a></span>
<span id="cb2-39"><a href="#cb2-39"></a>    <span class="kw">def</span> add_survey(<span class="va">self</span>):</span>
<span id="cb2-40"><a href="#cb2-40"></a>        <span class="co">"""</span></span>
<span id="cb2-41"><a href="#cb2-41"></a><span class="co">        Simulates an additional stated preference based on stated_prob and adds it to the list.</span></span>
<span id="cb2-42"><a href="#cb2-42"></a><span class="co">        This updates the self.stated_preferences tensor by concatenating on a new simulated survey result.</span></span>
<span id="cb2-43"><a href="#cb2-43"></a><span class="co">        """</span></span>
<span id="cb2-44"><a href="#cb2-44"></a>        <span class="co"># YOUR CODE HERE (~3 lines)</span></span>
<span id="cb2-45"><a href="#cb2-45"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb2-46"><a href="#cb2-46"></a></span>
<span id="cb2-47"><a href="#cb2-47"></a><span class="kw">def</span> log_likelihoods(revealed_preferences, stated_preferences, z_values):</span>
<span id="cb2-48"><a href="#cb2-48"></a>    <span class="co">"""</span></span>
<span id="cb2-49"><a href="#cb2-49"></a><span class="co">    Computes the log likelihoods across both revealed and stated preferences.</span></span>
<span id="cb2-50"><a href="#cb2-50"></a><span class="co">    Use your answer in part (a) to help.</span></span>
<span id="cb2-51"><a href="#cb2-51"></a><span class="co">    </span></span>
<span id="cb2-52"><a href="#cb2-52"></a><span class="co">    Args:</span></span>
<span id="cb2-53"><a href="#cb2-53"></a><span class="co">        revealed_preferences (torch.Tensor): Tensor containing revealed preferences (0 or 1).</span></span>
<span id="cb2-54"><a href="#cb2-54"></a><span class="co">        stated_preferences (torch.Tensor): Tensor containing stated preferences (0 or 1).</span></span>
<span id="cb2-55"><a href="#cb2-55"></a><span class="co">        z_values (torch.Tensor): Tensor of underlying z values to calculate likelihood for.</span></span>
<span id="cb2-56"><a href="#cb2-56"></a></span>
<span id="cb2-57"><a href="#cb2-57"></a><span class="co">    Returns:</span></span>
<span id="cb2-58"><a href="#cb2-58"></a><span class="co">        torch.Tensor: Log likelihood for each z value.</span></span>
<span id="cb2-59"><a href="#cb2-59"></a><span class="co">    """</span></span>
<span id="cb2-60"><a href="#cb2-60"></a>    <span class="co"># YOUR CODE HERE (~10-16 lines)</span></span>
<span id="cb2-61"><a href="#cb2-61"></a>    <span class="cf">pass</span></span>
<span id="cb2-62"><a href="#cb2-62"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE </span></span>
<span id="cb2-63"><a href="#cb2-63"></a></span>
<span id="cb2-64"><a href="#cb2-64"></a><span class="kw">def</span> average_mae_mle(R, z, survey_count, num_sims, z_sweep):</span>
<span id="cb2-65"><a href="#cb2-65"></a>    <span class="co">"""</span></span>
<span id="cb2-66"><a href="#cb2-66"></a><span class="co">    Runs multiple simulations to compute the average mean absolute error (MAE) of Maximum Likelihood Estimation (MLE) </span></span>
<span id="cb2-67"><a href="#cb2-67"></a><span class="co">    for z after increasing number of surveys.</span></span>
<span id="cb2-68"><a href="#cb2-68"></a><span class="co">    </span></span>
<span id="cb2-69"><a href="#cb2-69"></a><span class="co">    Args:</span></span>
<span id="cb2-70"><a href="#cb2-70"></a><span class="co">        R (int): Number of revealed preferences.</span></span>
<span id="cb2-71"><a href="#cb2-71"></a><span class="co">        z (float): The true probability of voting for Alice.</span></span>
<span id="cb2-72"><a href="#cb2-72"></a><span class="co">        survey_count (int): Number of additional surveys to perform.</span></span>
<span id="cb2-73"><a href="#cb2-73"></a><span class="co">        num_sims (int): Number of simulation runs to average over.</span></span>
<span id="cb2-74"><a href="#cb2-74"></a><span class="co">        z_sweep (torch.Tensor): Range of z values to consider for maximum likelihood estimation.</span></span>
<span id="cb2-75"><a href="#cb2-75"></a></span>
<span id="cb2-76"><a href="#cb2-76"></a><span class="co">    Returns:</span></span>
<span id="cb2-77"><a href="#cb2-77"></a><span class="co">        torch.Tensor: Tensor of mean absolute errors averaged over simulations.</span></span>
<span id="cb2-78"><a href="#cb2-78"></a><span class="co">                      Should have shape (survey_count, )</span></span>
<span id="cb2-79"><a href="#cb2-79"></a><span class="co">    """</span></span>
<span id="cb2-80"><a href="#cb2-80"></a>    all_errors <span class="op">=</span> []</span>
<span id="cb2-81"><a href="#cb2-81"></a>    <span class="cf">for</span> _ <span class="kw">in</span> tqdm(<span class="bu">range</span>(num_sims)):</span>
<span id="cb2-82"><a href="#cb2-82"></a>        errors <span class="op">=</span> []</span>
<span id="cb2-83"><a href="#cb2-83"></a>        vote_simulator <span class="op">=</span> VotingSimulation(R<span class="op">=</span>R, z<span class="op">=</span>z)</span>
<span id="cb2-84"><a href="#cb2-84"></a></span>
<span id="cb2-85"><a href="#cb2-85"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(survey_count):</span>
<span id="cb2-86"><a href="#cb2-86"></a>            revealed_preferences <span class="op">=</span> vote_simulator.revealed_preferences</span>
<span id="cb2-87"><a href="#cb2-87"></a>            stated_preferences <span class="op">=</span> vote_simulator.stated_preferences</span>
<span id="cb2-88"><a href="#cb2-88"></a></span>
<span id="cb2-89"><a href="#cb2-89"></a>            <span class="co"># YOUR CODE HERE (~6-8 lines)</span></span>
<span id="cb2-90"><a href="#cb2-90"></a>            <span class="cf">pass</span> <span class="co"># Compute log_likelihoods across z_sweep. Argmax to find MLE for z. </span></span>
<span id="cb2-91"><a href="#cb2-91"></a>                 <span class="co"># Append the absolute error to errors and add a survey to the simulator.</span></span>
<span id="cb2-92"><a href="#cb2-92"></a>            <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb2-93"><a href="#cb2-93"></a></span>
<span id="cb2-94"><a href="#cb2-94"></a>        errors_tensor <span class="op">=</span> torch.stack(errors) </span>
<span id="cb2-95"><a href="#cb2-95"></a>        all_errors.append(errors_tensor)</span>
<span id="cb2-96"><a href="#cb2-96"></a></span>
<span id="cb2-97"><a href="#cb2-97"></a>    <span class="co"># Calculate the average error across simulations </span></span>
<span id="cb2-98"><a href="#cb2-98"></a>    mean_errors <span class="op">=</span> torch.stack(all_errors).mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-99"><a href="#cb2-99"></a>    <span class="cf">return</span> mean_errors</span>
<span id="cb2-100"><a href="#cb2-100"></a></span>
<span id="cb2-101"><a href="#cb2-101"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb2-102"><a href="#cb2-102"></a>    <span class="co"># DO NOT CHANGE!</span></span>
<span id="cb2-103"><a href="#cb2-103"></a>    max_surveys <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb2-104"><a href="#cb2-104"></a>    z <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb2-105"><a href="#cb2-105"></a>    R <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-106"><a href="#cb2-106"></a>    num_sims <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb2-107"><a href="#cb2-107"></a>    z_sweep <span class="op">=</span> torch.linspace(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dv">981</span>)</span>
<span id="cb2-108"><a href="#cb2-108"></a></span>
<span id="cb2-109"><a href="#cb2-109"></a>    <span class="co"># Compute and plot the errors. Attach this plot to part (d).</span></span>
<span id="cb2-110"><a href="#cb2-110"></a>    mean_errors <span class="op">=</span> average_mae_mle(R, z, max_surveys, num_sims, z_sweep)</span>
<span id="cb2-111"><a href="#cb2-111"></a>    plt.plot(mean_errors)</span>
<span id="cb2-112"><a href="#cb2-112"></a></span>
<span id="cb2-113"><a href="#cb2-113"></a>    plt.xlabel(<span class="st">'Surveys Conducted'</span>)</span>
<span id="cb2-114"><a href="#cb2-114"></a>    plt.ylabel(<span class="st">'Average Error'</span>)</span>
<span id="cb2-115"><a href="#cb2-115"></a>    plt.title(<span class="ss">f'MLE MAE Error (z=</span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>num_sims<span class="sc">}</span><span class="ss"> simulations)'</span>)</span>
<span id="cb2-116"><a href="#cb2-116"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="question-3-probabilistic-multi-modal-preferences-25-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="question-3-probabilistic-multi-modal-preferences-25-points">Question 3: Probabilistic Multi-modal Preferences (25 points)</h3>
<p>Suppose you are part of the ML team on the movie streaming site CardinalStreams. After taking CS329H, you collect a movie preferences dataset with <span class="math inline">\(30000\)</span> examples of the form <span class="math inline">\((m_1, m_2, \text{user id})\)</span> where <span class="math inline">\(m_1\)</span> and <span class="math inline">\(m_2\)</span> are movies with <span class="math inline">\(m_1\succ m_2\)</span>. The preferences come from <span class="math inline">\(600\)</span> distinct users with <span class="math inline">\(50\)</span> examples per user. Each movie has a <span class="math inline">\(10\)</span>-dimensional feature vector <span class="math inline">\(m\)</span>, and each user has a <span class="math inline">\(10\)</span>-dimensional weight vector <span class="math inline">\(u\)</span>. Given movie features <span class="math inline">\(m_1, m_2\)</span> and user weights <span class="math inline">\(u\)</span>, the user’s preference between the movies is given by a Bradley-Terry reward model, i.e., <span class="math display">\[P(m_1\succ m_2)=\frac{e^{u\cdot m_1}}{e^{u\cdot m_1} + e^{u\cdot m_2}}=\frac{1}{1+e^{u\cdot (m_2-m_1)}}=\sigma(u\cdot (m_1-m_2)).\]</span></p>
<p>You realize that trying to estimate the weights for each user with only <span class="math inline">\(50\)</span> examples will not work due to the lack of data. Instead, you choose to drop the user IDs column and shuffle the dataset in order to take a <em>multi-modal preferences</em> approach. For simplicity, you assume a model where a proportion <span class="math inline">\(p\)</span> of the users have weights <span class="math inline">\(w_1\)</span> and the other <span class="math inline">\(1-p\)</span> have weights <span class="math inline">\(w_2\)</span>. In this setting, each user belongs to one of two groups: users with weights <span class="math inline">\(w_1\)</span> are part of Group 1, and users with weights <span class="math inline">\(w_2\)</span> are part of Group 2.</p>
<ol type="a">
<li><p><strong>(Written, 3 points)</strong>. For a datapoint <span class="math inline">\((m_1, m_2)\)</span> with label <span class="math inline">\(m_1\succ m_2\)</span>, compute the data likelihood <span class="math inline">\(P(m_1\succ m_2 | p, w_1, w_2)\)</span> assuming <span class="math inline">\(p, w_1, w_2\)</span> are given.</p></li>
<li><p><strong>(Written, 3 points)</strong>. As a follow up, use the likelihood to simplify the posterior distribution of <span class="math inline">\(p, w_1, w_2\)</span> after updating on <span class="math inline">\((m_1, m_2)\)</span> leaving terms for the priors unchanged.</p></li>
<li><p><strong>(Written, 4 points)</strong>. Assume priors <span class="math inline">\(p\sim B(1, 1)\)</span>, <span class="math inline">\(w_1\sim\mathcal{N}(0, \mathbf{I})\)</span>, and <span class="math inline">\(w_2\sim\mathcal{N}(0, \mathbf{I})\)</span> where <span class="math inline">\(B\)</span> represents the Beta distribution and <span class="math inline">\(\mathcal{N}\)</span> represents the normal distribution. You will notice that the posterior from part (b) has no simple closed-form. As a result, we must resort to <em>Markov Chain Monte Carlo (MCMC)</em> approaches to sample from the posterior. These approaches allow sampling from highly complex distributions by constructing a Markov chain <span class="math inline">\(\{x_t\}_{t=1}^\infty\)</span> so that <span class="math inline">\(\lim_{t\to\infty}x_t\)</span> act as desired samples from the target distribution. You can think of a Markov chain as a sequence with the special property that <span class="math inline">\(x_{t+1}\)</span> only depends on <span class="math inline">\(x_t\)</span> for all <span class="math inline">\(t\ge 1\)</span>.</p>
<p>The most basic version of MCMC is known as Metropolis-Hastings. Assume <span class="math inline">\(\pi\)</span> is the target distribution we wish to sample from where <span class="math inline">\(\pi(z)\)</span> represents the probability density at point <span class="math inline">\(z\)</span>. Metropolis-Hastings constructs the approximating Markov chain <span class="math inline">\(x_t\)</span> as follows: a proposal <span class="math inline">\(P\)</span> for <span class="math inline">\(x_{t+1}\)</span> is made via sampling from a chosen distribution <span class="math inline">\(Q(\,\cdot\,| x_t)\)</span> (e.g., adding Gaussian noise). The acceptance probability of the proposal is given by <span class="math display">\[A= \min \left( 1, \frac{\pi(P)Q(x_t | P)}{\pi(x_t)Q(P | x_t)} \right).\]</span> That is, <span class="math display">\[x_{t+1}=\begin{cases}
P &amp; \text{with probability } A, \\
x_t &amp; \text{with probability } 1 - A.
\end{cases}\]</span> To extract our samples from <span class="math inline">\(\pi\)</span>, we run the Markov chain for <span class="math inline">\(N\)</span> timesteps and disregard the first <span class="math inline">\(T&lt;N\)</span> timesteps in what is called the <em>burn-in or mixing time</em> (i.e., our final samples are <span class="math inline">\(x_{T+1}, x_{T+2},\cdots, x_{N}\)</span>). The mixing time is needed to ensure that the Markov chain elements are representative of the distribution <span class="math inline">\(\pi\)</span> – initial elements of the chain will not be a good approximation of <span class="math inline">\(\pi\)</span> and depend more on the choice of initialization <span class="math inline">\(x_1\)</span>.</p>
<p>To build some intuition, suppose we have a biased coin that turns heads with probability <span class="math inline">\(p_{\text{heads}}\)</span>. We observe <span class="math inline">\(12\)</span> coin flips to have <span class="math inline">\(9\)</span> heads and <span class="math inline">\(3\)</span> tails. If our prior for <span class="math inline">\(p_{\text{heads}}\)</span> was <span class="math inline">\(B(1, 1)\)</span>, then our posterior will be <span class="math inline">\(B(1+9, 1+3)=B(10, 4)\)</span>. The Bayesian update is given by</p>
<p><span class="math display">\[\begin{aligned}
    P(p_{\text{heads}}|9\text{ heads}, 3\text{ tails})&amp;=\frac{P(9\text{ heads}, 3\text{ tails} | p_{\text{heads}})B(1, 1)(p_{\text{heads}})}{\int_0^1 P(9\text{ heads}, 3\text{ tails} | p_{\text{heads}})B(1, 1)(p_{\text{heads}}) dp_{\text{heads}}}\\
    &amp;=\frac{P(9\text{ heads}, 3\text{ tails} | p_{\text{heads}})}{\int_0^1 P(9\text{ heads}, 3\text{ tails} | p_{\text{heads}})  dp_{\text{heads}}}.
\end{aligned}\]</span></p>
<p><strong>Find the acceptance probablity</strong> <span class="math inline">\(A\)</span> in the setting of the biased coin assuming the proposal distribution <span class="math inline">\(Q(\cdot|x_t)=x_t+N(0,\sigma)\)</span> for given <span class="math inline">\(\sigma\)</span>. Notice that this choice of <span class="math inline">\(Q\)</span> is symmetric, i.e., <span class="math inline">\(Q(x_t|P)=Q(P|x_t)\)</span>. In addition, you will realize that is unnecessary to compute the normalizing constant of the Bayesian update (i.e., the integral in the denominator) which is why MCMC is commonly used to sample from posteriors!</p></li>
<li><p><strong>(Written + Coding, 6 points)</strong>. Implement Metropolis-Hastings to sample from the posterior distribution of the biased coin in <code>multimodal_preferences/biased_coin.py</code>. Attach a histogram of your MCMC samples overlayed on top of the true posterior <span class="math inline">\(B(10, 4)\)</span> by running <code>python biased_coin.py</code>.</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="d40b50d3" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="im">from</span> scipy.stats <span class="im">import</span> beta</span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="kw">def</span> likelihood(p: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb3-6"><a href="#cb3-6"></a>    <span class="co">"""</span></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="co">    Computes the likelihood of 9 heads and 3 tails assuming p_heads is p.</span></span>
<span id="cb3-8"><a href="#cb3-8"></a></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="co">    Args:</span></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="co">    p (float): A value between 0 and 1 representing the probability of heads.</span></span>
<span id="cb3-11"><a href="#cb3-11"></a></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="co">    Returns:</span></span>
<span id="cb3-13"><a href="#cb3-13"></a><span class="co">    float: The likelihood value at p_heads=p. Return 0 if p is outside the range [0, 1].</span></span>
<span id="cb3-14"><a href="#cb3-14"></a><span class="co">    """</span></span>
<span id="cb3-15"><a href="#cb3-15"></a>    <span class="co"># YOUR CODE HERE (~1-3 lines)</span></span>
<span id="cb3-16"><a href="#cb3-16"></a>    <span class="cf">pass</span></span>
<span id="cb3-17"><a href="#cb3-17"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb3-18"><a href="#cb3-18"></a></span>
<span id="cb3-19"><a href="#cb3-19"></a></span>
<span id="cb3-20"><a href="#cb3-20"></a><span class="kw">def</span> propose(x_current: <span class="bu">float</span>, sigma: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb3-21"><a href="#cb3-21"></a>    <span class="co">"""</span></span>
<span id="cb3-22"><a href="#cb3-22"></a><span class="co">    Proposes a new sample from the proposal distribution Q.</span></span>
<span id="cb3-23"><a href="#cb3-23"></a><span class="co">    Here, Q is a normal distribution centered at x_current with standard deviation sigma.</span></span>
<span id="cb3-24"><a href="#cb3-24"></a></span>
<span id="cb3-25"><a href="#cb3-25"></a><span class="co">    Args:</span></span>
<span id="cb3-26"><a href="#cb3-26"></a><span class="co">    x_current (float): The current value in the Markov chain.</span></span>
<span id="cb3-27"><a href="#cb3-27"></a><span class="co">    sigma (float): Standard deviation of the normal proposal distribution.</span></span>
<span id="cb3-28"><a href="#cb3-28"></a></span>
<span id="cb3-29"><a href="#cb3-29"></a><span class="co">    Returns:</span></span>
<span id="cb3-30"><a href="#cb3-30"></a><span class="co">    float: The proposed new sample.</span></span>
<span id="cb3-31"><a href="#cb3-31"></a><span class="co">    """</span></span>
<span id="cb3-32"><a href="#cb3-32"></a>    <span class="co"># YOUR CODE HERE (~1-3 lines)</span></span>
<span id="cb3-33"><a href="#cb3-33"></a>    <span class="cf">pass</span></span>
<span id="cb3-34"><a href="#cb3-34"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb3-35"><a href="#cb3-35"></a></span>
<span id="cb3-36"><a href="#cb3-36"></a></span>
<span id="cb3-37"><a href="#cb3-37"></a><span class="kw">def</span> acceptance_probability(x_current: <span class="bu">float</span>, x_proposed: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb3-38"><a href="#cb3-38"></a>    <span class="co">"""</span></span>
<span id="cb3-39"><a href="#cb3-39"></a><span class="co">    Computes the acceptance probability A for the proposed sample.</span></span>
<span id="cb3-40"><a href="#cb3-40"></a><span class="co">    Since the proposal distribution is symmetric, Q cancels out.</span></span>
<span id="cb3-41"><a href="#cb3-41"></a></span>
<span id="cb3-42"><a href="#cb3-42"></a><span class="co">    Args:</span></span>
<span id="cb3-43"><a href="#cb3-43"></a><span class="co">    x_current (float): The current value in the Markov chain.</span></span>
<span id="cb3-44"><a href="#cb3-44"></a><span class="co">    x_proposed (float): The proposed new value.</span></span>
<span id="cb3-45"><a href="#cb3-45"></a></span>
<span id="cb3-46"><a href="#cb3-46"></a><span class="co">    Returns:</span></span>
<span id="cb3-47"><a href="#cb3-47"></a><span class="co">    float: The acceptance probability</span></span>
<span id="cb3-48"><a href="#cb3-48"></a><span class="co">    """</span></span>
<span id="cb3-49"><a href="#cb3-49"></a>    <span class="co"># YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb3-50"><a href="#cb3-50"></a>    <span class="cf">pass</span></span>
<span id="cb3-51"><a href="#cb3-51"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb3-52"><a href="#cb3-52"></a></span>
<span id="cb3-53"><a href="#cb3-53"></a></span>
<span id="cb3-54"><a href="#cb3-54"></a><span class="kw">def</span> metropolis_hastings(N: <span class="bu">int</span>, T: <span class="bu">int</span>, x_init: <span class="bu">float</span>, sigma: <span class="bu">float</span>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb3-55"><a href="#cb3-55"></a>    <span class="co">"""</span></span>
<span id="cb3-56"><a href="#cb3-56"></a><span class="co">    Runs the Metropolis-Hastings algorithm to sample from a posterior distribution.</span></span>
<span id="cb3-57"><a href="#cb3-57"></a></span>
<span id="cb3-58"><a href="#cb3-58"></a><span class="co">    Args:</span></span>
<span id="cb3-59"><a href="#cb3-59"></a><span class="co">    N (int): Total number of iterations.</span></span>
<span id="cb3-60"><a href="#cb3-60"></a><span class="co">    T (int): Burn-in period (number of initial samples to discard).</span></span>
<span id="cb3-61"><a href="#cb3-61"></a><span class="co">    x_init (float): Initial value of the chain.</span></span>
<span id="cb3-62"><a href="#cb3-62"></a><span class="co">    sigma (float): Standard deviation of the proposal distribution.</span></span>
<span id="cb3-63"><a href="#cb3-63"></a></span>
<span id="cb3-64"><a href="#cb3-64"></a><span class="co">    Returns:</span></span>
<span id="cb3-65"><a href="#cb3-65"></a><span class="co">    list: Samples collected after the burn-in period.</span></span>
<span id="cb3-66"><a href="#cb3-66"></a><span class="co">    """</span></span>
<span id="cb3-67"><a href="#cb3-67"></a>    samples <span class="op">=</span> []</span>
<span id="cb3-68"><a href="#cb3-68"></a>    x_current <span class="op">=</span> x_init</span>
<span id="cb3-69"><a href="#cb3-69"></a></span>
<span id="cb3-70"><a href="#cb3-70"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb3-71"><a href="#cb3-71"></a>        <span class="co"># YOUR CODE HERE (~7-10 lines)</span></span>
<span id="cb3-72"><a href="#cb3-72"></a>        <span class="co"># Use the propose and acceptance_probability functions to get x_{t+1} and store it in samples after the burn-in period T</span></span>
<span id="cb3-73"><a href="#cb3-73"></a>        <span class="cf">pass</span></span>
<span id="cb3-74"><a href="#cb3-74"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb3-75"><a href="#cb3-75"></a></span>
<span id="cb3-76"><a href="#cb3-76"></a>    <span class="cf">return</span> samples</span>
<span id="cb3-77"><a href="#cb3-77"></a></span>
<span id="cb3-78"><a href="#cb3-78"></a></span>
<span id="cb3-79"><a href="#cb3-79"></a><span class="kw">def</span> plot_results(samples: np.ndarray) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb3-80"><a href="#cb3-80"></a>    <span class="co">"""</span></span>
<span id="cb3-81"><a href="#cb3-81"></a><span class="co">    Plots the histogram of MCMC samples along with the true Beta(10, 4) PDF.</span></span>
<span id="cb3-82"><a href="#cb3-82"></a></span>
<span id="cb3-83"><a href="#cb3-83"></a><span class="co">    Args:</span></span>
<span id="cb3-84"><a href="#cb3-84"></a><span class="co">    samples (np.ndarray): Array of samples collected from the Metropolis-Hastings algorithm.</span></span>
<span id="cb3-85"><a href="#cb3-85"></a></span>
<span id="cb3-86"><a href="#cb3-86"></a><span class="co">    Returns:</span></span>
<span id="cb3-87"><a href="#cb3-87"></a><span class="co">    None</span></span>
<span id="cb3-88"><a href="#cb3-88"></a><span class="co">    """</span></span>
<span id="cb3-89"><a href="#cb3-89"></a>    <span class="co"># Histogram of the samples from the Metropolis-Hastings algorithm</span></span>
<span id="cb3-90"><a href="#cb3-90"></a>    plt.hist(samples, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">"MCMC Samples"</span>)</span>
<span id="cb3-91"><a href="#cb3-91"></a></span>
<span id="cb3-92"><a href="#cb3-92"></a>    <span class="co"># True Beta(10, 4) distribution for comparison</span></span>
<span id="cb3-93"><a href="#cb3-93"></a>    p <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb3-94"><a href="#cb3-94"></a>    beta_pdf <span class="op">=</span> beta.pdf(p, <span class="dv">10</span>, <span class="dv">4</span>)</span>
<span id="cb3-95"><a href="#cb3-95"></a>    plt.plot(p, beta_pdf, <span class="st">"r-"</span>, label<span class="op">=</span><span class="st">"Beta(10, 4) PDF"</span>)</span>
<span id="cb3-96"><a href="#cb3-96"></a></span>
<span id="cb3-97"><a href="#cb3-97"></a>    plt.xlabel(<span class="st">"p_heads"</span>)</span>
<span id="cb3-98"><a href="#cb3-98"></a>    plt.ylabel(<span class="st">"Density"</span>)</span>
<span id="cb3-99"><a href="#cb3-99"></a>    plt.title(<span class="st">"Metropolis-Hastings Sampling of Biased Coin Posterior"</span>)</span>
<span id="cb3-100"><a href="#cb3-100"></a>    plt.legend()</span>
<span id="cb3-101"><a href="#cb3-101"></a>    plt.show()</span>
<span id="cb3-102"><a href="#cb3-102"></a></span>
<span id="cb3-103"><a href="#cb3-103"></a></span>
<span id="cb3-104"><a href="#cb3-104"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb3-105"><a href="#cb3-105"></a>    <span class="co"># MCMC Parameters (DO NOT CHANGE!)</span></span>
<span id="cb3-106"><a href="#cb3-106"></a>    N <span class="op">=</span> <span class="dv">50000</span>  <span class="co"># Total number of iterations</span></span>
<span id="cb3-107"><a href="#cb3-107"></a>    T <span class="op">=</span> <span class="dv">10000</span>  <span class="co"># Burn-in period to discard</span></span>
<span id="cb3-108"><a href="#cb3-108"></a>    x_init <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># Initial guess for p_heads</span></span>
<span id="cb3-109"><a href="#cb3-109"></a>    sigma <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># Standard deviation of the proposal distribution</span></span>
<span id="cb3-110"><a href="#cb3-110"></a></span>
<span id="cb3-111"><a href="#cb3-111"></a>    <span class="co"># Run Metropolis-Hastings and plot the results</span></span>
<span id="cb3-112"><a href="#cb3-112"></a>    samples <span class="op">=</span> metropolis_hastings(N, T, x_init, sigma)</span>
<span id="cb3-113"><a href="#cb3-113"></a>    plot_results(samples)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<ol start="5" type="a">
<li><strong>(Coding, 9 points)</strong>. Implement Metropolis-Hastings in the movie setting inside<br>
<code>multimodal_preferences/movie_metropolis.py</code>. The movie dataset we use for grading will not be provided. However, randomly constructed datasets can be used to test your implementation by running <code>python movie_metropolis.py</code>. You should be able to achieve a <span class="math inline">\(90\%\)</span> success rate with most <code>fraction_accepted</code> values above <span class="math inline">\(0.1\)</span>. Success is measured by thresholded closeness of predicted parameters to true parameters. You may notice occasional failures that occur due to lack of convergence which we will account for in grading.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="be1bac50" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">import</span> torch</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="im">import</span> torch.distributions <span class="im">as</span> dist</span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="im">import</span> math</span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="im">from</span> typing <span class="im">import</span> Tuple</span>
<span id="cb4-6"><a href="#cb4-6"></a></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="kw">def</span> make_data(</span>
<span id="cb4-8"><a href="#cb4-8"></a>    true_p: torch.Tensor, true_weights_1: torch.Tensor, true_weights_2: torch.Tensor, num_movies: <span class="bu">int</span>, feature_dim: <span class="bu">int</span></span>
<span id="cb4-9"><a href="#cb4-9"></a>) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb4-10"><a href="#cb4-10"></a>    <span class="co">"""</span></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="co">    Generates a synthetic movie dataset according to the CardinalStreams model.</span></span>
<span id="cb4-12"><a href="#cb4-12"></a></span>
<span id="cb4-13"><a href="#cb4-13"></a><span class="co">    Args:</span></span>
<span id="cb4-14"><a href="#cb4-14"></a><span class="co">        true_p (torch.Tensor): Probability of coming from Group 1.</span></span>
<span id="cb4-15"><a href="#cb4-15"></a><span class="co">        true_weights_1 (torch.Tensor): Weights for Group 1.</span></span>
<span id="cb4-16"><a href="#cb4-16"></a><span class="co">        true_weights_2 (torch.Tensor): Weights for Group 2.</span></span>
<span id="cb4-17"><a href="#cb4-17"></a></span>
<span id="cb4-18"><a href="#cb4-18"></a><span class="co">    Returns:</span></span>
<span id="cb4-19"><a href="#cb4-19"></a><span class="co">        Tuple[torch.Tensor, torch.Tensor]: A tuple containing the dataset and labels.</span></span>
<span id="cb4-20"><a href="#cb4-20"></a><span class="co">    """</span></span>
<span id="cb4-21"><a href="#cb4-21"></a>    <span class="co"># Create movie features</span></span>
<span id="cb4-22"><a href="#cb4-22"></a>    first_movie_features <span class="op">=</span> torch.randn((num_movies, feature_dim))</span>
<span id="cb4-23"><a href="#cb4-23"></a>    second_movie_features <span class="op">=</span> torch.randn((num_movies, feature_dim))</span>
<span id="cb4-24"><a href="#cb4-24"></a></span>
<span id="cb4-25"><a href="#cb4-25"></a>    <span class="co"># Only care about difference of features for Bradley-Terry</span></span>
<span id="cb4-26"><a href="#cb4-26"></a>    dataset <span class="op">=</span> first_movie_features <span class="op">-</span> second_movie_features</span>
<span id="cb4-27"><a href="#cb4-27"></a></span>
<span id="cb4-28"><a href="#cb4-28"></a>    <span class="co"># Get probabilities that first movie is preferred assuming Group 1 or Group 2</span></span>
<span id="cb4-29"><a href="#cb4-29"></a>    weight_1_probs <span class="op">=</span> torch.sigmoid(dataset <span class="op">@</span> true_weights_1)</span>
<span id="cb4-30"><a href="#cb4-30"></a>    weight_2_probs <span class="op">=</span> torch.sigmoid(dataset <span class="op">@</span> true_weights_2)</span>
<span id="cb4-31"><a href="#cb4-31"></a></span>
<span id="cb4-32"><a href="#cb4-32"></a>    <span class="co"># Probability that first movie is preferred overall can be viewed as sum of conditioning on Group 1 and Group 2</span></span>
<span id="cb4-33"><a href="#cb4-33"></a>    first_movie_preferred_probs <span class="op">=</span> (</span>
<span id="cb4-34"><a href="#cb4-34"></a>        true_p <span class="op">*</span> weight_1_probs <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> true_p) <span class="op">*</span> weight_2_probs</span>
<span id="cb4-35"><a href="#cb4-35"></a>    )</span>
<span id="cb4-36"><a href="#cb4-36"></a>    labels <span class="op">=</span> dist.Bernoulli(first_movie_preferred_probs).sample()</span>
<span id="cb4-37"><a href="#cb4-37"></a>    <span class="cf">return</span> dataset, labels</span>
<span id="cb4-38"><a href="#cb4-38"></a></span>
<span id="cb4-39"><a href="#cb4-39"></a></span>
<span id="cb4-40"><a href="#cb4-40"></a><span class="kw">def</span> compute_likelihoods(</span>
<span id="cb4-41"><a href="#cb4-41"></a>    dataset: torch.Tensor,</span>
<span id="cb4-42"><a href="#cb4-42"></a>    labels: torch.Tensor,</span>
<span id="cb4-43"><a href="#cb4-43"></a>    p: torch.Tensor,</span>
<span id="cb4-44"><a href="#cb4-44"></a>    w_1: torch.Tensor,</span>
<span id="cb4-45"><a href="#cb4-45"></a>    w_2: torch.Tensor,</span>
<span id="cb4-46"><a href="#cb4-46"></a>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb4-47"><a href="#cb4-47"></a>    <span class="co">"""</span></span>
<span id="cb4-48"><a href="#cb4-48"></a><span class="co">    Computes the likelihood of each datapoint. Use your calculation from part (a) to help.</span></span>
<span id="cb4-49"><a href="#cb4-49"></a></span>
<span id="cb4-50"><a href="#cb4-50"></a><span class="co">    Args:</span></span>
<span id="cb4-51"><a href="#cb4-51"></a><span class="co">        dataset (torch.Tensor): The dataset of differences between movie features.</span></span>
<span id="cb4-52"><a href="#cb4-52"></a><span class="co">        labels (torch.Tensor): The labels where 1 indicates the first movie is preferred, and 0 indicates preference of the second movie.</span></span>
<span id="cb4-53"><a href="#cb4-53"></a><span class="co">        p (torch.Tensor): The probability of coming from Group 1.</span></span>
<span id="cb4-54"><a href="#cb4-54"></a><span class="co">        w_1 (torch.Tensor): Weights for Group 1.</span></span>
<span id="cb4-55"><a href="#cb4-55"></a><span class="co">        w_2 (torch.Tensor): Weights for Group 2.</span></span>
<span id="cb4-56"><a href="#cb4-56"></a></span>
<span id="cb4-57"><a href="#cb4-57"></a><span class="co">    Returns:</span></span>
<span id="cb4-58"><a href="#cb4-58"></a><span class="co">        torch.Tensor: The likelihoods for each datapoint. Should have shape (dataset.shape[0], )</span></span>
<span id="cb4-59"><a href="#cb4-59"></a><span class="co">    """</span></span>
<span id="cb4-60"><a href="#cb4-60"></a>    <span class="co"># YOUR CODE HERE (~6-8 lines)</span></span>
<span id="cb4-61"><a href="#cb4-61"></a>    <span class="cf">pass</span></span>
<span id="cb4-62"><a href="#cb4-62"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb4-63"><a href="#cb4-63"></a></span>
<span id="cb4-64"><a href="#cb4-64"></a><span class="kw">def</span> compute_prior_density(</span>
<span id="cb4-65"><a href="#cb4-65"></a>    p: torch.Tensor, w_1: torch.Tensor, w_2: torch.Tensor</span>
<span id="cb4-66"><a href="#cb4-66"></a>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb4-67"><a href="#cb4-67"></a>    <span class="co">"""</span></span>
<span id="cb4-68"><a href="#cb4-68"></a><span class="co">    Computes the prior density of the parameters.</span></span>
<span id="cb4-69"><a href="#cb4-69"></a></span>
<span id="cb4-70"><a href="#cb4-70"></a><span class="co">    Args:</span></span>
<span id="cb4-71"><a href="#cb4-71"></a><span class="co">        p (torch.Tensor): The probability of preferring model 1.</span></span>
<span id="cb4-72"><a href="#cb4-72"></a><span class="co">        w_1 (torch.Tensor): Weights for model 1.</span></span>
<span id="cb4-73"><a href="#cb4-73"></a><span class="co">        w_2 (torch.Tensor): Weights for model 2.</span></span>
<span id="cb4-74"><a href="#cb4-74"></a></span>
<span id="cb4-75"><a href="#cb4-75"></a><span class="co">    Returns:</span></span>
<span id="cb4-76"><a href="#cb4-76"></a><span class="co">        torch.Tensor: The prior densities of p, w_1, and w_2.</span></span>
<span id="cb4-77"><a href="#cb4-77"></a><span class="co">    """</span></span>
<span id="cb4-78"><a href="#cb4-78"></a>    <span class="co"># Adjusts p to stay in the range [0.3, 0.7] to prevent multiple equilibria issues at p=0 and p=1</span></span>
<span id="cb4-79"><a href="#cb4-79"></a>    p_prob <span class="op">=</span> torch.tensor([<span class="fl">2.5</span>]) <span class="cf">if</span> <span class="fl">0.3</span> <span class="op">&lt;=</span> p <span class="op">&lt;=</span> <span class="fl">0.7</span> <span class="cf">else</span> torch.tensor([<span class="fl">0.0</span>])</span>
<span id="cb4-80"><a href="#cb4-80"></a></span>
<span id="cb4-81"><a href="#cb4-81"></a>    <span class="kw">def</span> normal_pdf(x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb4-82"><a href="#cb4-82"></a>        <span class="co">"""Computes the PDF of the standard normal distribution at x."""</span></span>
<span id="cb4-83"><a href="#cb4-83"></a>        <span class="cf">return</span> (<span class="fl">1.0</span> <span class="op">/</span> torch.sqrt(torch.tensor(<span class="dv">2</span> <span class="op">*</span> math.pi))) <span class="op">*</span> torch.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb4-84"><a href="#cb4-84"></a></span>
<span id="cb4-85"><a href="#cb4-85"></a>    weights_1_prob <span class="op">=</span> normal_pdf(w_1)</span>
<span id="cb4-86"><a href="#cb4-86"></a>    weights_2_prob <span class="op">=</span> normal_pdf(w_2)</span>
<span id="cb4-87"><a href="#cb4-87"></a></span>
<span id="cb4-88"><a href="#cb4-88"></a>    <span class="co"># Concatenate the densities</span></span>
<span id="cb4-89"><a href="#cb4-89"></a>    concatenated_prob <span class="op">=</span> torch.cat([p_prob, weights_1_prob, weights_2_prob])</span>
<span id="cb4-90"><a href="#cb4-90"></a>    <span class="cf">return</span> concatenated_prob</span>
<span id="cb4-91"><a href="#cb4-91"></a></span>
<span id="cb4-92"><a href="#cb4-92"></a></span>
<span id="cb4-93"><a href="#cb4-93"></a><span class="kw">def</span> metropolis_hastings(</span>
<span id="cb4-94"><a href="#cb4-94"></a>    dataset: torch.Tensor,</span>
<span id="cb4-95"><a href="#cb4-95"></a>    labels: torch.Tensor,</span>
<span id="cb4-96"><a href="#cb4-96"></a>    sigma: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.01</span>,</span>
<span id="cb4-97"><a href="#cb4-97"></a>    num_iters: <span class="bu">int</span> <span class="op">=</span> <span class="dv">30000</span>,</span>
<span id="cb4-98"><a href="#cb4-98"></a>    burn_in: <span class="bu">int</span> <span class="op">=</span> <span class="dv">20000</span>,</span>
<span id="cb4-99"><a href="#cb4-99"></a>) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, <span class="bu">float</span>]:</span>
<span id="cb4-100"><a href="#cb4-100"></a>    <span class="co">"""</span></span>
<span id="cb4-101"><a href="#cb4-101"></a><span class="co">    Performs the Metropolis-Hastings algorithm to sample from the posterior distribution.</span></span>
<span id="cb4-102"><a href="#cb4-102"></a><span class="co">    DO NOT CHANGE THE DEFAULT VALUES!</span></span>
<span id="cb4-103"><a href="#cb4-103"></a></span>
<span id="cb4-104"><a href="#cb4-104"></a><span class="co">    Args:</span></span>
<span id="cb4-105"><a href="#cb4-105"></a><span class="co">        dataset (torch.Tensor): The dataset of differences between movie features.</span></span>
<span id="cb4-106"><a href="#cb4-106"></a><span class="co">        labels (torch.Tensor): The labels indicating which movie is preferred.</span></span>
<span id="cb4-107"><a href="#cb4-107"></a><span class="co">        sigma (float, optional): Standard deviation for proposal distribution.</span></span>
<span id="cb4-108"><a href="#cb4-108"></a><span class="co">            Defaults to 0.01.</span></span>
<span id="cb4-109"><a href="#cb4-109"></a><span class="co">        num_iters (int, optional): Total number of iterations. Defaults to 30000.</span></span>
<span id="cb4-110"><a href="#cb4-110"></a><span class="co">        burn_in (int, optional): Number of iterations to discard as burn-in.</span></span>
<span id="cb4-111"><a href="#cb4-111"></a><span class="co">            Defaults to 20000.</span></span>
<span id="cb4-112"><a href="#cb4-112"></a></span>
<span id="cb4-113"><a href="#cb4-113"></a><span class="co">    Returns:</span></span>
<span id="cb4-114"><a href="#cb4-114"></a><span class="co">        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]: Samples of p,</span></span>
<span id="cb4-115"><a href="#cb4-115"></a><span class="co">        w_1, w_2, and the fraction of accepted proposals.</span></span>
<span id="cb4-116"><a href="#cb4-116"></a><span class="co">    """</span></span>
<span id="cb4-117"><a href="#cb4-117"></a>    feature_dim <span class="op">=</span> dataset.shape[<span class="dv">1</span>]</span>
<span id="cb4-118"><a href="#cb4-118"></a></span>
<span id="cb4-119"><a href="#cb4-119"></a>    <span class="co"># Initialize random starting parameters by sampling priors</span></span>
<span id="cb4-120"><a href="#cb4-120"></a>    curr_p <span class="op">=</span> <span class="fl">0.3</span> <span class="op">+</span> <span class="fl">0.4</span> <span class="op">*</span> torch.rand(<span class="dv">1</span>)</span>
<span id="cb4-121"><a href="#cb4-121"></a>    curr_w_1 <span class="op">=</span> torch.randn(feature_dim)</span>
<span id="cb4-122"><a href="#cb4-122"></a>    curr_w_2 <span class="op">=</span> torch.randn(feature_dim)</span>
<span id="cb4-123"><a href="#cb4-123"></a></span>
<span id="cb4-124"><a href="#cb4-124"></a>    <span class="co"># Keep track of samples and total number of accepted proposals</span></span>
<span id="cb4-125"><a href="#cb4-125"></a>    p_samples <span class="op">=</span> []</span>
<span id="cb4-126"><a href="#cb4-126"></a>    w_1_samples <span class="op">=</span> []</span>
<span id="cb4-127"><a href="#cb4-127"></a>    w_2_samples <span class="op">=</span> []</span>
<span id="cb4-128"><a href="#cb4-128"></a>    accept_count <span class="op">=</span> <span class="dv">0</span> </span>
<span id="cb4-129"><a href="#cb4-129"></a></span>
<span id="cb4-130"><a href="#cb4-130"></a>    <span class="cf">for</span> T <span class="kw">in</span> tqdm(<span class="bu">range</span>(num_iters)):</span>
<span id="cb4-131"><a href="#cb4-131"></a>        <span class="co"># YOUR CODE HERE (~3 lines)</span></span>
<span id="cb4-132"><a href="#cb4-132"></a>        <span class="cf">pass</span> <span class="co"># Sample proposals for p, w_1, w_2</span></span>
<span id="cb4-133"><a href="#cb4-133"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb4-134"><a href="#cb4-134"></a></span>
<span id="cb4-135"><a href="#cb4-135"></a>        <span class="co"># YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb4-136"><a href="#cb4-136"></a>        <span class="cf">pass</span> <span class="co"># Compute likehoods and prior densities on both the proposed and current samples</span></span>
<span id="cb4-137"><a href="#cb4-137"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb4-138"><a href="#cb4-138"></a></span>
<span id="cb4-139"><a href="#cb4-139"></a>        <span class="co"># YOUR CODE HERE (~2-4 lines)</span></span>
<span id="cb4-140"><a href="#cb4-140"></a>        <span class="cf">pass</span> <span class="co"># Obtain the ratios of the likelihoods and prior densities between the proposed and current samples </span></span>
<span id="cb4-141"><a href="#cb4-141"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE </span></span>
<span id="cb4-142"><a href="#cb4-142"></a></span>
<span id="cb4-143"><a href="#cb4-143"></a>        <span class="co"># YOUR CODE HERE (~1-2 lines)</span></span>
<span id="cb4-144"><a href="#cb4-144"></a>        <span class="cf">pass</span> <span class="co"># Multiply all ratios (both likelihoods and prior densities) and use this to calculate the acceptance probability of the proposal</span></span>
<span id="cb4-145"><a href="#cb4-145"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb4-146"><a href="#cb4-146"></a></span>
<span id="cb4-147"><a href="#cb4-147"></a>        <span class="co"># YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb4-148"><a href="#cb4-148"></a>        <span class="cf">pass</span> <span class="co"># Sample randomness to determine whether the proposal should be accepted to update curr_p, curr_w_1, curr_w_2, and accept_count</span></span>
<span id="cb4-149"><a href="#cb4-149"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE </span></span>
<span id="cb4-150"><a href="#cb4-150"></a></span>
<span id="cb4-151"><a href="#cb4-151"></a>        <span class="co"># YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb4-152"><a href="#cb4-152"></a>        <span class="cf">pass</span> <span class="co"># Update p_samples, w_1_samples, w_2_samples if we have passed the burn in period T</span></span>
<span id="cb4-153"><a href="#cb4-153"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE </span></span>
<span id="cb4-154"><a href="#cb4-154"></a></span>
<span id="cb4-155"><a href="#cb4-155"></a>    fraction_accepted <span class="op">=</span> accept_count <span class="op">/</span> num_iters</span>
<span id="cb4-156"><a href="#cb4-156"></a>    <span class="bu">print</span>(<span class="ss">f"Fraction of accepted proposals: </span><span class="sc">{</span>fraction_accepted<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-157"><a href="#cb4-157"></a>    <span class="cf">return</span> (</span>
<span id="cb4-158"><a href="#cb4-158"></a>        torch.stack(p_samples),</span>
<span id="cb4-159"><a href="#cb4-159"></a>        torch.stack(w_1_samples),</span>
<span id="cb4-160"><a href="#cb4-160"></a>        torch.stack(w_2_samples),</span>
<span id="cb4-161"><a href="#cb4-161"></a>        fraction_accepted,</span>
<span id="cb4-162"><a href="#cb4-162"></a>    )</span>
<span id="cb4-163"><a href="#cb4-163"></a></span>
<span id="cb4-164"><a href="#cb4-164"></a></span>
<span id="cb4-165"><a href="#cb4-165"></a><span class="kw">def</span> evaluate_metropolis(num_sims: <span class="bu">int</span>, num_movies: <span class="bu">int</span>, feature_dim: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb4-166"><a href="#cb4-166"></a>    <span class="co">"""</span></span>
<span id="cb4-167"><a href="#cb4-167"></a><span class="co">    Runs the Metropolis-Hastings algorithm N times and compare estimated parameters</span></span>
<span id="cb4-168"><a href="#cb4-168"></a><span class="co">    with true parameters to obtain success rate. You should attain a success rate of around 90%. </span></span>
<span id="cb4-169"><a href="#cb4-169"></a></span>
<span id="cb4-170"><a href="#cb4-170"></a><span class="co">    Note that there are two successful equilibria to converge to. They are true_weights_1 and true_weights_2 with probabilities</span></span>
<span id="cb4-171"><a href="#cb4-171"></a><span class="co">    p and 1-p in addition to true_weights_2 and true_weights_1 with probabilities 1-p and p. This is why even though it may appear your</span></span>
<span id="cb4-172"><a href="#cb4-172"></a><span class="co">    predicted parameters don't match the true parameters, they are in fact equivalent. </span></span>
<span id="cb4-173"><a href="#cb4-173"></a></span>
<span id="cb4-174"><a href="#cb4-174"></a><span class="co">    Args:</span></span>
<span id="cb4-175"><a href="#cb4-175"></a><span class="co">        num_sims (int): Number of simulations to run.</span></span>
<span id="cb4-176"><a href="#cb4-176"></a></span>
<span id="cb4-177"><a href="#cb4-177"></a><span class="co">    Returns:</span></span>
<span id="cb4-178"><a href="#cb4-178"></a><span class="co">        None</span></span>
<span id="cb4-179"><a href="#cb4-179"></a><span class="co">    """</span></span>
<span id="cb4-180"><a href="#cb4-180"></a>    </span>
<span id="cb4-181"><a href="#cb4-181"></a>    success_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-182"><a href="#cb4-182"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_sims):</span>
<span id="cb4-183"><a href="#cb4-183"></a>        <span class="co"># Sample random ground truth parameters</span></span>
<span id="cb4-184"><a href="#cb4-184"></a>        true_p <span class="op">=</span> <span class="fl">0.3</span> <span class="op">+</span> <span class="fl">0.4</span> <span class="op">*</span> torch.rand(<span class="dv">1</span>)</span>
<span id="cb4-185"><a href="#cb4-185"></a>        true_weights_1 <span class="op">=</span> torch.randn(feature_dim)</span>
<span id="cb4-186"><a href="#cb4-186"></a>        true_weights_2 <span class="op">=</span> torch.randn(feature_dim)</span>
<span id="cb4-187"><a href="#cb4-187"></a></span>
<span id="cb4-188"><a href="#cb4-188"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">---- MCMC Simulation ----"</span>)</span>
<span id="cb4-189"><a href="#cb4-189"></a>        <span class="bu">print</span>(<span class="st">"True parameters:"</span>, true_p, true_weights_1, true_weights_2)</span>
<span id="cb4-190"><a href="#cb4-190"></a></span>
<span id="cb4-191"><a href="#cb4-191"></a>        dataset, labels <span class="op">=</span> make_data(true_p, true_weights_1, true_weights_2, num_movies, feature_dim)</span>
<span id="cb4-192"><a href="#cb4-192"></a>        p_samples, w_1_samples, w_2_samples, _ <span class="op">=</span> metropolis_hastings(dataset, labels)</span>
<span id="cb4-193"><a href="#cb4-193"></a></span>
<span id="cb4-194"><a href="#cb4-194"></a>        p_pred <span class="op">=</span> p_samples.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-195"><a href="#cb4-195"></a>        w_1_pred <span class="op">=</span> w_1_samples.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-196"><a href="#cb4-196"></a>        w_2_pred <span class="op">=</span> w_2_samples.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-197"><a href="#cb4-197"></a></span>
<span id="cb4-198"><a href="#cb4-198"></a>        <span class="bu">print</span>(<span class="st">"Predicted parameters:"</span>, p_pred, w_1_pred, w_2_pred)</span>
<span id="cb4-199"><a href="#cb4-199"></a></span>
<span id="cb4-200"><a href="#cb4-200"></a>        <span class="co"># Do casework on two equilibria cases to check for success</span></span>
<span id="cb4-201"><a href="#cb4-201"></a>        p_diff_case_1 <span class="op">=</span> torch.<span class="bu">abs</span>(p_pred <span class="op">-</span> true_p)</span>
<span id="cb4-202"><a href="#cb4-202"></a>        p_diff_case_2 <span class="op">=</span> torch.<span class="bu">abs</span>(p_pred <span class="op">-</span> (<span class="dv">1</span> <span class="op">-</span> true_p))</span>
<span id="cb4-203"><a href="#cb4-203"></a></span>
<span id="cb4-204"><a href="#cb4-204"></a>        w_1_diff_case_1 <span class="op">=</span> torch.<span class="bu">max</span>(torch.<span class="bu">abs</span>(w_1_pred <span class="op">-</span> true_weights_1))</span>
<span id="cb4-205"><a href="#cb4-205"></a>        w_1_diff_case_2 <span class="op">=</span> torch.<span class="bu">max</span>(torch.<span class="bu">abs</span>(w_1_pred <span class="op">-</span> true_weights_2))</span>
<span id="cb4-206"><a href="#cb4-206"></a></span>
<span id="cb4-207"><a href="#cb4-207"></a>        w_2_diff_case_1 <span class="op">=</span> torch.<span class="bu">max</span>(torch.<span class="bu">abs</span>(w_2_pred <span class="op">-</span> true_weights_2))</span>
<span id="cb4-208"><a href="#cb4-208"></a>        w_2_diff_case_2 <span class="op">=</span> torch.<span class="bu">max</span>(torch.<span class="bu">abs</span>(w_2_pred <span class="op">-</span> true_weights_1))</span>
<span id="cb4-209"><a href="#cb4-209"></a></span>
<span id="cb4-210"><a href="#cb4-210"></a>        pass_case_1 <span class="op">=</span> (</span>
<span id="cb4-211"><a href="#cb4-211"></a>            p_diff_case_1 <span class="op">&lt;</span> <span class="fl">0.1</span> <span class="kw">and</span> w_1_diff_case_1 <span class="op">&lt;</span> <span class="fl">0.5</span> <span class="kw">and</span> w_2_diff_case_1 <span class="op">&lt;</span> <span class="fl">0.5</span></span>
<span id="cb4-212"><a href="#cb4-212"></a>        )</span>
<span id="cb4-213"><a href="#cb4-213"></a>        pass_case_2 <span class="op">=</span> (</span>
<span id="cb4-214"><a href="#cb4-214"></a>            p_diff_case_2 <span class="op">&lt;</span> <span class="fl">0.1</span> <span class="kw">and</span> w_1_diff_case_2 <span class="op">&lt;</span> <span class="fl">0.5</span> <span class="kw">and</span> w_2_diff_case_2 <span class="op">&lt;</span> <span class="fl">0.5</span></span>
<span id="cb4-215"><a href="#cb4-215"></a>        )</span>
<span id="cb4-216"><a href="#cb4-216"></a>        passes <span class="op">=</span> pass_case_1 <span class="kw">or</span> pass_case_2</span>
<span id="cb4-217"><a href="#cb4-217"></a></span>
<span id="cb4-218"><a href="#cb4-218"></a>        <span class="bu">print</span>(<span class="ss">f'Result: </span><span class="sc">{</span><span class="st">"Success"</span> <span class="cf">if</span> passes <span class="cf">else</span> <span class="st">"FAILED"</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-219"><a href="#cb4-219"></a>        <span class="cf">if</span> passes:</span>
<span id="cb4-220"><a href="#cb4-220"></a>            success_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-221"><a href="#cb4-221"></a>    <span class="bu">print</span>(<span class="ss">f'Success rate: </span><span class="sc">{</span>success_count <span class="op">/</span> num_sims<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-222"><a href="#cb4-222"></a></span>
<span id="cb4-223"><a href="#cb4-223"></a></span>
<span id="cb4-224"><a href="#cb4-224"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb4-225"><a href="#cb4-225"></a>    evaluate_metropolis(num_sims<span class="op">=</span><span class="dv">10</span>, num_movies<span class="op">=</span><span class="dv">30000</span>, feature_dim<span class="op">=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="question-4-direct-preference-optimization-40-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="question-4-direct-preference-optimization-40-points">Question 4: Direct Preference Optimization (40 points)</h3>
<p>Note this question requires a GPU which is provided for free on Google Colab (T4 instance) or through the course cloud credits provided on Ed.<br>
Direct Preference Optimization (DPO) allows for policy alignment on a preference dataset without the need to train a separate reward model. The preference dataset is constructed by sampling generations <span class="math inline">\((y_1, y_2)\sim \pi_{\text{ref}}(\cdot\mid x)\)</span> where <span class="math inline">\(\pi_\text{ref}\)</span> is the base policy to be aligned, and <span class="math inline">\(x\)</span> comes from a set of previously collected prompts. The pairs of generations are then labeled by an annotator for which of the generations is preferred. Denote the preference dataset by <span class="math inline">\(\mathcal{D}=\left\{\left(x^{(i)}, y_+^{(i)}, y_-^{(i)}\right)\right\}_{i=1}^N\)</span>, where <span class="math inline">\(y_+\)</span> and <span class="math inline">\(y_-\)</span> are the preferred and non-preferred generations, respectively. DPO aims to solve the following: <span class="math display">\[\hat{\pi}=\arg \min_{\pi\in\Pi}\mathbb{E}_{(x, y_+, y_-)\sim\mathcal{D}}\left[-\log\sigma\left(
\beta\log\left(\frac{\pi(y_+ | x)}{\pi_{\text{ref}}(y_+ | x)}\right)-\beta\log\left(\frac{\pi(y_- | x)}{\pi_{\text{ref}}(y_- | x)}\right)\right)\right]\]</span> where <span class="math inline">\(\Pi\)</span> is the space of possible polices <span class="math inline">\(\pi\)</span> can take on. <span class="math inline">\(\pi\)</span> is typically parametrized.</p>
<ol type="a">
<li><p><strong>(Written, 6 points)</strong>. Consider the setting where <span class="math inline">\(\pi_{\text{ref}}\)</span> has no conditioning features and randomly outputs one of two possible values, <span class="math inline">\(\mathbf{A}\)</span> or <span class="math inline">\(\mathbf{B}\)</span> (also known as the “Bandit” setting). Suppose that <span class="math inline">\(\pi_{\text{ref}}(\mathbf{A})=p_0\)</span> and <span class="math inline">\(\pi_{\text{ref}}(\mathbf{B})=1-p_0\)</span>. Furthermore, assume that the preference dataset <span class="math inline">\(\mathcal{D}\)</span> is infinitely large, sampled from <span class="math inline">\(\pi_{\text{ref}}\)</span>, and that the preferred response is selected through a Bradley-Terry reward model where <span class="math inline">\(\mathbf{A}\)</span> has reward score <span class="math inline">\(r_A\)</span> and <span class="math inline">\(\mathbf{B}\)</span> has reward score <span class="math inline">\(r_B\)</span>. Set <span class="math inline">\(\Pi=\{\pi_p\mid 0&lt;p&lt;1\}\)</span> where <span class="math inline">\(\pi_p\)</span> is the policy defined by <span class="math inline">\(\pi_p(\mathbf{A})=p\)</span> and <span class="math inline">\(\pi_p(\mathbf{B})=1-p\)</span>. The DPO objective is to compute: <span class="math display">\[\pi_{\hat{p}}=\arg \min_{\pi_p\in \Pi} f(p, p_0, \beta, r_A, r_B),\]</span> for a function <span class="math inline">\(f\)</span>. Find <span class="math inline">\(f\)</span> by explicitly computing the relevant expectation.</p></li>
<li><p><strong>(Written, 8 points)</strong>. Assume that a solution to the optimization problem in part (a) exists. Find an expression for <span class="math inline">\(\hat{p}\)</span>. (Hint: Make sure to know your sigmoid derivative properties! Everything should simplify nicely. You may use the <a href="https://en.wikipedia.org/wiki/Logit"><em>logit function</em></a> denoted by <span class="math inline">\(\sigma^{-1}\)</span> in your final expression.)</p></li>
<li><p><strong>(Written, 3 points)</strong>. Show that <span class="math inline">\(\lim_{\beta\to\infty}\hat{p}=p_0.\)</span> (Very) briefly explain why this makes sense intuitively based on the role of <span class="math inline">\(\beta\)</span> in KL-constrained reward optimization (we suggest two sentences).</p></li>
<li><p><strong>(Written, 3 points)</strong>. Assume <span class="math inline">\(r_A=r_B\)</span> and <span class="math inline">\(\beta&gt;0\)</span>. Notice that <span class="math inline">\(\hat{p}=p_0\)</span>. Briefly explain why this makes sense intuitively (we suggest two sentences).</p></li>
</ol>
<p>Next, you will fine-tune the lightweight 2 billion parameter Gemma 2 model on the DPO objective. We will use the instruction fine-tuned variant of the model (i.e., designed for chat-based interactions).</p>
<ol type="1">
<li><p><strong>(Coding, 4 points)</strong>. Open the <code>dpo/dpo.ipynb</code> file of the PSET’s codebase. Execute the first few cells of the notebook until you see the <code>sample_chat_tokens</code> and their IDs printed out. The next cell requires you to implement the <code>get_response_idxs</code> function in <code>dpo/dpo.py</code>.</p>
<p>To implement it, you must find the indices of the first and last token of the model’s response in <code>sample_chat_tokens</code>. In the notebook’s example, this corresponds to the tokens “As” and “.”</p></li>
<li><p><strong>(Coding, 4 points)</strong>. The following cell asks you to implement the <code>get_response_next_token_probs</code> function. The next token logits for each token of the chat prompt are provided. Pass them through the softmax function and appropriately index the next token IDs.</p>
<pre><code>&lt;bos&gt;&lt;start_of_turn&gt;user
Where are you?&lt;end_of_turn&gt;
&lt;start_of_turn&gt;model
I am here.&lt;end_of_turn&gt;</code></pre>
<p>In the example above, we look for the next-token probabilities of “I”, “am”, “here”, and “.” To do so, you must extract the logits for “\n”, “I”, “am”, and “here” because the probability of generating a given token comes from the prediction of the token before. Use the return value of <code>get_response_idxs</code> as anchor points for indexing. Be careful of off-by-one indexing mistakes!</p></li>
<li><p><strong>(Coding, 6 points)</strong>. The training and reference LLM policies are loaded for you. We load the training policy in with LoRA for computational efficiency during fine-tuning in the next part. Implement <code>compute_dpo_objective</code> with the objective provided in the theory portion for your favorite positive value of <span class="math inline">\(\beta\)</span>. Does <span class="math inline">\(\beta\)</span> affect the loss printed out? Why or why not? You do not need to write why in your submission, but this line of thinking will help debug any issues with your DPO loss function.</p></li>
<li><p><strong>(Written + Coding, 6 points)</strong>. Finally, you will fine-tune the Gemma model on the DPO loss function with batch size (and dataset size) of <span class="math inline">\(1\)</span> by implementing <code>finetune</code>. The prompt and completions are provided in the notebook. The optimizer, <span class="math inline">\(\beta\)</span>, and the number of fine-tuning steps have also been provided. Make sure to use <code>torch.no_grad()</code> on the reference model to prevent unnecessary gradients!</p>
<p>Report the proportion of “because of” occurences before and after fine-tuning. Additionally, include a plot of the DPO loss curve.</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="0c67325a" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">import</span> torch</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM, set_seed</span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model</span>
<span id="cb6-4"><a href="#cb6-4"></a></span>
<span id="cb6-5"><a href="#cb6-5"></a>set_seed(<span class="dv">42</span>) <span class="co"># DO NOT CHANGE THE SEED</span></span>
<span id="cb6-6"><a href="#cb6-6"></a></span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="kw">def</span> get_response_idxs(tokenizer, chat_token_ids):</span>
<span id="cb6-8"><a href="#cb6-8"></a>    <span class="co">"""</span></span>
<span id="cb6-9"><a href="#cb6-9"></a><span class="co">    Finds the start and end indices of the response in the tokenized chat.</span></span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a><span class="co">    Args:</span></span>
<span id="cb6-12"><a href="#cb6-12"></a><span class="co">    tokenizer: The tokenizer object used to encode/decode text.</span></span>
<span id="cb6-13"><a href="#cb6-13"></a><span class="co">    chat_token_ids (list[int]): The token IDs representing the chat conversation.</span></span>
<span id="cb6-14"><a href="#cb6-14"></a></span>
<span id="cb6-15"><a href="#cb6-15"></a><span class="co">    Returns:</span></span>
<span id="cb6-16"><a href="#cb6-16"></a><span class="co">    tuple: A tuple (response_start_idx, response_end_idx), both of which are nonnegative integers.</span></span>
<span id="cb6-17"><a href="#cb6-17"></a><span class="co">    """</span></span>
<span id="cb6-18"><a href="#cb6-18"></a></span>
<span id="cb6-19"><a href="#cb6-19"></a>    start_of_turn_id <span class="op">=</span> tokenizer.convert_tokens_to_ids(<span class="st">"&lt;start_of_turn&gt;"</span>)</span>
<span id="cb6-20"><a href="#cb6-20"></a>    end_of_turn_id <span class="op">=</span> tokenizer.convert_tokens_to_ids(<span class="st">"&lt;end_of_turn&gt;"</span>)</span>
<span id="cb6-21"><a href="#cb6-21"></a></span>
<span id="cb6-22"><a href="#cb6-22"></a>    response_start_idx <span class="op">=</span> <span class="va">None</span> <span class="co"># Nonnegative integer</span></span>
<span id="cb6-23"><a href="#cb6-23"></a>    response_end_idx <span class="op">=</span> <span class="va">None</span> <span class="co"># Nonnegative integer</span></span>
<span id="cb6-24"><a href="#cb6-24"></a></span>
<span id="cb6-25"><a href="#cb6-25"></a>    <span class="co"># YOUR CODE HERE (~3-5 lines)</span></span>
<span id="cb6-26"><a href="#cb6-26"></a>    <span class="cf">pass</span></span>
<span id="cb6-27"><a href="#cb6-27"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb6-28"><a href="#cb6-28"></a></span>
<span id="cb6-29"><a href="#cb6-29"></a>    <span class="cf">return</span> response_start_idx, response_end_idx</span>
<span id="cb6-30"><a href="#cb6-30"></a></span>
<span id="cb6-31"><a href="#cb6-31"></a><span class="kw">def</span> get_response_next_token_probs(tokenizer, model, chat_token_ids):</span>
<span id="cb6-32"><a href="#cb6-32"></a>    <span class="co">"""</span></span>
<span id="cb6-33"><a href="#cb6-33"></a><span class="co">    Computes the next token probabilities for the response in a chat.</span></span>
<span id="cb6-34"><a href="#cb6-34"></a></span>
<span id="cb6-35"><a href="#cb6-35"></a><span class="co">    Args:</span></span>
<span id="cb6-36"><a href="#cb6-36"></a><span class="co">    tokenizer: The tokenizer object used to encode/decode text.</span></span>
<span id="cb6-37"><a href="#cb6-37"></a><span class="co">    model: The language model used to generate the logits.</span></span>
<span id="cb6-38"><a href="#cb6-38"></a><span class="co">    chat_token_ids (list[int]): The token IDs representing the chat conversation.</span></span>
<span id="cb6-39"><a href="#cb6-39"></a></span>
<span id="cb6-40"><a href="#cb6-40"></a><span class="co">    Returns:</span></span>
<span id="cb6-41"><a href="#cb6-41"></a><span class="co">    torch.Tensor: A 1D tensor containing the probabilities of the tokens in the response found by appropriately indexing</span></span>
<span id="cb6-42"><a href="#cb6-42"></a><span class="co">                  the next token probabilities of the preceding token.</span></span>
<span id="cb6-43"><a href="#cb6-43"></a><span class="co">    """</span></span>
<span id="cb6-44"><a href="#cb6-44"></a></span>
<span id="cb6-45"><a href="#cb6-45"></a>    response_start_idx, response_end_idx <span class="op">=</span> get_response_idxs(tokenizer, chat_token_ids)</span>
<span id="cb6-46"><a href="#cb6-46"></a>    chat_token_ids_tensor <span class="op">=</span> torch.tensor([chat_token_ids]).to(model.device)</span>
<span id="cb6-47"><a href="#cb6-47"></a>    logits <span class="op">=</span> model(chat_token_ids_tensor).logits[<span class="dv">0</span>, :, :] <span class="co"># shape (len(chat_token_ids), vocabulary_size)</span></span>
<span id="cb6-48"><a href="#cb6-48"></a></span>
<span id="cb6-49"><a href="#cb6-49"></a>    next_token_probs <span class="op">=</span> <span class="va">None</span> <span class="co"># Should be a 1D-tensor</span></span>
<span id="cb6-50"><a href="#cb6-50"></a></span>
<span id="cb6-51"><a href="#cb6-51"></a>    <span class="co"># YOUR CODE HERE (~3-5 lines)</span></span>
<span id="cb6-52"><a href="#cb6-52"></a>    <span class="cf">pass</span></span>
<span id="cb6-53"><a href="#cb6-53"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb6-54"><a href="#cb6-54"></a></span>
<span id="cb6-55"><a href="#cb6-55"></a>    <span class="cf">return</span> next_token_probs</span>
<span id="cb6-56"><a href="#cb6-56"></a></span>
<span id="cb6-57"><a href="#cb6-57"></a><span class="kw">def</span> compute_dpo_objective(preferred_train_probs, nonpreferred_train_probs, preferred_ref_probs, nonpreferred_ref_probs, beta):</span>
<span id="cb6-58"><a href="#cb6-58"></a>    <span class="co">"""</span></span>
<span id="cb6-59"><a href="#cb6-59"></a><span class="co">    Computes the Direct Preference Optimization (DPO) objective for training.</span></span>
<span id="cb6-60"><a href="#cb6-60"></a></span>
<span id="cb6-61"><a href="#cb6-61"></a><span class="co">    Args:</span></span>
<span id="cb6-62"><a href="#cb6-62"></a><span class="co">    preferred_train_probs (torch.Tensor): Token probabilities for the preferred chat sequence from the training model.</span></span>
<span id="cb6-63"><a href="#cb6-63"></a><span class="co">    nonpreferred_train_probs (torch.Tensor): Token probabilities for the non-preferred chat sequence from the training model.</span></span>
<span id="cb6-64"><a href="#cb6-64"></a><span class="co">    preferred_ref_probs (torch.Tensor): Token probabilities for the preferred chat sequence from the reference model.</span></span>
<span id="cb6-65"><a href="#cb6-65"></a><span class="co">    nonpreferred_ref_probs (torch.Tensor): Token probabilities for the non-preferred chat sequence from the reference model.</span></span>
<span id="cb6-66"><a href="#cb6-66"></a><span class="co">    beta (float): Controls the KL strength of staying close to the reference model.</span></span>
<span id="cb6-67"><a href="#cb6-67"></a></span>
<span id="cb6-68"><a href="#cb6-68"></a><span class="co">    Returns:</span></span>
<span id="cb6-69"><a href="#cb6-69"></a><span class="co">    torch.Tensor: The computed DPO objective, which is a float.</span></span>
<span id="cb6-70"><a href="#cb6-70"></a><span class="co">    """</span></span>
<span id="cb6-71"><a href="#cb6-71"></a></span>
<span id="cb6-72"><a href="#cb6-72"></a>    dpo_obj <span class="op">=</span> <span class="va">None</span> <span class="co"># Float value</span></span>
<span id="cb6-73"><a href="#cb6-73"></a>    </span>
<span id="cb6-74"><a href="#cb6-74"></a>    <span class="co"># YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb6-75"><a href="#cb6-75"></a>    <span class="cf">pass</span></span>
<span id="cb6-76"><a href="#cb6-76"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb6-77"><a href="#cb6-77"></a></span>
<span id="cb6-78"><a href="#cb6-78"></a>    <span class="cf">return</span> dpo_obj</span>
<span id="cb6-79"><a href="#cb6-79"></a></span>
<span id="cb6-80"><a href="#cb6-80"></a><span class="kw">def</span> finetune(tokenizer, optimizer, train_model, ref_model, preferred_chat_ids, nonpreferred_chat_ids, num_gradient_steps, beta):</span>
<span id="cb6-81"><a href="#cb6-81"></a>    <span class="co">"""</span></span>
<span id="cb6-82"><a href="#cb6-82"></a><span class="co">    Fine-tunes the training model using DPO. Make sure to disable gradients on the reference model!</span></span>
<span id="cb6-83"><a href="#cb6-83"></a></span>
<span id="cb6-84"><a href="#cb6-84"></a><span class="co">    Args:</span></span>
<span id="cb6-85"><a href="#cb6-85"></a><span class="co">    tokenizer: The tokenizer object used to encode/decode text.</span></span>
<span id="cb6-86"><a href="#cb6-86"></a><span class="co">    optimizer: The optimizer for updating the training model's parameters.</span></span>
<span id="cb6-87"><a href="#cb6-87"></a><span class="co">    train_model: The model being fine-tuned.</span></span>
<span id="cb6-88"><a href="#cb6-88"></a><span class="co">    ref_model: The reference model.</span></span>
<span id="cb6-89"><a href="#cb6-89"></a><span class="co">    preferred_chat_ids (list[int]): The token IDs representing the preferred chat sequence.</span></span>
<span id="cb6-90"><a href="#cb6-90"></a><span class="co">    nonpreferred_chat_ids (list[int]): The token IDs representing the non-preferred chat sequence.</span></span>
<span id="cb6-91"><a href="#cb6-91"></a><span class="co">    num_gradient_steps (int): The number of gradient updates to perform.</span></span>
<span id="cb6-92"><a href="#cb6-92"></a><span class="co">    beta (float): A parameter used in computing the DPO objective.</span></span>
<span id="cb6-93"><a href="#cb6-93"></a></span>
<span id="cb6-94"><a href="#cb6-94"></a><span class="co">    Returns:</span></span>
<span id="cb6-95"><a href="#cb6-95"></a><span class="co">    None</span></span>
<span id="cb6-96"><a href="#cb6-96"></a><span class="co">    """</span></span>
<span id="cb6-97"><a href="#cb6-97"></a></span>
<span id="cb6-98"><a href="#cb6-98"></a>    <span class="bu">print</span>(<span class="st">'Fine-tuning...'</span>)</span>
<span id="cb6-99"><a href="#cb6-99"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_gradient_steps):</span>
<span id="cb6-100"><a href="#cb6-100"></a>        <span class="co"># YOUR CODE HERE (~9-12 lines)</span></span>
<span id="cb6-101"><a href="#cb6-101"></a>        <span class="cf">pass</span></span>
<span id="cb6-102"><a href="#cb6-102"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb6-103"><a href="#cb6-103"></a>    <span class="bu">print</span>(<span class="st">"Fine-tuning complete!"</span>)</span>
<span id="cb6-104"><a href="#cb6-104"></a></span>
<span id="cb6-105"><a href="#cb6-105"></a><span class="co"># DO NOT CHANGE!</span></span>
<span id="cb6-106"><a href="#cb6-106"></a><span class="kw">def</span> sample_model(tokenizer, model, prompt, N<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb6-107"><a href="#cb6-107"></a>    <span class="co">"""</span></span>
<span id="cb6-108"><a href="#cb6-108"></a><span class="co">    Samples N different completions from the model based on the given prompt.</span></span>
<span id="cb6-109"><a href="#cb6-109"></a></span>
<span id="cb6-110"><a href="#cb6-110"></a><span class="co">    Args:</span></span>
<span id="cb6-111"><a href="#cb6-111"></a><span class="co">    tokenizer: The tokenizer object used to encode/decode text.</span></span>
<span id="cb6-112"><a href="#cb6-112"></a><span class="co">    model: The language model used for generation.</span></span>
<span id="cb6-113"><a href="#cb6-113"></a><span class="co">    prompt (str): The input prompt for which completions will be generated.</span></span>
<span id="cb6-114"><a href="#cb6-114"></a><span class="co">    N (int): The number of completions to generate.</span></span>
<span id="cb6-115"><a href="#cb6-115"></a></span>
<span id="cb6-116"><a href="#cb6-116"></a><span class="co">    Returns:</span></span>
<span id="cb6-117"><a href="#cb6-117"></a><span class="co">    list[str]: A list of N generated completions.</span></span>
<span id="cb6-118"><a href="#cb6-118"></a><span class="co">    """</span></span>
<span id="cb6-119"><a href="#cb6-119"></a></span>
<span id="cb6-120"><a href="#cb6-120"></a>    chat <span class="op">=</span> [{<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt}]</span>
<span id="cb6-121"><a href="#cb6-121"></a>    chat_tokens <span class="op">=</span> tokenizer.apply_chat_template(chat, tokenize<span class="op">=</span><span class="va">True</span>, add_generation_prompt<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-122"><a href="#cb6-122"></a></span>
<span id="cb6-123"><a href="#cb6-123"></a>    <span class="co"># Generate N different responses</span></span>
<span id="cb6-124"><a href="#cb6-124"></a>    outputs <span class="op">=</span> model.generate(</span>
<span id="cb6-125"><a href="#cb6-125"></a>        torch.tensor([chat_tokens], device<span class="op">=</span>model.device),</span>
<span id="cb6-126"><a href="#cb6-126"></a>        num_return_sequences<span class="op">=</span>N,</span>
<span id="cb6-127"><a href="#cb6-127"></a>        max_new_tokens<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb6-128"><a href="#cb6-128"></a>        temperature<span class="op">=</span><span class="fl">0.15</span>,</span>
<span id="cb6-129"><a href="#cb6-129"></a>        top_k<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb6-130"><a href="#cb6-130"></a>        top_p<span class="op">=</span><span class="fl">0.95</span>,</span>
<span id="cb6-131"><a href="#cb6-131"></a>        do_sample<span class="op">=</span><span class="va">True</span></span>
<span id="cb6-132"><a href="#cb6-132"></a>    )</span>
<span id="cb6-133"><a href="#cb6-133"></a></span>
<span id="cb6-134"><a href="#cb6-134"></a>    <span class="kw">def</span> extract_response(decoded_text):</span>
<span id="cb6-135"><a href="#cb6-135"></a>        <span class="cf">return</span> decoded_text.rsplit(<span class="st">'model</span><span class="ch">\n</span><span class="st">'</span>, <span class="dv">1</span>)[<span class="op">-</span><span class="dv">1</span>][:<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb6-136"><a href="#cb6-136"></a></span>
<span id="cb6-137"><a href="#cb6-137"></a>    responses <span class="op">=</span> [extract_response(tokenizer.decode(output, skip_special_tokens<span class="op">=</span><span class="va">True</span>)) <span class="cf">for</span> output <span class="kw">in</span> outputs]</span>
<span id="cb6-138"><a href="#cb6-138"></a>    <span class="cf">return</span> responses</span>
<span id="cb6-139"><a href="#cb6-139"></a></span>
<span id="cb6-140"><a href="#cb6-140"></a><span class="co"># DO NOT CHANGE!</span></span>
<span id="cb6-141"><a href="#cb6-141"></a><span class="kw">def</span> fraction_responses_with_because_of(responses):</span>
<span id="cb6-142"><a href="#cb6-142"></a>    <span class="co">"""</span></span>
<span id="cb6-143"><a href="#cb6-143"></a><span class="co">    Calculates the fraction of responses that start with a specific match string.</span></span>
<span id="cb6-144"><a href="#cb6-144"></a></span>
<span id="cb6-145"><a href="#cb6-145"></a><span class="co">    Args:</span></span>
<span id="cb6-146"><a href="#cb6-146"></a><span class="co">    responses (list[str]): A list of model-generated responses.</span></span>
<span id="cb6-147"><a href="#cb6-147"></a></span>
<span id="cb6-148"><a href="#cb6-148"></a><span class="co">    Returns:</span></span>
<span id="cb6-149"><a href="#cb6-149"></a><span class="co">    float: The fraction of responses that start with the phrase "The sky appears blue because of".</span></span>
<span id="cb6-150"><a href="#cb6-150"></a><span class="co">    """</span></span>
<span id="cb6-151"><a href="#cb6-151"></a></span>
<span id="cb6-152"><a href="#cb6-152"></a>    match_str <span class="op">=</span> <span class="st">"The sky appears blue because of"</span></span>
<span id="cb6-153"><a href="#cb6-153"></a>    match_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-154"><a href="#cb6-154"></a></span>
<span id="cb6-155"><a href="#cb6-155"></a>    <span class="cf">for</span> response <span class="kw">in</span> responses:</span>
<span id="cb6-156"><a href="#cb6-156"></a>        <span class="cf">if</span> response.startswith(match_str):</span>
<span id="cb6-157"><a href="#cb6-157"></a>            match_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb6-158"><a href="#cb6-158"></a></span>
<span id="cb6-159"><a href="#cb6-159"></a>    <span class="cf">return</span> match_count <span class="op">/</span> <span class="bu">len</span>(responses)</span>
<span id="cb6-160"><a href="#cb6-160"></a></span>
<span id="cb6-161"><a href="#cb6-161"></a></span>
<span id="cb6-162"><a href="#cb6-162"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">'__main__'</span>:</span>
<span id="cb6-163"><a href="#cb6-163"></a>    model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb6-164"><a href="#cb6-164"></a>        <span class="st">"google/gemma-2-2b-it"</span>,</span>
<span id="cb6-165"><a href="#cb6-165"></a>        torch_dtype<span class="op">=</span>torch.bfloat16,</span>
<span id="cb6-166"><a href="#cb6-166"></a>        device_map<span class="op">=</span><span class="st">'auto'</span></span>
<span id="cb6-167"><a href="#cb6-167"></a>    )</span>
<span id="cb6-168"><a href="#cb6-168"></a>    tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"google/gemma-2-2b-it"</span>)</span>
<span id="cb6-169"><a href="#cb6-169"></a></span>
<span id="cb6-170"><a href="#cb6-170"></a>    sample_prompt <span class="op">=</span> <span class="st">"How is it going?"</span></span>
<span id="cb6-171"><a href="#cb6-171"></a>    sample_completion <span class="op">=</span> <span class="st">"As an AI, I don't have feelings or experiences like humans do, so I don't have a </span><span class="ch">\"</span><span class="st">going</span><span class="ch">\"</span><span class="st"> in the same way."</span></span>
<span id="cb6-172"><a href="#cb6-172"></a></span>
<span id="cb6-173"><a href="#cb6-173"></a>    sample_chat <span class="op">=</span> [</span>
<span id="cb6-174"><a href="#cb6-174"></a>        {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: sample_prompt},</span>
<span id="cb6-175"><a href="#cb6-175"></a>        {<span class="st">"role"</span>: <span class="st">"assistant"</span>, <span class="st">"content"</span>: sample_completion}</span>
<span id="cb6-176"><a href="#cb6-176"></a>    ]</span>
<span id="cb6-177"><a href="#cb6-177"></a></span>
<span id="cb6-178"><a href="#cb6-178"></a>    sample_chat_tokens <span class="op">=</span> tokenizer.apply_chat_template(sample_chat, tokenize<span class="op">=</span><span class="va">False</span>, add_generation_prompt<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-179"><a href="#cb6-179"></a>    sample_chat_token_ids <span class="op">=</span> tokenizer.apply_chat_template(sample_chat, tokenize<span class="op">=</span><span class="va">True</span>, add_generation_prompt<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-180"><a href="#cb6-180"></a></span>
<span id="cb6-181"><a href="#cb6-181"></a>    <span class="bu">print</span>(<span class="st">"Chat tokens:"</span>)</span>
<span id="cb6-182"><a href="#cb6-182"></a>    <span class="bu">print</span>(sample_chat_tokens)</span>
<span id="cb6-183"><a href="#cb6-183"></a></span>
<span id="cb6-184"><a href="#cb6-184"></a>    <span class="bu">print</span>(<span class="st">"Chat token IDs:"</span>)</span>
<span id="cb6-185"><a href="#cb6-185"></a>    <span class="bu">print</span>(sample_chat_token_ids)</span>
<span id="cb6-186"><a href="#cb6-186"></a></span>
<span id="cb6-187"><a href="#cb6-187"></a>    response_start_idx, response_end_idx <span class="op">=</span> get_response_idxs(tokenizer, sample_chat_token_ids)</span>
<span id="cb6-188"><a href="#cb6-188"></a>    <span class="bu">print</span>(<span class="ss">f"Response tokens index in sample_chat_tokens range from </span><span class="sc">{</span>response_start_idx<span class="sc">}</span><span class="ss"> to </span><span class="sc">{</span>response_end_idx<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb6-189"><a href="#cb6-189"></a></span>
<span id="cb6-190"><a href="#cb6-190"></a>    first_response_token_id <span class="op">=</span> sample_chat_token_ids[response_start_idx]</span>
<span id="cb6-191"><a href="#cb6-191"></a>    last_response_token_id <span class="op">=</span> sample_chat_token_ids[response_end_idx]</span>
<span id="cb6-192"><a href="#cb6-192"></a>    <span class="bu">print</span>(<span class="ss">f'First response token is "</span><span class="sc">{</span>tokenizer<span class="sc">.</span>decode(first_response_token_id)<span class="sc">}</span><span class="ss">" with ID </span><span class="sc">{</span>first_response_token_id<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-193"><a href="#cb6-193"></a>    <span class="bu">print</span>(<span class="ss">f'Last response token is "</span><span class="sc">{</span>tokenizer<span class="sc">.</span>decode(last_response_token_id)<span class="sc">}</span><span class="ss">" with ID </span><span class="sc">{</span>last_response_token_id<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-194"><a href="#cb6-194"></a></span>
<span id="cb6-195"><a href="#cb6-195"></a>    <span class="co"># Make sure your code passes this test!</span></span>
<span id="cb6-196"><a href="#cb6-196"></a>    <span class="cf">assert</span> tokenizer.decode(first_response_token_id) <span class="op">==</span> <span class="st">"As"</span> <span class="kw">and</span> tokenizer.decode(last_response_token_id) <span class="op">==</span> <span class="st">"."</span></span>
<span id="cb6-197"><a href="#cb6-197"></a></span>
<span id="cb6-198"><a href="#cb6-198"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-199"><a href="#cb6-199"></a>        next_token_probs <span class="op">=</span> get_response_next_token_probs(tokenizer, model, sample_chat_token_ids)</span>
<span id="cb6-200"><a href="#cb6-200"></a>    <span class="bu">print</span>(<span class="ss">f'Next token probabilities: </span><span class="sc">{</span>next_token_probs<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-201"><a href="#cb6-201"></a></span>
<span id="cb6-202"><a href="#cb6-202"></a>    <span class="co"># Make sure your code passes this test!</span></span>
<span id="cb6-203"><a href="#cb6-203"></a>    <span class="cf">assert</span> next_token_probs.mean() <span class="op">&gt;</span> <span class="fl">0.7</span></span>
<span id="cb6-204"><a href="#cb6-204"></a></span>
<span id="cb6-205"><a href="#cb6-205"></a>    train_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb6-206"><a href="#cb6-206"></a>        <span class="st">"google/gemma-2-2b-it"</span>,</span>
<span id="cb6-207"><a href="#cb6-207"></a>        torch_dtype<span class="op">=</span>torch.bfloat16,</span>
<span id="cb6-208"><a href="#cb6-208"></a>        device_map<span class="op">=</span><span class="st">'auto'</span></span>
<span id="cb6-209"><a href="#cb6-209"></a>    )</span>
<span id="cb6-210"><a href="#cb6-210"></a>    lora_config <span class="op">=</span> LoraConfig()</span>
<span id="cb6-211"><a href="#cb6-211"></a>    train_model <span class="op">=</span> get_peft_model(train_model, lora_config)</span>
<span id="cb6-212"><a href="#cb6-212"></a>    train_model.train()</span>
<span id="cb6-213"><a href="#cb6-213"></a></span>
<span id="cb6-214"><a href="#cb6-214"></a>    ref_model <span class="op">=</span> model</span>
<span id="cb6-215"><a href="#cb6-215"></a>    ref_model.train()</span>
<span id="cb6-216"><a href="#cb6-216"></a>    <span class="bu">print</span>(<span class="st">'Loaded models!'</span>)</span>
<span id="cb6-217"><a href="#cb6-217"></a></span>
<span id="cb6-218"><a href="#cb6-218"></a>    <span class="co"># The model's response to the prompt usually includes the words "due to" - we want to change that to "because of" using DPO!</span></span>
<span id="cb6-219"><a href="#cb6-219"></a>    prompt <span class="op">=</span> <span class="st">"Explain why the sky is blue in one sentence."</span></span>
<span id="cb6-220"><a href="#cb6-220"></a>    preferred_completion <span class="op">=</span> <span class="st">"The sky appears blue because of"</span></span>
<span id="cb6-221"><a href="#cb6-221"></a>    nonpreferred_completion <span class="op">=</span> <span class="st">"The sky appears blue due to"</span></span>
<span id="cb6-222"><a href="#cb6-222"></a></span>
<span id="cb6-223"><a href="#cb6-223"></a>    preferred_chat <span class="op">=</span> [</span>
<span id="cb6-224"><a href="#cb6-224"></a>        {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt},</span>
<span id="cb6-225"><a href="#cb6-225"></a>        {<span class="st">"role"</span>: <span class="st">"assistant"</span>, <span class="st">"content"</span>: preferred_completion}</span>
<span id="cb6-226"><a href="#cb6-226"></a>    ]</span>
<span id="cb6-227"><a href="#cb6-227"></a></span>
<span id="cb6-228"><a href="#cb6-228"></a>    nonpreferred_chat <span class="op">=</span> [</span>
<span id="cb6-229"><a href="#cb6-229"></a>        {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt},</span>
<span id="cb6-230"><a href="#cb6-230"></a>        {<span class="st">"role"</span>: <span class="st">"assistant"</span>, <span class="st">"content"</span>: nonpreferred_completion}</span>
<span id="cb6-231"><a href="#cb6-231"></a>    ]</span>
<span id="cb6-232"><a href="#cb6-232"></a></span>
<span id="cb6-233"><a href="#cb6-233"></a>    preferred_chat_ids <span class="op">=</span> tokenizer.apply_chat_template(preferred_chat, tokenize<span class="op">=</span><span class="va">True</span>, add_generation_prompt<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-234"><a href="#cb6-234"></a>    nonpreferred_chat_ids <span class="op">=</span> tokenizer.apply_chat_template(nonpreferred_chat, tokenize<span class="op">=</span><span class="va">True</span>, add_generation_prompt<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-235"><a href="#cb6-235"></a></span>
<span id="cb6-236"><a href="#cb6-236"></a>    preferred_train_probs <span class="op">=</span> get_response_next_token_probs(tokenizer, train_model, preferred_chat_ids)</span>
<span id="cb6-237"><a href="#cb6-237"></a>    nonpreferred_train_probs <span class="op">=</span> get_response_next_token_probs(tokenizer, train_model, nonpreferred_chat_ids)</span>
<span id="cb6-238"><a href="#cb6-238"></a></span>
<span id="cb6-239"><a href="#cb6-239"></a>    <span class="co"># Gradients are not needed for the reference model since we will not be optimizing with respect to it</span></span>
<span id="cb6-240"><a href="#cb6-240"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-241"><a href="#cb6-241"></a>        preferred_ref_probs <span class="op">=</span> get_response_next_token_probs(tokenizer, ref_model, preferred_chat_ids)</span>
<span id="cb6-242"><a href="#cb6-242"></a>        nonpreferred_ref_probs <span class="op">=</span> get_response_next_token_probs(tokenizer, ref_model, nonpreferred_chat_ids)</span>
<span id="cb6-243"><a href="#cb6-243"></a></span>
<span id="cb6-244"><a href="#cb6-244"></a>    your_favorite_beta <span class="op">=</span> <span class="fl">1.0</span> <span class="co"># Feel free to play with beta here. Does anything change?</span></span>
<span id="cb6-245"><a href="#cb6-245"></a>    dpo_obj <span class="op">=</span> compute_dpo_objective(preferred_train_probs, nonpreferred_train_probs, preferred_ref_probs, nonpreferred_ref_probs, beta<span class="op">=</span>your_favorite_beta)</span>
<span id="cb6-246"><a href="#cb6-246"></a>    <span class="bu">print</span>(dpo_obj)</span>
<span id="cb6-247"><a href="#cb6-247"></a></span>
<span id="cb6-248"><a href="#cb6-248"></a>    prior_responses <span class="op">=</span> sample_model(tokenizer, train_model, prompt)</span>
<span id="cb6-249"><a href="#cb6-249"></a>    <span class="bu">print</span>(<span class="st">'Sampled responses before fine-tuning:</span><span class="ch">\n</span><span class="st">'</span> <span class="op">+</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>.join(prior_responses[:<span class="dv">10</span>]))</span>
<span id="cb6-250"><a href="#cb6-250"></a>    <span class="bu">print</span>(<span class="ss">f'Fraction responses with because of: </span><span class="sc">{</span>fraction_responses_with_because_of(prior_responses)<span class="sc">}</span><span class="ss">'</span>) <span class="co"># should start close to 0</span></span>
<span id="cb6-251"><a href="#cb6-251"></a></span>
<span id="cb6-252"><a href="#cb6-252"></a>    <span class="co"># DO NOT CHANGE THESE VALUES</span></span>
<span id="cb6-253"><a href="#cb6-253"></a>    num_gradient_steps <span class="op">=</span> <span class="dv">150</span> </span>
<span id="cb6-254"><a href="#cb6-254"></a>    learning_rate <span class="op">=</span> <span class="fl">2e-6</span></span>
<span id="cb6-255"><a href="#cb6-255"></a>    beta <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb6-256"><a href="#cb6-256"></a>    optimizer <span class="op">=</span> torch.optim.Adam(train_model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb6-257"><a href="#cb6-257"></a></span>
<span id="cb6-258"><a href="#cb6-258"></a>    finetune(tokenizer, optimizer, train_model, ref_model, preferred_chat_ids, nonpreferred_chat_ids, num_gradient_steps, beta)</span>
<span id="cb6-259"><a href="#cb6-259"></a></span>
<span id="cb6-260"><a href="#cb6-260"></a>    <span class="co"># Save GPU memory</span></span>
<span id="cb6-261"><a href="#cb6-261"></a>    <span class="kw">del</span> ref_model</span>
<span id="cb6-262"><a href="#cb6-262"></a>    <span class="kw">del</span> model</span>
<span id="cb6-263"><a href="#cb6-263"></a></span>
<span id="cb6-264"><a href="#cb6-264"></a>    post_tuning_responses <span class="op">=</span> sample_model(tokenizer, train_model, prompt)</span>
<span id="cb6-265"><a href="#cb6-265"></a>    <span class="bu">print</span>(<span class="st">'Sampled responses after fine-tuning:</span><span class="ch">\n</span><span class="st">'</span> <span class="op">+</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>.join(post_tuning_responses[:<span class="dv">10</span>]))</span>
<span id="cb6-266"><a href="#cb6-266"></a>    <span class="bu">print</span>(<span class="ss">f'Fraction responses with because of: </span><span class="sc">{</span>fraction_responses_with_because_of(post_tuning_responses)<span class="sc">}</span><span class="ss">'</span>) <span class="co"># should be more than half</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<p>Through our exploration of human preference models, we will ground ourselves in building a health coaching system that can provide meal recommendations aligned with a user’s dietary needs and preferences. Examples of scenarios which can benefit from a model of how humans make choices include:</p>
<ol type="1">
<li><p><strong>Health coaching:</strong> Humans express their preferences every time they pick lunch for consumption. Humans may have several goals related to nutrition, such as weight loss and improving concentration. We can learn how a given individual or set of individuals prefer to eat to provide personalized recommendations to help them attain their goals. This chapter will use this use case to ground human preference modeling in a real-life application.</p></li>
<li><p><strong>Social media:</strong> Platforms have a far greater amount of content than one can consume in a lifetime, yet such products must aim to maximize user engagement. To accomplish this, we can learn what specific things people like to see in their feeds to optimize the value they gain out of their time on social media. For example, the video feed social media platform <a href="https://www.tiktok.com/">TikTok</a> has had viral adoption due to its notorious ability to personalize a feed for its users based on their preferences.</p></li>
<li><p><strong>Shopping:</strong> Retail corporations largely aim to maximize revenue by making it easy for people to make purchases. Recommendation systems on online shopping platforms provide a mechanism for curating specific items based on an individual’s previous purchases (or even browsing history) to make shoppers aware of items they may like and, therefore, purchase.</p></li>
</ol>
<!--
Two key models used in pairwise sampling are the Thurstonian and Bradley-Terry models [@cattelan2012]. The Thurstonian model assumes each item $i$ has a true score $u_i$ following a normal distribution. The difference $d_{ij} = u_i - u_j$ is also normally distributed. The probability that item $i$ is preferred over item $j$ is given by $P(i \succ j) = \Phi \left( \frac{u_i - u_j}{\sqrt{2\sigma^2}} \right)$, where $\Phi$ is the cumulative normal distribution function. The denominator $\sqrt{2\sigma^2}$ is the standard deviation of the difference $d_{ij} = u_i - u_j$ when $u_i$ and $u_j$ are normally distributed with variance $\sigma^2$[@cattelan2012]. The Bradley-Terry model defines the probability of preference based on latent scores $\beta_i$ and $\beta_j$. The probability that item $i$ is preferred over item $j$ is $P(i \succ j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}$. This model is used to estimate relative strengths or preferences based on latent scores. [@cattelan2012].

::: {#tbl-philosophy}
  -----------------------------------------------------------------------
  Application                         Human Preference
  ----------------------------------- -----------------------------------
  Computer vision: train a neural     This is how humans process images
  network to predict bounding boxes   by identifying the position and
  delineating all instances of dogs   geometry of the things we see in
  in an image                         them

  Natural language processing: train  Coherent text is itself a
  a model to generate coherent text   human-created and defined concept,
                                      and we prefer that any
                                      synthetically generated text
                                      matches that of humans

  Computer vision: train a diffusion  Humans prefer that images
  model to generate realistic images  accurately capture the world as
  of nature                           observed by humans, and this
                                      generative model should reflect the
                                      details that comprise that
                                      preference
  -----------------------------------------------------------------------

  : Examples of machine learning tasks and their interpretation as
  modeling human preferences.
:::
-->


<!-- -->

</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bhatia2020preference" class="csl-entry" role="listitem">
Bhatia, Kush, Ashwin Pananjady, Peter L. Bartlett, Anca D. Dragan, and Martin J. Wainwright. 2020. <span>“Preference Learning Along Multiple Criteria: A Game-Theoretic Perspective.”</span> <em>Neural Information Processing Systems</em> 34 (1): 1–12.
</div>
<div id="ref-2001.04465" class="csl-entry" role="listitem">
Bobu, Andreea, Dexter R. R. Scobee, Jaime F. Fisac, S. Shankar Sastry, and Anca D. Dragan. 2020. <span>“LESS Is More: Rethinking Probabilistic Models of Human Behavior.”</span> <a href="https://doi.org/10.1145/3319502.3374811">https://doi.org/10.1145/3319502.3374811</a>.
</div>
<div id="ref-book_estimation_bock" class="csl-entry" role="listitem">
Bock, Hans Georg, Thomas Carraro, Willi Jäger, Stefan Körkel, Rolf Rannacher, and Johannes P. Schlöder. 2015. <em>Model Based Parameter Estimation: Theory and Applications</em>. Springer. <a href="https://api.semanticscholar.org/CorpusID:60333071">https://api.semanticscholar.org/CorpusID:60333071</a>.
</div>
<div id="ref-bolt2009" class="csl-entry" role="listitem">
Bolt, Daniel M., and James A. Wollack. 2009. <span>“Application of a Multidimensional Nested Logit Model to Multiple-Choice Test Items.”</span> <em>Journal of Educational Measurement</em> 46 (3): 181–98. <a href="https://doi.org/10.1111/j.1745-3984.2009.00081.x">https://doi.org/10.1111/j.1745-3984.2009.00081.x</a>.
</div>
<div id="ref-Liang2021" class="csl-entry" role="listitem">
Bommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, et al. 2021. <span>“On the Opportunities and Risks of Foundation Models.”</span>
</div>
<div id="ref-bradley1952rank" class="csl-entry" role="listitem">
Bradley, Ralph Allan, and Milton E Terry. 1952a. <span>“Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons.”</span> <em>Biometrika</em> 39 (3/4): 324–45.
</div>
<div id="ref-bradley-terry-model" class="csl-entry" role="listitem">
Bradley, Ralph Allan, and Milton E. Terry. 1952b. <span>“Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons.”</span> <em>Biometrika</em> 39 (3/4): 324–45. <a href="http://www.jstor.org/stable/2334029">http://www.jstor.org/stable/2334029</a>.
</div>
<div id="ref-campbell2015" class="csl-entry" role="listitem">
Campbell, Danny, and Seda Erdem. 2015. <span>“Position Bias in Best-Worst Scaling Surveys: A Case Study on Trust in Institutions.”</span> <em>American Journal of Agricultural Economics</em> 97 (2): 526–45. <a href="https://doi.org/10.1093/ajae/aau112">https://doi.org/10.1093/ajae/aau112</a>.
</div>
<div id="ref-book_estimation_casella" class="csl-entry" role="listitem">
Casella, George, and Roger L. Berger. 1990. <em>Statistical Inference</em>. Springer. <a href="https://api.semanticscholar.org/CorpusID:125727004">https://api.semanticscholar.org/CorpusID:125727004</a>.
</div>
<div id="ref-christiano2023deep" class="csl-entry" role="listitem">
Christiano, Paul, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2023. <span>“Deep Reinforcement Learning from Human Preferences.”</span> <a href="https://arxiv.org/abs/1706.03741">https://arxiv.org/abs/1706.03741</a>.
</div>
<div id="ref-finn2017model" class="csl-entry" role="listitem">
Finn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017. <span>“Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.”</span> In <em>International Conference on Machine Learning</em>, 1126–35. PMLR.
</div>
<div id="ref-idealpoints" class="csl-entry" role="listitem">
Greiner, James. 2005. <span>“Ideal Points.”</span> Harvard IQSS Blog. <a href="https://blogs.iq.harvard.edu/ideal_points_1">https://blogs.iq.harvard.edu/ideal_points_1</a>.
</div>
<div id="ref-haarnoja2018soft" class="csl-entry" role="listitem">
Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. <span>“Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.”</span> In <em>International Conference on Machine Learning</em>, 1861–70. PMLR.
</div>
<div id="ref-harpe2015" class="csl-entry" role="listitem">
Harpe, Spencer E. 2015. <span>“How to Analyze Likert and Other Rating Scale Data.”</span> <em>Currents in Pharmacy Teaching and Learning</em> 7 (5): 836–50. <a href="http://dx.doi.org/10.1016/j.cptl.2015.08.001">http://dx.doi.org/10.1016/j.cptl.2015.08.001</a>.
</div>
<div id="ref-hejna2023few" class="csl-entry" role="listitem">
Hejna III, Donald Joseph, and Dorsa Sadigh. 2023. <span>“Few-Shot Preference Learning for Human-in-the-Loop Rl.”</span> In <em>Conference on Robot Learning</em>, 2014–25. PMLR.
</div>
<div id="ref-huber1976ideal" class="csl-entry" role="listitem">
Huber, Joel. 1976. <span>“Ideal Point Models of Preference.”</span> In <em>Advances in Consumer Research</em>, 03:138–42. Association for Consumer Research.
</div>
<div id="ref-ideal_point" class="csl-entry" role="listitem">
Jamieson, Kevin G, and Robert Nowak. 2011. <span>“Active Ranking Using Pairwise Comparisons.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger. Vol. 24. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2011/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2011/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf</a>.
</div>
<div id="ref-keisler2003common" class="csl-entry" role="listitem">
Keisler, H. Jerome, and Byung Soo Lee. 2003. <span>“Common Assumption of Rationality.”</span> <em>Economic Theory Journal</em> 30 (2): 123–45.
</div>
<div id="ref-lee2021pebble" class="csl-entry" role="listitem">
Lee, Kimin, Laura Smith, and Pieter Abbeel. 2021. <span>“Pebble: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-Training.”</span> <em>arXiv Preprint arXiv:2106.05091</em>.
</div>
<div id="ref-Luce1977" class="csl-entry" role="listitem">
Luce, R.Duncan. 1977. <span>“The Choice Axiom After Twenty Years.”</span> <em>Journal of Mathematical Psychology</em> 15 (3): 215–33. <a href="https://doi.org/10.1016/0022-2496(77)90032-3">https://doi.org/10.1016/0022-2496(77)90032-3</a>.
</div>
<div id="ref-miljkovic2005rational" class="csl-entry" role="listitem">
Miljkovic, Dragan. 2005. <span>“Rational Choice and Irrational Individuals or Simply an Irrational Theory: A Critical Review of the Hypothesis of Perfect Rationality.”</span> <em>The Journal of Socio-Economics</em> 34 (5): 621–34. <a href="https://doi.org/10.1016/j.socec.2003.12.031">https://doi.org/10.1016/j.socec.2003.12.031</a>.
</div>
<div id="ref-myers2022learning" class="csl-entry" role="listitem">
Myers, Vivek, Erdem Biyik, Nima Anari, and Dorsa Sadigh. 2022. <span>“Learning Multimodal Rewards from Rankings.”</span> In <em>Conference on Robot Learning</em>, 342–52. PMLR.
</div>
<div id="ref-myers2021learning" class="csl-entry" role="listitem">
Myers, Vivek, Erdem Bıyık, Nima Anari, and Dorsa Sadigh. 2021. <span>“Learning Multimodal Rewards from Rankings.”</span> <a href="https://arxiv.org/abs/2109.12750">https://arxiv.org/abs/2109.12750</a>.
</div>
<div id="ref-padalkar2023open" class="csl-entry" role="listitem">
Padalkar, Abhishek, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, et al. 2023. <span>“Open x-Embodiment: Robotic Learning Datasets and RT-x Models.”</span> <em>arXiv Preprint arXiv:2310.08864</em>.
</div>
<div id="ref-plackett_luce" class="csl-entry" role="listitem">
Plackett, R. L. 1975. <span>“The Analysis of Permutations.”</span> <em>Journal of the Royal Statistical Society. Series C (Applied Statistics)</em> 24 (2): 193–202. <a href="http://www.jstor.org/stable/2346567">http://www.jstor.org/stable/2346567</a>.
</div>
<div id="ref-rafailov2023direct" class="csl-entry" role="listitem">
Rafailov, Rafael, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. <span>“Direct Preference Optimization: Your Language Model Is Secretly a Reward Model.”</span> <a href="https://arxiv.org/abs/2305.18290">https://arxiv.org/abs/2305.18290</a>.
</div>
<div id="ref-ragain2019" class="csl-entry" role="listitem">
Ragain, Stephen, and Johan Ugander. 2019. <span>“Choosing to Rank.”</span> <em>arXiv Preprint arXiv:1809.05139</em>. <a href="https://arxiv.org/abs/1809.05139">https://arxiv.org/abs/1809.05139</a>.
</div>
<div id="ref-gradient_descent" class="csl-entry" role="listitem">
Ruder, Sebastian. 2016. <span>“An Overview of Gradient Descent Optimization Algorithms.”</span> <em>ArXiv</em> abs/1609.04747. <a href="https://api.semanticscholar.org/CorpusID:17485266">https://api.semanticscholar.org/CorpusID:17485266</a>.
</div>
<div id="ref-simon1972theories" class="csl-entry" role="listitem">
Simon, Herbert A. 1972. <span>“Theories of Bounded Rationality.”</span> In <em>Decision and Organization</em>, edited by C. B. McGuire and Roy Radner, 161–76. North-Holland Publishing Company.
</div>
<div id="ref-tatli2022distancepreferences" class="csl-entry" role="listitem">
Tatli, Gokcan, Rob Nowak, and Ramya Korlakai Vinayak. 2022. <span>“Learning Preference Distributions from Distance Measurements.”</span> In <em>2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)</em>, 1–8. <a href="https://doi.org/10.1109/Allerton49937.2022.9929404">https://doi.org/10.1109/Allerton49937.2022.9929404</a>.
</div>
<div id="ref-2307.09288" class="csl-entry" role="listitem">
Touvron, Hugo et al. 2023. <span>“Llama 2: Open Foundation and Fine-Tuned Chat Models.”</span> <a href="https://arxiv.org/abs/2307.09288">https://arxiv.org/abs/2307.09288</a>.
</div>
<div id="ref-yu2020meta" class="csl-entry" role="listitem">
Yu, Tianhe, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. 2020. <span>“Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning.”</span> In <em>Conference on Robot Learning</em>, 1094–1100. PMLR.
</div>
<div id="ref-zhou2019watch" class="csl-entry" role="listitem">
Zhou, Allan, Eric Jang, Daniel Kappler, Alex Herzog, Mohi Khansari, Paul Wohlhart, Yunfei Bai, Mrinal Kalakrishnan, Sergey Levine, and Chelsea Finn. 2019. <span>“Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards.”</span> In <em>International Conference on Learning Representations</em>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../index.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../src/003-measure.html" class="pagination-link" aria-label="Model-Based Preference Optimization">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb7" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1"></a><span class="fu"># Models of Preferences and Decisions {#ch-human-decision-making-choice-models}</span></span>
<span id="cb7-2"><a href="#cb7-2"></a></span>
<span id="cb7-3"><a href="#cb7-3"></a>::: {.content-visible when-format="html"}</span>
<span id="cb7-4"><a href="#cb7-4"></a></span>
<span id="cb7-5"><a href="#cb7-5"></a>&lt;iframe</span>
<span id="cb7-6"><a href="#cb7-6"></a>  src="https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/"</span>
<span id="cb7-7"><a href="#cb7-7"></a>  style="width:45%; height:225px;"</span>
<span id="cb7-8"><a href="#cb7-8"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb7-9"><a href="#cb7-9"></a>&lt;iframe</span>
<span id="cb7-10"><a href="#cb7-10"></a>  src="https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/"</span>
<span id="cb7-11"><a href="#cb7-11"></a>  style="width:45%; height:225px;"</span>
<span id="cb7-12"><a href="#cb7-12"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb7-13"><a href="#cb7-13"></a><span class="co">[</span><span class="ot">Fullscreen Part 1</span><span class="co">](https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/)</span>{.btn .btn-outline-primary .btn role="button"}</span>
<span id="cb7-14"><a href="#cb7-14"></a><span class="co">[</span><span class="ot">Fullscreen Part 2</span><span class="co">](https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/)</span>{.btn .btn-outline-primary .btn role="button"}</span>
<span id="cb7-15"><a href="#cb7-15"></a></span>
<span id="cb7-16"><a href="#cb7-16"></a>:::</span>
<span id="cb7-17"><a href="#cb7-17"></a></span>
<span id="cb7-18"><a href="#cb7-18"></a><span class="fu">## Introduction</span></span>
<span id="cb7-19"><a href="#cb7-19"></a></span>
<span id="cb7-20"><a href="#cb7-20"></a>Human preference modeling aims to capture humans' decision making processes in a probabilistic framework. Many problems would benefit from a quantitative perspective, enabling an understanding of how humans engage with the world. In this chapter, we will explore how one can model human preferences, including different formulations of such models, how one can optimize these models given data, and considerations one should understand to create such systems. We describe these assumptions in @sec-foundations.</span>
<span id="cb7-21"><a href="#cb7-21"></a></span>
<span id="cb7-22"><a href="#cb7-22"></a><span class="fu">## Foundations of Preference Models {#sec-foundations}</span></span>
<span id="cb7-23"><a href="#cb7-23"></a></span>
<span id="cb7-24"><a href="#cb7-24"></a><span class="co">&lt;!--</span></span>
<span id="cb7-25"><a href="#cb7-25"></a><span class="co">An alternative framework we will explore is ranking, in which we can model an ordering of given choices from most to least desirable. It is possible that there is an infinite set of options; in this case, our model will have to reason about a discretized set of options and may fail to capture the full space of possibilities a human would choose from in the real world.</span></span>
<span id="cb7-26"><a href="#cb7-26"></a><span class="co">--&gt;</span></span>
<span id="cb7-27"><a href="#cb7-27"></a></span>
<span id="cb7-28"><a href="#cb7-28"></a><span class="fu">### Axiom 1: Construction of Choices Set {#axiom-1-preference-models-model-choice .unnumbered}</span></span>
<span id="cb7-29"><a href="#cb7-29"></a></span>
<span id="cb7-30"><a href="#cb7-30"></a>Human preference models model the preferred choices amongst a set of options. For example, this could be modeling which meal from a set of options a person will most likely choose. Preference models must enumerate the set of all possible choices included in a human decision. As such, we must ensure that the choices we enumerate capture the entire domain (collectively exhaustive) but are distinct (mutually exclusive) choices. A discrete set of choices is a constraint we canonically impose to ensure we can tractably model preferences and aptly estimate the parameters of preference models. We assume that if a new option is added to the choice set, the relative probabilities of choosing between the original options remain unchanged. This is known as Independence of Irrelevant Alternatives (IIA) property from Luce's axiom of choices <span class="co">[</span><span class="ot">@Luce1977</span><span class="co">]</span>.</span>
<span id="cb7-31"><a href="#cb7-31"></a></span>
<span id="cb7-32"><a href="#cb7-32"></a><span class="fu">### Axiom 2: Preference Centers around Utility {#axiom-3-preference-centers-around-utility .unnumbered}</span></span>
<span id="cb7-33"><a href="#cb7-33"></a></span>
<span id="cb7-34"><a href="#cb7-34"></a>Human preference models are centered around the notion of utility, a scalar quantity representing the benefit or value an individual attains from selecting a given choice. We assume that the underlying utility mechanism of a human preference model captures the final decision output from a human. We use the notation $u_{i,j}$ as the utility of person $i$ choosing item $j$. The utility is a random variable, decomposing into true utility $u_{i,j}^*$ and a random noise $\epsilon_{i,j}$: $u_{i,j} = u_{i,j}^* + \epsilon_{i,j}$. Only the relative difference in utility matters to predict the choice among options. As such, the scale of the utilities is irrelevant within a given set of human preference data. The scale of utilities is important when comparing across datasets or across different humans; since utility may be defined differently in various datasets or different human. A common practice to address this consideration is to standardize the utilities in each dataset based on its variance in the observed data.</span>
<span id="cb7-35"><a href="#cb7-35"></a></span>
<span id="cb7-36"><a href="#cb7-36"></a><span class="fu">### Axiom 3: Rationality {#human-rationality .unnumbered}</span></span>
<span id="cb7-37"><a href="#cb7-37"></a></span>
<span id="cb7-38"><a href="#cb7-38"></a>Modeling decision-making must also take into account rationality. Rationality assumption provides a framework for predicting and modeling human behavior by outlining the principles that guide decision-making processes <span class="co">[</span><span class="ot">@keisler2003common</span><span class="co">]</span>. By incorporating different types of rationality, researchers can create more accurate and realistic models that reflect the complexities of human decision-making <span class="co">[</span><span class="ot">@miljkovic2005rational; @simon1972theories</span><span class="co">]</span>. Perfect rationality posits that individuals make decisions that maximize their utility, assuming they have complete information and the cognitive ability to process this information to make optimal choices <span class="co">[</span><span class="ot">@miljkovic2005rational</span><span class="co">]</span>. Numerous studies have shown that this assumption frequently fails to describe actual human behavior, as individuals do not always act in ways that maximize their utility due to various constraints and biases <span class="co">[</span><span class="ot">@miljkovic2005rational</span><span class="co">]</span>. Bounded rationality acknowledges that individuals operate within the limits of their information and cognitive capabilities. Decisions are made using heuristics rather than through exhaustive analysis, reflecting the practical constraints of real-world decision-making <span class="co">[</span><span class="ot">@simon1972theories</span><span class="co">]</span>. Bounded rationality acknowledges that decisions are influenced by noise, resulting in probabilistic choice behavior: while individuals aim to maximize their utility, random factors can lead to deviations from perfectly rational choices <span class="co">[</span><span class="ot">@miljkovic2005rational</span><span class="co">]</span>.</span>
<span id="cb7-39"><a href="#cb7-39"></a></span>
<span id="cb7-40"><a href="#cb7-40"></a>Bounded rationality can be operationalized through Boltzmann rationalit. It addresses the likelihood of a human selecting an option $o$ from a set $O$. Desirability is represented by a value function $v : O \rightarrow \mathbb{R}^+$, with the selection probability calculated as $P(o) = \frac{v(o)}{\sum_{o' \in O} v(o')}$. Assuming there is an underlying reward for each option $R(o) \in \mathbb{R}$ such that $v(o) = e^{R(o)}$, we get $P(o) = \frac{e^{R(o)}}{\sum_{\bar{o} \in \mathcal{O}} e^{R(\bar{o})}}$. Essentially, "A human will act out a trajectory with a probability proportional to the exponentiated return they receive for the trajectory." When choices involve trajectories $\xi \in \Xi$ (sequences of actions), the reward $R$ is typically a function of a feature vector $\phi : \Xi \rightarrow \mathbb{R}^k$, and the probability density is given by $p(\xi) = \frac{e^{R(\phi(\xi))}}{\int_{\Xi} e^{R(\phi(\bar{\xi}))} d\bar{\xi}}$.</span>
<span id="cb7-41"><a href="#cb7-41"></a></span>
<span id="cb7-42"><a href="#cb7-42"></a>Boltzmann rationality has the "duplicates problem," where there is no concept of similar actions (e.g., choosing between using a car or a train for transportation, with no particular preference). The probability of making the decision is 50% for either option. However, if we now have 100 cars, under Boltzmann, we would have a 99% probability of choosing a car, which is unrealistic. To address this issue, various extensions have been proposed. One such extension is the attribute rule, which interprets options as bundles of attributes. In this rule, attributes $X$ are associated with options, and they have desirability values $w(x)$. An attribute intensity function $s(x, o)$ indicates the degree to which an attribute is expressed in an option. The probability of choosing option $o$ is</span>
<span id="cb7-43"><a href="#cb7-43"></a></span>
<span id="cb7-44"><a href="#cb7-44"></a>$$P(o) = \sum_{x \in \mathcal{X}_o} \frac{w(x)}{\sum_{\bar{x} \in \mathcal{X}_o} w(\bar{x})} \cdot \frac{s(x, o)}{\sum_{\tilde{o} \in \mathcal{O}} s(x, \bar{o})}$$</span>
<span id="cb7-45"><a href="#cb7-45"></a></span>
<span id="cb7-46"><a href="#cb7-46"></a>This equation describes a two-step process where an attribute $x \in X_O$ is first chosen according to a Boltzmann-like rule and then an option $o \in O$ with that attribute is selected using another Boltzmann-like rule. This approach handles duplicates gracefully by effectively creating a two-layer hierarchy in choosing an option. Boltzmann rationality finds practical applications in various fields, particularly in reinforcement learning, where it models decision-making in uncertain environments. It also applies to trajectory selection, where the probability of a sequence of actions (trajectory) is proportional to the exponential return. These applications enhance the accuracy of models that interact with or predict human behavior, making Boltzmann Rationality a vital component of the models of interaction.</span>
<span id="cb7-47"><a href="#cb7-47"></a></span>
<span id="cb7-48"><a href="#cb7-48"></a>We next explore a case study to deepen our understanding of rationality: Limiting Errors due to Similar Selection (LESS) <span class="co">[</span><span class="ot">@2001.04465</span><span class="co">]</span>. LESS takes inspiration from the attribute rule and extends it to continuous trajectories <span class="co">[</span><span class="ot">@2001.04465</span><span class="co">]</span>. The key insight is that instead of creating "attributes", which group together similar discrete options, it introduces a similarity metric on the space of continuous actions, thereby creating similar groupings on trajectories. The LESS similarity metric could be defined in trajectory space, where the trajectory is some theoretical notion of all states and actions one passes through over time. However, it is instead defined on the measured feature vector $\phi(\xi)$ associated with the agent's trajectory $\xi$. In practice, one can never measure the exact trajectory with perfect fidelity. The feature vector will almost necessarily map in a one-to-many fashion with trajectories. Formally,</span>
<span id="cb7-49"><a href="#cb7-49"></a>let $\phi \in \Phi$ be the set of all possible feature vectors $\xi \in \Xi$ the set of all trajectories. The set of feature vectors belonging to a set of trajectories $\Xi' \subseteq \Xi$ is $\Phi_{\Xi'}$. We begin with equation (4) and substitute our similarity metric on feature vectors of trajectories.</span>
<span id="cb7-50"><a href="#cb7-50"></a></span>
<span id="cb7-51"><a href="#cb7-51"></a>$$\begin{aligned}</span>
<span id="cb7-52"><a href="#cb7-52"></a>    P(\xi) = \frac{e^{R(\phi(\xi))}}{\sum_{\bar{\phi} \in \Phi_{\Xi}} e^{R(\hat{\phi})}} \cdot \frac{s(\phi(\xi), \bar{\xi})}{\sum_{\hat{\xi} \in \Xi} s(\phi(\xi), \bar{\xi})}</span>
<span id="cb7-53"><a href="#cb7-53"></a>\end{aligned}$$</span>
<span id="cb7-54"><a href="#cb7-54"></a></span>
<span id="cb7-55"><a href="#cb7-55"></a>The probability of choosing trajectory $\xi$ is proportional to the exponentiated reward for the agent's measured trajectory $\phi(\xi)$, normalized by the sum of all rewards over all possible measured trajectories. The second half of the product is a normalization factor based on how similar the current trajectory is to other trajectories in feature space. We can define the similarity function as an indicator function, where $s(x, \xi) = 1$ only if $x = \phi(\xi)$. That means that multiple trajectories with the same feature vector will effectively be considered a single option. Thus, we achieve the "bundling" of trajectories, in the same way that the attribute rule bundled options under different attributes.</span>
<span id="cb7-56"><a href="#cb7-56"></a></span>
<span id="cb7-57"><a href="#cb7-57"></a>However, setting the similarity metric as an indicator function isn't sufficiently flexible. We want a proper metric that acts more as a continuous distance over the feature space. We instead define $s$ to be a soft similarity metric $s : \Phi \times \Xi \rightarrow \mathbb{R}^+$ with the following properties:</span>
<span id="cb7-58"><a href="#cb7-58"></a></span>
<span id="cb7-59"><a href="#cb7-59"></a><span class="ss">1.  </span>$s(\phi(\xi), \xi) = \max_{x \in \phi, \bar{\xi} \in \Xi} s(x, \hat{\xi}) \forall (\xi \in \Xi)$</span>
<span id="cb7-60"><a href="#cb7-60"></a></span>
<span id="cb7-61"><a href="#cb7-61"></a><span class="ss">2.  </span>Symmetric: $s(\phi(\xi), \bar{\xi}) = s(\phi(\bar{\xi}), \xi)$</span>
<span id="cb7-62"><a href="#cb7-62"></a></span>
<span id="cb7-63"><a href="#cb7-63"></a><span class="ss">3.  </span>Positive Semidefinite: $s(x, \xi) \geq 0$</span>
<span id="cb7-64"><a href="#cb7-64"></a></span>
<span id="cb7-65"><a href="#cb7-65"></a>Using this redefined similarity metric $s$, we extend (5) to be a probability density on the continuous trajectory space $\mathcal{E}$, as in (3).</span>
<span id="cb7-66"><a href="#cb7-66"></a></span>
<span id="cb7-67"><a href="#cb7-67"></a>$$p(\hat{\xi}) = \frac{\frac{e^{R(\phi(\xi))}}{\int_{\Xi} s(\phi(\xi), \bar{\xi}) d\bar{\xi}}}{\int_{\Xi}\frac{e^{R(\phi(\hat{\xi}))}}{\int_{\Xi} s(\phi(\hat{\xi}), \bar{\xi}) d\bar{\xi}}d\hat{\xi}} \propto \frac{e^{R(\phi(\hat{\xi}))}}{\int_{\Xi} s(\phi(\xi), \bar{\xi}) d\bar{\xi}}$$</span>
<span id="cb7-68"><a href="#cb7-68"></a></span>
<span id="cb7-69"><a href="#cb7-69"></a>Under this formulation, the likelihood of selecting a trajectory is inversely proportional to its feature-space similarity with other trajectories. This de-weights similar trajectories, which is the desired effect for our LESS model of human decision-making. This means, though, that the "trajectory bundle" of similar trajectories still has a reasonable probability of being chosen.</span>
<span id="cb7-70"><a href="#cb7-70"></a></span>
<span id="cb7-71"><a href="#cb7-71"></a><span class="fu">### Axiom 4: Preference captures decision-making {#axiom-2-preference-captures-decision-making .unnumbered}</span></span>
<span id="cb7-72"><a href="#cb7-72"></a>Human preferences are classified into two categories: revealed preferences and stated preferences.</span>
<span id="cb7-73"><a href="#cb7-73"></a></span>
<span id="cb7-74"><a href="#cb7-74"></a>Revealed preferences are those one can observe retroactively from existing data. The implicit decision-making knowledge can be captured via learnable parameters and their usage in models which represent relationships between input decision attributes that may have little human interpretability, but enable powerful models of human preference. For health coaching, we may have information about which foods an individual has chosen previously in different contexts, allowing us to build a model from their decisions. Such data may be easier to acquire and can reflect real-world outcomes (since they are, at least theoretically, inherently based on human preferences). However, if we fail to capture sufficient context in such data, human preference models may not sufficiently capture human preferences.</span>
<span id="cb7-75"><a href="#cb7-75"></a></span>
<span id="cb7-76"><a href="#cb7-76"></a>Stated preferences are those individuals explicitly indicate in potentially experimental conditions. The explicit knowledge may be leveraged by including inductive biases during modeling (for example, the context used in a model) which are reasonable assumptions for how a human would consider a set of options.This may include controlled experiments or studies. This may be harder to obtain and somewhat biased, as they can be hypothetical or only accurately reflect a piece of the overall context of a decision. However, they enable greater control of the decision-making process.</span>
<span id="cb7-77"><a href="#cb7-77"></a></span>
<span id="cb7-78"><a href="#cb7-78"></a><span class="fu">## Methods for Collecting Preference Data {#sec-collect}</span></span>
<span id="cb7-79"><a href="#cb7-79"></a></span>
<span id="cb7-80"><a href="#cb7-80"></a>Next, we explore various mechanisms by which humans can express their preferences, including pairwise sampling, rank-order sampling, rating-scale sampling, best-worst scaling, and multiple-choice samples. In *pairwise sampling*, participants compare two options to determine which is preferred. One of the major advantage of this method is low cognitive demand for rater. Its disavantage is the limited amount of information content elicited by a sample. Next, we will see that we can trading cognitive demand for rater to elicit more nuance preference information. For example, *Rank-order sampling* captures human preferences by having participants rank items from most to least preferred. Used in voting, market research, and psychology, it provides rich preference data but is more complex and cognitively demanding than pairwise comparisons, especially for large item sets. Participants may also rank inconsistently <span class="co">[</span><span class="ot">@ragain2019</span><span class="co">]</span>. </span>
<span id="cb7-81"><a href="#cb7-81"></a></span>
<span id="cb7-82"><a href="#cb7-82"></a>*Rating-scale sampling*, such as the Likert scale, is a method in which participants rate items on a fixed-point scale (e.g., 1 to 5, "Strongly Disagree" to "Strongly Agree") to measure levels of preference towards items <span class="co">[</span><span class="ot">@harpe2015</span><span class="co">]</span>. Participants can also mark a point on a continuous rating scale to indicate their preference or attitude. Commonly used in surveys, product reviews, and psychological assessments, this method provides a more nuanced measure than discrete scales. Rating-scale sampling is simple for participants to understand and use, provides rich data on the intensity of preferences, and is flexible enough for various measurements (e.g., agreement, satisfaction) <span class="co">[</span><span class="ot">@harpe2015</span><span class="co">]</span>. However, rating-scale sampling methods also have limitations. Ratings can be influenced by personal biases and interpretations of scales, leading to subjectivity. There is a central tendency bias, where participants may avoid extreme ratings, resulting in clustering responses around the middle. Different participants might interpret scale points differently, and fixed-point scales may not capture the full nuance of participants' preferences or attitudes <span class="co">[</span><span class="ot">@harpe2015</span><span class="co">]</span>.</span>
<span id="cb7-83"><a href="#cb7-83"></a></span>
<span id="cb7-84"><a href="#cb7-84"></a>*In Best-worst scaling* (BWS), participants are presented with items and asked to identify the most and least preferred options. The primary objective of BWS is to discern the relative importance or preference of items, making it widely applicable in various fields such as market research, health economics, and social sciences <span class="co">[</span><span class="ot">@campbell2015</span><span class="co">]</span>. BWS provides rich data on the relative importance of items, helps clarify preferences, reduces biases found in traditional rating scales, and results in utility scores that are easy to interpret. However, BWS also has limitations, including potential scale interpretation differences among participants, and design challenges to avoid biases, such as the order effect or the context in which items are presented.</span>
<span id="cb7-85"><a href="#cb7-85"></a></span>
<span id="cb7-86"><a href="#cb7-86"></a>*Multiple-choice sampling* involve participants selecting one option from a set of alternatives. Multiple-choice sampling is simple for participants to understand and reflect on realistic decision-making scenarios where individuals choose one option from many. It is beneficial in complex choice scenarios, such as modes of transportation, where choices are not independent <span class="co">[</span><span class="ot">@bolt2009</span><span class="co">]</span>. Multiple-choice sampling often relies on simplistic assumptions such as the independence of irrelevant alternatives (IIA), which may not always hold true. This method may also fail to capture the variation in preferences among different individuals, as it typically records only the most preferred choice without accounting for the relative importance of other options.</span>
<span id="cb7-87"><a href="#cb7-87"></a></span>
<span id="cb7-88"><a href="#cb7-88"></a><span class="fu">## Models of Choices {#sec-models}</span></span>
<span id="cb7-89"><a href="#cb7-89"></a></span>
<span id="cb7-90"><a href="#cb7-90"></a><span class="fu">#### Binary Choice Model {#binary-choice-model .unnumbered}</span></span>
<span id="cb7-91"><a href="#cb7-91"></a></span>
<span id="cb7-92"><a href="#cb7-92"></a>Binary choice model is centered around one item. The model predicts, for that option, after observing user choices in the past, whether that option will be chosen or not. We use binary variable $y \in <span class="sc">\{</span>0, 1<span class="sc">\}</span>$ to represent whether that choice will be picked by the user in the next phase of selection. We denote $P = \mathbb{P}(y = 1)$. We can formally model $y$ as a function of the utility of the positive choice: $y = \mathbb{I}<span class="co">[</span><span class="ot">U&gt;0</span><span class="co">]</span>$. We explore two cases based on the noise distribution. $\psi$ is the logistic function or the standard normal cummulative distribution function if noise follows logistic distribution and the standard normal distribution, repsectively:</span>
<span id="cb7-93"><a href="#cb7-93"></a>$$</span>
<span id="cb7-94"><a href="#cb7-94"></a>\mathbb{P}(u_{i,j} &gt; 0) = \mathbb{P}(u_{i,j}^* + \epsilon &gt; 0) = 1 - \mathbb{P}( \epsilon &lt; -u_{i,j}^*) = \psi(u_{i,j}^*).</span>
<span id="cb7-95"><a href="#cb7-95"></a>$$</span>
<span id="cb7-96"><a href="#cb7-96"></a></span>
<span id="cb7-97"><a href="#cb7-97"></a><span class="fu">#### Bradley-Terry Model {#bradley-terry-model .unnumbered}</span></span>
<span id="cb7-98"><a href="#cb7-98"></a></span>
<span id="cb7-99"><a href="#cb7-99"></a>The Bradley-Terry model compares the utility of choice over all others <span class="co">[</span><span class="ot">@bradley-terry-model</span><span class="co">]</span> in the set of $J$ choices $i \in <span class="sc">\{</span>1, 2, \dots, J<span class="sc">\}</span>$. Each choice can also have its unique random noise variable representing the unobserved factor, although we can also choose to have all choices' unobserved factors follow the same distribution (e.g. independent and identically distributed, IID). The noise is represented as an extreme value distribution, although we can choose alternatives such as a multivariate Gaussian distribution: $\epsilon \sim \mathcal{N}(0, \Sigma)$. If $\Sigma$ is not a diagonal matrix, we effectively model correlations in the noise across choices, enabling us to avoid the IID assumption. In the case of the extreme value distribution, we model the probability of a user preferring choice $i$, which we denote as $P_i = Z^{-1}\exp(u_{i,j}^*)$ where $Z = \sum_{j = 1}^{J} \exp(u_{i,j}^*)$.</span>
<span id="cb7-100"><a href="#cb7-100"></a></span>
<span id="cb7-101"><a href="#cb7-101"></a><span class="fu">#### Ordered Preferences Model {#ordered-preferences-model .unnumbered}</span></span>
<span id="cb7-102"><a href="#cb7-102"></a></span>
<span id="cb7-103"><a href="#cb7-103"></a>Previous models do not leverage information about ordering of the available options a human can choose from: all choices were treated as independent by the model. The model aims to capture how an individual chooses between them. However, in many cases, we may introduce an inductive bias based on information about the options. For example, in a study for stated preferences, a user may be able to choose from intricately dependent options such as very poor, poor, fair, good, and great. In this case, it can be useful to include this bias in our model to represent a human's decision-making process better. Instead of comparing choices against alternatives, we can focus on a single example and use additional parameters to define classification criteria based on the utility determined by the model. Let us suppose we have a single example with attributes $z_i$, and wish to know which of $J$ predefined options an individual will choose from. We can define $J - 1$ parameters, which act as thresholds on the utility computed by $u_i = u_{i,j}^*$ to classify the predicted choice between these options. For example, if there are 3 predefined options, we can define parameters $a, b \in \mathbb{R}$ such that</span>
<span id="cb7-104"><a href="#cb7-104"></a></span>
<span id="cb7-105"><a href="#cb7-105"></a>$$</span>
<span id="cb7-106"><a href="#cb7-106"></a>y_i =</span>
<span id="cb7-107"><a href="#cb7-107"></a>\begin{cases} </span>
<span id="cb7-108"><a href="#cb7-108"></a>    1 &amp; u &lt; a <span class="sc">\\</span></span>
<span id="cb7-109"><a href="#cb7-109"></a>    2 &amp; a \le u &lt; b <span class="sc">\\</span></span>
<span id="cb7-110"><a href="#cb7-110"></a>    3 &amp; \text{else}</span>
<span id="cb7-111"><a href="#cb7-111"></a>\end{cases}</span>
<span id="cb7-112"><a href="#cb7-112"></a>$$</span>
<span id="cb7-113"><a href="#cb7-113"></a></span>
<span id="cb7-114"><a href="#cb7-114"></a>By assuming the noise distribution to be either logistic or standard normal, we have </span>
<span id="cb7-115"><a href="#cb7-115"></a>$$</span>
<span id="cb7-116"><a href="#cb7-116"></a>\begin{split}</span>
<span id="cb7-117"><a href="#cb7-117"></a>    \mathbb{P}(y_i = 1) &amp; = \mathbb{P}(u &lt; a) = \mathbb{P}(u_{i,j}^* + \epsilon &lt; a) = \psi(a-u_{i,j}^*) <span class="sc">\\</span></span>
<span id="cb7-118"><a href="#cb7-118"></a>    \mathbb{P}(y_i = 2) &amp; = \mathbb{P}(a \le u &lt; b) = \mathbb{P}(a - u_{i,j}^* \le \epsilon &lt; b - u_{i,j}^*) = \psi(b-u_{i,j}^*)  - \psi(u_{i,j}^*-a) <span class="sc">\\</span></span>
<span id="cb7-119"><a href="#cb7-119"></a>    \mathbb{P}(y_i = 3) &amp; = \mathbb{P}(u &gt; b) = \mathbb{P}(u_{i,j}^* + \epsilon &gt; b ) = \mathbb{P}( \epsilon &gt; b - u_{i,j}^*) = \psi(b-u_{i,j}^*)</span>
<span id="cb7-120"><a href="#cb7-120"></a>\end{split}</span>
<span id="cb7-121"><a href="#cb7-121"></a>$$</span>
<span id="cb7-122"><a href="#cb7-122"></a></span>
<span id="cb7-123"><a href="#cb7-123"></a><span class="fu">#### Plackett-Luce Model {#plackett-luce-model .unnumbered}</span></span>
<span id="cb7-124"><a href="#cb7-124"></a></span>
<span id="cb7-125"><a href="#cb7-125"></a>We can model an open-ended ranking of the available options with the Plackett-Luce model, in which we jointly model the full sequence of choice ordering <span class="co">[</span><span class="ot">@plackett_luce</span><span class="co">]</span>. The general form models the joint distribution as the product of conditional probabilities, where each is conditioned on the preceding ranking terms. Given an ordering of $J$ choices $<span class="sc">\{</span>y_1, \dots, y_J<span class="sc">\}</span>$, we factorize the joint probability into conditionals. Each conditional follows the Bradley-Terry model:</span>
<span id="cb7-126"><a href="#cb7-126"></a>$$</span>
<span id="cb7-127"><a href="#cb7-127"></a>\mathbb{P}(y_1, \dots, y_J) = \mathbb{P}(y_1) \cdot \mathbb{P}(y_2 | y_1) \cdot \dots \cdot \mathbb{P}(y_J | y_1, y_2, \dots y_{J - 1}) = \prod_{i = 1}^J \frac{\exp(u_{i,j}^*)}{\sum_{j \ge i} \exp(u_{i,j}^*)}</span>
<span id="cb7-128"><a href="#cb7-128"></a>$$</span>
<span id="cb7-129"><a href="#cb7-129"></a></span>
<span id="cb7-130"><a href="#cb7-130"></a><span class="fu">#### Ideal Point Model {#ideal-point-model .unnumbered}</span></span>
<span id="cb7-131"><a href="#cb7-131"></a></span>
<span id="cb7-132"><a href="#cb7-132"></a>The ideal point model uses distance functions to compute utility for individual-choice pairs <span class="co">[</span><span class="ot">@huber1976ideal</span><span class="co">]</span>. Given vector representation $z_i$ of choice $i$ and a vector $v_n$ representing an individual $n$, we can use a distance function to model a stochastic utility function with the unobserved factors following a specified distribution: $u_{n, i} = \texttt{dist}(z_i, v_n) + \epsilon_{n, i}$. We assume human preferences follow the choice with maximum utility: $y_{n, i} = \mathbb{I}<span class="co">[</span><span class="ot">u_{n, i} &gt; u_{n, j} \ \forall i \ne j</span><span class="co">]</span>$. The intuition is that vectors exist in a shared $n$-dimensional space, and as such we can use geometry to match choices whose representations are closest to that of a given individual. This model can often result in faster learning compared to non-geometric approaches <span class="co">[</span><span class="ot">@ideal_point; @tatli2022distancepreferences</span><span class="co">]</span> when equipped with a distance metric. Certain distance metrics, such as Euclidian distance or inner product, can easily be biased by the scale of vectors. A distance measure such as cosine similarity, which compensates for scale by normalizing the inner product of two vectors by the product of their magnitudes, can mitigate this bias yet may discard valuable information encoded by the length of the vectors. Beyond the distance metric alone, this model places a strong inductive bias that the individual and choice representations all share a common embedding space. In some contexts, this can be a robust bias to add to the model <span class="co">[</span><span class="ot">@idealpoints</span><span class="co">]</span>, but it is a key factor one must take into account before employing such a model, and is a key design choice for modeling.</span>
<span id="cb7-133"><a href="#cb7-133"></a></span>
<span id="cb7-134"><a href="#cb7-134"></a><span class="fu">## Choices Aggregation</span></span>
<span id="cb7-135"><a href="#cb7-135"></a></span>
<span id="cb7-136"><a href="#cb7-136"></a>Game theory provides a mathematical framework for analyzing strategic</span>
<span id="cb7-137"><a href="#cb7-137"></a>interactions among rational agents. These models help in understanding</span>
<span id="cb7-138"><a href="#cb7-138"></a>and predicting human behavior by considering multiple criteria and the</span>
<span id="cb7-139"><a href="#cb7-139"></a>associated trade-offs. They enhance the understanding of preferences</span>
<span id="cb7-140"><a href="#cb7-140"></a>across multiple criteria and allow for richer and more accurate feedback</span>
<span id="cb7-141"><a href="#cb7-141"></a>through structured comparisons. Game-theory framings capture the</span>
<span id="cb7-142"><a href="#cb7-142"></a>complexity of preferences and interactions in decision-making processes</span>
<span id="cb7-143"><a href="#cb7-143"></a><span class="co">[</span><span class="ot">@bhatia2020preference</span><span class="co">]</span>.</span>
<span id="cb7-144"><a href="#cb7-144"></a></span>
<span id="cb7-145"><a href="#cb7-145"></a>The most popular form of preference elicitation involves pairwise</span>
<span id="cb7-146"><a href="#cb7-146"></a>comparisons. Users are asked to choose between two options, such as</span>
<span id="cb7-147"><a href="#cb7-147"></a>product A or product B. This method is used in various applications like</span>
<span id="cb7-148"><a href="#cb7-148"></a>search engines, recommender systems, and interactive robotics. Key</span>
<span id="cb7-149"><a href="#cb7-149"></a>concepts include the Von Neumann Winner and the Blackwell Winner. The</span>
<span id="cb7-150"><a href="#cb7-150"></a>Von Neumann Winner refers to a distribution over objects that beats or</span>
<span id="cb7-151"><a href="#cb7-151"></a>ties every other object in the collection under the expected utility</span>
<span id="cb7-152"><a href="#cb7-152"></a>assumption. The Blackwell Winner generalizes the Von Neumann Winner for</span>
<span id="cb7-153"><a href="#cb7-153"></a>multi-criteria problems using a target set for acceptable payoff vectors</span>
<span id="cb7-154"><a href="#cb7-154"></a><span class="co">[</span><span class="ot">@bhatia2020preference</span><span class="co">]</span>.</span>
<span id="cb7-155"><a href="#cb7-155"></a></span>
<span id="cb7-156"><a href="#cb7-156"></a>Game-theory framings provide a framework for preference learning along</span>
<span id="cb7-157"><a href="#cb7-157"></a>multiple criteria. These models use tools from vector-valued payoffs in</span>
<span id="cb7-158"><a href="#cb7-158"></a>game theory, with Blackwell's approach being a key concept. This</span>
<span id="cb7-159"><a href="#cb7-159"></a>approach allows for a more comprehensive understanding of preferences by</span>
<span id="cb7-160"><a href="#cb7-160"></a>considering multiple criteria simultaneously <span class="co">[</span><span class="ot">@bhatia2020preference</span><span class="co">]</span>.</span>
<span id="cb7-161"><a href="#cb7-161"></a></span>
<span id="cb7-162"><a href="#cb7-162"></a>In game-theory framings, pairwise preferences are modeled as random</span>
<span id="cb7-163"><a href="#cb7-163"></a>variables. Comparisons between objects along different criteria are</span>
<span id="cb7-164"><a href="#cb7-164"></a>captured in a preference tensor $P$. This tensor models the probability</span>
<span id="cb7-165"><a href="#cb7-165"></a>that one object is preferred over another along a specific criterion,</span>
<span id="cb7-166"><a href="#cb7-166"></a>allowing for a detailed understanding of preferences across multiple</span>
<span id="cb7-167"><a href="#cb7-167"></a>dimensions <span class="co">[</span><span class="ot">@bhatia2020preference</span><span class="co">]</span>.</span>
<span id="cb7-168"><a href="#cb7-168"></a></span>
<span id="cb7-169"><a href="#cb7-169"></a>The preference tensor $P$ captures object comparisons along different</span>
<span id="cb7-170"><a href="#cb7-170"></a>criteria. It is defined as:</span>
<span id="cb7-171"><a href="#cb7-171"></a>$$P(i_1, i_2; j) = P(i_1 \succ i_2 \text{ along criterion } j)$$ where</span>
<span id="cb7-172"><a href="#cb7-172"></a>$P(i_2, i_1; j) = 1 - P(i_1, i_2; j)$. These values are aggregated to</span>
<span id="cb7-173"><a href="#cb7-173"></a>form an overall preference matrix $P_{ov}$ <span class="co">[</span><span class="ot">@bhatia2020preference</span><span class="co">]</span>.</span>
<span id="cb7-174"><a href="#cb7-174"></a></span>
<span id="cb7-175"><a href="#cb7-175"></a>The Blackwell Winner is defined using a target set $S$ of acceptable</span>
<span id="cb7-176"><a href="#cb7-176"></a>score vectors. The goal is to find a distribution $\pi^*$ such that</span>
<span id="cb7-177"><a href="#cb7-177"></a>$P(\pi^*, \pi) \in S$ for all $\pi$. This method minimizes the maximum</span>
<span id="cb7-178"><a href="#cb7-178"></a>distance to the target set, providing a robust solution to</span>
<span id="cb7-179"><a href="#cb7-179"></a>multi-criteria preference problems <span class="co">[</span><span class="ot">@bhatia2020preference</span><span class="co">]</span>.</span>
<span id="cb7-180"><a href="#cb7-180"></a></span>
<span id="cb7-181"><a href="#cb7-181"></a>The optimization problem for finding the Blackwell Winner is defined as:</span>
<span id="cb7-182"><a href="#cb7-182"></a>$$\pi(P, S, \|\cdot\|) = \arg \min_{\pi \in \Delta_d} \left<span class="co">[</span><span class="ot"> \max_{\pi' \in \Delta_d} \rho(P(\pi, \pi'), S) \right</span><span class="co">]</span>$$</span>
<span id="cb7-183"><a href="#cb7-183"></a>where $\rho(u, v) = \|u - v\|$. This measures the distance to the target</span>
<span id="cb7-184"><a href="#cb7-184"></a>set, ensuring that the selected distribution is as close as possible to</span>
<span id="cb7-185"><a href="#cb7-185"></a>the ideal preference vector <span class="co">[</span><span class="ot">@bhatia2020preference</span><span class="co">]</span>.</span>
<span id="cb7-186"><a href="#cb7-186"></a></span>
<span id="cb7-187"><a href="#cb7-187"></a></span>
<span id="cb7-188"><a href="#cb7-188"></a><span class="fu">## Parameterization and Learning of Utility Functions {#sec-learning}</span></span>
<span id="cb7-189"><a href="#cb7-189"></a>The attributes representing a choice $z_i$ are crucial in defining the human preference model, as they provide the context for capturing human behavior when choice $i$ is made. </span>
<span id="cb7-190"><a href="#cb7-190"></a></span>
<span id="cb7-191"><a href="#cb7-191"></a>With an understanding of the various techniques we can use to model</span>
<span id="cb7-192"><a href="#cb7-192"></a>human preferences, we can now create robust models which utilize context</span>
<span id="cb7-193"><a href="#cb7-193"></a>attributes about the options an individual has in front of them and</span>
<span id="cb7-194"><a href="#cb7-194"></a>model their choices. However, these models on their own are powerless;</span>
<span id="cb7-195"><a href="#cb7-195"></a>their parameters are initialized randomly and we must fit the models to</span>
<span id="cb7-196"><a href="#cb7-196"></a>the actual human choice data!</span>
<span id="cb7-197"><a href="#cb7-197"></a></span>
<span id="cb7-198"><a href="#cb7-198"></a>Each of the models we have studied contain distinct parameters which aim</span>
<span id="cb7-199"><a href="#cb7-199"></a>to capture human preferences; for example $\beta$ is a parameter vector</span>
<span id="cb7-200"><a href="#cb7-200"></a>containing variables which represent a linear function to compute</span>
<span id="cb7-201"><a href="#cb7-201"></a>utility given a choice's attributes. We can also choose to represent</span>
<span id="cb7-202"><a href="#cb7-202"></a>stochastic utility functions or embedding functions for Ideal Point</span>
<span id="cb7-203"><a href="#cb7-203"></a>Models as neural networks. But how can we compute the optimal values of</span>
<span id="cb7-204"><a href="#cb7-204"></a>these parameters?</span>
<span id="cb7-205"><a href="#cb7-205"></a></span>
<span id="cb7-206"><a href="#cb7-206"></a>In this section, we give the reader an overview of the different methods</span>
<span id="cb7-207"><a href="#cb7-207"></a>available to tune human preference model parameters using given data. We</span>
<span id="cb7-208"><a href="#cb7-208"></a>refer the reader to <span class="co">[</span><span class="ot">@book_estimation_casella; @book_estimation_bock</span><span class="co">]</span></span>
<span id="cb7-209"><a href="#cb7-209"></a>for first-principle derivations of these methods and a deeper dive into</span>
<span id="cb7-210"><a href="#cb7-210"></a>their theoretical properties (convergence, generalization,</span>
<span id="cb7-211"><a href="#cb7-211"></a>data-hungriness, etc.).</span>
<span id="cb7-212"><a href="#cb7-212"></a></span>
<span id="cb7-213"><a href="#cb7-213"></a>A common and powerful approach for computing the parameters of a model</span>
<span id="cb7-214"><a href="#cb7-214"></a>is maximum likelihood estimation</span>
<span id="cb7-215"><a href="#cb7-215"></a><span class="co">[</span><span class="ot">@book_estimation_casella; @book_estimation_bock</span><span class="co">]</span>. The likelihood of a</span>
<span id="cb7-216"><a href="#cb7-216"></a>model is the probability of the observed data given the model</span>
<span id="cb7-217"><a href="#cb7-217"></a>parameters; intuitively we wish to maximize this likelihood, as that</span>
<span id="cb7-218"><a href="#cb7-218"></a>would mean that our model associates observed human preferences in the</span>
<span id="cb7-219"><a href="#cb7-219"></a>data with high probability. We can formally define the likelihood for a</span>
<span id="cb7-220"><a href="#cb7-220"></a>model with parameters $\beta$ and a given data point $(z_i, y_i)$ as:</span>
<span id="cb7-221"><a href="#cb7-221"></a>$$\mathcal{L}(z_i, y_i; \beta) = \mathbb{P}(y = y_i | z_i; \beta)$$</span>
<span id="cb7-222"><a href="#cb7-222"></a></span>
<span id="cb7-223"><a href="#cb7-223"></a>Assuming our data is independent and identically distributed (iid), the</span>
<span id="cb7-224"><a href="#cb7-224"></a>likelihood over the entire dataset is the joint probability of all</span>
<span id="cb7-225"><a href="#cb7-225"></a>observed data as defined by the model:</span>
<span id="cb7-226"><a href="#cb7-226"></a>$$\mathcal{L}(z, Y; \beta) = \prod_{i = 1}^J \mathbb{P}(y = y_i | z_i; \beta)$$</span>
<span id="cb7-227"><a href="#cb7-227"></a></span>
<span id="cb7-228"><a href="#cb7-228"></a>In our very first example of binary choice with logistic noise, this was</span>
<span id="cb7-229"><a href="#cb7-229"></a>simply the model's probability of the observed preference value:</span>
<span id="cb7-230"><a href="#cb7-230"></a>$$\mathcal{L}(z_i, y_i; \beta) = \frac{1}{1 + \exp^{-u_{i,j}^*}}$$</span>
<span id="cb7-231"><a href="#cb7-231"></a></span>
<span id="cb7-232"><a href="#cb7-232"></a>In the same case with noise following a standard normal distribution,</span>
<span id="cb7-233"><a href="#cb7-233"></a>this took the form: $$\mathcal{L}(z_i, y_i; \beta) = \Phi(u_{i,j}^*)$$</span>
<span id="cb7-234"><a href="#cb7-234"></a></span>
<span id="cb7-235"><a href="#cb7-235"></a>Fortunately, in these cases, there are straightforward methods for</span>
<span id="cb7-236"><a href="#cb7-236"></a>parameter estimation: logistic regression and probit regression (binary</span>
<span id="cb7-237"><a href="#cb7-237"></a>or multinomial, depending on the model), respectively. We can use</span>
<span id="cb7-238"><a href="#cb7-238"></a>ordinal regression to estimate the model's parameters for our ordered</span>
<span id="cb7-239"><a href="#cb7-239"></a>preference model.</span>
<span id="cb7-240"><a href="#cb7-240"></a></span>
<span id="cb7-241"><a href="#cb7-241"></a>Generally, the objective function commonly found in parameter learning</span>
<span id="cb7-242"><a href="#cb7-242"></a>can be optimized with stochastic gradient descent (SGD)</span>
<span id="cb7-243"><a href="#cb7-243"></a><span class="co">[</span><span class="ot">@gradient_descent</span><span class="co">]</span>. We can define an objective function as the</span>
<span id="cb7-244"><a href="#cb7-244"></a>likelihood to maximize this objective. Since SGD minimizes a given</span>
<span id="cb7-245"><a href="#cb7-245"></a>objective, we must negate the likelihood, which ensures that a converged</span>
<span id="cb7-246"><a href="#cb7-246"></a>solution maximizes the likelihood. SGD operates by computing the</span>
<span id="cb7-247"><a href="#cb7-247"></a>gradient of the objective with respect to the parameters of the model,</span>
<span id="cb7-248"><a href="#cb7-248"></a>which provides a signal of the direction in which the parameters must</span>
<span id="cb7-249"><a href="#cb7-249"></a>move to *maximize* the objective. Then, SGD makes an update step by</span>
<span id="cb7-250"><a href="#cb7-250"></a>subtracting this gradient from the parameters (most often with a scale</span>
<span id="cb7-251"><a href="#cb7-251"></a>factor called a *learning rate*), to move the parameters in a direction</span>
<span id="cb7-252"><a href="#cb7-252"></a>which *minimizes* the objective. When the objective is the negative</span>
<span id="cb7-253"><a href="#cb7-253"></a>likelihood (or sometimes negative log-likelihood for convenience or</span>
<span id="cb7-254"><a href="#cb7-254"></a>tractability), the result is an increase in the overall likelihood.</span>
<span id="cb7-255"><a href="#cb7-255"></a></span>
<span id="cb7-256"><a href="#cb7-256"></a>In the case of logistic and Gaussian models, SGD may yield a challenging</span>
<span id="cb7-257"><a href="#cb7-257"></a>optimization problem as its stochasticity can lead to noisy updates, for</span>
<span id="cb7-258"><a href="#cb7-258"></a>example, if certain examples or batches of examples are biased.</span>
<span id="cb7-259"><a href="#cb7-259"></a>Mitigations include batched SGD, in which multiple samples are randomly</span>
<span id="cb7-260"><a href="#cb7-260"></a>sampled from the dataset at each iteration, learning rates, which reduce</span>
<span id="cb7-261"><a href="#cb7-261"></a>the impact of noisy gradient updates, and momentum and higher-order</span>
<span id="cb7-262"><a href="#cb7-262"></a>optimizers which reduce noise by using movering averages of gradients or</span>
<span id="cb7-263"><a href="#cb7-263"></a>provide better estimates of the best direction in which to update the</span>
<span id="cb7-264"><a href="#cb7-264"></a>gradients. Some models, such as those that use neural networks, may, in</span>
<span id="cb7-265"><a href="#cb7-265"></a>fact, be intractable to estimate without a method such as SGD (or its</span>
<span id="cb7-266"><a href="#cb7-266"></a>momentum-based derivatives). For example, neural networks with many</span>
<span id="cb7-267"><a href="#cb7-267"></a>layers, non-linearities, and parameters can only be efficiently computed</span>
<span id="cb7-268"><a href="#cb7-268"></a>with gradient-based methods.</span>
<span id="cb7-269"><a href="#cb7-269"></a></span>
<span id="cb7-270"><a href="#cb7-270"></a><span class="fu">### Reward Learning with Large Language Models</span></span>
<span id="cb7-271"><a href="#cb7-271"></a></span>
<span id="cb7-272"><a href="#cb7-272"></a>Taking a step away from explicitly modeling human bias and preference,</span>
<span id="cb7-273"><a href="#cb7-273"></a>we consider applying a deep learning approach to state-of-the-art</span>
<span id="cb7-274"><a href="#cb7-274"></a>language models. We begin by introducing the concepts of *foundation</span>
<span id="cb7-275"><a href="#cb7-275"></a>models* and *alignment*. A foundation model <span class="co">[</span><span class="ot">@Liang2021</span><span class="co">]</span> in machine</span>
<span id="cb7-276"><a href="#cb7-276"></a>learning typically refers to a large and pre-trained neural network</span>
<span id="cb7-277"><a href="#cb7-277"></a>model that serves as the basis for various downstream tasks. In natural</span>
<span id="cb7-278"><a href="#cb7-278"></a>language processing, models like GPT-3, Llama, and BERT are considered</span>
<span id="cb7-279"><a href="#cb7-279"></a>foundation models. They are pre-trained on a massive corpus of text</span>
<span id="cb7-280"><a href="#cb7-280"></a>data, learning to understand language and context, and are capable of</span>
<span id="cb7-281"><a href="#cb7-281"></a>various language-related tasks such as text classification, language</span>
<span id="cb7-282"><a href="#cb7-282"></a>generation, and question answering. Foundation models are important</span>
<span id="cb7-283"><a href="#cb7-283"></a>because they alleviate the need to train massive neural networks from</span>
<span id="cb7-284"><a href="#cb7-284"></a>scratch, a compute and data expensive endeavor. However, a raw</span>
<span id="cb7-285"><a href="#cb7-285"></a>foundation model, trained on a pretraining objective such as a language</span>
<span id="cb7-286"><a href="#cb7-286"></a>modeling objective, is not useful on its own. It must be aligned to</span>
<span id="cb7-287"><a href="#cb7-287"></a>respond correctly based on human preferences.</span>
<span id="cb7-288"><a href="#cb7-288"></a></span>
<span id="cb7-289"><a href="#cb7-289"></a>In short, alignment for foundation models is the process by which model</span>
<span id="cb7-290"><a href="#cb7-290"></a>behavior is aligned with human values, ethics, and societal norms. Large</span>
<span id="cb7-291"><a href="#cb7-291"></a>Language Models (LLMs) are a foundation model for natural language</span>
<span id="cb7-292"><a href="#cb7-292"></a>processing. They are trained using a next-word prediction objective,</span>
<span id="cb7-293"><a href="#cb7-293"></a>allowing them to generate coherent language. A simple way to align a</span>
<span id="cb7-294"><a href="#cb7-294"></a>Large Language Model is to train it to follow instructions in a</span>
<span id="cb7-295"><a href="#cb7-295"></a>supervised way, using instruction-response pairs curated by hand.</span>
<span id="cb7-296"><a href="#cb7-296"></a>However, this limits the upper limit of LLM performance to the</span>
<span id="cb7-297"><a href="#cb7-297"></a>performance of the annotators' writing abilities. This type of</span>
<span id="cb7-298"><a href="#cb7-298"></a>annotation is also expensive.</span>
<span id="cb7-299"><a href="#cb7-299"></a></span>
<span id="cb7-300"><a href="#cb7-300"></a>An alternative, more promising approach is to train LLMs using</span>
<span id="cb7-301"><a href="#cb7-301"></a>reinforcement learning, potentially enabling them to surpass human-level</span>
<span id="cb7-302"><a href="#cb7-302"></a>performance. The main challenge with this method lies in defining an</span>
<span id="cb7-303"><a href="#cb7-303"></a>explicit reward function for generating free-form text. To address this,</span>
<span id="cb7-304"><a href="#cb7-304"></a>a reward model (RM) can be trained based on human preferences, providing</span>
<span id="cb7-305"><a href="#cb7-305"></a>a mechanism to score the quality of the generated text. This approach,</span>
<span id="cb7-306"><a href="#cb7-306"></a>known as Reinforcement Learning from Human Feedback (RLHF), leverages</span>
<span id="cb7-307"><a href="#cb7-307"></a>human feedback to guide model training, allowing LLMs to better align</span>
<span id="cb7-308"><a href="#cb7-308"></a>with human expectations while continuously improving performance.</span>
<span id="cb7-309"><a href="#cb7-309"></a></span>
<span id="cb7-310"><a href="#cb7-310"></a><span class="al">![Overall architecture of a reward model based on LLM](Figures/arch.png)</span>{#fig-rm-arch}</span>
<span id="cb7-311"><a href="#cb7-311"></a></span>
<span id="cb7-312"><a href="#cb7-312"></a>The Llama2 reward model <span class="co">[</span><span class="ot">@2307.09288</span><span class="co">]</span> is initialized from the pretrained</span>
<span id="cb7-313"><a href="#cb7-313"></a>Llama2 LLM. In the LLM, the last layer is a mapping</span>
<span id="cb7-314"><a href="#cb7-314"></a>$L: \mathbb{R}^D \rightarrow \mathbb{R}^V$, where $D$ is the embedding dimension</span>
<span id="cb7-315"><a href="#cb7-315"></a>from the transformer decoder stack and $V$ is the vocabulary size. To</span>
<span id="cb7-316"><a href="#cb7-316"></a>get the RM, we replace that last layer with a randomly initialized</span>
<span id="cb7-317"><a href="#cb7-317"></a>scalar head that maps $L: \mathbb{R}^D \rightarrow \mathbb{R}^1$. It's important to</span>
<span id="cb7-318"><a href="#cb7-318"></a>initialize the RM from the LLM it's meant to evaluate. This is because:</span>
<span id="cb7-319"><a href="#cb7-319"></a></span>
<span id="cb7-320"><a href="#cb7-320"></a><span class="ss">1.  </span>The RM will have the same "knowledge" as the LLM. This is</span>
<span id="cb7-321"><a href="#cb7-321"></a>    particularly useful if evaluating things like "does the LLM know</span>
<span id="cb7-322"><a href="#cb7-322"></a>    when it doesn't know?". However, in cases where the RM is simply</span>
<span id="cb7-323"><a href="#cb7-323"></a>    evaluating helpfulness or factuality, it may be useful to have the</span>
<span id="cb7-324"><a href="#cb7-324"></a>    RM know more.</span>
<span id="cb7-325"><a href="#cb7-325"></a></span>
<span id="cb7-326"><a href="#cb7-326"></a><span class="ss">2.  </span>The RM is on distribution for the LLM - it is initialized in a way</span>
<span id="cb7-327"><a href="#cb7-327"></a>    where it semantically understands the LLM's outputs.</span>
<span id="cb7-328"><a href="#cb7-328"></a></span>
<span id="cb7-329"><a href="#cb7-329"></a>An RM is trained with paired preferences, following the format:</span>
<span id="cb7-330"><a href="#cb7-330"></a>$$\begin{aligned}</span>
<span id="cb7-331"><a href="#cb7-331"></a>    \langle prompt<span class="sc">\_</span>history, response<span class="sc">\_</span>accepted, response<span class="sc">\_</span>rejected \rangle</span>
<span id="cb7-332"><a href="#cb7-332"></a>\end{aligned}$$ Prompt_history is a multiturn history of user prompts</span>
<span id="cb7-333"><a href="#cb7-333"></a>and model generations, response_accepted is the preferred final model</span>
<span id="cb7-334"><a href="#cb7-334"></a>generation by an annotator, and response_rejected is the unpreferred</span>
<span id="cb7-335"><a href="#cb7-335"></a>response. The RM is trained with a binary ranking loss with an optional</span>
<span id="cb7-336"><a href="#cb7-336"></a>margin term m(r), shown in equation (7). There is also often a small</span>
<span id="cb7-337"><a href="#cb7-337"></a>regularization term added to center the score distribution on 0.</span>
<span id="cb7-338"><a href="#cb7-338"></a>$$\mathcal{L}_{\text{ranking}} = -\log(\sigma(r_\theta(x,y_c) - r_\theta(x,y_r) - m(r)))$$</span>
<span id="cb7-339"><a href="#cb7-339"></a>The margin term increases the distance in scores specifically for</span>
<span id="cb7-340"><a href="#cb7-340"></a>preference pairs annotators rate as easier to separate.</span>
<span id="cb7-341"><a href="#cb7-341"></a></span>
<span id="cb7-342"><a href="#cb7-342"></a>::: {#tbl-margin_nums}</span>
<span id="cb7-343"><a href="#cb7-343"></a>  -------------- --------------- -------- ---------- -----------------</span>
<span id="cb7-344"><a href="#cb7-344"></a><span class="in">                  Significantly   Better   Slightly     Negligibly</span></span>
<span id="cb7-345"><a href="#cb7-345"></a><span class="in">                     Better                 Better    Better / Unsure</span></span>
<span id="cb7-346"><a href="#cb7-346"></a>  Margin Small          1          2/3       1/3             0</span>
<span id="cb7-347"><a href="#cb7-347"></a>  Margin Large          3           2         1              0</span>
<span id="cb7-348"><a href="#cb7-348"></a>  -------------- --------------- -------- ---------- -----------------</span>
<span id="cb7-349"><a href="#cb7-349"></a></span>
<span id="cb7-350"><a href="#cb7-350"></a>  : Two variants of preference rating based margin with different magnitude.</span>
<span id="cb7-351"><a href="#cb7-351"></a>:::</span>
<span id="cb7-352"><a href="#cb7-352"></a></span>
<span id="cb7-353"><a href="#cb7-353"></a>![Reward model score distribution shift caused by incorporating</span>
<span id="cb7-354"><a href="#cb7-354"></a>preference rating based margin in ranking loss. With the margin term,</span>
<span id="cb7-355"><a href="#cb7-355"></a>we observe a binary split pattern in reward distribution, especially for</span>
<span id="cb7-356"><a href="#cb7-356"></a>a larger margin.](Figures/margin-2.png){#fig-margin-2</span>
<span id="cb7-357"><a href="#cb7-357"></a>width="<span class="sc">\\</span>linewidth"}</span>
<span id="cb7-358"><a href="#cb7-358"></a></span>
<span id="cb7-359"><a href="#cb7-359"></a>It may seem confusing how the margins were chosen. It's primarily</span>
<span id="cb7-360"><a href="#cb7-360"></a>because the sigmoid function, which is used to normalize the raw reward</span>
<span id="cb7-361"><a href="#cb7-361"></a>model score, flattens out beyond the range of $<span class="co">[</span><span class="ot">-4, 4</span><span class="co">]</span>$. Thus, the</span>
<span id="cb7-362"><a href="#cb7-362"></a>maximum possible margin is eight.</span>
<span id="cb7-363"><a href="#cb7-363"></a></span>
<span id="cb7-364"><a href="#cb7-364"></a>When training or using a reward model, watching for the following is</span>
<span id="cb7-365"><a href="#cb7-365"></a>important:</span>
<span id="cb7-366"><a href="#cb7-366"></a></span>
<span id="cb7-367"><a href="#cb7-367"></a><span class="ss">1.  </span>**LLM Distribution Shift**: With each finetune of the LLM, the RM</span>
<span id="cb7-368"><a href="#cb7-368"></a>    should be updated through a collection of fresh human preferences</span>
<span id="cb7-369"><a href="#cb7-369"></a>    using generations from the new LLM. This ensures that the RM stays</span>
<span id="cb7-370"><a href="#cb7-370"></a>    aligned with the current distribution of the LLM and avoids drifting</span>
<span id="cb7-371"><a href="#cb7-371"></a>    off-distribution.</span>
<span id="cb7-372"><a href="#cb7-372"></a></span>
<span id="cb7-373"><a href="#cb7-373"></a><span class="ss">2.  </span>**RM and LLM are coupled**: An RM is generally optimized to</span>
<span id="cb7-374"><a href="#cb7-374"></a>    distinguish human preferences more efficiently within the specific</span>
<span id="cb7-375"><a href="#cb7-375"></a>    distribution of the LLM to be optimized. However, this</span>
<span id="cb7-376"><a href="#cb7-376"></a>    specialization poses a challenge: such an RM will underperform when</span>
<span id="cb7-377"><a href="#cb7-377"></a>    dealing with generations not aligned with this specific LLM</span>
<span id="cb7-378"><a href="#cb7-378"></a>    distribution, such as generations from a completely different LLM.</span>
<span id="cb7-379"><a href="#cb7-379"></a></span>
<span id="cb7-380"><a href="#cb7-380"></a><span class="ss">3.  </span>**Training Sensitivities of RMs**: Training RMs can be unstable and</span>
<span id="cb7-381"><a href="#cb7-381"></a>    prone to overfitting, especially with multiple training epochs. It's</span>
<span id="cb7-382"><a href="#cb7-382"></a>    generally advisable to limit the number of epochs during RM training</span>
<span id="cb7-383"><a href="#cb7-383"></a>    to avoid this issue.</span>
<span id="cb7-384"><a href="#cb7-384"></a></span>
<span id="cb7-385"><a href="#cb7-385"></a>The industry has centered around optimizing for two primary qualities in</span>
<span id="cb7-386"><a href="#cb7-386"></a>LLMs: helpfulness and harmlessness (safety). There are also other axes</span>
<span id="cb7-387"><a href="#cb7-387"></a>such as factuality, reasoning, tool use, code, multilingual, and more,</span>
<span id="cb7-388"><a href="#cb7-388"></a>but these are out of scope for us. In the Llama2 paper, preference data</span>
<span id="cb7-389"><a href="#cb7-389"></a>was collected from humans for each quality, with separate guidelines.</span>
<span id="cb7-390"><a href="#cb7-390"></a>This presents a challenge for co-optimizing the final LLM towards both</span>
<span id="cb7-391"><a href="#cb7-391"></a>goals.</span>
<span id="cb7-392"><a href="#cb7-392"></a></span>
<span id="cb7-393"><a href="#cb7-393"></a>Two main approaches can be taken for Reinforcement Learning from Human</span>
<span id="cb7-394"><a href="#cb7-394"></a>Feedback (RLHF) in this context:</span>
<span id="cb7-395"><a href="#cb7-395"></a></span>
<span id="cb7-396"><a href="#cb7-396"></a><span class="ss">1.  </span>Train a unified reward model that integrates both datasets.</span>
<span id="cb7-397"><a href="#cb7-397"></a></span>
<span id="cb7-398"><a href="#cb7-398"></a><span class="ss">2.  </span>Train two separate reward models, one for each quality, and optimize</span>
<span id="cb7-399"><a href="#cb7-399"></a>    the LLM toward both.</span>
<span id="cb7-400"><a href="#cb7-400"></a></span>
<span id="cb7-401"><a href="#cb7-401"></a>Option 1 is difficult because of the tension between helpfulness and</span>
<span id="cb7-402"><a href="#cb7-402"></a>harmlessness. They trade off against each other, confusing an RM trained</span>
<span id="cb7-403"><a href="#cb7-403"></a>on both. The chosen solution was option 2, where two RMs are used to</span>
<span id="cb7-404"><a href="#cb7-404"></a>train the LLM in a piecewise fashion. The helpfulness RM is used as the</span>
<span id="cb7-405"><a href="#cb7-405"></a>primary optimization term, while the harmlessness RM acts as a penalty</span>
<span id="cb7-406"><a href="#cb7-406"></a>term, driving the behavior of the LLM away from unsafe territory only</span>
<span id="cb7-407"><a href="#cb7-407"></a>when the LLM veers beyond a certain threshold. This is formalized as</span>
<span id="cb7-408"><a href="#cb7-408"></a>follows, where $R_s$, $R_h$, and $R_c$ are the safety, helpfulness, and</span>
<span id="cb7-409"><a href="#cb7-409"></a>combined reward, respectively. $g$ and $p$ are the model generation and</span>
<span id="cb7-410"><a href="#cb7-410"></a>the user prompt: $$\begin{aligned}</span>
<span id="cb7-411"><a href="#cb7-411"></a>    R_c(g \mid p) =</span>
<span id="cb7-412"><a href="#cb7-412"></a>    \begin{cases}</span>
<span id="cb7-413"><a href="#cb7-413"></a>        R_s(g \mid p) &amp; \text{if } \text{is<span class="sc">\_</span>safety}(p) \text{ or } R_s(g \mid p) &lt; 0.15 <span class="sc">\\</span></span>
<span id="cb7-414"><a href="#cb7-414"></a>        R_h(g \mid p) &amp; \text{otherwise}</span>
<span id="cb7-415"><a href="#cb7-415"></a>    \end{cases}</span>
<span id="cb7-416"><a href="#cb7-416"></a>\end{aligned}$$</span>
<span id="cb7-417"><a href="#cb7-417"></a></span>
<span id="cb7-418"><a href="#cb7-418"></a>There are several open issues with reward models alluded to in the</span>
<span id="cb7-419"><a href="#cb7-419"></a>paper. For example, how best to collect human feedback? Training</span>
<span id="cb7-420"><a href="#cb7-420"></a>annotators and making sure they do the correct thing is hard. What</span>
<span id="cb7-421"><a href="#cb7-421"></a>should the guidelines be? Another question is whether RMs can be made</span>
<span id="cb7-422"><a href="#cb7-422"></a>robust to adversarial prompts. Last but not least, do RMs have</span>
<span id="cb7-423"><a href="#cb7-423"></a>well-calibrated scores? This matters for RLHF - pure preference accuracy</span>
<span id="cb7-424"><a href="#cb7-424"></a>isn't enough.</span>
<span id="cb7-425"><a href="#cb7-425"></a></span>
<span id="cb7-426"><a href="#cb7-426"></a><span class="fu">### Reward Learning in Robotics</span></span>
<span id="cb7-427"><a href="#cb7-427"></a></span>
<span id="cb7-428"><a href="#cb7-428"></a>To help set up our basic reward learning problem, consider a user and a</span>
<span id="cb7-429"><a href="#cb7-429"></a>robot. The user's preferences or goals can be represented by an internal</span>
<span id="cb7-430"><a href="#cb7-430"></a>reward function, R($\xi$), which the robot needs to learn. Since the</span>
<span id="cb7-431"><a href="#cb7-431"></a>reward function isn't explicit, there are a variety of ways that the</span>
<span id="cb7-432"><a href="#cb7-432"></a>robot can learn this reward function, which we will discuss in the next</span>
<span id="cb7-433"><a href="#cb7-433"></a>section. An example method of learning a reward function from human data</span>
<span id="cb7-434"><a href="#cb7-434"></a>is using pairwise comparison. Consider the robot example from section</span>
<span id="cb7-435"><a href="#cb7-435"></a>one, but now, the robot shows the human two possible trajectories</span>
<span id="cb7-436"><a href="#cb7-436"></a>$\xi_A$ and $\xi_B$ as depicted in the diagram below.</span>
<span id="cb7-437"><a href="#cb7-437"></a></span>
<span id="cb7-438"><a href="#cb7-438"></a>![Two different trajectories taken by a robot to prompt</span>
<span id="cb7-439"><a href="#cb7-439"></a>user ranking.](Figures/robots.png){#fig-reward-robot-1 width="70%"}</span>
<span id="cb7-440"><a href="#cb7-440"></a></span>
<span id="cb7-441"><a href="#cb7-441"></a>The user is show both the trajectories above and asked to rank which one</span>
<span id="cb7-442"><a href="#cb7-442"></a>is better. Based on iterations of multiple trajectories and ranking, the</span>
<span id="cb7-443"><a href="#cb7-443"></a>robot is able to learn the user's internal reward function. There quite</span>
<span id="cb7-444"><a href="#cb7-444"></a>a lot of ways that models can learn a reward function from human data.</span>
<span id="cb7-445"><a href="#cb7-445"></a>Here's a list <span class="co">[</span><span class="ot">@myers2021learning</span><span class="co">]</span> of some of them:</span>
<span id="cb7-446"><a href="#cb7-446"></a></span>
<span id="cb7-447"><a href="#cb7-447"></a><span class="ss">1.  </span>Pairwise comparison: This is the method that we saw illustrated in</span>
<span id="cb7-448"><a href="#cb7-448"></a>    the previous example. The robot is able to learn based on a</span>
<span id="cb7-449"><a href="#cb7-449"></a>    comparison ranking provided by the user.</span>
<span id="cb7-450"><a href="#cb7-450"></a></span>
<span id="cb7-451"><a href="#cb7-451"></a><span class="ss">2.  </span>Expert demonstrations: Experts perform the task and the robot learns</span>
<span id="cb7-452"><a href="#cb7-452"></a>    the optimal reward function from these demonstrations.</span>
<span id="cb7-453"><a href="#cb7-453"></a></span>
<span id="cb7-454"><a href="#cb7-454"></a><span class="ss">3.  </span>Sub-optimal demonstrations: The robot is provided with</span>
<span id="cb7-455"><a href="#cb7-455"></a>    demonstrations that are not quite as good as the expert</span>
<span id="cb7-456"><a href="#cb7-456"></a>    demonstrations but it is still able to learn a noisy reward function</span>
<span id="cb7-457"><a href="#cb7-457"></a>    from the demonstrations.</span>
<span id="cb7-458"><a href="#cb7-458"></a></span>
<span id="cb7-459"><a href="#cb7-459"></a><span class="ss">4.  </span>Physical Corrections: While the robot is performing the task, at</span>
<span id="cb7-460"><a href="#cb7-460"></a>    each point in its trajectory (or at an arbitrary point in its</span>
<span id="cb7-461"><a href="#cb7-461"></a>    trajectory) its arm is corrected to a more suitable position. Based</span>
<span id="cb7-462"><a href="#cb7-462"></a>    on these corrections, the robot is able to learn the reward</span>
<span id="cb7-463"><a href="#cb7-463"></a>    function.</span>
<span id="cb7-464"><a href="#cb7-464"></a></span>
<span id="cb7-465"><a href="#cb7-465"></a><span class="ss">5.  </span>Ranking: This method is similar to pairwise comparison but involves</span>
<span id="cb7-466"><a href="#cb7-466"></a>    more trajectories than 2. All the trajectories may have subtle</span>
<span id="cb7-467"><a href="#cb7-467"></a>    differences from each other, but these differences help provide</span>
<span id="cb7-468"><a href="#cb7-468"></a>    insight to the model.</span>
<span id="cb7-469"><a href="#cb7-469"></a></span>
<span id="cb7-470"><a href="#cb7-470"></a><span class="ss">6.  </span>Trajectory Assessment: Given a single trajectory, the user rates how</span>
<span id="cb7-471"><a href="#cb7-471"></a>    close it is to optimal, typically using a ranking scale.</span>
<span id="cb7-472"><a href="#cb7-472"></a></span>
<span id="cb7-473"><a href="#cb7-473"></a>    Each of these methods allows the robot to refine its understanding</span>
<span id="cb7-474"><a href="#cb7-474"></a>    of the user's reward function, but their effectiveness can vary</span>
<span id="cb7-475"><a href="#cb7-475"></a>    depending on the application. For instance, expert demonstrations</span>
<span id="cb7-476"><a href="#cb7-476"></a>    tend to produce more reliable results but may not always be feasible</span>
<span id="cb7-477"><a href="#cb7-477"></a>    in everyday tasks. Pairwise comparison and ranking methods offer</span>
<span id="cb7-478"><a href="#cb7-478"></a>    more flexibility but might require a higher number of iterations.</span>
<span id="cb7-479"><a href="#cb7-479"></a></span>
<span id="cb7-480"><a href="#cb7-480"></a><span class="fu">### Reward Learning with Meta Learning</span></span>
<span id="cb7-481"><a href="#cb7-481"></a></span>
<span id="cb7-482"><a href="#cb7-482"></a>Learning a reward function from human preferences is an intricate and</span>
<span id="cb7-483"><a href="#cb7-483"></a>complicated task. At its core, this task is about designing algorithms</span>
<span id="cb7-484"><a href="#cb7-484"></a>that can capture what humans value based on their elicited preferences.</span>
<span id="cb7-485"><a href="#cb7-485"></a>However, due to the nuanced and multifaceted nature of human desires,</span>
<span id="cb7-486"><a href="#cb7-486"></a>learning reward functions from human can be a difficult task. Therefore,</span>
<span id="cb7-487"><a href="#cb7-487"></a>meta-learning rewards may be considered to facilitate the reward</span>
<span id="cb7-488"><a href="#cb7-488"></a>learning processes. Meta-learning, often referred to as "learning to</span>
<span id="cb7-489"><a href="#cb7-489"></a>learn," aims to design models that can adapt to new tasks with minimal</span>
<span id="cb7-490"><a href="#cb7-490"></a>additional efforts. We discuss paper <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span> in @sec-few-shot showing how meta-learning can be leveraged for</span>
<span id="cb7-491"><a href="#cb7-491"></a>few-shot preference learning, where a system can quickly adapt to a new</span>
<span id="cb7-492"><a href="#cb7-492"></a>task after only a few queries to pairwise preferences from human.</span>
<span id="cb7-493"><a href="#cb7-493"></a></span>
<span id="cb7-494"><a href="#cb7-494"></a>Moving beyond the concept of learning from pairwise preferences, in @sec-watch we discuss a different approach where</span>
<span id="cb7-495"><a href="#cb7-495"></a>meta-learning intersects with both demonstrations and</span>
<span id="cb7-496"><a href="#cb7-496"></a>rewards <span class="co">[</span><span class="ot">@zhou2019watch</span><span class="co">]</span>. This paper considers the use of both</span>
<span id="cb7-497"><a href="#cb7-497"></a>demonstrations and rewards elicited from human that guide the learning</span>
<span id="cb7-498"><a href="#cb7-498"></a>process.</span>
<span id="cb7-499"><a href="#cb7-499"></a></span>
<span id="cb7-500"><a href="#cb7-500"></a>In the regular learning setting, a model is fitted to a dataset with</span>
<span id="cb7-501"><a href="#cb7-501"></a>certain learning algorithm. The learning algorithm, for example, can be</span>
<span id="cb7-502"><a href="#cb7-502"></a>the minimization of a loss function. To formulate the "regular" learning</span>
<span id="cb7-503"><a href="#cb7-503"></a>procedure, let's denote the training dataset as $D$, and the test</span>
<span id="cb7-504"><a href="#cb7-504"></a>dataset as $S$. Given a model parameterized by $\theta$; training loss</span>
<span id="cb7-505"><a href="#cb7-505"></a>function $L(\theta, D)$; and test loss function $L(\theta, S)$, we can</span>
<span id="cb7-506"><a href="#cb7-506"></a>formulate a process of "regular" machine learning process as</span>
<span id="cb7-507"><a href="#cb7-507"></a>$$\begin{aligned}</span>
<span id="cb7-508"><a href="#cb7-508"></a>    \theta^\star = \arg\min_\theta\quad L(\theta, D).</span>
<span id="cb7-509"><a href="#cb7-509"></a>\end{aligned}$$ Note that the minimization of the training loss function</span>
<span id="cb7-510"><a href="#cb7-510"></a>is essentially *one* possible learning algorithm. For example, instead</span>
<span id="cb7-511"><a href="#cb7-511"></a>of minimizing the loss function, one may do gradient descent with model</span>
<span id="cb7-512"><a href="#cb7-512"></a>regularization on the loss function, where the final solution may not be</span>
<span id="cb7-513"><a href="#cb7-513"></a>the one that actually minimizes the loss function. As a result, we may</span>
<span id="cb7-514"><a href="#cb7-514"></a>want to be more general and more abstract for the moment, and denote the</span>
<span id="cb7-515"><a href="#cb7-515"></a>learning algorithm as $\mathcal{A}$. Thus, we can write</span>
<span id="cb7-516"><a href="#cb7-516"></a>$$\begin{aligned}</span>
<span id="cb7-517"><a href="#cb7-517"></a>    \theta^\star = \mathcal{A}(D),</span>
<span id="cb7-518"><a href="#cb7-518"></a>\end{aligned}$$ i.e., the learning algorithm $\mathcal{A}$ takes in a</span>
<span id="cb7-519"><a href="#cb7-519"></a>training dataset and outputs a model parameter $\theta^\star$. Then, the</span>
<span id="cb7-520"><a href="#cb7-520"></a>performance of the model is evaluated by the test loss</span>
<span id="cb7-521"><a href="#cb7-521"></a>$L(\mathcal{A}(D), S)$. As we can see, in the regime of "regular"</span>
<span id="cb7-522"><a href="#cb7-522"></a>learning, the learning algorithm $\mathcal{A}$ is pre-defined and fixed.</span>
<span id="cb7-523"><a href="#cb7-523"></a></span>
<span id="cb7-524"><a href="#cb7-524"></a>Meta-learning, or learning-to-learn, essentially asks the question of</span>
<span id="cb7-525"><a href="#cb7-525"></a>whether one can *learn* the learning algorithm $\mathcal{A}$ from prior</span>
<span id="cb7-526"><a href="#cb7-526"></a>tasks, such that the modal can adapt to a new task more</span>
<span id="cb7-527"><a href="#cb7-527"></a>quickly/proficiently. For example, different human languages share</span>
<span id="cb7-528"><a href="#cb7-528"></a>similar ideas, and therefore a human expert who has learned many</span>
<span id="cb7-529"><a href="#cb7-529"></a>languages should be able to learn a new language easier than an average</span>
<span id="cb7-530"><a href="#cb7-530"></a>person. In other words, the human expert should have learned how to</span>
<span id="cb7-531"><a href="#cb7-531"></a>learn new languages more quickly based on their past experiences on</span>
<span id="cb7-532"><a href="#cb7-532"></a>learning languages.</span>
<span id="cb7-533"><a href="#cb7-533"></a></span>
<span id="cb7-534"><a href="#cb7-534"></a>To mathematically formulate meta-learning, we consider a family of</span>
<span id="cb7-535"><a href="#cb7-535"></a>learning algorithms $\mathcal{A}_\omega$ parameterized by $\omega$. The</span>
<span id="cb7-536"><a href="#cb7-536"></a>"prior" tasks are represented by a set of meta-training datasets</span>
<span id="cb7-537"><a href="#cb7-537"></a>$<span class="sc">\{</span>(D_i, S_i)<span class="sc">\}</span>_{i=1}^N$ consists of $N$ pairs of training dataset $D_i$</span>
<span id="cb7-538"><a href="#cb7-538"></a>and test dataset $S_i$. As we noted before, a learning algorithm</span>
<span id="cb7-539"><a href="#cb7-539"></a>$\mathcal{A}_\omega$ takes in a training dataset, and outputs a model,</span>
<span id="cb7-540"><a href="#cb7-540"></a>i.e., $$\begin{aligned}</span>
<span id="cb7-541"><a href="#cb7-541"></a>    \forall i: \quad \theta^\star_i=\mathcal{A}_\omega(D_i).</span>
<span id="cb7-542"><a href="#cb7-542"></a>\end{aligned}$$</span>
<span id="cb7-543"><a href="#cb7-543"></a></span>
<span id="cb7-544"><a href="#cb7-544"></a>Therefore, the **meta-learning objective** is $$\begin{aligned}</span>
<span id="cb7-545"><a href="#cb7-545"></a>    \min_\omega \quad \sum_{i}\ L(\mathcal{A}_\omega(D_i), S_i).</span>
<span id="cb7-546"><a href="#cb7-546"></a>\end{aligned}$$ The above optimization problem gives a solution</span>
<span id="cb7-547"><a href="#cb7-547"></a>$\omega^\star$ which we use as the meta-parameter. Then, when a new task</span>
<span id="cb7-548"><a href="#cb7-548"></a>comes with a new training dataset $D_{new}$, we can simply apply</span>
<span id="cb7-549"><a href="#cb7-549"></a>$\theta^\star_{new}=\mathcal{A}_{\omega^\star}(D_{new})$ to obtain the</span>
<span id="cb7-550"><a href="#cb7-550"></a>adapted model $\theta^\star_{new}$. Note that we usually assume the</span>
<span id="cb7-551"><a href="#cb7-551"></a>meta-training datasets $D_i, S_i$ and the new dataset $D_{new}$ share</span>
<span id="cb7-552"><a href="#cb7-552"></a>the same underlying structure, or they come from the same distribution</span>
<span id="cb7-553"><a href="#cb7-553"></a>of datasets.</span>
<span id="cb7-554"><a href="#cb7-554"></a></span>
<span id="cb7-555"><a href="#cb7-555"></a>One of the most popular meta-learning method is Model-Agnosic</span>
<span id="cb7-556"><a href="#cb7-556"></a>Meta-Learning (MAML) <span class="co">[</span><span class="ot">@finn2017model</span><span class="co">]</span>. In MAML, the meta-parameter</span>
<span id="cb7-557"><a href="#cb7-557"></a>$\omega$ shares the same space as the model parameter $\theta$. At its</span>
<span id="cb7-558"><a href="#cb7-558"></a>core, in MAML the learning algorithm is defined to be $$\begin{aligned}</span>
<span id="cb7-559"><a href="#cb7-559"></a>    \mathcal{A}_\omega(D_i)=\omega-\alpha \nabla_\omega L(\omega, D_i),</span>
<span id="cb7-560"><a href="#cb7-560"></a>\end{aligned}$$ where $\alpha$ is the step size. As we can see, in fact</span>
<span id="cb7-561"><a href="#cb7-561"></a>$\omega$ is defined as the initialization of fine-tuning $\theta$. With</span>
<span id="cb7-562"><a href="#cb7-562"></a>a good $\omega$ learned, the model can adapt to a new task very quickly.</span>
<span id="cb7-563"><a href="#cb7-563"></a>In general, meta-learning can be summarized as follows: Given data from</span>
<span id="cb7-564"><a href="#cb7-564"></a>prior tasks, learn to solve a new task more quickly/proficiently. Given</span>
<span id="cb7-565"><a href="#cb7-565"></a>the general nature of meta-learning, one may be curious about whether</span>
<span id="cb7-566"><a href="#cb7-566"></a>preference learning can be benefited from meta-learning, which we</span>
<span id="cb7-567"><a href="#cb7-567"></a>discuss in the following section.</span>
<span id="cb7-568"><a href="#cb7-568"></a></span>
<span id="cb7-569"><a href="#cb7-569"></a><span class="fu">#### Few-Shot Preference Learning for Reinforcement Learning {#sec-few-shot}</span></span>
<span id="cb7-570"><a href="#cb7-570"></a></span>
<span id="cb7-571"><a href="#cb7-571"></a>Reinforcement learning (RL) in robotics often stumbles when it comes to</span>
<span id="cb7-572"><a href="#cb7-572"></a>devising reward functions aligning with human intentions.</span>
<span id="cb7-573"><a href="#cb7-573"></a>Preference-based RL algorithms aim to solve this by learning from human</span>
<span id="cb7-574"><a href="#cb7-574"></a>feedback, but this often demands a *highly impractical number of</span>
<span id="cb7-575"><a href="#cb7-575"></a>queries* or leads to oversimplified reward functions that don't hold up</span>
<span id="cb7-576"><a href="#cb7-576"></a>in real-world tasks.</span>
<span id="cb7-577"><a href="#cb7-577"></a></span>
<span id="cb7-578"><a href="#cb7-578"></a>To address the impractical requirement of human queries, as we discussed</span>
<span id="cb7-579"><a href="#cb7-579"></a>in the previous section, one may apply meta-learning so that the RL</span>
<span id="cb7-580"><a href="#cb7-580"></a>agent can adapt to new tasks with fewer human queries. <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span></span>
<span id="cb7-581"><a href="#cb7-581"></a>proposes to pre-training models on previous tasks with the meta-learning</span>
<span id="cb7-582"><a href="#cb7-582"></a>method MAML <span class="co">[</span><span class="ot">@finn2017model</span><span class="co">]</span>, and then the meta-trained model can adapt</span>
<span id="cb7-583"><a href="#cb7-583"></a>to new tasks with fewer queries.</span>
<span id="cb7-584"><a href="#cb7-584"></a></span>
<span id="cb7-585"><a href="#cb7-585"></a>We consider Reinforcement Learning (RL) settings where a state is</span>
<span id="cb7-586"><a href="#cb7-586"></a>denoted as $s\in S$, and action is denoted as $a\in A$, for state space</span>
<span id="cb7-587"><a href="#cb7-587"></a>$S$ and action space $A$. The reward function $r:S\times A \to \mathbb{R}$ is</span>
<span id="cb7-588"><a href="#cb7-588"></a>unknown and need to be learned from eliciting human preferences. There</span>
<span id="cb7-589"><a href="#cb7-589"></a>are multiple tasks, where each task has its own reward function and</span>
<span id="cb7-590"><a href="#cb7-590"></a>transition probabilities. The reward model is parameterized by $\psi$.</span>
<span id="cb7-591"><a href="#cb7-591"></a>We denote $\hat{r}_\psi(s,a)$ to be a learned estimate of an unknown</span>
<span id="cb7-592"><a href="#cb7-592"></a>ground-truth reward function $r(s,a)$, parameterized by $\psi$.</span>
<span id="cb7-593"><a href="#cb7-593"></a>Accordingly, a reward model determines a RL policy $\phi$ by maximizing</span>
<span id="cb7-594"><a href="#cb7-594"></a>the accumulated rewards. The preferences is learned via pairwise</span>
<span id="cb7-595"><a href="#cb7-595"></a>comparison of trajectory segments $$\begin{aligned}</span>
<span id="cb7-596"><a href="#cb7-596"></a>    \sigma = (s_t, a_t, s_{t+1}, a_{t+1}, ..., s_{t+k-1}, s_{t+k-1})</span>
<span id="cb7-597"><a href="#cb7-597"></a>\end{aligned}$$ of $k$ states and actions.</span>
<span id="cb7-598"><a href="#cb7-598"></a></span>
<span id="cb7-599"><a href="#cb7-599"></a>For each pre-training task, there is a dataset $D$ consists of labeled</span>
<span id="cb7-600"><a href="#cb7-600"></a>queries $(\sigma_1, \sigma_2, y)$ where $y\in <span class="sc">\{</span>0, 1<span class="sc">\}</span>$ is the label</span>
<span id="cb7-601"><a href="#cb7-601"></a>representing which trajectory is preferred. Therefore, a loss function</span>
<span id="cb7-602"><a href="#cb7-602"></a>$L(\psi, D)$ captures how well the reward model characterizes the</span>
<span id="cb7-603"><a href="#cb7-603"></a>preferences in dataset $D$. In <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span> they the preference</span>
<span id="cb7-604"><a href="#cb7-604"></a>predictor over segments using the Bradley-Terry model of paired</span>
<span id="cb7-605"><a href="#cb7-605"></a>comparisons <span class="co">[</span><span class="ot">@bradley1952rank</span><span class="co">]</span>, i.e., $$\begin{aligned}</span>
<span id="cb7-606"><a href="#cb7-606"></a>    P<span class="co">[</span><span class="ot">\sigma_1 \succ \sigma_2 </span><span class="co">]</span> = \frac{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1})}{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1}) + \exp \sum_t \hat{r}_\psi(s_t^{2}, a_t^{2})}.</span>
<span id="cb7-607"><a href="#cb7-607"></a>\end{aligned}$$ Then, the loss function is essentially a binary</span>
<span id="cb7-608"><a href="#cb7-608"></a>cross-entropy which the reward model $\psi$ aims to minimize, i.e.,</span>
<span id="cb7-609"><a href="#cb7-609"></a>$$\begin{aligned}</span>
<span id="cb7-610"><a href="#cb7-610"></a>    {L}(\psi,  {D}) = - \mathbb{E}_{(\sigma^1, \sigma^2, y) \sim {D}} \left<span class="co">[</span><span class="ot"> y(1) \log (P[\sigma_1 \succ \sigma_2 ]) + y(2)\log(1 - P[\sigma_1 \succ \sigma_2 ]) \right</span><span class="co">]</span>.</span>
<span id="cb7-611"><a href="#cb7-611"></a>\end{aligned}$$</span>
<span id="cb7-612"><a href="#cb7-612"></a></span>
<span id="cb7-613"><a href="#cb7-613"></a><span class="fu">##### Method Component 1: Pre-Training with Meta Learning {#method-component-1-pre-training-with-meta-learning .unnumbered}</span></span>
<span id="cb7-614"><a href="#cb7-614"></a></span>
<span id="cb7-615"><a href="#cb7-615"></a>To efficiently approximate the reward function $r_\text{new}$ for a new</span>
<span id="cb7-616"><a href="#cb7-616"></a>task with minimal queries, as described in <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span>, we aim to</span>
<span id="cb7-617"><a href="#cb7-617"></a>utilize a pre-trained reward function $\hat{r}_\psi$ that can be quickly</span>
<span id="cb7-618"><a href="#cb7-618"></a>fine-tuned using just a few preference comparisons. By pre-training on</span>
<span id="cb7-619"><a href="#cb7-619"></a>data from prior tasks, we can leverage the common structure across tasks</span>
<span id="cb7-620"><a href="#cb7-620"></a>to speed up the adaptation process. Although any meta-learning method is</span>
<span id="cb7-621"><a href="#cb7-621"></a>compatible, <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span> opt for Model Agnostic Meta-Learning (MAML)</span>
<span id="cb7-622"><a href="#cb7-622"></a>due to its simplicity. Therefore, the pre-training update for the reward</span>
<span id="cb7-623"><a href="#cb7-623"></a>model $\psi$ is $$\begin{aligned}</span>
<span id="cb7-624"><a href="#cb7-624"></a>    \psi \xleftarrow{} \psi - \beta \nabla_\psi \sum_{i = 1}^N {L} (\psi - \alpha \nabla_\psi {L}(\psi, {D}_i), {D}_i),</span>
<span id="cb7-625"><a href="#cb7-625"></a>\end{aligned}$$ where $\alpha, \beta$ are the inner and outer learning</span>
<span id="cb7-626"><a href="#cb7-626"></a>rate, respectively. We note that data $<span class="sc">\{</span>D_i<span class="sc">\}</span>_i$ of labeled preferences</span>
<span id="cb7-627"><a href="#cb7-627"></a>queries for prior tasks can come from offline datasets, simulated</span>
<span id="cb7-628"><a href="#cb7-628"></a>policies, or actual humans.</span>
<span id="cb7-629"><a href="#cb7-629"></a></span>
<span id="cb7-630"><a href="#cb7-630"></a><span class="fu">##### Method Component 2: Few-Shot Adaptation {#method-component-2-few-shot-adaptation .unnumbered}</span></span>
<span id="cb7-631"><a href="#cb7-631"></a></span>
<span id="cb7-632"><a href="#cb7-632"></a>With the aforementioned pre-training with meta learning, the</span>
<span id="cb7-633"><a href="#cb7-633"></a>meta-learned reward model can then be used for few-shot preference based</span>
<span id="cb7-634"><a href="#cb7-634"></a>RL during an online adaptation phase. The core procedure of the few-shot</span>
<span id="cb7-635"><a href="#cb7-635"></a>adaption is descibed as below</span>
<span id="cb7-636"><a href="#cb7-636"></a></span>
<span id="cb7-637"><a href="#cb7-637"></a><span class="ss">1.  </span>Given a pre-trained reward model $\psi$</span>
<span id="cb7-638"><a href="#cb7-638"></a></span>
<span id="cb7-639"><a href="#cb7-639"></a><span class="ss">2.  </span>For time step $t=1, 2, \dots$</span>
<span id="cb7-640"><a href="#cb7-640"></a></span>
<span id="cb7-641"><a href="#cb7-641"></a><span class="ss">    1.  </span>Find pairs of trajectories $(\sigma_1, \sigma_2)$ with</span>
<span id="cb7-642"><a href="#cb7-642"></a>        preference uncertainty based on $\psi$.</span>
<span id="cb7-643"><a href="#cb7-643"></a></span>
<span id="cb7-644"><a href="#cb7-644"></a><span class="ss">    2.  </span>Query human preference $y$ and forms a new dataset $D_{new}$</span>
<span id="cb7-645"><a href="#cb7-645"></a></span>
<span id="cb7-646"><a href="#cb7-646"></a><span class="ss">    3.  </span>Update the reward model by</span>
<span id="cb7-647"><a href="#cb7-647"></a>        $\psi'\leftarrow \psi - \alpha \nabla_\psi L(\psi, D_{new})$</span>
<span id="cb7-648"><a href="#cb7-648"></a></span>
<span id="cb7-649"><a href="#cb7-649"></a><span class="ss">    4.  </span>Update the policy with the new reward model $\psi'$</span>
<span id="cb7-650"><a href="#cb7-650"></a></span>
<span id="cb7-651"><a href="#cb7-651"></a>As mentioned in <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span>, uncertain queries are selected using</span>
<span id="cb7-652"><a href="#cb7-652"></a>the disagreement of an ensemble of reward functions over the preference</span>
<span id="cb7-653"><a href="#cb7-653"></a>predictors. Specifically, comparisons that maximize</span>
<span id="cb7-654"><a href="#cb7-654"></a>$\texttt{std}(P<span class="co">[</span><span class="ot">\sigma_1 \succ \sigma_2</span><span class="co">]</span>)$ are selected each time</span>
<span id="cb7-655"><a href="#cb7-655"></a>feedback is collected.</span>
<span id="cb7-656"><a href="#cb7-656"></a></span>
<span id="cb7-657"><a href="#cb7-657"></a>The whole pipeline of the method is outlined in @fig-few-1.</span>
<span id="cb7-658"><a href="#cb7-658"></a></span>
<span id="cb7-659"><a href="#cb7-659"></a>!<span class="co">[</span><span class="ot">An overview of the proposed method in [@hejna2023few</span><span class="co">]</span>. **Pre-training</span>
<span id="cb7-660"><a href="#cb7-660"></a>(left):** In the pre-training phase, trajectory segment comparisons are</span>
<span id="cb7-661"><a href="#cb7-661"></a>generated using data from previously learned tasks. Then, they are used</span>
<span id="cb7-662"><a href="#cb7-662"></a>to train a reward model. **Online-Adaptation (Right)**: After</span>
<span id="cb7-663"><a href="#cb7-663"></a>pre-training the reward model, it is adapted to new data from human</span>
<span id="cb7-664"><a href="#cb7-664"></a>feedback. The adapted reward model is then used to train a policy for a</span>
<span id="cb7-665"><a href="#cb7-665"></a>new task in a closed loop manner.](Figures/overview-few.png){#fig-few-1</span>
<span id="cb7-666"><a href="#cb7-666"></a>width="<span class="sc">\\</span>linewidth"}</span>
<span id="cb7-667"><a href="#cb7-667"></a></span>
<span id="cb7-668"><a href="#cb7-668"></a>We present one set of experiment from the paper, as it illustrates the</span>
<span id="cb7-669"><a href="#cb7-669"></a>effectiveness of the proposed method in a straightforward way. The</span>
<span id="cb7-670"><a href="#cb7-670"></a>experiment test the propoesed method on the Meta-World</span>
<span id="cb7-671"><a href="#cb7-671"></a>benchmark <span class="co">[</span><span class="ot">@yu2020meta</span><span class="co">]</span>. Three baselines are compared with the proposed</span>
<span id="cb7-672"><a href="#cb7-672"></a>method:</span>
<span id="cb7-673"><a href="#cb7-673"></a></span>
<span id="cb7-674"><a href="#cb7-674"></a><span class="ss">1.  </span>SAC: The Soft-Actor Critic RL algorithm trained from ground truth</span>
<span id="cb7-675"><a href="#cb7-675"></a>    rewards. This represents the standard best possible method given the</span>
<span id="cb7-676"><a href="#cb7-676"></a>    ground-truth reward.</span>
<span id="cb7-677"><a href="#cb7-677"></a></span>
<span id="cb7-678"><a href="#cb7-678"></a><span class="ss">2.  </span>PEBBLE: The PEBBLE algorithm <span class="co">[</span><span class="ot">@lee2021pebble</span><span class="co">]</span>. It does not use</span>
<span id="cb7-679"><a href="#cb7-679"></a>    information from pripor tasks.</span>
<span id="cb7-680"><a href="#cb7-680"></a></span>
<span id="cb7-681"><a href="#cb7-681"></a><span class="ss">3.  </span>Init: This method initialize the reward model with the pretained</span>
<span id="cb7-682"><a href="#cb7-682"></a>    weights from meta learning. However, instead of adapting the reward</span>
<span id="cb7-683"><a href="#cb7-683"></a>    model to the new task, it performs standard updates as in PEBBLE.</span>
<span id="cb7-684"><a href="#cb7-684"></a></span>
<span id="cb7-685"><a href="#cb7-685"></a>The results are shown in @fig-few-exp, where we can see that the proposed methord</span>
<span id="cb7-686"><a href="#cb7-686"></a>outperforms all of the baselines.</span>
<span id="cb7-687"><a href="#cb7-687"></a></span>
<span id="cb7-688"><a href="#cb7-688"></a>![Results on MetaWorld tasks. The title of each subplot indicates the</span>
<span id="cb7-689"><a href="#cb7-689"></a>task and number of artificial feedback queries used in training. Results</span>
<span id="cb7-690"><a href="#cb7-690"></a>for each method are shown across five seeds.</span>
<span id="cb7-691"><a href="#cb7-691"></a>](Figures/few-exp.png){#fig-few-exp width="<span class="sc">\\</span>linewidth"}</span>
<span id="cb7-692"><a href="#cb7-692"></a></span>
<span id="cb7-693"><a href="#cb7-693"></a>This paper <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span> shows that meta reward learning indeed reduce</span>
<span id="cb7-694"><a href="#cb7-694"></a>the number of queries of human preferences. However, as mentioned in the</span>
<span id="cb7-695"><a href="#cb7-695"></a>paper, there are still some drawbacks, as shown in the following.</span>
<span id="cb7-696"><a href="#cb7-696"></a></span>
<span id="cb7-697"><a href="#cb7-697"></a>Many of the queries the model pick for human preference elicitation are</span>
<span id="cb7-698"><a href="#cb7-698"></a>actually almost identical to human. After all, the model would pick the</span>
<span id="cb7-699"><a href="#cb7-699"></a>most uncertain pair of trajectories for human preference queries, and</span>
<span id="cb7-700"><a href="#cb7-700"></a>similar trajectories are for sure having high uncertainty in their</span>
<span id="cb7-701"><a href="#cb7-701"></a>preference. This suggest the need of new ways for designing the query</span>
<span id="cb7-702"><a href="#cb7-702"></a>selection strategy.</span>
<span id="cb7-703"><a href="#cb7-703"></a></span>
<span id="cb7-704"><a href="#cb7-704"></a>Moreover, despite the improved query complexity, it still needs an</span>
<span id="cb7-705"><a href="#cb7-705"></a>impractical amount of queries. As shown in @fig-few-exp, the "sweep into" task still needs 2500 human</span>
<span id="cb7-706"><a href="#cb7-706"></a>queries for it to work properly, which is still not ideal for what we</span>
<span id="cb7-707"><a href="#cb7-707"></a>want them to be.</span>
<span id="cb7-708"><a href="#cb7-708"></a></span>
<span id="cb7-709"><a href="#cb7-709"></a>In addition, it is mentioned in the paper that the proposed method may</span>
<span id="cb7-710"><a href="#cb7-710"></a>be even worse than training from scratch, if the new task is too</span>
<span id="cb7-711"><a href="#cb7-711"></a>out-of-distribution. Certainly, since meta-learning assumes</span>
<span id="cb7-712"><a href="#cb7-712"></a>in-distribution tasks, we cannot expect the proposed method to be good</span>
<span id="cb7-713"><a href="#cb7-713"></a>for out-of-distribution task. It is thus an interesting future direction</span>
<span id="cb7-714"><a href="#cb7-714"></a>to investigate whether one can design a method that automatically</span>
<span id="cb7-715"><a href="#cb7-715"></a>balance between using the prior information or training from scratch.</span>
<span id="cb7-716"><a href="#cb7-716"></a></span>
<span id="cb7-717"><a href="#cb7-717"></a><span class="fu">#### Watch Try Learn {#sec-watch}</span></span>
<span id="cb7-718"><a href="#cb7-718"></a></span>
<span id="cb7-719"><a href="#cb7-719"></a>Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards</span>
<span id="cb7-720"><a href="#cb7-720"></a><span class="co">[</span><span class="ot">@zhou2019watch</span><span class="co">]</span> asks the question "How can we efficiently learn both</span>
<span id="cb7-721"><a href="#cb7-721"></a>from expert demonstrations and from trials where we only get **binary**</span>
<span id="cb7-722"><a href="#cb7-722"></a>feedback from a human\". Why do we care about this question? In the</span>
<span id="cb7-723"><a href="#cb7-723"></a>context of robotics, a very compelling answer is the *cost of</span>
<span id="cb7-724"><a href="#cb7-724"></a>data-collection*. In a hypothetical world in which we have a vast number</span>
<span id="cb7-725"><a href="#cb7-725"></a>of **expert demonstrations** of robots accomplishing a large number of</span>
<span id="cb7-726"><a href="#cb7-726"></a>diverse tasks, we don't necessarily need to worry about learning from</span>
<span id="cb7-727"><a href="#cb7-727"></a>trials or from humans. We could simply learn a very capable imitation</span>
<span id="cb7-728"><a href="#cb7-728"></a>agent to perform any task. Natural Language Processing could be seen as</span>
<span id="cb7-729"><a href="#cb7-729"></a>living in this world, because internet-scale data is available.</span>
<span id="cb7-730"><a href="#cb7-730"></a>**Robots, however, are expensive**, so people generally don't have</span>
<span id="cb7-731"><a href="#cb7-731"></a>access to them, and therefore cannot use them to produce information to</span>
<span id="cb7-732"><a href="#cb7-732"></a>imitate. Similarly, **human time is expensive**, so even for large</span>
<span id="cb7-733"><a href="#cb7-733"></a>organizations that do have access to a lot of robots, it's still hard to</span>
<span id="cb7-734"><a href="#cb7-734"></a>collect a lot of expert demonstrations.</span>
<span id="cb7-735"><a href="#cb7-735"></a></span>
<span id="cb7-736"><a href="#cb7-736"></a>The largest available collection of robotics datasets today is Open</span>
<span id="cb7-737"><a href="#cb7-737"></a>X-Embodiment (<span class="co">[</span><span class="ot">@padalkar2023open</span><span class="co">]</span>), which consists of around 1M episodes</span>
<span id="cb7-738"><a href="#cb7-738"></a>from more than 300 different scenes. Even such large datastes are not</span>
<span id="cb7-739"><a href="#cb7-739"></a>enough to learn generally-capable robotic policies from imitation</span>
<span id="cb7-740"><a href="#cb7-740"></a>learning alone.</span>
<span id="cb7-741"><a href="#cb7-741"></a></span>
<span id="cb7-742"><a href="#cb7-742"></a>![Visualization of the Open X-Embodiment dataset collection. Even this</span>
<span id="cb7-743"><a href="#cb7-743"></a>large-scale dataset for robot learning is not yet enough to learn</span>
<span id="cb7-744"><a href="#cb7-744"></a>generally-capable robotic</span>
<span id="cb7-745"><a href="#cb7-745"></a>policies.](Figures/open_x_embodiment.png){#fig-open-x-embodiment}</span>
<span id="cb7-746"><a href="#cb7-746"></a></span>
<span id="cb7-747"><a href="#cb7-747"></a>**Main insight:** binary feedback is much cheaper to obtain than expert</span>
<span id="cb7-748"><a href="#cb7-748"></a>demonstrations! Instead of hiring people to act as robot operators to</span>
<span id="cb7-749"><a href="#cb7-749"></a>tell the robot exactly what to do, if there was a way of having many</span>
<span id="cb7-750"><a href="#cb7-750"></a>robots trying things in parallel, we can have humans watch videos of</span>
<span id="cb7-751"><a href="#cb7-751"></a>what the robots did and then give a success classification of whether</span>
<span id="cb7-752"><a href="#cb7-752"></a>the robot accomplished the goal. This is a much cheaper form of human</span>
<span id="cb7-753"><a href="#cb7-753"></a>supervision because the human labels don't necessarily need to be given</span>
<span id="cb7-754"><a href="#cb7-754"></a>in real time, so one human labeler can label many trajectories in</span>
<span id="cb7-755"><a href="#cb7-755"></a>parallel, and the human doesn't need to be a skilled robot operator.</span>
<span id="cb7-756"><a href="#cb7-756"></a></span>
<span id="cb7-757"><a href="#cb7-757"></a>Concretely, this paper seeks to learn new tasks with the following</span>
<span id="cb7-758"><a href="#cb7-758"></a>general problem setting:</span>
<span id="cb7-759"><a href="#cb7-759"></a></span>
<span id="cb7-760"><a href="#cb7-760"></a><span class="ss">1.  </span>We only get 1 expert demonstration of the target task</span>
<span id="cb7-761"><a href="#cb7-761"></a></span>
<span id="cb7-762"><a href="#cb7-762"></a><span class="ss">2.  </span>After seeing the expert demonstration, we have robots try to solve</span>
<span id="cb7-763"><a href="#cb7-763"></a>    the task 1 or more times.</span>
<span id="cb7-764"><a href="#cb7-764"></a></span>
<span id="cb7-765"><a href="#cb7-765"></a><span class="ss">3.  </span>The user (or some pre-defined reward function) annotates each trial</span>
<span id="cb7-766"><a href="#cb7-766"></a>    as success/failure.</span>
<span id="cb7-767"><a href="#cb7-767"></a></span>
<span id="cb7-768"><a href="#cb7-768"></a><span class="ss">4.  </span>The agent learns from both the demos and the annotated trials to</span>
<span id="cb7-769"><a href="#cb7-769"></a>    perform well on the target task.</span>
<span id="cb7-770"><a href="#cb7-770"></a></span>
<span id="cb7-771"><a href="#cb7-771"></a>Note that this work falls under the **meta-learning** umbrella, because</span>
<span id="cb7-772"><a href="#cb7-772"></a>we are learning an algorithm for quickly learning new tasks given new</span>
<span id="cb7-773"><a href="#cb7-773"></a>observations (demos, trials, and success labels.)</span>
<span id="cb7-774"><a href="#cb7-774"></a></span>
<span id="cb7-775"><a href="#cb7-775"></a>The **main contribution** of this paper is a meta-learning algorithm for</span>
<span id="cb7-776"><a href="#cb7-776"></a>incorporating demonstrations and binary feedback from trials to solve</span>
<span id="cb7-777"><a href="#cb7-777"></a>new tasks.</span>
<span id="cb7-778"><a href="#cb7-778"></a></span>
<span id="cb7-779"><a href="#cb7-779"></a>Meta-Learning deals with efficient learning of new tasks. In the context</span>
<span id="cb7-780"><a href="#cb7-780"></a>of robotics or reinforcement learning in general, **how do we define</span>
<span id="cb7-781"><a href="#cb7-781"></a>tasks**? We will use the Markov decision process (**MDP**) formalism. A</span>
<span id="cb7-782"><a href="#cb7-782"></a>task $T_i$ is described with the tuple $<span class="sc">\{</span>S, A, r_i, P_i<span class="sc">\}</span>$.</span>
<span id="cb7-783"><a href="#cb7-783"></a></span>
<span id="cb7-784"><a href="#cb7-784"></a><span class="ss">1.  </span>$S$ represents the *state-space* of the task, or all possible states</span>
<span id="cb7-785"><a href="#cb7-785"></a>    the agent could find itself in. This work uses image-observations,</span>
<span id="cb7-786"><a href="#cb7-786"></a>    so $S$ is the space of all possible RGB images.</span>
<span id="cb7-787"><a href="#cb7-787"></a></span>
<span id="cb7-788"><a href="#cb7-788"></a><span class="ss">2.  </span>$A$ is the action space, meaning the set of all possible actions the</span>
<span id="cb7-789"><a href="#cb7-789"></a>    agent could take. In robotics there are many ways of representing</span>
<span id="cb7-790"><a href="#cb7-790"></a>    action spaces, and this work considers end-effector positions,</span>
<span id="cb7-791"><a href="#cb7-791"></a>    rotations, and opening.</span>
<span id="cb7-792"><a href="#cb7-792"></a></span>
<span id="cb7-793"><a href="#cb7-793"></a><span class="ss">3.  </span>$r_i$ is the reward function for the task, with function signature</span>
<span id="cb7-794"><a href="#cb7-794"></a>    $r_i : S \times A \to \mathbb{R}$. This work assumes all reward functions</span>
<span id="cb7-795"><a href="#cb7-795"></a>    are binary.</span>
<span id="cb7-796"><a href="#cb7-796"></a></span>
<span id="cb7-797"><a href="#cb7-797"></a><span class="ss">4.  </span>$P_i$ is the transition dynamics function. It's a function that maps</span>
<span id="cb7-798"><a href="#cb7-798"></a>    state-action pairs to probability distributions over next states.</span>
<span id="cb7-799"><a href="#cb7-799"></a></span>
<span id="cb7-800"><a href="#cb7-800"></a>Notice that $S$ and $A$ are shared across tasks. Transition dynamics</span>
<span id="cb7-801"><a href="#cb7-801"></a>functions are normally also shared between tasks because they represent</span>
<span id="cb7-802"><a href="#cb7-802"></a>the laws of physics. However, this work considers environments with</span>
<span id="cb7-803"><a href="#cb7-803"></a>different objects, so they don't share the dynamics function. Given this</span>
<span id="cb7-804"><a href="#cb7-804"></a>definition for tasks, they assume that the tasks from the data that they</span>
<span id="cb7-805"><a href="#cb7-805"></a>get come from some unknown task-generating distribution $p(T)$.</span>
<span id="cb7-806"><a href="#cb7-806"></a></span>
<span id="cb7-807"><a href="#cb7-807"></a>Let's give a more precise definition of the problem statement considered</span>
<span id="cb7-808"><a href="#cb7-808"></a>by **Watch, Try, Learn**. As the paper name suggests, there are 3 phases</span>
<span id="cb7-809"><a href="#cb7-809"></a>for the problem statement.</span>
<span id="cb7-810"><a href="#cb7-810"></a></span>
<span id="cb7-811"><a href="#cb7-811"></a>**Watch:** During the *watch* phase, we give the agent $K$</span>
<span id="cb7-812"><a href="#cb7-812"></a>demonstrations of the target tasks. This paper considers the case where</span>
<span id="cb7-813"><a href="#cb7-813"></a>$K$ always equals 1, and all demonstrations are successful. That is,</span>
<span id="cb7-814"><a href="#cb7-814"></a>each demonstration consists of a trajectory</span>
<span id="cb7-815"><a href="#cb7-815"></a>$<span class="sc">\{</span>(s_0, a_0), \ldots, (s_H, a_H)<span class="sc">\}</span>$ where $H$ is the task horizon, and</span>
<span id="cb7-816"><a href="#cb7-816"></a>the final state is always successful, that is</span>
<span id="cb7-817"><a href="#cb7-817"></a>$r_i(s_H, a_H) = 1, r_i(s_j, a_j) = 0$ for every $j \neq H$.</span>
<span id="cb7-818"><a href="#cb7-818"></a></span>
<span id="cb7-819"><a href="#cb7-819"></a>Importantly, these demonstrations alone might not be sufficient for</span>
<span id="cb7-820"><a href="#cb7-820"></a>**full task specification**. As an example, consider a demonstration in</span>
<span id="cb7-821"><a href="#cb7-821"></a>which an apple is moved to the right, next to a pan. Seeing this</span>
<span id="cb7-822"><a href="#cb7-822"></a>demonstration alone, the task could be always moving the apple to the</span>
<span id="cb7-823"><a href="#cb7-823"></a>right, or it could be always moving the apple next to the pan,</span>
<span id="cb7-824"><a href="#cb7-824"></a>irrespective of where the pan is. The expected output after the Watch</span>
<span id="cb7-825"><a href="#cb7-825"></a>phase is a policy capable of gathering information about a task, given</span>
<span id="cb7-826"><a href="#cb7-826"></a>demonstrations.</span>
<span id="cb7-827"><a href="#cb7-827"></a></span>
<span id="cb7-828"><a href="#cb7-828"></a>**Try:** In the Try phase, we use the agent learned during the Watch</span>
<span id="cb7-829"><a href="#cb7-829"></a>phase to attempt the task for $L$ trials. As specified earlier, this</span>
<span id="cb7-830"><a href="#cb7-830"></a>paper considers the casae where $L$ always equals 1. After the agent</span>
<span id="cb7-831"><a href="#cb7-831"></a>completes the trials, humans (or pre-programmed reward functions)</span>
<span id="cb7-832"><a href="#cb7-832"></a>provide one binary reward for each trial, indicating whether the trial</span>
<span id="cb7-833"><a href="#cb7-833"></a>was successful. The expected output of this phase is $L$ trajectories</span>
<span id="cb7-834"><a href="#cb7-834"></a>and corresponding feedback that hopefully *disambiguate* the task.</span>
<span id="cb7-835"><a href="#cb7-835"></a></span>
<span id="cb7-836"><a href="#cb7-836"></a>**Learn:** After completing the trials, the agent must learn from both</span>
<span id="cb7-837"><a href="#cb7-837"></a>the original expert demonstrations and the trials, and become capable of</span>
<span id="cb7-838"><a href="#cb7-838"></a>solving the target task.</span>
<span id="cb7-839"><a href="#cb7-839"></a></span>
<span id="cb7-840"><a href="#cb7-840"></a>**Given Data:** To train agents that can Watch, Try, and Learn, we are</span>
<span id="cb7-841"><a href="#cb7-841"></a>given a dataset of expert demonstrations containing multiple demos for</span>
<span id="cb7-842"><a href="#cb7-842"></a>each task, and the dataset contains hundreds of tasks. Importantly, **no</span>
<span id="cb7-843"><a href="#cb7-843"></a>online interaction** is needed for training, and this method trains only</span>
<span id="cb7-844"><a href="#cb7-844"></a>with **supervised learning** and no reinforcement learning.</span>
<span id="cb7-845"><a href="#cb7-845"></a></span>
<span id="cb7-846"><a href="#cb7-846"></a>This section describes exactly how this paper trains an agent from the</span>
<span id="cb7-847"><a href="#cb7-847"></a>given expert demonstrations, and how to incorporate the trials and human</span>
<span id="cb7-848"><a href="#cb7-848"></a>feedback into the loop.</span>
<span id="cb7-849"><a href="#cb7-849"></a></span>
<span id="cb7-850"><a href="#cb7-850"></a>**Training to Watch:** We now describe the algorithm to obtain an agent</span>
<span id="cb7-851"><a href="#cb7-851"></a>conditioned on the given expert demonstration. In particular, what we</span>
<span id="cb7-852"><a href="#cb7-852"></a>want to obtain out of the Watch phase is a policy conditioned on a set</span>
<span id="cb7-853"><a href="#cb7-853"></a>of expert demonstrations. Formally, we want to obtain</span>
<span id="cb7-854"><a href="#cb7-854"></a>$\pi_\theta^{\text{watch}}(a | s, <span class="sc">\{</span>d_{i,k}<span class="sc">\}</span>)$.</span>
<span id="cb7-855"><a href="#cb7-855"></a></span>
<span id="cb7-856"><a href="#cb7-856"></a>The way we can obtain this policy is through **meta-imitation</span>
<span id="cb7-857"><a href="#cb7-857"></a>learning**. Given the demonstrations $<span class="sc">\{</span>\textbf{d}_{i,k}<span class="sc">\}</span>$ for task</span>
<span id="cb7-858"><a href="#cb7-858"></a>$i$, we sample another *different* demonstration coming from the same</span>
<span id="cb7-859"><a href="#cb7-859"></a>task $\textbf{d}_i^{\text{test}}$. The key insight here is that</span>
<span id="cb7-860"><a href="#cb7-860"></a>$\textbf{d}_i^{\text{test}}$ is an example of **optimal behavior** given</span>
<span id="cb7-861"><a href="#cb7-861"></a>the demonstrations. Therefore, to obtain</span>
<span id="cb7-862"><a href="#cb7-862"></a>$\pi_\theta^{\text{watch}}(a | s, <span class="sc">\{</span>d_{i,k}<span class="sc">\}</span>)$, we simply regress the</span>
<span id="cb7-863"><a href="#cb7-863"></a>policy to imitate actions taken on $\textbf{d}_i^{\text{test}}$.</span>
<span id="cb7-864"><a href="#cb7-864"></a>Concretely, we train policy parameters $\theta$ to minimize the</span>
<span id="cb7-865"><a href="#cb7-865"></a>following loss:</span>
<span id="cb7-866"><a href="#cb7-866"></a></span>
<span id="cb7-867"><a href="#cb7-867"></a>$\mathcal{L}^\text{watch}(\theta, \mathcal{D}_i^*) = \mathbb{E}_{\{d_{i,k}\} \sim \mathcal{D}_i^*} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^*  \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[ </span>
<span id="cb7-868"><a href="#cb7-868"></a><span class="ss">- </span>\log \pi_\theta^{\text{watch}} (a_t | s_t, \{d_{i,k}<span class="sc">\}</span>) \big]$</span>
<span id="cb7-869"><a href="#cb7-869"></a></span>
<span id="cb7-870"><a href="#cb7-870"></a>This corresponds to doing imitation learning by minimizing the negative</span>
<span id="cb7-871"><a href="#cb7-871"></a>log-likelihood of the test trajectory actions, conditioning the policy</span>
<span id="cb7-872"><a href="#cb7-872"></a>on the entire demo set. However, how is the conditioning on the demo set</span>
<span id="cb7-873"><a href="#cb7-873"></a>achieved?</span>
<span id="cb7-874"><a href="#cb7-874"></a></span>
<span id="cb7-875"><a href="#cb7-875"></a>![Vision-based policy architecture that conditions on a set of</span>
<span id="cb7-876"><a href="#cb7-876"></a>demonstrations.](Figures/watch-try-learn-architecture.png){#fig-watch-try-learn-arch}</span>
<span id="cb7-877"><a href="#cb7-877"></a></span>
<span id="cb7-878"><a href="#cb7-878"></a>@fig-watch-try-learn-arch visualizes how Watch Try Learn</span>
<span id="cb7-879"><a href="#cb7-879"></a>deals with conditioning on demonstrations. In addition to using features</span>
<span id="cb7-880"><a href="#cb7-880"></a>obtained from the images of the current state, the architecture uses</span>
<span id="cb7-881"><a href="#cb7-881"></a>features from frames sampled (in order) from the demonstration episodes,</span>
<span id="cb7-882"><a href="#cb7-882"></a>which are concatenated together.</span>
<span id="cb7-883"><a href="#cb7-883"></a></span>
<span id="cb7-884"><a href="#cb7-884"></a>**Trying:** On the **Try** phase, when the agent is given a set of</span>
<span id="cb7-885"><a href="#cb7-885"></a>demonstrations $<span class="sc">\{</span>\textbf{d}_{i,k}<span class="sc">\}</span>$, we deploy the policy</span>
<span id="cb7-886"><a href="#cb7-886"></a>$\pi_\theta^{\text{watch}}(a | s, <span class="sc">\{</span>\textbf{d}_{i,k}<span class="sc">\}</span>)$ to collect $L$</span>
<span id="cb7-887"><a href="#cb7-887"></a>trials. There is no training involved in the Try phase, we simply</span>
<span id="cb7-888"><a href="#cb7-888"></a>condition the policy on the given demonstrations</span>
<span id="cb7-889"><a href="#cb7-889"></a></span>
<span id="cb7-890"><a href="#cb7-890"></a>**Training to Learn:** During the Watch phase the objective was to train</span>
<span id="cb7-891"><a href="#cb7-891"></a>a policy conditioned on demonstrations</span>
<span id="cb7-892"><a href="#cb7-892"></a>$\pi_\theta^{\text{watch}}(a | s, <span class="sc">\{</span>\textbf{d}_{i,k}<span class="sc">\}</span>)$. The authors of</span>
<span id="cb7-893"><a href="#cb7-893"></a>Watch, Try, Learn use a similar strategy as the Watch phase for the</span>
<span id="cb7-894"><a href="#cb7-894"></a>Learn phase. We now want to train a policy that is conditioned on the</span>
<span id="cb7-895"><a href="#cb7-895"></a>demonstrations, as well as the trials and binary feedback. That is, we</span>
<span id="cb7-896"><a href="#cb7-896"></a>want to learn</span>
<span id="cb7-897"><a href="#cb7-897"></a>$\pi_\phi^{\text{watch}}(a | s, <span class="sc">\{</span>\textbf{d}_{i,k}\}, \{\mathbf{\tau}_{i, l}<span class="sc">\}</span>)$.</span>
<span id="cb7-898"><a href="#cb7-898"></a>To train the policy, we again use meta-imitation learning where we</span>
<span id="cb7-899"><a href="#cb7-899"></a>additionally sample yet another trajectory from the same task.</span>
<span id="cb7-900"><a href="#cb7-900"></a>Concretely, we train policy parameters $\phi$ to minimize the following</span>
<span id="cb7-901"><a href="#cb7-901"></a>loss:</span>
<span id="cb7-902"><a href="#cb7-902"></a></span>
<span id="cb7-903"><a href="#cb7-903"></a>$\mathcal{L}^{\text{learn}}(\phi, \mathcal{D}_i, \mathcal{D}_i^*) = \mathbb{E}_{(\{d_{i,k}\}, \{\mathbf{\tau}_{i,l}\}) \sim \mathcal{D}_i} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^* \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[ </span>
<span id="cb7-904"><a href="#cb7-904"></a><span class="ss">- </span>\log \pi_\theta^{\text{learn}} (a_t | s_t, \{d_{i,k}\}, \{\tau_{i,l}<span class="sc">\}</span>) \big]$</span>
<span id="cb7-905"><a href="#cb7-905"></a></span>
<span id="cb7-906"><a href="#cb7-906"></a>The conditioning on both the demo episodes and the trial episodes is</span>
<span id="cb7-907"><a href="#cb7-907"></a>achieved in the exact same way as in the Watch phase, and is visualized</span>
<span id="cb7-908"><a href="#cb7-908"></a>in @fig-watch-try-learn-arch. The architecture is simply</span>
<span id="cb7-909"><a href="#cb7-909"></a>adjusted to be able to take in more images fro mthe trial episodes.</span>
<span id="cb7-910"><a href="#cb7-910"></a></span>
<span id="cb7-911"><a href="#cb7-911"></a>In this section, we describe the evaluation suite for the paper,</span>
<span id="cb7-912"><a href="#cb7-912"></a>including the simulation benchmark used, the baselines considered, and</span>
<span id="cb7-913"><a href="#cb7-913"></a>the results.</span>
<span id="cb7-914"><a href="#cb7-914"></a></span>
<span id="cb7-915"><a href="#cb7-915"></a>**Gripper environment setup:**</span>
<span id="cb7-916"><a href="#cb7-916"></a></span>
<span id="cb7-917"><a href="#cb7-917"></a>![Visualization of different tasks from the simulated benchmark for</span>
<span id="cb7-918"><a href="#cb7-918"></a>Watch Try Learn.](Figures/watch-try-learn-envs.png){#fig-envs}</span>
<span id="cb7-919"><a href="#cb7-919"></a></span>
<span id="cb7-920"><a href="#cb7-920"></a>@fig-envs illustrates the different task families considered in the simulated</span>
<span id="cb7-921"><a href="#cb7-921"></a>Gripper environment. Button Pressing, Grasping, Pushing, and Pick and</span>
<span id="cb7-922"><a href="#cb7-922"></a>Place. For each task family, the environment supports hundreds of</span>
<span id="cb7-923"><a href="#cb7-923"></a>different tasks by changing the objects in the scene and the objectives</span>
<span id="cb7-924"><a href="#cb7-924"></a>(e.g. which object to pick and where to place). For each task in each</span>
<span id="cb7-925"><a href="#cb7-925"></a>task family, a handful of expert demonstrations are given in a</span>
<span id="cb7-926"><a href="#cb7-926"></a>demonstrations dataset. As mentioned previously, the environment gives</span>
<span id="cb7-927"><a href="#cb7-927"></a>the agent image observations, and take in actions as end-effector</span>
<span id="cb7-928"><a href="#cb7-928"></a>(gripper) positions, angles, and opening.</span>
<span id="cb7-929"><a href="#cb7-929"></a></span>
<span id="cb7-930"><a href="#cb7-930"></a>**Baselines:** The following three baselines are considered:</span>
<span id="cb7-931"><a href="#cb7-931"></a></span>
<span id="cb7-932"><a href="#cb7-932"></a><span class="ss">1.  </span>**Behavior Cloning**: simple imitation learning based on maximum</span>
<span id="cb7-933"><a href="#cb7-933"></a>    log-likelihood training using data from all tasks.</span>
<span id="cb7-934"><a href="#cb7-934"></a></span>
<span id="cb7-935"><a href="#cb7-935"></a><span class="ss">2.  </span>**Meta-imitation learning**: This baseline corresponds to simply</span>
<span id="cb7-936"><a href="#cb7-936"></a>    running the policy from the Watch step, without using any trial</span>
<span id="cb7-937"><a href="#cb7-937"></a>    data. That is, we only condition on the set of expert</span>
<span id="cb7-938"><a href="#cb7-938"></a>    demonstrations, but no online trials.</span>
<span id="cb7-939"><a href="#cb7-939"></a></span>
<span id="cb7-940"><a href="#cb7-940"></a><span class="ss">3.  </span>**Behavior Cloning + SAC**: Pre-train a policy with Behavior Cloning</span>
<span id="cb7-941"><a href="#cb7-941"></a>    on all data, and follow that with Reinforcement Learning fine-tuning</span>
<span id="cb7-942"><a href="#cb7-942"></a>    for the specific target task, using the maximum-entropy algorithm</span>
<span id="cb7-943"><a href="#cb7-943"></a>    SAC (<span class="co">[</span><span class="ot">@haarnoja2018soft</span><span class="co">]</span>).</span>
<span id="cb7-944"><a href="#cb7-944"></a></span>
<span id="cb7-945"><a href="#cb7-945"></a>![Results for Watch Try Learn on the gripper control environment, and</span>
<span id="cb7-946"><a href="#cb7-946"></a>comparisons with</span>
<span id="cb7-947"><a href="#cb7-947"></a>baselines.](Figures/watch-try-learn-results.png){#fig-watch-try-learn-results</span>
<span id="cb7-948"><a href="#cb7-948"></a>width="50%"}</span>
<span id="cb7-949"><a href="#cb7-949"></a></span>
<span id="cb7-950"><a href="#cb7-950"></a>::: {#tbl-watch-try-learn-table}</span>
<span id="cb7-951"><a href="#cb7-951"></a>  **METHOD**                     **SUCCESS RATE**</span>
<span id="cb7-952"><a href="#cb7-952"></a>  ----------------------------- ------------------</span>
<span id="cb7-953"><a href="#cb7-953"></a>  BC                              .09 $\pm$ .01</span>
<span id="cb7-954"><a href="#cb7-954"></a>  MIL                             .30 $\pm$ .02</span>
<span id="cb7-955"><a href="#cb7-955"></a>  WTL, 1 TRIAL (OURS)             .42 $\pm$ .02</span>
<span id="cb7-956"><a href="#cb7-956"></a>  **RL FINE-TUNING WITH SAC**   </span>
<span id="cb7-957"><a href="#cb7-957"></a>  BC + SAC, 1500 TRIALS           .11 $\pm$ .07</span>
<span id="cb7-958"><a href="#cb7-958"></a>  BC + SAC, 2000 TRIALS           .29 $\pm$ .10</span>
<span id="cb7-959"><a href="#cb7-959"></a>  BC + SAC, 2500 TRIALS           .39 $\pm$ .11</span>
<span id="cb7-960"><a href="#cb7-960"></a></span>
<span id="cb7-961"><a href="#cb7-961"></a>  : Average success rates over all tasks.</span>
<span id="cb7-962"><a href="#cb7-962"></a>:::</span>
<span id="cb7-963"><a href="#cb7-963"></a></span>
<span id="cb7-964"><a href="#cb7-964"></a>@fig-watch-try-learn-results shows average success rates for</span>
<span id="cb7-965"><a href="#cb7-965"></a>Watch Try Learn compared to baselines. Watch Try Learn significantly</span>
<span id="cb7-966"><a href="#cb7-966"></a>outperforms baselines on every task family. In particular, it is far</span>
<span id="cb7-967"><a href="#cb7-967"></a>superior to Behavior Cloning, which is a very weak baseline, and it</span>
<span id="cb7-968"><a href="#cb7-968"></a>significantly surpasses Meta-Imitation Learning on 3 out of 4 task</span>
<span id="cb7-969"><a href="#cb7-969"></a>families. @tbl-watch-try-learn-table includes comparison with BC</span>
<span id="cb7-970"><a href="#cb7-970"></a>fine-tuned with Reinforcement Learning. Even after 2500 online trials,</span>
<span id="cb7-971"><a href="#cb7-971"></a>SAC is not able to obtain the success rate that Watch Try Learn achieves</span>
<span id="cb7-972"><a href="#cb7-972"></a>after only 1 trial. Overall, Watch Try Learn exhibits very significant</span>
<span id="cb7-973"><a href="#cb7-973"></a>performance gains over prior methods.</span>
<span id="cb7-974"><a href="#cb7-974"></a></span>
<span id="cb7-975"><a href="#cb7-975"></a><span class="fu">### Direct Preference Optimization</span></span>
<span id="cb7-976"><a href="#cb7-976"></a></span>
<span id="cb7-977"><a href="#cb7-977"></a>A modern method for estimating the parameters of a human preference</span>
<span id="cb7-978"><a href="#cb7-978"></a>model is direct preference optimization <span class="co">[</span><span class="ot">@rafailov2023direct</span><span class="co">]</span>, which is</span>
<span id="cb7-979"><a href="#cb7-979"></a>used in the context of aligning language models to human preferences. A</span>
<span id="cb7-980"><a href="#cb7-980"></a>recent approach <span class="co">[</span><span class="ot">@christiano2023deep</span><span class="co">]</span> first trains a reward model that</span>
<span id="cb7-981"><a href="#cb7-981"></a>captures human preferences and then uses proximal policy optimization to</span>
<span id="cb7-982"><a href="#cb7-982"></a>train a language model-based policy to reflect those learned</span>
<span id="cb7-983"><a href="#cb7-983"></a>preferences. Direct Preference Optimization (DPO), on the other hand,</span>
<span id="cb7-984"><a href="#cb7-984"></a>removes the need for a reward model by directly using the model</span>
<span id="cb7-985"><a href="#cb7-985"></a>likelihood of two outcomes (a preferred or highly-ranked sequence and an</span>
<span id="cb7-986"><a href="#cb7-986"></a>unpreferred or low-ranked sequence) to capture the preference</span>
<span id="cb7-987"><a href="#cb7-987"></a>represented in the data. DPO provides a simpler framework than its</span>
<span id="cb7-988"><a href="#cb7-988"></a>reinforcement learning approach and results in comparable performance</span>
<span id="cb7-989"><a href="#cb7-989"></a>with improved stability. Furthermore, it obviates the need to train a</span>
<span id="cb7-990"><a href="#cb7-990"></a>reward model, instead using a language model policy and human preference</span>
<span id="cb7-991"><a href="#cb7-991"></a>dataset to align the policy directly to human preferences.</span>
<span id="cb7-992"><a href="#cb7-992"></a></span>
<span id="cb7-993"><a href="#cb7-993"></a><span class="fu">### Model Design Consideration</span></span>
<span id="cb7-994"><a href="#cb7-994"></a></span>
<span id="cb7-995"><a href="#cb7-995"></a>When designing models and learning their parameters, one must account</span>
<span id="cb7-996"><a href="#cb7-996"></a>for important tradeoffs when designing and optimizing a model to learn</span>
<span id="cb7-997"><a href="#cb7-997"></a>human preferences.</span>
<span id="cb7-998"><a href="#cb7-998"></a></span>
<span id="cb7-999"><a href="#cb7-999"></a>**Bias vs. Variance Trade-off.** In modeling human preferences, we aim</span>
<span id="cb7-1000"><a href="#cb7-1000"></a>to ensure that predicted utilities accurately reflect overall human</span>
<span id="cb7-1001"><a href="#cb7-1001"></a>preferences. One key challenge is managing the bias and variance</span>
<span id="cb7-1002"><a href="#cb7-1002"></a>trade-off.</span>
<span id="cb7-1003"><a href="#cb7-1003"></a></span>
<span id="cb7-1004"><a href="#cb7-1004"></a>Bias refers to assumptions made during model design and training that</span>
<span id="cb7-1005"><a href="#cb7-1005"></a>can skew predictions. For example, in Ideal Point Models, we make the</span>
<span id="cb7-1006"><a href="#cb7-1006"></a>assumption that the representations we use for individuals and choices</span>
<span id="cb7-1007"><a href="#cb7-1007"></a>are aligned in the embedding space, and that this representation is</span>
<span id="cb7-1008"><a href="#cb7-1008"></a>sufficient to capture human preferences using distance metrics. However,</span>
<span id="cb7-1009"><a href="#cb7-1009"></a>there are myriad cases in which this may break down, for example if the</span>
<span id="cb7-1010"><a href="#cb7-1010"></a>two sets of vectors follow different distributions each with their own</span>
<span id="cb7-1011"><a href="#cb7-1011"></a>unique biases. If the representations do not come from the same domain,</span>
<span id="cb7-1012"><a href="#cb7-1012"></a>one may have little visibility into how a distance metric computes the</span>
<span id="cb7-1013"><a href="#cb7-1013"></a>final utility value for a choice for a given individual. Some ways to</span>
<span id="cb7-1014"><a href="#cb7-1014"></a>mitigate bias in human preference models include increasing the number</span>
<span id="cb7-1015"><a href="#cb7-1015"></a>of parameters in a model (allowing for better learning of patterns in</span>
<span id="cb7-1016"><a href="#cb7-1016"></a>the data) or removing inductive biases based on our assumptions of the</span>
<span id="cb7-1017"><a href="#cb7-1017"></a>underlying data.</span>
<span id="cb7-1018"><a href="#cb7-1018"></a></span>
<span id="cb7-1019"><a href="#cb7-1019"></a>On the other hand, variance refers to the model's sensitivity to small</span>
<span id="cb7-1020"><a href="#cb7-1020"></a>changes in the input, which leads to significant changes in the outp ut.</span>
<span id="cb7-1021"><a href="#cb7-1021"></a>This phenomenon is often termed 'overfitting' or 'overparameterization.'</span>
<span id="cb7-1022"><a href="#cb7-1022"></a>This behavior can occur in models that have many parameters, and learn</span>
<span id="cb7-1023"><a href="#cb7-1023"></a>correlations in the data that do not contribute to learning human</span>
<span id="cb7-1024"><a href="#cb7-1024"></a>preferences, but are artifacts of noise in the dataset that one should</span>
<span id="cb7-1025"><a href="#cb7-1025"></a>ultimately ignore. One can address variance in models by reducing the</span>
<span id="cb7-1026"><a href="#cb7-1026"></a>number of parameters or incorporating biases in the model based on</span>
<span id="cb7-1027"><a href="#cb7-1027"></a>factors we can assume about the data.</span>
<span id="cb7-1028"><a href="#cb7-1028"></a></span>
<span id="cb7-1029"><a href="#cb7-1029"></a>**Model Scope.** One important consideration unique to human preference</span>
<span id="cb7-1030"><a href="#cb7-1030"></a>models is that we wish to model individual preferences, and we may</span>
<span id="cb7-1031"><a href="#cb7-1031"></a>choose to do so at arbitrary granularity. For example, we can fit models</span>
<span id="cb7-1032"><a href="#cb7-1032"></a>to a specific individual or even multiple models for an individual, each</span>
<span id="cb7-1033"><a href="#cb7-1033"></a>for different purposes or contexts. On the other end of the spectrum, we</span>
<span id="cb7-1034"><a href="#cb7-1034"></a>may create a model to capture human preferences across large populations</span>
<span id="cb7-1035"><a href="#cb7-1035"></a>or the world.</span>
<span id="cb7-1036"><a href="#cb7-1036"></a></span>
<span id="cb7-1037"><a href="#cb7-1037"></a>Individual models may certainly prove to be more powerful, as they do</span>
<span id="cb7-1038"><a href="#cb7-1038"></a>not need to generalize across multiple individuals and can dedicate all</span>
<span id="cb7-1039"><a href="#cb7-1039"></a>of their parameters to learning the preferences of a single user. In the</span>
<span id="cb7-1040"><a href="#cb7-1040"></a>context of human behavior, this can be a significant advantage as any</span>
<span id="cb7-1041"><a href="#cb7-1041"></a>two individuals can be arbitrarily different or even opposite in their</span>
<span id="cb7-1042"><a href="#cb7-1042"></a>preferences. On the other hand, models fit only one person can</span>
<span id="cb7-1043"><a href="#cb7-1043"></a>tremendously overfit to the training distribution and capture noise in</span>
<span id="cb7-1044"><a href="#cb7-1044"></a>the data, which is not truly representative of human preferences.</span>
<span id="cb7-1045"><a href="#cb7-1045"></a></span>
<span id="cb7-1046"><a href="#cb7-1046"></a>On the end of the spectrum, models fit to the entire world may be</span>
<span id="cb7-1047"><a href="#cb7-1047"></a>inadequate to model human preferences for arbitrary individuals,</span>
<span id="cb7-1048"><a href="#cb7-1048"></a>especially those whose data it has not been fit to. As such, models may</span>
<span id="cb7-1049"><a href="#cb7-1049"></a>underfit the given training distribution. These models aim to generalize</span>
<span id="cb7-1050"><a href="#cb7-1050"></a>to many people but may fail to capture the nuances of individual</span>
<span id="cb7-1051"><a href="#cb7-1051"></a>preferences, especially for those whose data is not represented in the</span>
<span id="cb7-1052"><a href="#cb7-1052"></a>training set. As a result, they may not perform well for arbitrary</span>
<span id="cb7-1053"><a href="#cb7-1053"></a>individuals within the target population</span>
<span id="cb7-1054"><a href="#cb7-1054"></a></span>
<span id="cb7-1055"><a href="#cb7-1055"></a>Choosing the appropriate scope for a model is crucial. ne must balance</span>
<span id="cb7-1056"><a href="#cb7-1056"></a>the trade-off between overfitting to noise in highly granular models and</span>
<span id="cb7-1057"><a href="#cb7-1057"></a>underfitting in broader models that may not capture individual nuances.</span>
<span id="cb7-1058"><a href="#cb7-1058"></a></span>
<span id="cb7-1059"><a href="#cb7-1059"></a><span class="fu">## Multimodal Preferences</span></span>
<span id="cb7-1060"><a href="#cb7-1060"></a></span>
<span id="cb7-1061"><a href="#cb7-1061"></a>One of the core assumptions about learning a reward function is that it</span>
<span id="cb7-1062"><a href="#cb7-1062"></a>is unimodal, meaning that it consists of data from one person with a</span>
<span id="cb7-1063"><a href="#cb7-1063"></a>certain set of preferences or a group of people with similar</span>
<span id="cb7-1064"><a href="#cb7-1064"></a>preferences. However, the model of unimodality often oversimplifies</span>
<span id="cb7-1065"><a href="#cb7-1065"></a>human preferences and their often conflicting nature. To accurately</span>
<span id="cb7-1066"><a href="#cb7-1066"></a>capture all the nuances of human preference, we examine a multi-modal</span>
<span id="cb7-1067"><a href="#cb7-1067"></a>distribution with some baseline assumptions. Consider a scenario where</span>
<span id="cb7-1068"><a href="#cb7-1068"></a>we, as regular drivers, make a left-hand turn at an intersection</span>
<span id="cb7-1069"><a href="#cb7-1069"></a><span class="co">[</span><span class="ot">@myers2021learning</span><span class="co">]</span>. What would we do if we saw a car speeding down the</span>
<span id="cb7-1070"><a href="#cb7-1070"></a>road approaching us? The figure below describes some options. Following</span>
<span id="cb7-1071"><a href="#cb7-1071"></a>a timid driving pattern, some vehicles would stop to let the other car</span>
<span id="cb7-1072"><a href="#cb7-1072"></a>go, preventing a collision. Other vehicles would be more aggressive and</span>
<span id="cb7-1073"><a href="#cb7-1073"></a>try to make the turn before colliding with the oncoming vehicle. Given</span>
<span id="cb7-1074"><a href="#cb7-1074"></a>the data of one of these driving patterns, our model (our autonomous</span>
<span id="cb7-1075"><a href="#cb7-1075"></a>vehicle) can make an appropriate decision. However, what if our model</span>
<span id="cb7-1076"><a href="#cb7-1076"></a>was given data from both aggressive and timid drivers, and we don't know</span>
<span id="cb7-1077"><a href="#cb7-1077"></a>which data corresponds to which type of driver? If we applied standard</span>
<span id="cb7-1078"><a href="#cb7-1078"></a>learning based on comparison techniques, we see, as illustrated by the</span>
<span id="cb7-1079"><a href="#cb7-1079"></a>figure below, that the car would have an accident trying to find a</span>
<span id="cb7-1080"><a href="#cb7-1080"></a>policy close enough to both driving patterns.</span>
<span id="cb7-1081"><a href="#cb7-1081"></a></span>
<span id="cb7-1082"><a href="#cb7-1082"></a>![<span class="co">[</span><span class="ot">@myers2021learning</span><span class="co">]</span> shows the possibilities of 2 different driving</span>
<span id="cb7-1083"><a href="#cb7-1083"></a>patterns when a car is taking a left-hand turn at an intersection and</span>
<span id="cb7-1084"><a href="#cb7-1084"></a>sees another car approaching</span>
<span id="cb7-1085"><a href="#cb7-1085"></a>head-on.](Figures/driving-patt.png){#fig-driving-patt}</span>
<span id="cb7-1086"><a href="#cb7-1086"></a></span>
<span id="cb7-1087"><a href="#cb7-1087"></a>!<span class="co">[</span><span class="ot">The figure [@myers2021learning</span><span class="co">]</span> depicts the resultant collision when we</span>
<span id="cb7-1088"><a href="#cb7-1088"></a>try to find a policy close enough to both the driving</span>
<span id="cb7-1089"><a href="#cb7-1089"></a>patterns.](Figures/driving-coll.png){#fig-driving-coll}</span>
<span id="cb7-1090"><a href="#cb7-1090"></a></span>
<span id="cb7-1091"><a href="#cb7-1091"></a>As illustrated by the driving example, we see that multi-modality for</span>
<span id="cb7-1092"><a href="#cb7-1092"></a>our reward function is extremely important and, in some cases, if it is</span>
<span id="cb7-1093"><a href="#cb7-1093"></a>not considered, can lead to fatal decisions <span class="co">[</span><span class="ot">@myers2021learning</span><span class="co">]</span>. But why</span>
<span id="cb7-1094"><a href="#cb7-1094"></a>can't we label the groups, which would be the timid and aggressive</span>
<span id="cb7-1095"><a href="#cb7-1095"></a>drivers in the driving case, and then learn separate reward functions</span>
<span id="cb7-1096"><a href="#cb7-1096"></a>for each driver? The first problem with this approach is that it is</span>
<span id="cb7-1097"><a href="#cb7-1097"></a>inefficient and time-consuming to separate the data into groups because</span>
<span id="cb7-1098"><a href="#cb7-1098"></a>we would have to cluster and label the data. Secondly, it would not be</span>
<span id="cb7-1099"><a href="#cb7-1099"></a>accurate just to split the data because a more timid driver can be</span>
<span id="cb7-1100"><a href="#cb7-1100"></a>aggressive when they are in a hurry.</span>
<span id="cb7-1101"><a href="#cb7-1101"></a></span>
<span id="cb7-1102"><a href="#cb7-1102"></a>To formulate this problem of learning reward functions and mixing</span>
<span id="cb7-1103"><a href="#cb7-1103"></a>coefficients from ranking queries in a fully observable deterministic</span>
<span id="cb7-1104"><a href="#cb7-1104"></a>dynamical system, we begin by describing the system as a trajectory</span>
<span id="cb7-1105"><a href="#cb7-1105"></a>$\xi = (s_0, a_0, ..., s_T, a_T)$, where the sequence of states and</span>
<span id="cb7-1106"><a href="#cb7-1106"></a>actions represents the system's evolution over time. Assume there are</span>
<span id="cb7-1107"><a href="#cb7-1107"></a>$M$ different reward functions, each representing an expert's</span>
<span id="cb7-1108"><a href="#cb7-1108"></a>preferences. Using the linearity assumption in reward learning, we model</span>
<span id="cb7-1109"><a href="#cb7-1109"></a>each expert's reward function as a linear combination of features in a</span>
<span id="cb7-1110"><a href="#cb7-1110"></a>known, fixed feature space $\phi(\xi)$. The reward for the $m$-th expert</span>
<span id="cb7-1111"><a href="#cb7-1111"></a>is given by: $$R_m(\xi) = \omega^T_m \phi(\xi),$$ where $\omega_m$ is a</span>
<span id="cb7-1112"><a href="#cb7-1112"></a>vector of parameters corresponding to the $m$-th expert's preferences.</span>
<span id="cb7-1113"><a href="#cb7-1113"></a>There exists an unknown distribution over the reward parameters and we</span>
<span id="cb7-1114"><a href="#cb7-1114"></a>can represent this distribution with mixing coefficients $\alpha_m$ such</span>
<span id="cb7-1115"><a href="#cb7-1115"></a>that $\sum_M^{m = 1} \alpha_m = 1$. Our goal is to learn reward</span>
<span id="cb7-1116"><a href="#cb7-1116"></a>functions and mixing coefficients using ranking queries.</span>
<span id="cb7-1117"><a href="#cb7-1117"></a></span>
<span id="cb7-1118"><a href="#cb7-1118"></a>To define our problem, let's consider a robot who performs the following</span>
<span id="cb7-1119"><a href="#cb7-1119"></a>trajectories and asks a user to rank all the trajectories.</span>
<span id="cb7-1120"><a href="#cb7-1120"></a></span>
<span id="cb7-1121"><a href="#cb7-1121"></a>!<span class="co">[</span><span class="ot">The figure [@myers2022learning</span><span class="co">]</span> depicts a few different trajectories</span>
<span id="cb7-1122"><a href="#cb7-1122"></a>for an example multi-modal ranking</span>
<span id="cb7-1123"><a href="#cb7-1123"></a>scenario.](Figures/robot-traj.png){#fig-robot-traj}</span>
<span id="cb7-1124"><a href="#cb7-1124"></a></span>
<span id="cb7-1125"><a href="#cb7-1125"></a>The robot will be given back a set of trajectory rankings, coming from M</span>
<span id="cb7-1126"><a href="#cb7-1126"></a>humans and the objective is to learn the underlying reward function. We</span>
<span id="cb7-1127"><a href="#cb7-1127"></a>can represent the response of the ranking query as</span>
<span id="cb7-1128"><a href="#cb7-1128"></a>$x = (\xi_{a_1},\ ...\ ,\xi_{a_K})$ where $a_1$ is the index of the</span>
<span id="cb7-1129"><a href="#cb7-1129"></a>expert's top choice, $a_2$ is the index of the expert's second choice,</span>
<span id="cb7-1130"><a href="#cb7-1130"></a><span class="sc">\.</span>.. and so on. With the response $x$, we generate a probability</span>
<span id="cb7-1131"><a href="#cb7-1131"></a>distribution with the softmax rule <span class="co">[</span><span class="ot">@myers2022learning</span><span class="co">]</span>:</span>
<span id="cb7-1132"><a href="#cb7-1132"></a>$Pr(x_1 = \xi_{a_1} | R = R_m) = \frac{e^R_m(\xi_{a_1})}{\sum_{j=1}^Ke^R_m(\xi_{a_j})}$.</span>
<span id="cb7-1133"><a href="#cb7-1133"></a>where $R_m(\xi_{a_i})$ denotes the reward assigned by the $m$-th expert</span>
<span id="cb7-1134"><a href="#cb7-1134"></a>to trajectory $\xi_{a_i}$. Then, we randomly sample our probability</span>
<span id="cb7-1135"><a href="#cb7-1135"></a>distribution to pick our top choice. From the remaining trajectories, we</span>
<span id="cb7-1136"><a href="#cb7-1136"></a>noisily choose from our distribution to rank our second-best option. We</span>
<span id="cb7-1137"><a href="#cb7-1137"></a>repeat this process until we have ranked all our trajectories. This</span>
<span id="cb7-1138"><a href="#cb7-1138"></a>follows what is known as the Plackett-Luce Ranking Model.</span>
<span id="cb7-1139"><a href="#cb7-1139"></a></span>
<span id="cb7-1140"><a href="#cb7-1140"></a>Given knowledge of the true reward function weights $\omega_m$ and</span>
<span id="cb7-1141"><a href="#cb7-1141"></a>mixing coefficients $\alpha_m$, we have the following joint mass over</span>
<span id="cb7-1142"><a href="#cb7-1142"></a>observations x from a query Q:</span>
<span id="cb7-1143"><a href="#cb7-1143"></a>$Pr(x\ |\ Q) = \sum_{m = 1}^M \alpha_m\prod_{i = 1}^K\frac{e^{\omega_m^T \Phi(\xi_{a_i})}}{\sum_{j = i}^K e^{\omega_m^T \Phi(\xi_{a_j})}}$.</span>
<span id="cb7-1144"><a href="#cb7-1144"></a></span>
<span id="cb7-1145"><a href="#cb7-1145"></a>With the above formulation of the joint mass distribution over</span>
<span id="cb7-1146"><a href="#cb7-1146"></a>observation and queries, we can now formulate an objective.</span>
<span id="cb7-1147"><a href="#cb7-1147"></a>Specifically, it is to present users with the best set of queries that</span>
<span id="cb7-1148"><a href="#cb7-1148"></a>learn reward weights, $\omega$, and mixing coefficient, $\alpha$, based</span>
<span id="cb7-1149"><a href="#cb7-1149"></a>upon user rankings of preferred query responses. By learning these</span>
<span id="cb7-1150"><a href="#cb7-1150"></a>parameters, we can have an accurate estimation of the joint mass</span>
<span id="cb7-1151"><a href="#cb7-1151"></a>distribution of the observations.</span>
<span id="cb7-1152"><a href="#cb7-1152"></a></span>
<span id="cb7-1153"><a href="#cb7-1153"></a>To learn these parameters, we use a Bayesian learning framework. The</span>
<span id="cb7-1154"><a href="#cb7-1154"></a>goal will be to learn the reward weights, $\omega_m$, and all mixing</span>
<span id="cb7-1155"><a href="#cb7-1155"></a>coefficients $\alpha_m$. Thus, define the parameters to be</span>
<span id="cb7-1156"><a href="#cb7-1156"></a>$\theta = <span class="sc">\{</span>\omega, \alpha<span class="sc">\}</span>$. We start by simplifying the posterior</span>
<span id="cb7-1157"><a href="#cb7-1157"></a>over the parameters.</span>
<span id="cb7-1158"><a href="#cb7-1158"></a></span>
<span id="cb7-1159"><a href="#cb7-1159"></a>$$\begin{aligned}</span>
<span id="cb7-1160"><a href="#cb7-1160"></a>\Pr(\Theta | Q^{(1)}, x^{(1)}, Q^{(2)}, x^{(2)}, \ldots) &amp; \propto \Pr(\Theta) \Pr(Q^{(1)} | x^{(1)}, Q^{(2)}, x^{(2)}, \ldots | \Theta) <span class="sc">\\</span></span>
<span id="cb7-1161"><a href="#cb7-1161"></a>&amp; = \Pr(\Theta) \prod_t \Pr(x^{(t)} | Q^{(t)}, \Theta, Q^{(1)}, x^{(1)}, \ldots, Q^{(t-1)}, x^{(t-1)}) <span class="sc">\\</span></span>
<span id="cb7-1162"><a href="#cb7-1162"></a>&amp; \propto \Pr(\Theta) \prod_t \Pr(x^{(t)} | \Theta, Q^{(t)})</span>
<span id="cb7-1163"><a href="#cb7-1163"></a>\end{aligned}$$</span>
<span id="cb7-1164"><a href="#cb7-1164"></a></span>
<span id="cb7-1165"><a href="#cb7-1165"></a>Note that the first proportionality term is directly from Bayes rule</span>
<span id="cb7-1166"><a href="#cb7-1166"></a>(removing normalization constant). The first equation comes directly</span>
<span id="cb7-1167"><a href="#cb7-1167"></a>from the assumption that the queries at timestamp $t$ are conditionally</span>
<span id="cb7-1168"><a href="#cb7-1168"></a>independent of the parameters given previous queries &amp; rankings. This</span>
<span id="cb7-1169"><a href="#cb7-1169"></a>assumption is reasonable because the previous queries &amp; rankings ideally</span>
<span id="cb7-1170"><a href="#cb7-1170"></a>give all the information to inform the choice of the next set of. The</span>
<span id="cb7-1171"><a href="#cb7-1171"></a>last proportionality term comes from the assumption that the ranked</span>
<span id="cb7-1172"><a href="#cb7-1172"></a>queries are conditionally independent given the parameters</span>
<span id="cb7-1173"><a href="#cb7-1173"></a></span>
<span id="cb7-1174"><a href="#cb7-1174"></a>The prior distribution is dependent on use case. For example, in the</span>
<span id="cb7-1175"><a href="#cb7-1175"></a>user studies conducted by the authors to verify this method, they use a</span>
<span id="cb7-1176"><a href="#cb7-1176"></a>standard Gaussian for the reward weights and the mixing coefficients to</span>
<span id="cb7-1177"><a href="#cb7-1177"></a>be uniform on a $M - 1$ simplex to ensure that they add up to 1. Then we</span>
<span id="cb7-1178"><a href="#cb7-1178"></a>can use maximum likelihood estimation to compute the parameters with the</span>
<span id="cb7-1179"><a href="#cb7-1179"></a>simplified posterior.</span>
<span id="cb7-1180"><a href="#cb7-1180"></a></span>
<span id="cb7-1181"><a href="#cb7-1181"></a>{{&lt; include psets/pset1.qmd &gt;}}</span>
<span id="cb7-1182"><a href="#cb7-1182"></a></span>
<span id="cb7-1183"><a href="#cb7-1183"></a>Through our exploration of human preference models, we will ground ourselves in</span>
<span id="cb7-1184"><a href="#cb7-1184"></a>building a health coaching system that can provide meal recommendations aligned with a user's dietary needs and preferences. Examples of scenarios which can benefit from a model of how humans make choices include:</span>
<span id="cb7-1185"><a href="#cb7-1185"></a></span>
<span id="cb7-1186"><a href="#cb7-1186"></a><span class="ss">1.  </span>**Health coaching:** Humans express their preferences every time</span>
<span id="cb7-1187"><a href="#cb7-1187"></a>    they pick lunch for consumption. Humans may have several goals</span>
<span id="cb7-1188"><a href="#cb7-1188"></a>    related to nutrition, such as weight loss and improving</span>
<span id="cb7-1189"><a href="#cb7-1189"></a>    concentration. We can learn how a given individual or set of</span>
<span id="cb7-1190"><a href="#cb7-1190"></a>    individuals prefer to eat to provide personalized recommendations to</span>
<span id="cb7-1191"><a href="#cb7-1191"></a>    help them attain their goals. This chapter will use this use case to</span>
<span id="cb7-1192"><a href="#cb7-1192"></a>    ground human preference modeling in a real-life application.</span>
<span id="cb7-1193"><a href="#cb7-1193"></a></span>
<span id="cb7-1194"><a href="#cb7-1194"></a><span class="ss">2.  </span>**Social media:** Platforms have a far greater amount of content</span>
<span id="cb7-1195"><a href="#cb7-1195"></a>    than one can consume in a lifetime, yet such products must aim to</span>
<span id="cb7-1196"><a href="#cb7-1196"></a>    maximize user engagement. To accomplish this, we can learn what</span>
<span id="cb7-1197"><a href="#cb7-1197"></a>    specific things people like to see in their feeds to optimize the</span>
<span id="cb7-1198"><a href="#cb7-1198"></a>    value they gain out of their time on social media. For example, the</span>
<span id="cb7-1199"><a href="#cb7-1199"></a>    video feed social media platform <span class="co">[</span><span class="ot">TikTok</span><span class="co">](https://www.tiktok.com/)</span></span>
<span id="cb7-1200"><a href="#cb7-1200"></a>    has had viral adoption due to its notorious ability to personalize a</span>
<span id="cb7-1201"><a href="#cb7-1201"></a>    feed for its users based on their preferences.</span>
<span id="cb7-1202"><a href="#cb7-1202"></a></span>
<span id="cb7-1203"><a href="#cb7-1203"></a><span class="ss">3.  </span>**Shopping:** Retail corporations largely aim to maximize revenue by</span>
<span id="cb7-1204"><a href="#cb7-1204"></a>    making it easy for people to make purchases. Recommendation systems</span>
<span id="cb7-1205"><a href="#cb7-1205"></a>    on online shopping platforms provide a mechanism for curating</span>
<span id="cb7-1206"><a href="#cb7-1206"></a>    specific items based on an individual's previous purchases (or even</span>
<span id="cb7-1207"><a href="#cb7-1207"></a>    browsing history) to make shoppers aware of items they may like and,</span>
<span id="cb7-1208"><a href="#cb7-1208"></a>    therefore, purchase.</span>
<span id="cb7-1209"><a href="#cb7-1209"></a></span>
<span id="cb7-1210"><a href="#cb7-1210"></a><span class="co">&lt;!--</span></span>
<span id="cb7-1211"><a href="#cb7-1211"></a><span class="co">Two key models used in pairwise sampling are the Thurstonian and Bradley-Terry models [@cattelan2012]. The Thurstonian model assumes each item $i$ has a true score $u_i$ following a normal distribution. The difference $d_{ij} = u_i - u_j$ is also normally distributed. The probability that item $i$ is preferred over item $j$ is given by $P(i \succ j) = \Phi \left( \frac{u_i - u_j}{\sqrt{2\sigma^2}} \right)$, where $\Phi$ is the cumulative normal distribution function. The denominator $\sqrt{2\sigma^2}$ is the standard deviation of the difference $d_{ij} = u_i - u_j$ when $u_i$ and $u_j$ are normally distributed with variance $\sigma^2$[@cattelan2012]. The Bradley-Terry model defines the probability of preference based on latent scores $\beta_i$ and $\beta_j$. The probability that item $i$ is preferred over item $j$ is $P(i \succ j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}$. This model is used to estimate relative strengths or preferences based on latent scores. [@cattelan2012].</span></span>
<span id="cb7-1212"><a href="#cb7-1212"></a></span>
<span id="cb7-1213"><a href="#cb7-1213"></a><span class="co">::: {#tbl-philosophy}</span></span>
<span id="cb7-1214"><a href="#cb7-1214"></a><span class="co">  -----------------------------------------------------------------------</span></span>
<span id="cb7-1215"><a href="#cb7-1215"></a><span class="co">  Application                         Human Preference</span></span>
<span id="cb7-1216"><a href="#cb7-1216"></a><span class="co">  ----------------------------------- -----------------------------------</span></span>
<span id="cb7-1217"><a href="#cb7-1217"></a><span class="co">  Computer vision: train a neural     This is how humans process images</span></span>
<span id="cb7-1218"><a href="#cb7-1218"></a><span class="co">  network to predict bounding boxes   by identifying the position and</span></span>
<span id="cb7-1219"><a href="#cb7-1219"></a><span class="co">  delineating all instances of dogs   geometry of the things we see in</span></span>
<span id="cb7-1220"><a href="#cb7-1220"></a><span class="co">  in an image                         them</span></span>
<span id="cb7-1221"><a href="#cb7-1221"></a></span>
<span id="cb7-1222"><a href="#cb7-1222"></a><span class="co">  Natural language processing: train  Coherent text is itself a</span></span>
<span id="cb7-1223"><a href="#cb7-1223"></a><span class="co">  a model to generate coherent text   human-created and defined concept,</span></span>
<span id="cb7-1224"><a href="#cb7-1224"></a><span class="co">                                      and we prefer that any</span></span>
<span id="cb7-1225"><a href="#cb7-1225"></a><span class="co">                                      synthetically generated text</span></span>
<span id="cb7-1226"><a href="#cb7-1226"></a><span class="co">                                      matches that of humans</span></span>
<span id="cb7-1227"><a href="#cb7-1227"></a></span>
<span id="cb7-1228"><a href="#cb7-1228"></a><span class="co">  Computer vision: train a diffusion  Humans prefer that images</span></span>
<span id="cb7-1229"><a href="#cb7-1229"></a><span class="co">  model to generate realistic images  accurately capture the world as</span></span>
<span id="cb7-1230"><a href="#cb7-1230"></a><span class="co">  of nature                           observed by humans, and this</span></span>
<span id="cb7-1231"><a href="#cb7-1231"></a><span class="co">                                      generative model should reflect the</span></span>
<span id="cb7-1232"><a href="#cb7-1232"></a><span class="co">                                      details that comprise that</span></span>
<span id="cb7-1233"><a href="#cb7-1233"></a><span class="co">                                      preference</span></span>
<span id="cb7-1234"><a href="#cb7-1234"></a><span class="co">  -----------------------------------------------------------------------</span></span>
<span id="cb7-1235"><a href="#cb7-1235"></a></span>
<span id="cb7-1236"><a href="#cb7-1236"></a><span class="co">  : Examples of machine learning tasks and their interpretation as</span></span>
<span id="cb7-1237"><a href="#cb7-1237"></a><span class="co">  modeling human preferences.</span></span>
<span id="cb7-1238"><a href="#cb7-1238"></a><span class="co">:::</span></span>
<span id="cb7-1239"><a href="#cb7-1239"></a><span class="co">--&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
    <footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/002-reward_model.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer><script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




</body></html>