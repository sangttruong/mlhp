<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Reward Model – Machine Learning from Human Preferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../src/003-measure.html" rel="next">
<link href="../src/001-preference_decision_model.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-07ba0ad10f5680c660e360ac31d2f3b6.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8b864f0777c60eecff11d75b6b2e1175.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-fa3d1c749edcb96cd5cb7d620f3e5237.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-6f24586c8b15e78d85e3983c622e3e8a.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../src/002-reward_model.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Reward Model</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning from Human Preferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sangttruong/mlhp" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Machine-Learning-from-Human-Preferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/001-preference_decision_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Models of Preferences and Decisions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/002-reward_model.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Reward Model</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/003-measure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/004-optim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Model-Free Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/005-align.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/006-conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>license.qmd</p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-learning" id="toc-sec-learning" class="nav-link active" data-scroll-target="#sec-learning"><span class="header-section-number">3.1</span> Parameterization and Learning of Utility Functions</a>
  <ul class="collapse">
  <li><a href="#reward-learning-with-large-language-models" id="toc-reward-learning-with-large-language-models" class="nav-link" data-scroll-target="#reward-learning-with-large-language-models"><span class="header-section-number">3.1.1</span> Reward Learning with Large Language Models</a></li>
  <li><a href="#reward-learning-in-robotics" id="toc-reward-learning-in-robotics" class="nav-link" data-scroll-target="#reward-learning-in-robotics"><span class="header-section-number">3.1.2</span> Reward Learning in Robotics</a></li>
  <li><a href="#reward-learning-with-meta-learning" id="toc-reward-learning-with-meta-learning" class="nav-link" data-scroll-target="#reward-learning-with-meta-learning"><span class="header-section-number">3.1.3</span> Reward Learning with Meta Learning</a></li>
  <li><a href="#direct-preference-optimization" id="toc-direct-preference-optimization" class="nav-link" data-scroll-target="#direct-preference-optimization"><span class="header-section-number">3.1.4</span> Direct Preference Optimization</a></li>
  <li><a href="#model-design-consideration" id="toc-model-design-consideration" class="nav-link" data-scroll-target="#model-design-consideration"><span class="header-section-number">3.1.5</span> Model Design Consideration</a></li>
  </ul></li>
  <li><a href="#multimodal-preferences" id="toc-multimodal-preferences" class="nav-link" data-scroll-target="#multimodal-preferences"><span class="header-section-number">3.2</span> Multimodal Preferences</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/002-reward_model.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="ch-reward-models" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Reward Model</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="sec-learning" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-learning"><span class="header-section-number">3.1</span> Parameterization and Learning of Utility Functions</h2>
<p>The attributes representing a choice <span class="math inline">\(z_i\)</span> are crucial in defining the human preference model, as they provide the context for capturing human behavior when choice <span class="math inline">\(i\)</span> is made.</p>
<p>With an understanding of the various techniques we can use to model human preferences, we can now create robust models which utilize context attributes about the options an individual has in front of them and model their choices. However, these models on their own are powerless; their parameters are initialized randomly and we must fit the models to the actual human choice data!</p>
<p>Each of the models we have studied contain distinct parameters which aim to capture human preferences; for example <span class="math inline">\(\beta\)</span> is a parameter vector containing variables which represent a linear function to compute utility given a choice’s attributes. We can also choose to represent stochastic utility functions or embedding functions for Ideal Point Models as neural networks. But how can we compute the optimal values of these parameters?</p>
<p>In this section, we give the reader an overview of the different methods available to tune human preference model parameters using given data. We refer the reader to <span class="citation" data-cites="book_estimation_casella book_estimation_bock">(<a href="#ref-book_estimation_casella" role="doc-biblioref">Casella and Berger 1990</a>; <a href="#ref-book_estimation_bock" role="doc-biblioref">Bock et al. 2015</a>)</span> for first-principle derivations of these methods and a deeper dive into their theoretical properties (convergence, generalization, data-hungriness, etc.).</p>
<p>A common and powerful approach for computing the parameters of a model is maximum likelihood estimation <span class="citation" data-cites="book_estimation_casella book_estimation_bock">(<a href="#ref-book_estimation_casella" role="doc-biblioref">Casella and Berger 1990</a>; <a href="#ref-book_estimation_bock" role="doc-biblioref">Bock et al. 2015</a>)</span>. The likelihood of a model is the probability of the observed data given the model parameters; intuitively we wish to maximize this likelihood, as that would mean that our model associates observed human preferences in the data with high probability. We can formally define the likelihood for a model with parameters <span class="math inline">\(\beta\)</span> and a given data point <span class="math inline">\((z_i, y_i)\)</span> as: <span class="math display">\[\mathcal{L}(z_i, y_i; \beta) = \mathbb{P}(y = y_i | z_i; \beta)\]</span></p>
<p>Assuming our data is independent and identically distributed (iid), the likelihood over the entire dataset is the joint probability of all observed data as defined by the model: <span class="math display">\[\mathcal{L}(z, Y; \beta) = \prod_{i = 1}^J \mathbb{P}(y = y_i | z_i; \beta)\]</span></p>
<p>In our very first example of binary choice with logistic noise, this was simply the model’s probability of the observed preference value: <span class="math display">\[\mathcal{L}(z_i, y_i; \beta) = \frac{1}{1 + \exp^{-u_{i,j}^*}}\]</span></p>
<p>In the same case with noise following a standard normal distribution, this took the form: <span class="math display">\[\mathcal{L}(z_i, y_i; \beta) = \Phi(u_{i,j}^*)\]</span></p>
<p>Fortunately, in these cases, there are straightforward methods for parameter estimation: logistic regression and probit regression (binary or multinomial, depending on the model), respectively. We can use ordinal regression to estimate the model’s parameters for our ordered preference model.</p>
<p>Generally, the objective function commonly found in parameter learning can be optimized with stochastic gradient descent (SGD) <span class="citation" data-cites="gradient_descent">(<a href="#ref-gradient_descent" role="doc-biblioref">Ruder 2016</a>)</span>. We can define an objective function as the likelihood to maximize this objective. Since SGD minimizes a given objective, we must negate the likelihood, which ensures that a converged solution maximizes the likelihood. SGD operates by computing the gradient of the objective with respect to the parameters of the model, which provides a signal of the direction in which the parameters must move to <em>maximize</em> the objective. Then, SGD makes an update step by subtracting this gradient from the parameters (most often with a scale factor called a <em>learning rate</em>), to move the parameters in a direction which <em>minimizes</em> the objective. When the objective is the negative likelihood (or sometimes negative log-likelihood for convenience or tractability), the result is an increase in the overall likelihood.</p>
<p>In the case of logistic and Gaussian models, SGD may yield a challenging optimization problem as its stochasticity can lead to noisy updates, for example, if certain examples or batches of examples are biased. Mitigations include batched SGD, in which multiple samples are randomly sampled from the dataset at each iteration, learning rates, which reduce the impact of noisy gradient updates, and momentum and higher-order optimizers which reduce noise by using movering averages of gradients or provide better estimates of the best direction in which to update the gradients. Some models, such as those that use neural networks, may, in fact, be intractable to estimate without a method such as SGD (or its momentum-based derivatives). For example, neural networks with many layers, non-linearities, and parameters can only be efficiently computed with gradient-based methods.</p>
<section id="reward-learning-with-large-language-models" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="reward-learning-with-large-language-models"><span class="header-section-number">3.1.1</span> Reward Learning with Large Language Models</h3>
<p>Taking a step away from explicitly modeling human bias and preference, we consider applying a deep learning approach to state-of-the-art language models. We begin by introducing the concepts of <em>foundation models</em> and <em>alignment</em>. A foundation model <span class="citation" data-cites="Liang2021">(<a href="#ref-Liang2021" role="doc-biblioref">Bommasani et al. 2021</a>)</span> in machine learning typically refers to a large and pre-trained neural network model that serves as the basis for various downstream tasks. In natural language processing, models like GPT-3, Llama, and BERT are considered foundation models. They are pre-trained on a massive corpus of text data, learning to understand language and context, and are capable of various language-related tasks such as text classification, language generation, and question answering. Foundation models are important because they alleviate the need to train massive neural networks from scratch, a compute and data expensive endeavor. However, a raw foundation model, trained on a pretraining objective such as a language modeling objective, is not useful on its own. It must be aligned to respond correctly based on human preferences.</p>
<p>In short, alignment for foundation models is the process by which model behavior is aligned with human values, ethics, and societal norms. Large Language Models (LLMs) are a foundation model for natural language processing. They are trained using a next-word prediction objective, allowing them to generate coherent language. A simple way to align a Large Language Model is to train it to follow instructions in a supervised way, using instruction-response pairs curated by hand. However, this limits the upper limit of LLM performance to the performance of the annotators’ writing abilities. This type of annotation is also expensive.</p>
<p>An alternative, more promising approach is to train LLMs using reinforcement learning, potentially enabling them to surpass human-level performance. The main challenge with this method lies in defining an explicit reward function for generating free-form text. To address this, a reward model (RM) can be trained based on human preferences, providing a mechanism to score the quality of the generated text. This approach, known as Reinforcement Learning from Human Feedback (RLHF), leverages human feedback to guide model training, allowing LLMs to better align with human expectations while continuously improving performance.</p>
<div id="fig-rm-arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rm-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/arch.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rm-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: Overall architecture of a reward model based on LLM
</figcaption>
</figure>
</div>
<p>The Llama2 reward model <span class="citation" data-cites="2307.09288">(<a href="#ref-2307.09288" role="doc-biblioref">Touvron et al. 2023</a>)</span> is initialized from the pretrained Llama2 LLM. In the LLM, the last layer is a mapping <span class="math inline">\(L: \mathbb{R}^D \rightarrow \mathbb{R}^V\)</span>, where <span class="math inline">\(D\)</span> is the embedding dimension from the transformer decoder stack and <span class="math inline">\(V\)</span> is the vocabulary size. To get the RM, we replace that last layer with a randomly initialized scalar head that maps <span class="math inline">\(L: \mathbb{R}^D \rightarrow \mathbb{R}^1\)</span>. It’s important to initialize the RM from the LLM it’s meant to evaluate. This is because:</p>
<ol type="1">
<li><p>The RM will have the same “knowledge” as the LLM. This is particularly useful if evaluating things like “does the LLM know when it doesn’t know?”. However, in cases where the RM is simply evaluating helpfulness or factuality, it may be useful to have the RM know more.</p></li>
<li><p>The RM is on distribution for the LLM - it is initialized in a way where it semantically understands the LLM’s outputs.</p></li>
</ol>
<p>An RM is trained with paired preferences, following the format: <span class="math display">\[\begin{aligned}
    \langle prompt\_history, response\_accepted, response\_rejected \rangle
\end{aligned}\]</span> Prompt_history is a multiturn history of user prompts and model generations, response_accepted is the preferred final model generation by an annotator, and response_rejected is the unpreferred response. The RM is trained with a binary ranking loss with an optional margin term m(r), shown in equation (7). There is also often a small regularization term added to center the score distribution on 0. <span class="math display">\[\mathcal{L}_{\text{ranking}} = -\log(\sigma(r_\theta(x,y_c) - r_\theta(x,y_r) - m(r)))\]</span> The margin term increases the distance in scores specifically for preference pairs annotators rate as easier to separate.</p>
<div id="tbl-margin_nums" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-margin_nums-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Two variants of preference rating based margin with different magnitude.
</figcaption>
<div aria-describedby="tbl-margin_nums-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;">Significantly</td>
<td style="text-align: center;">Better</td>
<td style="text-align: center;">Slightly</td>
<td style="text-align: center;">Negligibly</td>
</tr>
<tr class="even">
<td></td>
<td style="text-align: center;">Better</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Better</td>
<td style="text-align: center;">Better / Unsure</td>
</tr>
<tr class="odd">
<td>Margin Small</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2/3</td>
<td style="text-align: center;">1/3</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td>Margin Large</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="fig-margin-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-margin-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/margin-2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-margin-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: Reward model score distribution shift caused by incorporating preference rating based margin in ranking loss. With the margin term, we observe a binary split pattern in reward distribution, especially for a larger margin.
</figcaption>
</figure>
</div>
<p>It may seem confusing how the margins were chosen. It’s primarily because the sigmoid function, which is used to normalize the raw reward model score, flattens out beyond the range of <span class="math inline">\([-4, 4]\)</span>. Thus, the maximum possible margin is eight.</p>
<p>When training or using a reward model, watching for the following is important:</p>
<ol type="1">
<li><p><strong>LLM Distribution Shift</strong>: With each finetune of the LLM, the RM should be updated through a collection of fresh human preferences using generations from the new LLM. This ensures that the RM stays aligned with the current distribution of the LLM and avoids drifting off-distribution.</p></li>
<li><p><strong>RM and LLM are coupled</strong>: An RM is generally optimized to distinguish human preferences more efficiently within the specific distribution of the LLM to be optimized. However, this specialization poses a challenge: such an RM will underperform when dealing with generations not aligned with this specific LLM distribution, such as generations from a completely different LLM.</p></li>
<li><p><strong>Training Sensitivities of RMs</strong>: Training RMs can be unstable and prone to overfitting, especially with multiple training epochs. It’s generally advisable to limit the number of epochs during RM training to avoid this issue.</p></li>
</ol>
<p>The industry has centered around optimizing for two primary qualities in LLMs: helpfulness and harmlessness (safety). There are also other axes such as factuality, reasoning, tool use, code, multilingual, and more, but these are out of scope for us. In the Llama2 paper, preference data was collected from humans for each quality, with separate guidelines. This presents a challenge for co-optimizing the final LLM towards both goals.</p>
<p>Two main approaches can be taken for Reinforcement Learning from Human Feedback (RLHF) in this context:</p>
<ol type="1">
<li><p>Train a unified reward model that integrates both datasets.</p></li>
<li><p>Train two separate reward models, one for each quality, and optimize the LLM toward both.</p></li>
</ol>
<p>Option 1 is difficult because of the tension between helpfulness and harmlessness. They trade off against each other, confusing an RM trained on both. The chosen solution was option 2, where two RMs are used to train the LLM in a piecewise fashion. The helpfulness RM is used as the primary optimization term, while the harmlessness RM acts as a penalty term, driving the behavior of the LLM away from unsafe territory only when the LLM veers beyond a certain threshold. This is formalized as follows, where <span class="math inline">\(R_s\)</span>, <span class="math inline">\(R_h\)</span>, and <span class="math inline">\(R_c\)</span> are the safety, helpfulness, and combined reward, respectively. <span class="math inline">\(g\)</span> and <span class="math inline">\(p\)</span> are the model generation and the user prompt: <span class="math display">\[\begin{aligned}
    R_c(g \mid p) =
    \begin{cases}
        R_s(g \mid p) &amp; \text{if } \text{is\_safety}(p) \text{ or } R_s(g \mid p) &lt; 0.15 \\
        R_h(g \mid p) &amp; \text{otherwise}
    \end{cases}
\end{aligned}\]</span></p>
<p>There are several open issues with reward models alluded to in the paper. For example, how best to collect human feedback? Training annotators and making sure they do the correct thing is hard. What should the guidelines be? Another question is whether RMs can be made robust to adversarial prompts. Last but not least, do RMs have well-calibrated scores? This matters for RLHF - pure preference accuracy isn’t enough.</p>
</section>
<section id="reward-learning-in-robotics" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="reward-learning-in-robotics"><span class="header-section-number">3.1.2</span> Reward Learning in Robotics</h3>
<p>To help set up our basic reward learning problem, consider a user and a robot. The user’s preferences or goals can be represented by an internal reward function, R(<span class="math inline">\(\xi\)</span>), which the robot needs to learn. Since the reward function isn’t explicit, there are a variety of ways that the robot can learn this reward function, which we will discuss in the next section. An example method of learning a reward function from human data is using pairwise comparison. Consider the robot example from section one, but now, the robot shows the human two possible trajectories <span class="math inline">\(\xi_A\)</span> and <span class="math inline">\(\xi_B\)</span> as depicted in the diagram below.</p>
<div id="fig-reward-robot-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-reward-robot-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/robots.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-reward-robot-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3: Two different trajectories taken by a robot to prompt user ranking.
</figcaption>
</figure>
</div>
<p>The user is show both the trajectories above and asked to rank which one is better. Based on iterations of multiple trajectories and ranking, the robot is able to learn the user’s internal reward function. There quite a lot of ways that models can learn a reward function from human data. Here’s a list <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span> of some of them:</p>
<ol type="1">
<li><p>Pairwise comparison: This is the method that we saw illustrated in the previous example. The robot is able to learn based on a comparison ranking provided by the user.</p></li>
<li><p>Expert demonstrations: Experts perform the task and the robot learns the optimal reward function from these demonstrations.</p></li>
<li><p>Sub-optimal demonstrations: The robot is provided with demonstrations that are not quite as good as the expert demonstrations but it is still able to learn a noisy reward function from the demonstrations.</p></li>
<li><p>Physical Corrections: While the robot is performing the task, at each point in its trajectory (or at an arbitrary point in its trajectory) its arm is corrected to a more suitable position. Based on these corrections, the robot is able to learn the reward function.</p></li>
<li><p>Ranking: This method is similar to pairwise comparison but involves more trajectories than 2. All the trajectories may have subtle differences from each other, but these differences help provide insight to the model.</p></li>
<li><p>Trajectory Assessment: Given a single trajectory, the user rates how close it is to optimal, typically using a ranking scale.</p>
<p>Each of these methods allows the robot to refine its understanding of the user’s reward function, but their effectiveness can vary depending on the application. For instance, expert demonstrations tend to produce more reliable results but may not always be feasible in everyday tasks. Pairwise comparison and ranking methods offer more flexibility but might require a higher number of iterations.</p></li>
</ol>
</section>
<section id="reward-learning-with-meta-learning" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="reward-learning-with-meta-learning"><span class="header-section-number">3.1.3</span> Reward Learning with Meta Learning</h3>
<p>Learning a reward function from human preferences is an intricate and complicated task. At its core, this task is about designing algorithms that can capture what humans value based on their elicited preferences. However, due to the nuanced and multifaceted nature of human desires, learning reward functions from human can be a difficult task. Therefore, meta-learning rewards may be considered to facilitate the reward learning processes. Meta-learning, often referred to as “learning to learn,” aims to design models that can adapt to new tasks with minimal additional efforts. We discuss paper <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> in <a href="#sec-few-shot" class="quarto-xref"><span>Section 3.1.3.1</span></a> showing how meta-learning can be leveraged for few-shot preference learning, where a system can quickly adapt to a new task after only a few queries to pairwise preferences from human.</p>
<p>Moving beyond the concept of learning from pairwise preferences, in <a href="#sec-watch" class="quarto-xref"><span>Section 3.1.3.2</span></a> we discuss a different approach where meta-learning intersects with both demonstrations and rewards <span class="citation" data-cites="zhou2019watch">(<a href="#ref-zhou2019watch" role="doc-biblioref">Zhou et al. 2019</a>)</span>. This paper considers the use of both demonstrations and rewards elicited from human that guide the learning process.</p>
<p>In the regular learning setting, a model is fitted to a dataset with certain learning algorithm. The learning algorithm, for example, can be the minimization of a loss function. To formulate the “regular” learning procedure, let’s denote the training dataset as <span class="math inline">\(D\)</span>, and the test dataset as <span class="math inline">\(S\)</span>. Given a model parameterized by <span class="math inline">\(\theta\)</span>; training loss function <span class="math inline">\(L(\theta, D)\)</span>; and test loss function <span class="math inline">\(L(\theta, S)\)</span>, we can formulate a process of “regular” machine learning process as <span class="math display">\[\begin{aligned}
    \theta^\star = \arg\min_\theta\quad L(\theta, D).
\end{aligned}\]</span> Note that the minimization of the training loss function is essentially <em>one</em> possible learning algorithm. For example, instead of minimizing the loss function, one may do gradient descent with model regularization on the loss function, where the final solution may not be the one that actually minimizes the loss function. As a result, we may want to be more general and more abstract for the moment, and denote the learning algorithm as <span class="math inline">\(\mathcal{A}\)</span>. Thus, we can write <span class="math display">\[\begin{aligned}
    \theta^\star = \mathcal{A}(D),
\end{aligned}\]</span> i.e., the learning algorithm <span class="math inline">\(\mathcal{A}\)</span> takes in a training dataset and outputs a model parameter <span class="math inline">\(\theta^\star\)</span>. Then, the performance of the model is evaluated by the test loss <span class="math inline">\(L(\mathcal{A}(D), S)\)</span>. As we can see, in the regime of “regular” learning, the learning algorithm <span class="math inline">\(\mathcal{A}\)</span> is pre-defined and fixed.</p>
<p>Meta-learning, or learning-to-learn, essentially asks the question of whether one can <em>learn</em> the learning algorithm <span class="math inline">\(\mathcal{A}\)</span> from prior tasks, such that the modal can adapt to a new task more quickly/proficiently. For example, different human languages share similar ideas, and therefore a human expert who has learned many languages should be able to learn a new language easier than an average person. In other words, the human expert should have learned how to learn new languages more quickly based on their past experiences on learning languages.</p>
<p>To mathematically formulate meta-learning, we consider a family of learning algorithms <span class="math inline">\(\mathcal{A}_\omega\)</span> parameterized by <span class="math inline">\(\omega\)</span>. The “prior” tasks are represented by a set of meta-training datasets <span class="math inline">\(\{(D_i, S_i)\}_{i=1}^N\)</span> consists of <span class="math inline">\(N\)</span> pairs of training dataset <span class="math inline">\(D_i\)</span> and test dataset <span class="math inline">\(S_i\)</span>. As we noted before, a learning algorithm <span class="math inline">\(\mathcal{A}_\omega\)</span> takes in a training dataset, and outputs a model, i.e., <span class="math display">\[\begin{aligned}
    \forall i: \quad \theta^\star_i=\mathcal{A}_\omega(D_i).
\end{aligned}\]</span></p>
<p>Therefore, the <strong>meta-learning objective</strong> is <span class="math display">\[\begin{aligned}
    \min_\omega \quad \sum_{i}\ L(\mathcal{A}_\omega(D_i), S_i).
\end{aligned}\]</span> The above optimization problem gives a solution <span class="math inline">\(\omega^\star\)</span> which we use as the meta-parameter. Then, when a new task comes with a new training dataset <span class="math inline">\(D_{new}\)</span>, we can simply apply <span class="math inline">\(\theta^\star_{new}=\mathcal{A}_{\omega^\star}(D_{new})\)</span> to obtain the adapted model <span class="math inline">\(\theta^\star_{new}\)</span>. Note that we usually assume the meta-training datasets <span class="math inline">\(D_i, S_i\)</span> and the new dataset <span class="math inline">\(D_{new}\)</span> share the same underlying structure, or they come from the same distribution of datasets.</p>
<p>One of the most popular meta-learning method is Model-Agnosic Meta-Learning (MAML) <span class="citation" data-cites="finn2017model">(<a href="#ref-finn2017model" role="doc-biblioref">Finn, Abbeel, and Levine 2017</a>)</span>. In MAML, the meta-parameter <span class="math inline">\(\omega\)</span> shares the same space as the model parameter <span class="math inline">\(\theta\)</span>. At its core, in MAML the learning algorithm is defined to be <span class="math display">\[\begin{aligned}
    \mathcal{A}_\omega(D_i)=\omega-\alpha \nabla_\omega L(\omega, D_i),
\end{aligned}\]</span> where <span class="math inline">\(\alpha\)</span> is the step size. As we can see, in fact <span class="math inline">\(\omega\)</span> is defined as the initialization of fine-tuning <span class="math inline">\(\theta\)</span>. With a good <span class="math inline">\(\omega\)</span> learned, the model can adapt to a new task very quickly. In general, meta-learning can be summarized as follows: Given data from prior tasks, learn to solve a new task more quickly/proficiently. Given the general nature of meta-learning, one may be curious about whether preference learning can be benefited from meta-learning, which we discuss in the following section.</p>
<section id="sec-few-shot" class="level4" data-number="3.1.3.1">
<h4 data-number="3.1.3.1" class="anchored" data-anchor-id="sec-few-shot"><span class="header-section-number">3.1.3.1</span> Few-Shot Preference Learning for Reinforcement Learning</h4>
<p>Reinforcement learning (RL) in robotics often stumbles when it comes to devising reward functions aligning with human intentions. Preference-based RL algorithms aim to solve this by learning from human feedback, but this often demands a <em>highly impractical number of queries</em> or leads to oversimplified reward functions that don’t hold up in real-world tasks.</p>
<p>To address the impractical requirement of human queries, as we discussed in the previous section, one may apply meta-learning so that the RL agent can adapt to new tasks with fewer human queries. <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> proposes to pre-training models on previous tasks with the meta-learning method MAML <span class="citation" data-cites="finn2017model">(<a href="#ref-finn2017model" role="doc-biblioref">Finn, Abbeel, and Levine 2017</a>)</span>, and then the meta-trained model can adapt to new tasks with fewer queries.</p>
<p>We consider Reinforcement Learning (RL) settings where a state is denoted as <span class="math inline">\(s\in S\)</span>, and action is denoted as <span class="math inline">\(a\in A\)</span>, for state space <span class="math inline">\(S\)</span> and action space <span class="math inline">\(A\)</span>. The reward function <span class="math inline">\(r:S\times A \to \mathbb{R}\)</span> is unknown and need to be learned from eliciting human preferences. There are multiple tasks, where each task has its own reward function and transition probabilities. The reward model is parameterized by <span class="math inline">\(\psi\)</span>. We denote <span class="math inline">\(\hat{r}_\psi(s,a)\)</span> to be a learned estimate of an unknown ground-truth reward function <span class="math inline">\(r(s,a)\)</span>, parameterized by <span class="math inline">\(\psi\)</span>. Accordingly, a reward model determines a RL policy <span class="math inline">\(\phi\)</span> by maximizing the accumulated rewards. The preferences is learned via pairwise comparison of trajectory segments <span class="math display">\[\begin{aligned}
    \sigma = (s_t, a_t, s_{t+1}, a_{t+1}, ..., s_{t+k-1}, s_{t+k-1})
\end{aligned}\]</span> of <span class="math inline">\(k\)</span> states and actions.</p>
<p>For each pre-training task, there is a dataset <span class="math inline">\(D\)</span> consists of labeled queries <span class="math inline">\((\sigma_1, \sigma_2, y)\)</span> where <span class="math inline">\(y\in \{0, 1\}\)</span> is the label representing which trajectory is preferred. Therefore, a loss function <span class="math inline">\(L(\psi, D)\)</span> captures how well the reward model characterizes the preferences in dataset <span class="math inline">\(D\)</span>. In <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> they the preference predictor over segments using the Bradley-Terry model of paired comparisons <span class="citation" data-cites="bradley1952rank">(<a href="#ref-bradley1952rank" role="doc-biblioref">Bradley and Terry 1952</a>)</span>, i.e., <span class="math display">\[\begin{aligned}
    P[\sigma_1 \succ \sigma_2 ] = \frac{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1})}{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1}) + \exp \sum_t \hat{r}_\psi(s_t^{2}, a_t^{2})}.
\end{aligned}\]</span> Then, the loss function is essentially a binary cross-entropy which the reward model <span class="math inline">\(\psi\)</span> aims to minimize, i.e., <span class="math display">\[\begin{aligned}
    {L}(\psi,  {D}) = - \mathbb{E}_{(\sigma^1, \sigma^2, y) \sim {D}} \left[ y(1) \log (P[\sigma_1 \succ \sigma_2 ]) + y(2)\log(1 - P[\sigma_1 \succ \sigma_2 ]) \right].
\end{aligned}\]</span></p>
<section id="method-component-1-pre-training-with-meta-learning" class="level5" data-number="3.1.3.1.1">
<h5 data-number="3.1.3.1.1" class="anchored" data-anchor-id="method-component-1-pre-training-with-meta-learning"><span class="header-section-number">3.1.3.1.1</span> Method Component 1: Pre-Training with Meta Learning</h5>
<p>To efficiently approximate the reward function <span class="math inline">\(r_\text{new}\)</span> for a new task with minimal queries, as described in <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span>, we aim to utilize a pre-trained reward function <span class="math inline">\(\hat{r}_\psi\)</span> that can be quickly fine-tuned using just a few preference comparisons. By pre-training on data from prior tasks, we can leverage the common structure across tasks to speed up the adaptation process. Although any meta-learning method is compatible, <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> opt for Model Agnostic Meta-Learning (MAML) due to its simplicity. Therefore, the pre-training update for the reward model <span class="math inline">\(\psi\)</span> is <span class="math display">\[\begin{aligned}
    \psi \xleftarrow{} \psi - \beta \nabla_\psi \sum_{i = 1}^N {L} (\psi - \alpha \nabla_\psi {L}(\psi, {D}_i), {D}_i),
\end{aligned}\]</span> where <span class="math inline">\(\alpha, \beta\)</span> are the inner and outer learning rate, respectively. We note that data <span class="math inline">\(\{D_i\}_i\)</span> of labeled preferences queries for prior tasks can come from offline datasets, simulated policies, or actual humans.</p>
</section>
<section id="method-component-2-few-shot-adaptation" class="level5" data-number="3.1.3.1.2">
<h5 data-number="3.1.3.1.2" class="anchored" data-anchor-id="method-component-2-few-shot-adaptation"><span class="header-section-number">3.1.3.1.2</span> Method Component 2: Few-Shot Adaptation</h5>
<p>With the aforementioned pre-training with meta learning, the meta-learned reward model can then be used for few-shot preference based RL during an online adaptation phase. The core procedure of the few-shot adaption is descibed as below</p>
<ol type="1">
<li><p>Given a pre-trained reward model <span class="math inline">\(\psi\)</span></p></li>
<li><p>For time step <span class="math inline">\(t=1, 2, \dots\)</span></p>
<ol type="1">
<li><p>Find pairs of trajectories <span class="math inline">\((\sigma_1, \sigma_2)\)</span> with preference uncertainty based on <span class="math inline">\(\psi\)</span>.</p></li>
<li><p>Query human preference <span class="math inline">\(y\)</span> and forms a new dataset <span class="math inline">\(D_{new}\)</span></p></li>
<li><p>Update the reward model by <span class="math inline">\(\psi'\leftarrow \psi - \alpha \nabla_\psi L(\psi, D_{new})\)</span></p></li>
<li><p>Update the policy with the new reward model <span class="math inline">\(\psi'\)</span></p></li>
</ol></li>
</ol>
<p>As mentioned in <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span>, uncertain queries are selected using the disagreement of an ensemble of reward functions over the preference predictors. Specifically, comparisons that maximize <span class="math inline">\(\texttt{std}(P[\sigma_1 \succ \sigma_2])\)</span> are selected each time feedback is collected.</p>
<p>The whole pipeline of the method is outlined in <a href="#fig-few-1" class="quarto-xref">Figure&nbsp;<span>3.4</span></a>.</p>
<div id="fig-few-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-few-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/overview-few.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-few-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: An overview of the proposed method in <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span>. <strong>Pre-training (left):</strong> In the pre-training phase, trajectory segment comparisons are generated using data from previously learned tasks. Then, they are used to train a reward model. <strong>Online-Adaptation (Right)</strong>: After pre-training the reward model, it is adapted to new data from human feedback. The adapted reward model is then used to train a policy for a new task in a closed loop manner.
</figcaption>
</figure>
</div>
<p>We present one set of experiment from the paper, as it illustrates the effectiveness of the proposed method in a straightforward way. The experiment test the propoesed method on the Meta-World benchmark <span class="citation" data-cites="yu2020meta">(<a href="#ref-yu2020meta" role="doc-biblioref">Yu et al. 2020</a>)</span>. Three baselines are compared with the proposed method:</p>
<ol type="1">
<li><p>SAC: The Soft-Actor Critic RL algorithm trained from ground truth rewards. This represents the standard best possible method given the ground-truth reward.</p></li>
<li><p>PEBBLE: The PEBBLE algorithm <span class="citation" data-cites="lee2021pebble">(<a href="#ref-lee2021pebble" role="doc-biblioref">Lee, Smith, and Abbeel 2021</a>)</span>. It does not use information from pripor tasks.</p></li>
<li><p>Init: This method initialize the reward model with the pretained weights from meta learning. However, instead of adapting the reward model to the new task, it performs standard updates as in PEBBLE.</p></li>
</ol>
<p>The results are shown in <a href="#fig-few-exp" class="quarto-xref">Figure&nbsp;<span>3.5</span></a>, where we can see that the proposed methord outperforms all of the baselines.</p>
<div id="fig-few-exp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-few-exp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/few-exp.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-few-exp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.5: Results on MetaWorld tasks. The title of each subplot indicates the task and number of artificial feedback queries used in training. Results for each method are shown across five seeds.
</figcaption>
</figure>
</div>
<p>This paper <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> shows that meta reward learning indeed reduce the number of queries of human preferences. However, as mentioned in the paper, there are still some drawbacks, as shown in the following.</p>
<p>Many of the queries the model pick for human preference elicitation are actually almost identical to human. After all, the model would pick the most uncertain pair of trajectories for human preference queries, and similar trajectories are for sure having high uncertainty in their preference. This suggest the need of new ways for designing the query selection strategy.</p>
<p>Moreover, despite the improved query complexity, it still needs an impractical amount of queries. As shown in <a href="#fig-few-exp" class="quarto-xref">Figure&nbsp;<span>3.5</span></a>, the “sweep into” task still needs 2500 human queries for it to work properly, which is still not ideal for what we want them to be.</p>
<p>In addition, it is mentioned in the paper that the proposed method may be even worse than training from scratch, if the new task is too out-of-distribution. Certainly, since meta-learning assumes in-distribution tasks, we cannot expect the proposed method to be good for out-of-distribution task. It is thus an interesting future direction to investigate whether one can design a method that automatically balance between using the prior information or training from scratch.</p>
</section>
</section>
<section id="sec-watch" class="level4" data-number="3.1.3.2">
<h4 data-number="3.1.3.2" class="anchored" data-anchor-id="sec-watch"><span class="header-section-number">3.1.3.2</span> Watch Try Learn</h4>
<p>Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards <span class="citation" data-cites="zhou2019watch">(<a href="#ref-zhou2019watch" role="doc-biblioref">Zhou et al. 2019</a>)</span> asks the question “How can we efficiently learn both from expert demonstrations and from trials where we only get <strong>binary</strong> feedback from a human". Why do we care about this question? In the context of robotics, a very compelling answer is the <em>cost of data-collection</em>. In a hypothetical world in which we have a vast number of <strong>expert demonstrations</strong> of robots accomplishing a large number of diverse tasks, we don’t necessarily need to worry about learning from trials or from humans. We could simply learn a very capable imitation agent to perform any task. Natural Language Processing could be seen as living in this world, because internet-scale data is available. <strong>Robots, however, are expensive</strong>, so people generally don’t have access to them, and therefore cannot use them to produce information to imitate. Similarly, <strong>human time is expensive</strong>, so even for large organizations that do have access to a lot of robots, it’s still hard to collect a lot of expert demonstrations.</p>
<p>The largest available collection of robotics datasets today is Open X-Embodiment (<span class="citation" data-cites="padalkar2023open">(<a href="#ref-padalkar2023open" role="doc-biblioref">Padalkar et al. 2023</a>)</span>), which consists of around 1M episodes from more than 300 different scenes. Even such large datastes are not enough to learn generally-capable robotic policies from imitation learning alone.</p>
<div id="fig-open-x-embodiment" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-open-x-embodiment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/open_x_embodiment.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-open-x-embodiment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.6: Visualization of the Open X-Embodiment dataset collection. Even this large-scale dataset for robot learning is not yet enough to learn generally-capable robotic policies.
</figcaption>
</figure>
</div>
<p><strong>Main insight:</strong> binary feedback is much cheaper to obtain than expert demonstrations! Instead of hiring people to act as robot operators to tell the robot exactly what to do, if there was a way of having many robots trying things in parallel, we can have humans watch videos of what the robots did and then give a success classification of whether the robot accomplished the goal. This is a much cheaper form of human supervision because the human labels don’t necessarily need to be given in real time, so one human labeler can label many trajectories in parallel, and the human doesn’t need to be a skilled robot operator.</p>
<p>Concretely, this paper seeks to learn new tasks with the following general problem setting:</p>
<ol type="1">
<li><p>We only get 1 expert demonstration of the target task</p></li>
<li><p>After seeing the expert demonstration, we have robots try to solve the task 1 or more times.</p></li>
<li><p>The user (or some pre-defined reward function) annotates each trial as success/failure.</p></li>
<li><p>The agent learns from both the demos and the annotated trials to perform well on the target task.</p></li>
</ol>
<p>Note that this work falls under the <strong>meta-learning</strong> umbrella, because we are learning an algorithm for quickly learning new tasks given new observations (demos, trials, and success labels.)</p>
<p>The <strong>main contribution</strong> of this paper is a meta-learning algorithm for incorporating demonstrations and binary feedback from trials to solve new tasks.</p>
<p>Meta-Learning deals with efficient learning of new tasks. In the context of robotics or reinforcement learning in general, <strong>how do we define tasks</strong>? We will use the Markov decision process (<strong>MDP</strong>) formalism. A task <span class="math inline">\(T_i\)</span> is described with the tuple <span class="math inline">\(\{S, A, r_i, P_i\}\)</span>.</p>
<ol type="1">
<li><p><span class="math inline">\(S\)</span> represents the <em>state-space</em> of the task, or all possible states the agent could find itself in. This work uses image-observations, so <span class="math inline">\(S\)</span> is the space of all possible RGB images.</p></li>
<li><p><span class="math inline">\(A\)</span> is the action space, meaning the set of all possible actions the agent could take. In robotics there are many ways of representing action spaces, and this work considers end-effector positions, rotations, and opening.</p></li>
<li><p><span class="math inline">\(r_i\)</span> is the reward function for the task, with function signature <span class="math inline">\(r_i : S \times A \to \mathbb{R}\)</span>. This work assumes all reward functions are binary.</p></li>
<li><p><span class="math inline">\(P_i\)</span> is the transition dynamics function. It’s a function that maps state-action pairs to probability distributions over next states.</p></li>
</ol>
<p>Notice that <span class="math inline">\(S\)</span> and <span class="math inline">\(A\)</span> are shared across tasks. Transition dynamics functions are normally also shared between tasks because they represent the laws of physics. However, this work considers environments with different objects, so they don’t share the dynamics function. Given this definition for tasks, they assume that the tasks from the data that they get come from some unknown task-generating distribution <span class="math inline">\(p(T)\)</span>.</p>
<p>Let’s give a more precise definition of the problem statement considered by <strong>Watch, Try, Learn</strong>. As the paper name suggests, there are 3 phases for the problem statement.</p>
<p><strong>Watch:</strong> During the <em>watch</em> phase, we give the agent <span class="math inline">\(K\)</span> demonstrations of the target tasks. This paper considers the case where <span class="math inline">\(K\)</span> always equals 1, and all demonstrations are successful. That is, each demonstration consists of a trajectory <span class="math inline">\(\{(s_0, a_0), \ldots, (s_H, a_H)\}\)</span> where <span class="math inline">\(H\)</span> is the task horizon, and the final state is always successful, that is <span class="math inline">\(r_i(s_H, a_H) = 1, r_i(s_j, a_j) = 0\)</span> for every <span class="math inline">\(j \neq H\)</span>.</p>
<p>Importantly, these demonstrations alone might not be sufficient for <strong>full task specification</strong>. As an example, consider a demonstration in which an apple is moved to the right, next to a pan. Seeing this demonstration alone, the task could be always moving the apple to the right, or it could be always moving the apple next to the pan, irrespective of where the pan is. The expected output after the Watch phase is a policy capable of gathering information about a task, given demonstrations.</p>
<p><strong>Try:</strong> In the Try phase, we use the agent learned during the Watch phase to attempt the task for <span class="math inline">\(L\)</span> trials. As specified earlier, this paper considers the casae where <span class="math inline">\(L\)</span> always equals 1. After the agent completes the trials, humans (or pre-programmed reward functions) provide one binary reward for each trial, indicating whether the trial was successful. The expected output of this phase is <span class="math inline">\(L\)</span> trajectories and corresponding feedback that hopefully <em>disambiguate</em> the task.</p>
<p><strong>Learn:</strong> After completing the trials, the agent must learn from both the original expert demonstrations and the trials, and become capable of solving the target task.</p>
<p><strong>Given Data:</strong> To train agents that can Watch, Try, and Learn, we are given a dataset of expert demonstrations containing multiple demos for each task, and the dataset contains hundreds of tasks. Importantly, <strong>no online interaction</strong> is needed for training, and this method trains only with <strong>supervised learning</strong> and no reinforcement learning.</p>
<p>This section describes exactly how this paper trains an agent from the given expert demonstrations, and how to incorporate the trials and human feedback into the loop.</p>
<p><strong>Training to Watch:</strong> We now describe the algorithm to obtain an agent conditioned on the given expert demonstration. In particular, what we want to obtain out of the Watch phase is a policy conditioned on a set of expert demonstrations. Formally, we want to obtain <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{d_{i,k}\})\)</span>.</p>
<p>The way we can obtain this policy is through <strong>meta-imitation learning</strong>. Given the demonstrations <span class="math inline">\(\{\textbf{d}_{i,k}\}\)</span> for task <span class="math inline">\(i\)</span>, we sample another <em>different</em> demonstration coming from the same task <span class="math inline">\(\textbf{d}_i^{\text{test}}\)</span>. The key insight here is that <span class="math inline">\(\textbf{d}_i^{\text{test}}\)</span> is an example of <strong>optimal behavior</strong> given the demonstrations. Therefore, to obtain <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{d_{i,k}\})\)</span>, we simply regress the policy to imitate actions taken on <span class="math inline">\(\textbf{d}_i^{\text{test}}\)</span>. Concretely, we train policy parameters <span class="math inline">\(\theta\)</span> to minimize the following loss:</p>
<p><span class="math inline">\(\mathcal{L}^\text{watch}(\theta, \mathcal{D}_i^*) = \mathbb{E}_{\{d_{i,k}\} \sim \mathcal{D}_i^*} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^*  \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[
- \log \pi_\theta^{\text{watch}} (a_t | s_t, \{d_{i,k}\}) \big]\)</span></p>
<p>This corresponds to doing imitation learning by minimizing the negative log-likelihood of the test trajectory actions, conditioning the policy on the entire demo set. However, how is the conditioning on the demo set achieved?</p>
<div id="fig-watch-try-learn-arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-watch-try-learn-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/watch-try-learn-architecture.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-watch-try-learn-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.7: Vision-based policy architecture that conditions on a set of demonstrations.
</figcaption>
</figure>
</div>
<p><a href="#fig-watch-try-learn-arch" class="quarto-xref">Figure&nbsp;<span>3.7</span></a> visualizes how Watch Try Learn deals with conditioning on demonstrations. In addition to using features obtained from the images of the current state, the architecture uses features from frames sampled (in order) from the demonstration episodes, which are concatenated together.</p>
<p><strong>Trying:</strong> On the <strong>Try</strong> phase, when the agent is given a set of demonstrations <span class="math inline">\(\{\textbf{d}_{i,k}\}\)</span>, we deploy the policy <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{\textbf{d}_{i,k}\})\)</span> to collect <span class="math inline">\(L\)</span> trials. There is no training involved in the Try phase, we simply condition the policy on the given demonstrations</p>
<p><strong>Training to Learn:</strong> During the Watch phase the objective was to train a policy conditioned on demonstrations <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{\textbf{d}_{i,k}\})\)</span>. The authors of Watch, Try, Learn use a similar strategy as the Watch phase for the Learn phase. We now want to train a policy that is conditioned on the demonstrations, as well as the trials and binary feedback. That is, we want to learn <span class="math inline">\(\pi_\phi^{\text{watch}}(a | s, \{\textbf{d}_{i,k}\}, \{\mathbf{\tau}_{i, l}\})\)</span>. To train the policy, we again use meta-imitation learning where we additionally sample yet another trajectory from the same task. Concretely, we train policy parameters <span class="math inline">\(\phi\)</span> to minimize the following loss:</p>
<p><span class="math inline">\(\mathcal{L}^{\text{learn}}(\phi, \mathcal{D}_i, \mathcal{D}_i^*) = \mathbb{E}_{(\{d_{i,k}\}, \{\mathbf{\tau}_{i,l}\}) \sim \mathcal{D}_i} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^* \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[
- \log \pi_\theta^{\text{learn}} (a_t | s_t, \{d_{i,k}\}, \{\tau_{i,l}\}) \big]\)</span></p>
<p>The conditioning on both the demo episodes and the trial episodes is achieved in the exact same way as in the Watch phase, and is visualized in <a href="#fig-watch-try-learn-arch" class="quarto-xref">Figure&nbsp;<span>3.7</span></a>. The architecture is simply adjusted to be able to take in more images fro mthe trial episodes.</p>
<p>In this section, we describe the evaluation suite for the paper, including the simulation benchmark used, the baselines considered, and the results.</p>
<p><strong>Gripper environment setup:</strong></p>
<div id="fig-envs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-envs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/watch-try-learn-envs.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-envs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.8: Visualization of different tasks from the simulated benchmark for Watch Try Learn.
</figcaption>
</figure>
</div>
<p><a href="#fig-envs" class="quarto-xref">Figure&nbsp;<span>3.8</span></a> illustrates the different task families considered in the simulated Gripper environment. Button Pressing, Grasping, Pushing, and Pick and Place. For each task family, the environment supports hundreds of different tasks by changing the objects in the scene and the objectives (e.g.&nbsp;which object to pick and where to place). For each task in each task family, a handful of expert demonstrations are given in a demonstrations dataset. As mentioned previously, the environment gives the agent image observations, and take in actions as end-effector (gripper) positions, angles, and opening.</p>
<p><strong>Baselines:</strong> The following three baselines are considered:</p>
<ol type="1">
<li><p><strong>Behavior Cloning</strong>: simple imitation learning based on maximum log-likelihood training using data from all tasks.</p></li>
<li><p><strong>Meta-imitation learning</strong>: This baseline corresponds to simply running the policy from the Watch step, without using any trial data. That is, we only condition on the set of expert demonstrations, but no online trials.</p></li>
<li><p><strong>Behavior Cloning + SAC</strong>: Pre-train a policy with Behavior Cloning on all data, and follow that with Reinforcement Learning fine-tuning for the specific target task, using the maximum-entropy algorithm SAC (<span class="citation" data-cites="haarnoja2018soft">(<a href="#ref-haarnoja2018soft" role="doc-biblioref">Haarnoja et al. 2018</a>)</span>).</p></li>
</ol>
<div id="fig-watch-try-learn-results" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-watch-try-learn-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/watch-try-learn-results.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-watch-try-learn-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.9: Results for Watch Try Learn on the gripper control environment, and comparisons with baselines.
</figcaption>
</figure>
</div>
<div id="tbl-watch-try-learn-table" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-watch-try-learn-table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.2: Average success rates over all tasks.
</figcaption>
<div aria-describedby="tbl-watch-try-learn-table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"><strong>METHOD</strong></th>
<th style="text-align: center;"><strong>SUCCESS RATE</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">BC</td>
<td style="text-align: center;">.09 <span class="math inline">\(\pm\)</span> .01</td>
</tr>
<tr class="even">
<td style="text-align: left;">MIL</td>
<td style="text-align: center;">.30 <span class="math inline">\(\pm\)</span> .02</td>
</tr>
<tr class="odd">
<td style="text-align: left;">WTL, 1 TRIAL (OURS)</td>
<td style="text-align: center;">.42 <span class="math inline">\(\pm\)</span> .02</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>RL FINE-TUNING WITH SAC</strong></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">BC + SAC, 1500 TRIALS</td>
<td style="text-align: center;">.11 <span class="math inline">\(\pm\)</span> .07</td>
</tr>
<tr class="even">
<td style="text-align: left;">BC + SAC, 2000 TRIALS</td>
<td style="text-align: center;">.29 <span class="math inline">\(\pm\)</span> .10</td>
</tr>
<tr class="odd">
<td style="text-align: left;">BC + SAC, 2500 TRIALS</td>
<td style="text-align: center;">.39 <span class="math inline">\(\pm\)</span> .11</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#fig-watch-try-learn-results" class="quarto-xref">Figure&nbsp;<span>3.9</span></a> shows average success rates for Watch Try Learn compared to baselines. Watch Try Learn significantly outperforms baselines on every task family. In particular, it is far superior to Behavior Cloning, which is a very weak baseline, and it significantly surpasses Meta-Imitation Learning on 3 out of 4 task families. <a href="#tbl-watch-try-learn-table" class="quarto-xref">Table&nbsp;<span>3.2</span></a> includes comparison with BC fine-tuned with Reinforcement Learning. Even after 2500 online trials, SAC is not able to obtain the success rate that Watch Try Learn achieves after only 1 trial. Overall, Watch Try Learn exhibits very significant performance gains over prior methods.</p>
</section>
</section>
<section id="direct-preference-optimization" class="level3" data-number="3.1.4">
<h3 data-number="3.1.4" class="anchored" data-anchor-id="direct-preference-optimization"><span class="header-section-number">3.1.4</span> Direct Preference Optimization</h3>
<p>A modern method for estimating the parameters of a human preference model is direct preference optimization <span class="citation" data-cites="rafailov2023direct">(<a href="#ref-rafailov2023direct" role="doc-biblioref">Rafailov et al. 2023</a>)</span>, which is used in the context of aligning language models to human preferences. A recent approach <span class="citation" data-cites="christiano2023deep">(<a href="#ref-christiano2023deep" role="doc-biblioref">Christiano et al. 2023</a>)</span> first trains a reward model that captures human preferences and then uses proximal policy optimization to train a language model-based policy to reflect those learned preferences. Direct Preference Optimization (DPO), on the other hand, removes the need for a reward model by directly using the model likelihood of two outcomes (a preferred or highly-ranked sequence and an unpreferred or low-ranked sequence) to capture the preference represented in the data. DPO provides a simpler framework than its reinforcement learning approach and results in comparable performance with improved stability. Furthermore, it obviates the need to train a reward model, instead using a language model policy and human preference dataset to align the policy directly to human preferences.</p>
</section>
<section id="model-design-consideration" class="level3" data-number="3.1.5">
<h3 data-number="3.1.5" class="anchored" data-anchor-id="model-design-consideration"><span class="header-section-number">3.1.5</span> Model Design Consideration</h3>
<p>When designing models and learning their parameters, one must account for important tradeoffs when designing and optimizing a model to learn human preferences.</p>
<p><strong>Bias vs.&nbsp;Variance Trade-off.</strong> In modeling human preferences, we aim to ensure that predicted utilities accurately reflect overall human preferences. One key challenge is managing the bias and variance trade-off.</p>
<p>Bias refers to assumptions made during model design and training that can skew predictions. For example, in Ideal Point Models, we make the assumption that the representations we use for individuals and choices are aligned in the embedding space, and that this representation is sufficient to capture human preferences using distance metrics. However, there are myriad cases in which this may break down, for example if the two sets of vectors follow different distributions each with their own unique biases. If the representations do not come from the same domain, one may have little visibility into how a distance metric computes the final utility value for a choice for a given individual. Some ways to mitigate bias in human preference models include increasing the number of parameters in a model (allowing for better learning of patterns in the data) or removing inductive biases based on our assumptions of the underlying data.</p>
<p>On the other hand, variance refers to the model’s sensitivity to small changes in the input, which leads to significant changes in the outp ut. This phenomenon is often termed ‘overfitting’ or ‘overparameterization.’ This behavior can occur in models that have many parameters, and learn correlations in the data that do not contribute to learning human preferences, but are artifacts of noise in the dataset that one should ultimately ignore. One can address variance in models by reducing the number of parameters or incorporating biases in the model based on factors we can assume about the data.</p>
<p><strong>Model Scope.</strong> One important consideration unique to human preference models is that we wish to model individual preferences, and we may choose to do so at arbitrary granularity. For example, we can fit models to a specific individual or even multiple models for an individual, each for different purposes or contexts. On the other end of the spectrum, we may create a model to capture human preferences across large populations or the world.</p>
<p>Individual models may certainly prove to be more powerful, as they do not need to generalize across multiple individuals and can dedicate all of their parameters to learning the preferences of a single user. In the context of human behavior, this can be a significant advantage as any two individuals can be arbitrarily different or even opposite in their preferences. On the other hand, models fit only one person can tremendously overfit to the training distribution and capture noise in the data, which is not truly representative of human preferences.</p>
<p>On the end of the spectrum, models fit to the entire world may be inadequate to model human preferences for arbitrary individuals, especially those whose data it has not been fit to. As such, models may underfit the given training distribution. These models aim to generalize to many people but may fail to capture the nuances of individual preferences, especially for those whose data is not represented in the training set. As a result, they may not perform well for arbitrary individuals within the target population</p>
<p>Choosing the appropriate scope for a model is crucial. ne must balance the trade-off between overfitting to noise in highly granular models and underfitting in broader models that may not capture individual nuances.</p>
</section>
</section>
<section id="multimodal-preferences" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="multimodal-preferences"><span class="header-section-number">3.2</span> Multimodal Preferences</h2>
<p>One of the core assumptions about learning a reward function is that it is unimodal, meaning that it consists of data from one person with a certain set of preferences or a group of people with similar preferences. However, the model of unimodality often oversimplifies human preferences and their often conflicting nature. To accurately capture all the nuances of human preference, we examine a multi-modal distribution with some baseline assumptions. Consider a scenario where we, as regular drivers, make a left-hand turn at an intersection <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span>. What would we do if we saw a car speeding down the road approaching us? The figure below describes some options. Following a timid driving pattern, some vehicles would stop to let the other car go, preventing a collision. Other vehicles would be more aggressive and try to make the turn before colliding with the oncoming vehicle. Given the data of one of these driving patterns, our model (our autonomous vehicle) can make an appropriate decision. However, what if our model was given data from both aggressive and timid drivers, and we don’t know which data corresponds to which type of driver? If we applied standard learning based on comparison techniques, we see, as illustrated by the figure below, that the car would have an accident trying to find a policy close enough to both driving patterns.</p>
<div id="fig-driving-patt" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-driving-patt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/driving-patt.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-driving-patt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.10: <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span> shows the possibilities of 2 different driving patterns when a car is taking a left-hand turn at an intersection and sees another car approaching head-on.
</figcaption>
</figure>
</div>
<div id="fig-driving-coll" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-driving-coll-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/driving-coll.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-driving-coll-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.11: The figure <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span> depicts the resultant collision when we try to find a policy close enough to both the driving patterns.
</figcaption>
</figure>
</div>
<p>As illustrated by the driving example, we see that multi-modality for our reward function is extremely important and, in some cases, if it is not considered, can lead to fatal decisions <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span>. But why can’t we label the groups, which would be the timid and aggressive drivers in the driving case, and then learn separate reward functions for each driver? The first problem with this approach is that it is inefficient and time-consuming to separate the data into groups because we would have to cluster and label the data. Secondly, it would not be accurate just to split the data because a more timid driver can be aggressive when they are in a hurry.</p>
<p>To formulate this problem of learning reward functions and mixing coefficients from ranking queries in a fully observable deterministic dynamical system, we begin by describing the system as a trajectory <span class="math inline">\(\xi = (s_0, a_0, ..., s_T, a_T)\)</span>, where the sequence of states and actions represents the system’s evolution over time. Assume there are <span class="math inline">\(M\)</span> different reward functions, each representing an expert’s preferences. Using the linearity assumption in reward learning, we model each expert’s reward function as a linear combination of features in a known, fixed feature space <span class="math inline">\(\phi(\xi)\)</span>. The reward for the <span class="math inline">\(m\)</span>-th expert is given by: <span class="math display">\[R_m(\xi) = \omega^T_m \phi(\xi),\]</span> where <span class="math inline">\(\omega_m\)</span> is a vector of parameters corresponding to the <span class="math inline">\(m\)</span>-th expert’s preferences. There exists an unknown distribution over the reward parameters and we can represent this distribution with mixing coefficients <span class="math inline">\(\alpha_m\)</span> such that <span class="math inline">\(\sum_M^{m = 1} \alpha_m = 1\)</span>. Our goal is to learn reward functions and mixing coefficients using ranking queries.</p>
<p>To define our problem, let’s consider a robot who performs the following trajectories and asks a user to rank all the trajectories.</p>
<div id="fig-robot-traj" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-robot-traj-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/robot-traj.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-robot-traj-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.12: The figure <span class="citation" data-cites="myers2022learning">(<a href="#ref-myers2022learning" role="doc-biblioref">Myers et al. 2022</a>)</span> depicts a few different trajectories for an example multi-modal ranking scenario.
</figcaption>
</figure>
</div>
<p>The robot will be given back a set of trajectory rankings, coming from M humans and the objective is to learn the underlying reward function. We can represent the response of the ranking query as <span class="math inline">\(x = (\xi_{a_1},\ ...\ ,\xi_{a_K})\)</span> where <span class="math inline">\(a_1\)</span> is the index of the expert’s top choice, <span class="math inline">\(a_2\)</span> is the index of the expert’s second choice, ... and so on. With the response <span class="math inline">\(x\)</span>, we generate a probability distribution with the softmax rule <span class="citation" data-cites="myers2022learning">(<a href="#ref-myers2022learning" role="doc-biblioref">Myers et al. 2022</a>)</span>: <span class="math inline">\(Pr(x_1 = \xi_{a_1} | R = R_m) = \frac{e^R_m(\xi_{a_1})}{\sum_{j=1}^Ke^R_m(\xi_{a_j})}\)</span>. where <span class="math inline">\(R_m(\xi_{a_i})\)</span> denotes the reward assigned by the <span class="math inline">\(m\)</span>-th expert to trajectory <span class="math inline">\(\xi_{a_i}\)</span>. Then, we randomly sample our probability distribution to pick our top choice. From the remaining trajectories, we noisily choose from our distribution to rank our second-best option. We repeat this process until we have ranked all our trajectories. This follows what is known as the Plackett-Luce Ranking Model.</p>
<p>Given knowledge of the true reward function weights <span class="math inline">\(\omega_m\)</span> and mixing coefficients <span class="math inline">\(\alpha_m\)</span>, we have the following joint mass over observations x from a query Q: <span class="math inline">\(Pr(x\ |\ Q) = \sum_{m = 1}^M \alpha_m\prod_{i = 1}^K\frac{e^{\omega_m^T \Phi(\xi_{a_i})}}{\sum_{j = i}^K e^{\omega_m^T \Phi(\xi_{a_j})}}\)</span>.</p>
<p>With the above formulation of the joint mass distribution over observation and queries, we can now formulate an objective. Specifically, it is to present users with the best set of queries that learn reward weights, <span class="math inline">\(\omega\)</span>, and mixing coefficient, <span class="math inline">\(\alpha\)</span>, based upon user rankings of preferred query responses. By learning these parameters, we can have an accurate estimation of the joint mass distribution of the observations.</p>
<p>To learn these parameters, we use a Bayesian learning framework. The goal will be to learn the reward weights, <span class="math inline">\(\omega_m\)</span>, and all mixing coefficients <span class="math inline">\(\alpha_m\)</span>. Thus, define the parameters to be <span class="math inline">\(\theta = \{\omega, \alpha\}\)</span>. We start by simplifying the posterior over the parameters.</p>
<p><span class="math display">\[\begin{aligned}
\Pr(\Theta | Q^{(1)}, x^{(1)}, Q^{(2)}, x^{(2)}, \ldots) &amp; \propto \Pr(\Theta) \Pr(Q^{(1)} | x^{(1)}, Q^{(2)}, x^{(2)}, \ldots | \Theta) \\
&amp; = \Pr(\Theta) \prod_t \Pr(x^{(t)} | Q^{(t)}, \Theta, Q^{(1)}, x^{(1)}, \ldots, Q^{(t-1)}, x^{(t-1)}) \\
&amp; \propto \Pr(\Theta) \prod_t \Pr(x^{(t)} | \Theta, Q^{(t)})
\end{aligned}\]</span></p>
<p>Note that the first proportionality term is directly from Bayes rule (removing normalization constant). The first equation comes directly from the assumption that the queries at timestamp <span class="math inline">\(t\)</span> are conditionally independent of the parameters given previous queries &amp; rankings. This assumption is reasonable because the previous queries &amp; rankings ideally give all the information to inform the choice of the next set of. The last proportionality term comes from the assumption that the ranked queries are conditionally independent given the parameters</p>
<p>The prior distribution is dependent on use case. For example, in the user studies conducted by the authors to verify this method, they use a standard Gaussian for the reward weights and the mixing coefficients to be uniform on a <span class="math inline">\(M - 1\)</span> simplex to ensure that they add up to 1. Then we can use maximum likelihood estimation to compute the parameters with the simplified posterior.</p>
<!--{{< include psets/pset1.qmd >}}-->
<!--
Through our exploration of human preference models, we will ground ourselves in
building a health coaching system that can provide meal recommendations aligned with a user's dietary needs and preferences. Examples of scenarios which can benefit from a model of how humans make choices include:

1.  **Health coaching:** Humans express their preferences every time
    they pick lunch for consumption. Humans may have several goals
    related to nutrition, such as weight loss and improving
    concentration. We can learn how a given individual or set of
    individuals prefer to eat to provide personalized recommendations to
    help them attain their goals. This chapter will use this use case to
    ground human preference modeling in a real-life application.

2.  **Social media:** Platforms have a far greater amount of content
    than one can consume in a lifetime, yet such products must aim to
    maximize user engagement. To accomplish this, we can learn what
    specific things people like to see in their feeds to optimize the
    value they gain out of their time on social media. For example, the
    video feed social media platform [TikTok](https://www.tiktok.com/)
    has had viral adoption due to its notorious ability to personalize a
    feed for its users based on their preferences.

3.  **Shopping:** Retail corporations largely aim to maximize revenue by
    making it easy for people to make purchases. Recommendation systems
    on online shopping platforms provide a mechanism for curating
    specific items based on an individual's previous purchases (or even
    browsing history) to make shoppers aware of items they may like and,
    therefore, purchase.

Two key models used in pairwise sampling are the Thurstonian and Bradley-Terry models [@cattelan2012]. The Thurstonian model assumes each item $i$ has a true score $u_i$ following a normal distribution. The difference $d_{ij} = u_i - u_j$ is also normally distributed. The probability that item $i$ is preferred over item $j$ is given by $P(i \succ j) = \Phi \left( \frac{u_i - u_j}{\sqrt{2\sigma^2}} \right)$, where $\Phi$ is the cumulative normal distribution function. The denominator $\sqrt{2\sigma^2}$ is the standard deviation of the difference $d_{ij} = u_i - u_j$ when $u_i$ and $u_j$ are normally distributed with variance $\sigma^2$[@cattelan2012]. The Bradley-Terry model defines the probability of preference based on latent scores $\beta_i$ and $\beta_j$. The probability that item $i$ is preferred over item $j$ is $P(i \succ j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}$. This model is used to estimate relative strengths or preferences based on latent scores. [@cattelan2012].

::: {#tbl-philosophy}
  -----------------------------------------------------------------------
  Application                         Human Preference
  ----------------------------------- -----------------------------------
  Computer vision: train a neural     This is how humans process images
  network to predict bounding boxes   by identifying the position and
  delineating all instances of dogs   geometry of the things we see in
  in an image                         them

  Natural language processing: train  Coherent text is itself a
  a model to generate coherent text   human-created and defined concept,
                                      and we prefer that any
                                      synthetically generated text
                                      matches that of humans

  Computer vision: train a diffusion  Humans prefer that images
  model to generate realistic images  accurately capture the world as
  of nature                           observed by humans, and this
                                      generative model should reflect the
                                      details that comprise that
                                      preference
  -----------------------------------------------------------------------

  : Examples of machine learning tasks and their interpretation as
  modeling human preferences.
:::
-->
<!--
Game theory provides a mathematical framework for analyzing strategic
interactions among rational agents. These models help in understanding
and predicting human behavior by considering multiple criteria and the
associated trade-offs. They enhance the understanding of preferences
across multiple criteria and allow for richer and more accurate feedback
through structured comparisons. Game-theory framings capture the
complexity of preferences and interactions in decision-making processes
[@bhatia2020preference].

The most popular form of preference elicitation involves pairwise
comparisons. Users are asked to choose between two options, such as
product A or product B. This method is used in various applications like
search engines, recommender systems, and interactive robotics. Key
concepts include the Von Neumann Winner and the Blackwell Winner. The
Von Neumann Winner refers to a distribution over objects that beats or
ties every other object in the collection under the expected utility
assumption. The Blackwell Winner generalizes the Von Neumann Winner for
multi-criteria problems using a target set for acceptable payoff vectors
[@bhatia2020preference].

Game-theory framings provide a framework for preference learning along
multiple criteria. These models use tools from vector-valued payoffs in
game theory, with Blackwell's approach being a key concept. This
approach allows for a more comprehensive understanding of preferences by
considering multiple criteria simultaneously [@bhatia2020preference].

In game-theory framings, pairwise preferences are modeled as random
variables. Comparisons between objects along different criteria are
captured in a preference tensor $P$. This tensor models the probability
that one object is preferred over another along a specific criterion,
allowing for a detailed understanding of preferences across multiple
dimensions [@bhatia2020preference].

The preference tensor $P$ captures object comparisons along different
criteria. It is defined as:
$$P(i_1, i_2; j) = P(i_1 \succ i_2 \text{ along criterion } j)$$ where
$P(i_2, i_1; j) = 1 - P(i_1, i_2; j)$. These values are aggregated to
form an overall preference matrix $P_{ov}$ [@bhatia2020preference].

The Blackwell Winner is defined using a target set $S$ of acceptable
score vectors. The goal is to find a distribution $\pi^*$ such that
$P(\pi^*, \pi) \in S$ for all $\pi$. This method minimizes the maximum
distance to the target set, providing a robust solution to
multi-criteria preference problems [@bhatia2020preference].

The optimization problem for finding the Blackwell Winner is defined as:
$$\pi(P, S, \|\cdot\|) = \arg \min_{\pi \in \Delta_d} \left[ \max_{\pi' \in \Delta_d} \rho(P(\pi, \pi'), S) \right]$$
where $\rho(u, v) = \|u - v\|$. This measures the distance to the target
set, ensuring that the selected distribution is as close as possible to
the ideal preference vector [@bhatia2020preference].
-->


<!-- -->

</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-book_estimation_bock" class="csl-entry" role="listitem">
Bock, Hans Georg, Thomas Carraro, Willi Jäger, Stefan Körkel, Rolf Rannacher, and Johannes P. Schlöder. 2015. <em>Model Based Parameter Estimation: Theory and Applications</em>. Springer. <a href="https://api.semanticscholar.org/CorpusID:60333071">https://api.semanticscholar.org/CorpusID:60333071</a>.
</div>
<div id="ref-Liang2021" class="csl-entry" role="listitem">
Bommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, et al. 2021. <span>“On the Opportunities and Risks of Foundation Models.”</span>
</div>
<div id="ref-bradley1952rank" class="csl-entry" role="listitem">
Bradley, Ralph Allan, and Milton E Terry. 1952. <span>“Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons.”</span> <em>Biometrika</em> 39 (3/4): 324–45.
</div>
<div id="ref-book_estimation_casella" class="csl-entry" role="listitem">
Casella, George, and Roger L. Berger. 1990. <em>Statistical Inference</em>. Springer. <a href="https://api.semanticscholar.org/CorpusID:125727004">https://api.semanticscholar.org/CorpusID:125727004</a>.
</div>
<div id="ref-christiano2023deep" class="csl-entry" role="listitem">
Christiano, Paul, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2023. <span>“Deep Reinforcement Learning from Human Preferences.”</span> <a href="https://arxiv.org/abs/1706.03741">https://arxiv.org/abs/1706.03741</a>.
</div>
<div id="ref-finn2017model" class="csl-entry" role="listitem">
Finn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017. <span>“Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.”</span> In <em>International Conference on Machine Learning</em>, 1126–35. PMLR.
</div>
<div id="ref-haarnoja2018soft" class="csl-entry" role="listitem">
Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. <span>“Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.”</span> In <em>International Conference on Machine Learning</em>, 1861–70. PMLR.
</div>
<div id="ref-hejna2023few" class="csl-entry" role="listitem">
Hejna III, Donald Joseph, and Dorsa Sadigh. 2023. <span>“Few-Shot Preference Learning for Human-in-the-Loop Rl.”</span> In <em>Conference on Robot Learning</em>, 2014–25. PMLR.
</div>
<div id="ref-lee2021pebble" class="csl-entry" role="listitem">
Lee, Kimin, Laura Smith, and Pieter Abbeel. 2021. <span>“Pebble: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-Training.”</span> <em>arXiv Preprint arXiv:2106.05091</em>.
</div>
<div id="ref-myers2022learning" class="csl-entry" role="listitem">
Myers, Vivek, Erdem Biyik, Nima Anari, and Dorsa Sadigh. 2022. <span>“Learning Multimodal Rewards from Rankings.”</span> In <em>Conference on Robot Learning</em>, 342–52. PMLR.
</div>
<div id="ref-myers2021learning" class="csl-entry" role="listitem">
Myers, Vivek, Erdem Bıyık, Nima Anari, and Dorsa Sadigh. 2021. <span>“Learning Multimodal Rewards from Rankings.”</span> <a href="https://arxiv.org/abs/2109.12750">https://arxiv.org/abs/2109.12750</a>.
</div>
<div id="ref-padalkar2023open" class="csl-entry" role="listitem">
Padalkar, Abhishek, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, et al. 2023. <span>“Open x-Embodiment: Robotic Learning Datasets and RT-x Models.”</span> <em>arXiv Preprint arXiv:2310.08864</em>.
</div>
<div id="ref-rafailov2023direct" class="csl-entry" role="listitem">
Rafailov, Rafael, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. <span>“Direct Preference Optimization: Your Language Model Is Secretly a Reward Model.”</span> <a href="https://arxiv.org/abs/2305.18290">https://arxiv.org/abs/2305.18290</a>.
</div>
<div id="ref-gradient_descent" class="csl-entry" role="listitem">
Ruder, Sebastian. 2016. <span>“An Overview of Gradient Descent Optimization Algorithms.”</span> <em>ArXiv</em> abs/1609.04747. <a href="https://api.semanticscholar.org/CorpusID:17485266">https://api.semanticscholar.org/CorpusID:17485266</a>.
</div>
<div id="ref-2307.09288" class="csl-entry" role="listitem">
Touvron, Hugo et al. 2023. <span>“Llama 2: Open Foundation and Fine-Tuned Chat Models.”</span> <a href="https://arxiv.org/abs/2307.09288">https://arxiv.org/abs/2307.09288</a>.
</div>
<div id="ref-yu2020meta" class="csl-entry" role="listitem">
Yu, Tianhe, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. 2020. <span>“Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning.”</span> In <em>Conference on Robot Learning</em>, 1094–1100. PMLR.
</div>
<div id="ref-zhou2019watch" class="csl-entry" role="listitem">
Zhou, Allan, Eric Jang, Daniel Kappler, Alex Herzog, Mohi Khansari, Paul Wohlhart, Yunfei Bai, Mrinal Kalakrishnan, Sergey Levine, and Chelsea Finn. 2019. <span>“Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards.”</span> In <em>International Conference on Learning Representations</em>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../src/001-preference_decision_model.html" class="pagination-link" aria-label="Models of Preferences and Decisions">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Models of Preferences and Decisions</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../src/003-measure.html" class="pagination-link" aria-label="Model-Based Preference Optimization">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu"># Reward Model {#ch-reward-models}</span></span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="fu">## Parameterization and Learning of Utility Functions {#sec-learning}</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>The attributes representing a choice $z_i$ are crucial in defining the human preference model, as they provide the context for capturing human behavior when choice $i$ is made. </span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a>With an understanding of the various techniques we can use to model</span>
<span id="cb1-7"><a href="#cb1-7"></a>human preferences, we can now create robust models which utilize context</span>
<span id="cb1-8"><a href="#cb1-8"></a>attributes about the options an individual has in front of them and</span>
<span id="cb1-9"><a href="#cb1-9"></a>model their choices. However, these models on their own are powerless;</span>
<span id="cb1-10"><a href="#cb1-10"></a>their parameters are initialized randomly and we must fit the models to</span>
<span id="cb1-11"><a href="#cb1-11"></a>the actual human choice data!</span>
<span id="cb1-12"><a href="#cb1-12"></a></span>
<span id="cb1-13"><a href="#cb1-13"></a>Each of the models we have studied contain distinct parameters which aim</span>
<span id="cb1-14"><a href="#cb1-14"></a>to capture human preferences; for example $\beta$ is a parameter vector</span>
<span id="cb1-15"><a href="#cb1-15"></a>containing variables which represent a linear function to compute</span>
<span id="cb1-16"><a href="#cb1-16"></a>utility given a choice's attributes. We can also choose to represent</span>
<span id="cb1-17"><a href="#cb1-17"></a>stochastic utility functions or embedding functions for Ideal Point</span>
<span id="cb1-18"><a href="#cb1-18"></a>Models as neural networks. But how can we compute the optimal values of</span>
<span id="cb1-19"><a href="#cb1-19"></a>these parameters?</span>
<span id="cb1-20"><a href="#cb1-20"></a></span>
<span id="cb1-21"><a href="#cb1-21"></a>In this section, we give the reader an overview of the different methods</span>
<span id="cb1-22"><a href="#cb1-22"></a>available to tune human preference model parameters using given data. We</span>
<span id="cb1-23"><a href="#cb1-23"></a>refer the reader to <span class="co">[</span><span class="ot">@book_estimation_casella; @book_estimation_bock</span><span class="co">]</span></span>
<span id="cb1-24"><a href="#cb1-24"></a>for first-principle derivations of these methods and a deeper dive into</span>
<span id="cb1-25"><a href="#cb1-25"></a>their theoretical properties (convergence, generalization,</span>
<span id="cb1-26"><a href="#cb1-26"></a>data-hungriness, etc.).</span>
<span id="cb1-27"><a href="#cb1-27"></a></span>
<span id="cb1-28"><a href="#cb1-28"></a>A common and powerful approach for computing the parameters of a model</span>
<span id="cb1-29"><a href="#cb1-29"></a>is maximum likelihood estimation</span>
<span id="cb1-30"><a href="#cb1-30"></a><span class="co">[</span><span class="ot">@book_estimation_casella; @book_estimation_bock</span><span class="co">]</span>. The likelihood of a</span>
<span id="cb1-31"><a href="#cb1-31"></a>model is the probability of the observed data given the model</span>
<span id="cb1-32"><a href="#cb1-32"></a>parameters; intuitively we wish to maximize this likelihood, as that</span>
<span id="cb1-33"><a href="#cb1-33"></a>would mean that our model associates observed human preferences in the</span>
<span id="cb1-34"><a href="#cb1-34"></a>data with high probability. We can formally define the likelihood for a</span>
<span id="cb1-35"><a href="#cb1-35"></a>model with parameters $\beta$ and a given data point $(z_i, y_i)$ as:</span>
<span id="cb1-36"><a href="#cb1-36"></a>$$\mathcal{L}(z_i, y_i; \beta) = \mathbb{P}(y = y_i | z_i; \beta)$$</span>
<span id="cb1-37"><a href="#cb1-37"></a></span>
<span id="cb1-38"><a href="#cb1-38"></a>Assuming our data is independent and identically distributed (iid), the</span>
<span id="cb1-39"><a href="#cb1-39"></a>likelihood over the entire dataset is the joint probability of all</span>
<span id="cb1-40"><a href="#cb1-40"></a>observed data as defined by the model:</span>
<span id="cb1-41"><a href="#cb1-41"></a>$$\mathcal{L}(z, Y; \beta) = \prod_{i = 1}^J \mathbb{P}(y = y_i | z_i; \beta)$$</span>
<span id="cb1-42"><a href="#cb1-42"></a></span>
<span id="cb1-43"><a href="#cb1-43"></a>In our very first example of binary choice with logistic noise, this was</span>
<span id="cb1-44"><a href="#cb1-44"></a>simply the model's probability of the observed preference value:</span>
<span id="cb1-45"><a href="#cb1-45"></a>$$\mathcal{L}(z_i, y_i; \beta) = \frac{1}{1 + \exp^{-u_{i,j}^*}}$$</span>
<span id="cb1-46"><a href="#cb1-46"></a></span>
<span id="cb1-47"><a href="#cb1-47"></a>In the same case with noise following a standard normal distribution,</span>
<span id="cb1-48"><a href="#cb1-48"></a>this took the form: $$\mathcal{L}(z_i, y_i; \beta) = \Phi(u_{i,j}^*)$$</span>
<span id="cb1-49"><a href="#cb1-49"></a></span>
<span id="cb1-50"><a href="#cb1-50"></a>Fortunately, in these cases, there are straightforward methods for</span>
<span id="cb1-51"><a href="#cb1-51"></a>parameter estimation: logistic regression and probit regression (binary</span>
<span id="cb1-52"><a href="#cb1-52"></a>or multinomial, depending on the model), respectively. We can use</span>
<span id="cb1-53"><a href="#cb1-53"></a>ordinal regression to estimate the model's parameters for our ordered</span>
<span id="cb1-54"><a href="#cb1-54"></a>preference model.</span>
<span id="cb1-55"><a href="#cb1-55"></a></span>
<span id="cb1-56"><a href="#cb1-56"></a>Generally, the objective function commonly found in parameter learning</span>
<span id="cb1-57"><a href="#cb1-57"></a>can be optimized with stochastic gradient descent (SGD)</span>
<span id="cb1-58"><a href="#cb1-58"></a><span class="co">[</span><span class="ot">@gradient_descent</span><span class="co">]</span>. We can define an objective function as the</span>
<span id="cb1-59"><a href="#cb1-59"></a>likelihood to maximize this objective. Since SGD minimizes a given</span>
<span id="cb1-60"><a href="#cb1-60"></a>objective, we must negate the likelihood, which ensures that a converged</span>
<span id="cb1-61"><a href="#cb1-61"></a>solution maximizes the likelihood. SGD operates by computing the</span>
<span id="cb1-62"><a href="#cb1-62"></a>gradient of the objective with respect to the parameters of the model,</span>
<span id="cb1-63"><a href="#cb1-63"></a>which provides a signal of the direction in which the parameters must</span>
<span id="cb1-64"><a href="#cb1-64"></a>move to *maximize* the objective. Then, SGD makes an update step by</span>
<span id="cb1-65"><a href="#cb1-65"></a>subtracting this gradient from the parameters (most often with a scale</span>
<span id="cb1-66"><a href="#cb1-66"></a>factor called a *learning rate*), to move the parameters in a direction</span>
<span id="cb1-67"><a href="#cb1-67"></a>which *minimizes* the objective. When the objective is the negative</span>
<span id="cb1-68"><a href="#cb1-68"></a>likelihood (or sometimes negative log-likelihood for convenience or</span>
<span id="cb1-69"><a href="#cb1-69"></a>tractability), the result is an increase in the overall likelihood.</span>
<span id="cb1-70"><a href="#cb1-70"></a></span>
<span id="cb1-71"><a href="#cb1-71"></a>In the case of logistic and Gaussian models, SGD may yield a challenging</span>
<span id="cb1-72"><a href="#cb1-72"></a>optimization problem as its stochasticity can lead to noisy updates, for</span>
<span id="cb1-73"><a href="#cb1-73"></a>example, if certain examples or batches of examples are biased.</span>
<span id="cb1-74"><a href="#cb1-74"></a>Mitigations include batched SGD, in which multiple samples are randomly</span>
<span id="cb1-75"><a href="#cb1-75"></a>sampled from the dataset at each iteration, learning rates, which reduce</span>
<span id="cb1-76"><a href="#cb1-76"></a>the impact of noisy gradient updates, and momentum and higher-order</span>
<span id="cb1-77"><a href="#cb1-77"></a>optimizers which reduce noise by using movering averages of gradients or</span>
<span id="cb1-78"><a href="#cb1-78"></a>provide better estimates of the best direction in which to update the</span>
<span id="cb1-79"><a href="#cb1-79"></a>gradients. Some models, such as those that use neural networks, may, in</span>
<span id="cb1-80"><a href="#cb1-80"></a>fact, be intractable to estimate without a method such as SGD (or its</span>
<span id="cb1-81"><a href="#cb1-81"></a>momentum-based derivatives). For example, neural networks with many</span>
<span id="cb1-82"><a href="#cb1-82"></a>layers, non-linearities, and parameters can only be efficiently computed</span>
<span id="cb1-83"><a href="#cb1-83"></a>with gradient-based methods.</span>
<span id="cb1-84"><a href="#cb1-84"></a></span>
<span id="cb1-85"><a href="#cb1-85"></a><span class="fu">### Reward Learning with Large Language Models</span></span>
<span id="cb1-86"><a href="#cb1-86"></a></span>
<span id="cb1-87"><a href="#cb1-87"></a>Taking a step away from explicitly modeling human bias and preference,</span>
<span id="cb1-88"><a href="#cb1-88"></a>we consider applying a deep learning approach to state-of-the-art</span>
<span id="cb1-89"><a href="#cb1-89"></a>language models. We begin by introducing the concepts of *foundation</span>
<span id="cb1-90"><a href="#cb1-90"></a>models* and *alignment*. A foundation model <span class="co">[</span><span class="ot">@Liang2021</span><span class="co">]</span> in machine</span>
<span id="cb1-91"><a href="#cb1-91"></a>learning typically refers to a large and pre-trained neural network</span>
<span id="cb1-92"><a href="#cb1-92"></a>model that serves as the basis for various downstream tasks. In natural</span>
<span id="cb1-93"><a href="#cb1-93"></a>language processing, models like GPT-3, Llama, and BERT are considered</span>
<span id="cb1-94"><a href="#cb1-94"></a>foundation models. They are pre-trained on a massive corpus of text</span>
<span id="cb1-95"><a href="#cb1-95"></a>data, learning to understand language and context, and are capable of</span>
<span id="cb1-96"><a href="#cb1-96"></a>various language-related tasks such as text classification, language</span>
<span id="cb1-97"><a href="#cb1-97"></a>generation, and question answering. Foundation models are important</span>
<span id="cb1-98"><a href="#cb1-98"></a>because they alleviate the need to train massive neural networks from</span>
<span id="cb1-99"><a href="#cb1-99"></a>scratch, a compute and data expensive endeavor. However, a raw</span>
<span id="cb1-100"><a href="#cb1-100"></a>foundation model, trained on a pretraining objective such as a language</span>
<span id="cb1-101"><a href="#cb1-101"></a>modeling objective, is not useful on its own. It must be aligned to</span>
<span id="cb1-102"><a href="#cb1-102"></a>respond correctly based on human preferences.</span>
<span id="cb1-103"><a href="#cb1-103"></a></span>
<span id="cb1-104"><a href="#cb1-104"></a>In short, alignment for foundation models is the process by which model</span>
<span id="cb1-105"><a href="#cb1-105"></a>behavior is aligned with human values, ethics, and societal norms. Large</span>
<span id="cb1-106"><a href="#cb1-106"></a>Language Models (LLMs) are a foundation model for natural language</span>
<span id="cb1-107"><a href="#cb1-107"></a>processing. They are trained using a next-word prediction objective,</span>
<span id="cb1-108"><a href="#cb1-108"></a>allowing them to generate coherent language. A simple way to align a</span>
<span id="cb1-109"><a href="#cb1-109"></a>Large Language Model is to train it to follow instructions in a</span>
<span id="cb1-110"><a href="#cb1-110"></a>supervised way, using instruction-response pairs curated by hand.</span>
<span id="cb1-111"><a href="#cb1-111"></a>However, this limits the upper limit of LLM performance to the</span>
<span id="cb1-112"><a href="#cb1-112"></a>performance of the annotators' writing abilities. This type of</span>
<span id="cb1-113"><a href="#cb1-113"></a>annotation is also expensive.</span>
<span id="cb1-114"><a href="#cb1-114"></a></span>
<span id="cb1-115"><a href="#cb1-115"></a>An alternative, more promising approach is to train LLMs using</span>
<span id="cb1-116"><a href="#cb1-116"></a>reinforcement learning, potentially enabling them to surpass human-level</span>
<span id="cb1-117"><a href="#cb1-117"></a>performance. The main challenge with this method lies in defining an</span>
<span id="cb1-118"><a href="#cb1-118"></a>explicit reward function for generating free-form text. To address this,</span>
<span id="cb1-119"><a href="#cb1-119"></a>a reward model (RM) can be trained based on human preferences, providing</span>
<span id="cb1-120"><a href="#cb1-120"></a>a mechanism to score the quality of the generated text. This approach,</span>
<span id="cb1-121"><a href="#cb1-121"></a>known as Reinforcement Learning from Human Feedback (RLHF), leverages</span>
<span id="cb1-122"><a href="#cb1-122"></a>human feedback to guide model training, allowing LLMs to better align</span>
<span id="cb1-123"><a href="#cb1-123"></a>with human expectations while continuously improving performance.</span>
<span id="cb1-124"><a href="#cb1-124"></a></span>
<span id="cb1-125"><a href="#cb1-125"></a><span class="al">![Overall architecture of a reward model based on LLM](Figures/arch.png)</span>{#fig-rm-arch}</span>
<span id="cb1-126"><a href="#cb1-126"></a></span>
<span id="cb1-127"><a href="#cb1-127"></a>The Llama2 reward model <span class="co">[</span><span class="ot">@2307.09288</span><span class="co">]</span> is initialized from the pretrained</span>
<span id="cb1-128"><a href="#cb1-128"></a>Llama2 LLM. In the LLM, the last layer is a mapping</span>
<span id="cb1-129"><a href="#cb1-129"></a>$L: \mathbb{R}^D \rightarrow \mathbb{R}^V$, where $D$ is the embedding dimension</span>
<span id="cb1-130"><a href="#cb1-130"></a>from the transformer decoder stack and $V$ is the vocabulary size. To</span>
<span id="cb1-131"><a href="#cb1-131"></a>get the RM, we replace that last layer with a randomly initialized</span>
<span id="cb1-132"><a href="#cb1-132"></a>scalar head that maps $L: \mathbb{R}^D \rightarrow \mathbb{R}^1$. It's important to</span>
<span id="cb1-133"><a href="#cb1-133"></a>initialize the RM from the LLM it's meant to evaluate. This is because:</span>
<span id="cb1-134"><a href="#cb1-134"></a></span>
<span id="cb1-135"><a href="#cb1-135"></a><span class="ss">1.  </span>The RM will have the same "knowledge" as the LLM. This is</span>
<span id="cb1-136"><a href="#cb1-136"></a>    particularly useful if evaluating things like "does the LLM know</span>
<span id="cb1-137"><a href="#cb1-137"></a>    when it doesn't know?". However, in cases where the RM is simply</span>
<span id="cb1-138"><a href="#cb1-138"></a>    evaluating helpfulness or factuality, it may be useful to have the</span>
<span id="cb1-139"><a href="#cb1-139"></a>    RM know more.</span>
<span id="cb1-140"><a href="#cb1-140"></a></span>
<span id="cb1-141"><a href="#cb1-141"></a><span class="ss">2.  </span>The RM is on distribution for the LLM - it is initialized in a way</span>
<span id="cb1-142"><a href="#cb1-142"></a>    where it semantically understands the LLM's outputs.</span>
<span id="cb1-143"><a href="#cb1-143"></a></span>
<span id="cb1-144"><a href="#cb1-144"></a>An RM is trained with paired preferences, following the format:</span>
<span id="cb1-145"><a href="#cb1-145"></a>$$\begin{aligned}</span>
<span id="cb1-146"><a href="#cb1-146"></a>    \langle prompt<span class="sc">\_</span>history, response<span class="sc">\_</span>accepted, response<span class="sc">\_</span>rejected \rangle</span>
<span id="cb1-147"><a href="#cb1-147"></a>\end{aligned}$$ Prompt_history is a multiturn history of user prompts</span>
<span id="cb1-148"><a href="#cb1-148"></a>and model generations, response_accepted is the preferred final model</span>
<span id="cb1-149"><a href="#cb1-149"></a>generation by an annotator, and response_rejected is the unpreferred</span>
<span id="cb1-150"><a href="#cb1-150"></a>response. The RM is trained with a binary ranking loss with an optional</span>
<span id="cb1-151"><a href="#cb1-151"></a>margin term m(r), shown in equation (7). There is also often a small</span>
<span id="cb1-152"><a href="#cb1-152"></a>regularization term added to center the score distribution on 0.</span>
<span id="cb1-153"><a href="#cb1-153"></a>$$\mathcal{L}_{\text{ranking}} = -\log(\sigma(r_\theta(x,y_c) - r_\theta(x,y_r) - m(r)))$$</span>
<span id="cb1-154"><a href="#cb1-154"></a>The margin term increases the distance in scores specifically for</span>
<span id="cb1-155"><a href="#cb1-155"></a>preference pairs annotators rate as easier to separate.</span>
<span id="cb1-156"><a href="#cb1-156"></a></span>
<span id="cb1-157"><a href="#cb1-157"></a>::: {#tbl-margin_nums}</span>
<span id="cb1-158"><a href="#cb1-158"></a>  -------------- --------------- -------- ---------- -----------------</span>
<span id="cb1-159"><a href="#cb1-159"></a><span class="in">                  Significantly   Better   Slightly     Negligibly</span></span>
<span id="cb1-160"><a href="#cb1-160"></a><span class="in">                     Better                 Better    Better / Unsure</span></span>
<span id="cb1-161"><a href="#cb1-161"></a>  Margin Small          1          2/3       1/3             0</span>
<span id="cb1-162"><a href="#cb1-162"></a>  Margin Large          3           2         1              0</span>
<span id="cb1-163"><a href="#cb1-163"></a>  -------------- --------------- -------- ---------- -----------------</span>
<span id="cb1-164"><a href="#cb1-164"></a></span>
<span id="cb1-165"><a href="#cb1-165"></a>  : Two variants of preference rating based margin with different magnitude.</span>
<span id="cb1-166"><a href="#cb1-166"></a>:::</span>
<span id="cb1-167"><a href="#cb1-167"></a></span>
<span id="cb1-168"><a href="#cb1-168"></a>![Reward model score distribution shift caused by incorporating</span>
<span id="cb1-169"><a href="#cb1-169"></a>preference rating based margin in ranking loss. With the margin term,</span>
<span id="cb1-170"><a href="#cb1-170"></a>we observe a binary split pattern in reward distribution, especially for</span>
<span id="cb1-171"><a href="#cb1-171"></a>a larger margin.](Figures/margin-2.png){#fig-margin-2</span>
<span id="cb1-172"><a href="#cb1-172"></a>width="<span class="sc">\\</span>linewidth"}</span>
<span id="cb1-173"><a href="#cb1-173"></a></span>
<span id="cb1-174"><a href="#cb1-174"></a>It may seem confusing how the margins were chosen. It's primarily</span>
<span id="cb1-175"><a href="#cb1-175"></a>because the sigmoid function, which is used to normalize the raw reward</span>
<span id="cb1-176"><a href="#cb1-176"></a>model score, flattens out beyond the range of $<span class="co">[</span><span class="ot">-4, 4</span><span class="co">]</span>$. Thus, the</span>
<span id="cb1-177"><a href="#cb1-177"></a>maximum possible margin is eight.</span>
<span id="cb1-178"><a href="#cb1-178"></a></span>
<span id="cb1-179"><a href="#cb1-179"></a>When training or using a reward model, watching for the following is</span>
<span id="cb1-180"><a href="#cb1-180"></a>important:</span>
<span id="cb1-181"><a href="#cb1-181"></a></span>
<span id="cb1-182"><a href="#cb1-182"></a><span class="ss">1.  </span>**LLM Distribution Shift**: With each finetune of the LLM, the RM</span>
<span id="cb1-183"><a href="#cb1-183"></a>    should be updated through a collection of fresh human preferences</span>
<span id="cb1-184"><a href="#cb1-184"></a>    using generations from the new LLM. This ensures that the RM stays</span>
<span id="cb1-185"><a href="#cb1-185"></a>    aligned with the current distribution of the LLM and avoids drifting</span>
<span id="cb1-186"><a href="#cb1-186"></a>    off-distribution.</span>
<span id="cb1-187"><a href="#cb1-187"></a></span>
<span id="cb1-188"><a href="#cb1-188"></a><span class="ss">2.  </span>**RM and LLM are coupled**: An RM is generally optimized to</span>
<span id="cb1-189"><a href="#cb1-189"></a>    distinguish human preferences more efficiently within the specific</span>
<span id="cb1-190"><a href="#cb1-190"></a>    distribution of the LLM to be optimized. However, this</span>
<span id="cb1-191"><a href="#cb1-191"></a>    specialization poses a challenge: such an RM will underperform when</span>
<span id="cb1-192"><a href="#cb1-192"></a>    dealing with generations not aligned with this specific LLM</span>
<span id="cb1-193"><a href="#cb1-193"></a>    distribution, such as generations from a completely different LLM.</span>
<span id="cb1-194"><a href="#cb1-194"></a></span>
<span id="cb1-195"><a href="#cb1-195"></a><span class="ss">3.  </span>**Training Sensitivities of RMs**: Training RMs can be unstable and</span>
<span id="cb1-196"><a href="#cb1-196"></a>    prone to overfitting, especially with multiple training epochs. It's</span>
<span id="cb1-197"><a href="#cb1-197"></a>    generally advisable to limit the number of epochs during RM training</span>
<span id="cb1-198"><a href="#cb1-198"></a>    to avoid this issue.</span>
<span id="cb1-199"><a href="#cb1-199"></a></span>
<span id="cb1-200"><a href="#cb1-200"></a>The industry has centered around optimizing for two primary qualities in</span>
<span id="cb1-201"><a href="#cb1-201"></a>LLMs: helpfulness and harmlessness (safety). There are also other axes</span>
<span id="cb1-202"><a href="#cb1-202"></a>such as factuality, reasoning, tool use, code, multilingual, and more,</span>
<span id="cb1-203"><a href="#cb1-203"></a>but these are out of scope for us. In the Llama2 paper, preference data</span>
<span id="cb1-204"><a href="#cb1-204"></a>was collected from humans for each quality, with separate guidelines.</span>
<span id="cb1-205"><a href="#cb1-205"></a>This presents a challenge for co-optimizing the final LLM towards both</span>
<span id="cb1-206"><a href="#cb1-206"></a>goals.</span>
<span id="cb1-207"><a href="#cb1-207"></a></span>
<span id="cb1-208"><a href="#cb1-208"></a>Two main approaches can be taken for Reinforcement Learning from Human</span>
<span id="cb1-209"><a href="#cb1-209"></a>Feedback (RLHF) in this context:</span>
<span id="cb1-210"><a href="#cb1-210"></a></span>
<span id="cb1-211"><a href="#cb1-211"></a><span class="ss">1.  </span>Train a unified reward model that integrates both datasets.</span>
<span id="cb1-212"><a href="#cb1-212"></a></span>
<span id="cb1-213"><a href="#cb1-213"></a><span class="ss">2.  </span>Train two separate reward models, one for each quality, and optimize</span>
<span id="cb1-214"><a href="#cb1-214"></a>    the LLM toward both.</span>
<span id="cb1-215"><a href="#cb1-215"></a></span>
<span id="cb1-216"><a href="#cb1-216"></a>Option 1 is difficult because of the tension between helpfulness and</span>
<span id="cb1-217"><a href="#cb1-217"></a>harmlessness. They trade off against each other, confusing an RM trained</span>
<span id="cb1-218"><a href="#cb1-218"></a>on both. The chosen solution was option 2, where two RMs are used to</span>
<span id="cb1-219"><a href="#cb1-219"></a>train the LLM in a piecewise fashion. The helpfulness RM is used as the</span>
<span id="cb1-220"><a href="#cb1-220"></a>primary optimization term, while the harmlessness RM acts as a penalty</span>
<span id="cb1-221"><a href="#cb1-221"></a>term, driving the behavior of the LLM away from unsafe territory only</span>
<span id="cb1-222"><a href="#cb1-222"></a>when the LLM veers beyond a certain threshold. This is formalized as</span>
<span id="cb1-223"><a href="#cb1-223"></a>follows, where $R_s$, $R_h$, and $R_c$ are the safety, helpfulness, and</span>
<span id="cb1-224"><a href="#cb1-224"></a>combined reward, respectively. $g$ and $p$ are the model generation and</span>
<span id="cb1-225"><a href="#cb1-225"></a>the user prompt: $$\begin{aligned}</span>
<span id="cb1-226"><a href="#cb1-226"></a>    R_c(g \mid p) =</span>
<span id="cb1-227"><a href="#cb1-227"></a>    \begin{cases}</span>
<span id="cb1-228"><a href="#cb1-228"></a>        R_s(g \mid p) &amp; \text{if } \text{is<span class="sc">\_</span>safety}(p) \text{ or } R_s(g \mid p) &lt; 0.15 <span class="sc">\\</span></span>
<span id="cb1-229"><a href="#cb1-229"></a>        R_h(g \mid p) &amp; \text{otherwise}</span>
<span id="cb1-230"><a href="#cb1-230"></a>    \end{cases}</span>
<span id="cb1-231"><a href="#cb1-231"></a>\end{aligned}$$</span>
<span id="cb1-232"><a href="#cb1-232"></a></span>
<span id="cb1-233"><a href="#cb1-233"></a>There are several open issues with reward models alluded to in the</span>
<span id="cb1-234"><a href="#cb1-234"></a>paper. For example, how best to collect human feedback? Training</span>
<span id="cb1-235"><a href="#cb1-235"></a>annotators and making sure they do the correct thing is hard. What</span>
<span id="cb1-236"><a href="#cb1-236"></a>should the guidelines be? Another question is whether RMs can be made</span>
<span id="cb1-237"><a href="#cb1-237"></a>robust to adversarial prompts. Last but not least, do RMs have</span>
<span id="cb1-238"><a href="#cb1-238"></a>well-calibrated scores? This matters for RLHF - pure preference accuracy</span>
<span id="cb1-239"><a href="#cb1-239"></a>isn't enough.</span>
<span id="cb1-240"><a href="#cb1-240"></a></span>
<span id="cb1-241"><a href="#cb1-241"></a><span class="fu">### Reward Learning in Robotics</span></span>
<span id="cb1-242"><a href="#cb1-242"></a></span>
<span id="cb1-243"><a href="#cb1-243"></a>To help set up our basic reward learning problem, consider a user and a</span>
<span id="cb1-244"><a href="#cb1-244"></a>robot. The user's preferences or goals can be represented by an internal</span>
<span id="cb1-245"><a href="#cb1-245"></a>reward function, R($\xi$), which the robot needs to learn. Since the</span>
<span id="cb1-246"><a href="#cb1-246"></a>reward function isn't explicit, there are a variety of ways that the</span>
<span id="cb1-247"><a href="#cb1-247"></a>robot can learn this reward function, which we will discuss in the next</span>
<span id="cb1-248"><a href="#cb1-248"></a>section. An example method of learning a reward function from human data</span>
<span id="cb1-249"><a href="#cb1-249"></a>is using pairwise comparison. Consider the robot example from section</span>
<span id="cb1-250"><a href="#cb1-250"></a>one, but now, the robot shows the human two possible trajectories</span>
<span id="cb1-251"><a href="#cb1-251"></a>$\xi_A$ and $\xi_B$ as depicted in the diagram below.</span>
<span id="cb1-252"><a href="#cb1-252"></a></span>
<span id="cb1-253"><a href="#cb1-253"></a>![Two different trajectories taken by a robot to prompt</span>
<span id="cb1-254"><a href="#cb1-254"></a>user ranking.](Figures/robots.png){#fig-reward-robot-1 width="70%"}</span>
<span id="cb1-255"><a href="#cb1-255"></a></span>
<span id="cb1-256"><a href="#cb1-256"></a>The user is show both the trajectories above and asked to rank which one</span>
<span id="cb1-257"><a href="#cb1-257"></a>is better. Based on iterations of multiple trajectories and ranking, the</span>
<span id="cb1-258"><a href="#cb1-258"></a>robot is able to learn the user's internal reward function. There quite</span>
<span id="cb1-259"><a href="#cb1-259"></a>a lot of ways that models can learn a reward function from human data.</span>
<span id="cb1-260"><a href="#cb1-260"></a>Here's a list <span class="co">[</span><span class="ot">@myers2021learning</span><span class="co">]</span> of some of them:</span>
<span id="cb1-261"><a href="#cb1-261"></a></span>
<span id="cb1-262"><a href="#cb1-262"></a><span class="ss">1.  </span>Pairwise comparison: This is the method that we saw illustrated in</span>
<span id="cb1-263"><a href="#cb1-263"></a>    the previous example. The robot is able to learn based on a</span>
<span id="cb1-264"><a href="#cb1-264"></a>    comparison ranking provided by the user.</span>
<span id="cb1-265"><a href="#cb1-265"></a></span>
<span id="cb1-266"><a href="#cb1-266"></a><span class="ss">2.  </span>Expert demonstrations: Experts perform the task and the robot learns</span>
<span id="cb1-267"><a href="#cb1-267"></a>    the optimal reward function from these demonstrations.</span>
<span id="cb1-268"><a href="#cb1-268"></a></span>
<span id="cb1-269"><a href="#cb1-269"></a><span class="ss">3.  </span>Sub-optimal demonstrations: The robot is provided with</span>
<span id="cb1-270"><a href="#cb1-270"></a>    demonstrations that are not quite as good as the expert</span>
<span id="cb1-271"><a href="#cb1-271"></a>    demonstrations but it is still able to learn a noisy reward function</span>
<span id="cb1-272"><a href="#cb1-272"></a>    from the demonstrations.</span>
<span id="cb1-273"><a href="#cb1-273"></a></span>
<span id="cb1-274"><a href="#cb1-274"></a><span class="ss">4.  </span>Physical Corrections: While the robot is performing the task, at</span>
<span id="cb1-275"><a href="#cb1-275"></a>    each point in its trajectory (or at an arbitrary point in its</span>
<span id="cb1-276"><a href="#cb1-276"></a>    trajectory) its arm is corrected to a more suitable position. Based</span>
<span id="cb1-277"><a href="#cb1-277"></a>    on these corrections, the robot is able to learn the reward</span>
<span id="cb1-278"><a href="#cb1-278"></a>    function.</span>
<span id="cb1-279"><a href="#cb1-279"></a></span>
<span id="cb1-280"><a href="#cb1-280"></a><span class="ss">5.  </span>Ranking: This method is similar to pairwise comparison but involves</span>
<span id="cb1-281"><a href="#cb1-281"></a>    more trajectories than 2. All the trajectories may have subtle</span>
<span id="cb1-282"><a href="#cb1-282"></a>    differences from each other, but these differences help provide</span>
<span id="cb1-283"><a href="#cb1-283"></a>    insight to the model.</span>
<span id="cb1-284"><a href="#cb1-284"></a></span>
<span id="cb1-285"><a href="#cb1-285"></a><span class="ss">6.  </span>Trajectory Assessment: Given a single trajectory, the user rates how</span>
<span id="cb1-286"><a href="#cb1-286"></a>    close it is to optimal, typically using a ranking scale.</span>
<span id="cb1-287"><a href="#cb1-287"></a></span>
<span id="cb1-288"><a href="#cb1-288"></a>    Each of these methods allows the robot to refine its understanding</span>
<span id="cb1-289"><a href="#cb1-289"></a>    of the user's reward function, but their effectiveness can vary</span>
<span id="cb1-290"><a href="#cb1-290"></a>    depending on the application. For instance, expert demonstrations</span>
<span id="cb1-291"><a href="#cb1-291"></a>    tend to produce more reliable results but may not always be feasible</span>
<span id="cb1-292"><a href="#cb1-292"></a>    in everyday tasks. Pairwise comparison and ranking methods offer</span>
<span id="cb1-293"><a href="#cb1-293"></a>    more flexibility but might require a higher number of iterations.</span>
<span id="cb1-294"><a href="#cb1-294"></a></span>
<span id="cb1-295"><a href="#cb1-295"></a><span class="fu">### Reward Learning with Meta Learning</span></span>
<span id="cb1-296"><a href="#cb1-296"></a></span>
<span id="cb1-297"><a href="#cb1-297"></a>Learning a reward function from human preferences is an intricate and</span>
<span id="cb1-298"><a href="#cb1-298"></a>complicated task. At its core, this task is about designing algorithms</span>
<span id="cb1-299"><a href="#cb1-299"></a>that can capture what humans value based on their elicited preferences.</span>
<span id="cb1-300"><a href="#cb1-300"></a>However, due to the nuanced and multifaceted nature of human desires,</span>
<span id="cb1-301"><a href="#cb1-301"></a>learning reward functions from human can be a difficult task. Therefore,</span>
<span id="cb1-302"><a href="#cb1-302"></a>meta-learning rewards may be considered to facilitate the reward</span>
<span id="cb1-303"><a href="#cb1-303"></a>learning processes. Meta-learning, often referred to as "learning to</span>
<span id="cb1-304"><a href="#cb1-304"></a>learn," aims to design models that can adapt to new tasks with minimal</span>
<span id="cb1-305"><a href="#cb1-305"></a>additional efforts. We discuss paper <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span> in @sec-few-shot showing how meta-learning can be leveraged for</span>
<span id="cb1-306"><a href="#cb1-306"></a>few-shot preference learning, where a system can quickly adapt to a new</span>
<span id="cb1-307"><a href="#cb1-307"></a>task after only a few queries to pairwise preferences from human.</span>
<span id="cb1-308"><a href="#cb1-308"></a></span>
<span id="cb1-309"><a href="#cb1-309"></a>Moving beyond the concept of learning from pairwise preferences, in @sec-watch we discuss a different approach where</span>
<span id="cb1-310"><a href="#cb1-310"></a>meta-learning intersects with both demonstrations and</span>
<span id="cb1-311"><a href="#cb1-311"></a>rewards <span class="co">[</span><span class="ot">@zhou2019watch</span><span class="co">]</span>. This paper considers the use of both</span>
<span id="cb1-312"><a href="#cb1-312"></a>demonstrations and rewards elicited from human that guide the learning</span>
<span id="cb1-313"><a href="#cb1-313"></a>process.</span>
<span id="cb1-314"><a href="#cb1-314"></a></span>
<span id="cb1-315"><a href="#cb1-315"></a>In the regular learning setting, a model is fitted to a dataset with</span>
<span id="cb1-316"><a href="#cb1-316"></a>certain learning algorithm. The learning algorithm, for example, can be</span>
<span id="cb1-317"><a href="#cb1-317"></a>the minimization of a loss function. To formulate the "regular" learning</span>
<span id="cb1-318"><a href="#cb1-318"></a>procedure, let's denote the training dataset as $D$, and the test</span>
<span id="cb1-319"><a href="#cb1-319"></a>dataset as $S$. Given a model parameterized by $\theta$; training loss</span>
<span id="cb1-320"><a href="#cb1-320"></a>function $L(\theta, D)$; and test loss function $L(\theta, S)$, we can</span>
<span id="cb1-321"><a href="#cb1-321"></a>formulate a process of "regular" machine learning process as</span>
<span id="cb1-322"><a href="#cb1-322"></a>$$\begin{aligned}</span>
<span id="cb1-323"><a href="#cb1-323"></a>    \theta^\star = \arg\min_\theta\quad L(\theta, D).</span>
<span id="cb1-324"><a href="#cb1-324"></a>\end{aligned}$$ Note that the minimization of the training loss function</span>
<span id="cb1-325"><a href="#cb1-325"></a>is essentially *one* possible learning algorithm. For example, instead</span>
<span id="cb1-326"><a href="#cb1-326"></a>of minimizing the loss function, one may do gradient descent with model</span>
<span id="cb1-327"><a href="#cb1-327"></a>regularization on the loss function, where the final solution may not be</span>
<span id="cb1-328"><a href="#cb1-328"></a>the one that actually minimizes the loss function. As a result, we may</span>
<span id="cb1-329"><a href="#cb1-329"></a>want to be more general and more abstract for the moment, and denote the</span>
<span id="cb1-330"><a href="#cb1-330"></a>learning algorithm as $\mathcal{A}$. Thus, we can write</span>
<span id="cb1-331"><a href="#cb1-331"></a>$$\begin{aligned}</span>
<span id="cb1-332"><a href="#cb1-332"></a>    \theta^\star = \mathcal{A}(D),</span>
<span id="cb1-333"><a href="#cb1-333"></a>\end{aligned}$$ i.e., the learning algorithm $\mathcal{A}$ takes in a</span>
<span id="cb1-334"><a href="#cb1-334"></a>training dataset and outputs a model parameter $\theta^\star$. Then, the</span>
<span id="cb1-335"><a href="#cb1-335"></a>performance of the model is evaluated by the test loss</span>
<span id="cb1-336"><a href="#cb1-336"></a>$L(\mathcal{A}(D), S)$. As we can see, in the regime of "regular"</span>
<span id="cb1-337"><a href="#cb1-337"></a>learning, the learning algorithm $\mathcal{A}$ is pre-defined and fixed.</span>
<span id="cb1-338"><a href="#cb1-338"></a></span>
<span id="cb1-339"><a href="#cb1-339"></a>Meta-learning, or learning-to-learn, essentially asks the question of</span>
<span id="cb1-340"><a href="#cb1-340"></a>whether one can *learn* the learning algorithm $\mathcal{A}$ from prior</span>
<span id="cb1-341"><a href="#cb1-341"></a>tasks, such that the modal can adapt to a new task more</span>
<span id="cb1-342"><a href="#cb1-342"></a>quickly/proficiently. For example, different human languages share</span>
<span id="cb1-343"><a href="#cb1-343"></a>similar ideas, and therefore a human expert who has learned many</span>
<span id="cb1-344"><a href="#cb1-344"></a>languages should be able to learn a new language easier than an average</span>
<span id="cb1-345"><a href="#cb1-345"></a>person. In other words, the human expert should have learned how to</span>
<span id="cb1-346"><a href="#cb1-346"></a>learn new languages more quickly based on their past experiences on</span>
<span id="cb1-347"><a href="#cb1-347"></a>learning languages.</span>
<span id="cb1-348"><a href="#cb1-348"></a></span>
<span id="cb1-349"><a href="#cb1-349"></a>To mathematically formulate meta-learning, we consider a family of</span>
<span id="cb1-350"><a href="#cb1-350"></a>learning algorithms $\mathcal{A}_\omega$ parameterized by $\omega$. The</span>
<span id="cb1-351"><a href="#cb1-351"></a>"prior" tasks are represented by a set of meta-training datasets</span>
<span id="cb1-352"><a href="#cb1-352"></a>$<span class="sc">\{</span>(D_i, S_i)<span class="sc">\}</span>_{i=1}^N$ consists of $N$ pairs of training dataset $D_i$</span>
<span id="cb1-353"><a href="#cb1-353"></a>and test dataset $S_i$. As we noted before, a learning algorithm</span>
<span id="cb1-354"><a href="#cb1-354"></a>$\mathcal{A}_\omega$ takes in a training dataset, and outputs a model,</span>
<span id="cb1-355"><a href="#cb1-355"></a>i.e., $$\begin{aligned}</span>
<span id="cb1-356"><a href="#cb1-356"></a>    \forall i: \quad \theta^\star_i=\mathcal{A}_\omega(D_i).</span>
<span id="cb1-357"><a href="#cb1-357"></a>\end{aligned}$$</span>
<span id="cb1-358"><a href="#cb1-358"></a></span>
<span id="cb1-359"><a href="#cb1-359"></a>Therefore, the **meta-learning objective** is $$\begin{aligned}</span>
<span id="cb1-360"><a href="#cb1-360"></a>    \min_\omega \quad \sum_{i}\ L(\mathcal{A}_\omega(D_i), S_i).</span>
<span id="cb1-361"><a href="#cb1-361"></a>\end{aligned}$$ The above optimization problem gives a solution</span>
<span id="cb1-362"><a href="#cb1-362"></a>$\omega^\star$ which we use as the meta-parameter. Then, when a new task</span>
<span id="cb1-363"><a href="#cb1-363"></a>comes with a new training dataset $D_{new}$, we can simply apply</span>
<span id="cb1-364"><a href="#cb1-364"></a>$\theta^\star_{new}=\mathcal{A}_{\omega^\star}(D_{new})$ to obtain the</span>
<span id="cb1-365"><a href="#cb1-365"></a>adapted model $\theta^\star_{new}$. Note that we usually assume the</span>
<span id="cb1-366"><a href="#cb1-366"></a>meta-training datasets $D_i, S_i$ and the new dataset $D_{new}$ share</span>
<span id="cb1-367"><a href="#cb1-367"></a>the same underlying structure, or they come from the same distribution</span>
<span id="cb1-368"><a href="#cb1-368"></a>of datasets.</span>
<span id="cb1-369"><a href="#cb1-369"></a></span>
<span id="cb1-370"><a href="#cb1-370"></a>One of the most popular meta-learning method is Model-Agnosic</span>
<span id="cb1-371"><a href="#cb1-371"></a>Meta-Learning (MAML) <span class="co">[</span><span class="ot">@finn2017model</span><span class="co">]</span>. In MAML, the meta-parameter</span>
<span id="cb1-372"><a href="#cb1-372"></a>$\omega$ shares the same space as the model parameter $\theta$. At its</span>
<span id="cb1-373"><a href="#cb1-373"></a>core, in MAML the learning algorithm is defined to be $$\begin{aligned}</span>
<span id="cb1-374"><a href="#cb1-374"></a>    \mathcal{A}_\omega(D_i)=\omega-\alpha \nabla_\omega L(\omega, D_i),</span>
<span id="cb1-375"><a href="#cb1-375"></a>\end{aligned}$$ where $\alpha$ is the step size. As we can see, in fact</span>
<span id="cb1-376"><a href="#cb1-376"></a>$\omega$ is defined as the initialization of fine-tuning $\theta$. With</span>
<span id="cb1-377"><a href="#cb1-377"></a>a good $\omega$ learned, the model can adapt to a new task very quickly.</span>
<span id="cb1-378"><a href="#cb1-378"></a>In general, meta-learning can be summarized as follows: Given data from</span>
<span id="cb1-379"><a href="#cb1-379"></a>prior tasks, learn to solve a new task more quickly/proficiently. Given</span>
<span id="cb1-380"><a href="#cb1-380"></a>the general nature of meta-learning, one may be curious about whether</span>
<span id="cb1-381"><a href="#cb1-381"></a>preference learning can be benefited from meta-learning, which we</span>
<span id="cb1-382"><a href="#cb1-382"></a>discuss in the following section.</span>
<span id="cb1-383"><a href="#cb1-383"></a></span>
<span id="cb1-384"><a href="#cb1-384"></a><span class="fu">#### Few-Shot Preference Learning for Reinforcement Learning {#sec-few-shot}</span></span>
<span id="cb1-385"><a href="#cb1-385"></a></span>
<span id="cb1-386"><a href="#cb1-386"></a>Reinforcement learning (RL) in robotics often stumbles when it comes to</span>
<span id="cb1-387"><a href="#cb1-387"></a>devising reward functions aligning with human intentions.</span>
<span id="cb1-388"><a href="#cb1-388"></a>Preference-based RL algorithms aim to solve this by learning from human</span>
<span id="cb1-389"><a href="#cb1-389"></a>feedback, but this often demands a *highly impractical number of</span>
<span id="cb1-390"><a href="#cb1-390"></a>queries* or leads to oversimplified reward functions that don't hold up</span>
<span id="cb1-391"><a href="#cb1-391"></a>in real-world tasks.</span>
<span id="cb1-392"><a href="#cb1-392"></a></span>
<span id="cb1-393"><a href="#cb1-393"></a>To address the impractical requirement of human queries, as we discussed</span>
<span id="cb1-394"><a href="#cb1-394"></a>in the previous section, one may apply meta-learning so that the RL</span>
<span id="cb1-395"><a href="#cb1-395"></a>agent can adapt to new tasks with fewer human queries. <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span></span>
<span id="cb1-396"><a href="#cb1-396"></a>proposes to pre-training models on previous tasks with the meta-learning</span>
<span id="cb1-397"><a href="#cb1-397"></a>method MAML <span class="co">[</span><span class="ot">@finn2017model</span><span class="co">]</span>, and then the meta-trained model can adapt</span>
<span id="cb1-398"><a href="#cb1-398"></a>to new tasks with fewer queries.</span>
<span id="cb1-399"><a href="#cb1-399"></a></span>
<span id="cb1-400"><a href="#cb1-400"></a>We consider Reinforcement Learning (RL) settings where a state is</span>
<span id="cb1-401"><a href="#cb1-401"></a>denoted as $s\in S$, and action is denoted as $a\in A$, for state space</span>
<span id="cb1-402"><a href="#cb1-402"></a>$S$ and action space $A$. The reward function $r:S\times A \to \mathbb{R}$ is</span>
<span id="cb1-403"><a href="#cb1-403"></a>unknown and need to be learned from eliciting human preferences. There</span>
<span id="cb1-404"><a href="#cb1-404"></a>are multiple tasks, where each task has its own reward function and</span>
<span id="cb1-405"><a href="#cb1-405"></a>transition probabilities. The reward model is parameterized by $\psi$.</span>
<span id="cb1-406"><a href="#cb1-406"></a>We denote $\hat{r}_\psi(s,a)$ to be a learned estimate of an unknown</span>
<span id="cb1-407"><a href="#cb1-407"></a>ground-truth reward function $r(s,a)$, parameterized by $\psi$.</span>
<span id="cb1-408"><a href="#cb1-408"></a>Accordingly, a reward model determines a RL policy $\phi$ by maximizing</span>
<span id="cb1-409"><a href="#cb1-409"></a>the accumulated rewards. The preferences is learned via pairwise</span>
<span id="cb1-410"><a href="#cb1-410"></a>comparison of trajectory segments $$\begin{aligned}</span>
<span id="cb1-411"><a href="#cb1-411"></a>    \sigma = (s_t, a_t, s_{t+1}, a_{t+1}, ..., s_{t+k-1}, s_{t+k-1})</span>
<span id="cb1-412"><a href="#cb1-412"></a>\end{aligned}$$ of $k$ states and actions.</span>
<span id="cb1-413"><a href="#cb1-413"></a></span>
<span id="cb1-414"><a href="#cb1-414"></a>For each pre-training task, there is a dataset $D$ consists of labeled</span>
<span id="cb1-415"><a href="#cb1-415"></a>queries $(\sigma_1, \sigma_2, y)$ where $y\in <span class="sc">\{</span>0, 1<span class="sc">\}</span>$ is the label</span>
<span id="cb1-416"><a href="#cb1-416"></a>representing which trajectory is preferred. Therefore, a loss function</span>
<span id="cb1-417"><a href="#cb1-417"></a>$L(\psi, D)$ captures how well the reward model characterizes the</span>
<span id="cb1-418"><a href="#cb1-418"></a>preferences in dataset $D$. In <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span> they the preference</span>
<span id="cb1-419"><a href="#cb1-419"></a>predictor over segments using the Bradley-Terry model of paired</span>
<span id="cb1-420"><a href="#cb1-420"></a>comparisons <span class="co">[</span><span class="ot">@bradley1952rank</span><span class="co">]</span>, i.e., $$\begin{aligned}</span>
<span id="cb1-421"><a href="#cb1-421"></a>    P<span class="co">[</span><span class="ot">\sigma_1 \succ \sigma_2 </span><span class="co">]</span> = \frac{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1})}{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1}) + \exp \sum_t \hat{r}_\psi(s_t^{2}, a_t^{2})}.</span>
<span id="cb1-422"><a href="#cb1-422"></a>\end{aligned}$$ Then, the loss function is essentially a binary</span>
<span id="cb1-423"><a href="#cb1-423"></a>cross-entropy which the reward model $\psi$ aims to minimize, i.e.,</span>
<span id="cb1-424"><a href="#cb1-424"></a>$$\begin{aligned}</span>
<span id="cb1-425"><a href="#cb1-425"></a>    {L}(\psi,  {D}) = - \mathbb{E}_{(\sigma^1, \sigma^2, y) \sim {D}} \left<span class="co">[</span><span class="ot"> y(1) \log (P[\sigma_1 \succ \sigma_2 ]) + y(2)\log(1 - P[\sigma_1 \succ \sigma_2 ]) \right</span><span class="co">]</span>.</span>
<span id="cb1-426"><a href="#cb1-426"></a>\end{aligned}$$</span>
<span id="cb1-427"><a href="#cb1-427"></a></span>
<span id="cb1-428"><a href="#cb1-428"></a><span class="fu">##### Method Component 1: Pre-Training with Meta Learning {#method-component-1-pre-training-with-meta-learning}</span></span>
<span id="cb1-429"><a href="#cb1-429"></a></span>
<span id="cb1-430"><a href="#cb1-430"></a>To efficiently approximate the reward function $r_\text{new}$ for a new</span>
<span id="cb1-431"><a href="#cb1-431"></a>task with minimal queries, as described in <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span>, we aim to</span>
<span id="cb1-432"><a href="#cb1-432"></a>utilize a pre-trained reward function $\hat{r}_\psi$ that can be quickly</span>
<span id="cb1-433"><a href="#cb1-433"></a>fine-tuned using just a few preference comparisons. By pre-training on</span>
<span id="cb1-434"><a href="#cb1-434"></a>data from prior tasks, we can leverage the common structure across tasks</span>
<span id="cb1-435"><a href="#cb1-435"></a>to speed up the adaptation process. Although any meta-learning method is</span>
<span id="cb1-436"><a href="#cb1-436"></a>compatible, <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span> opt for Model Agnostic Meta-Learning (MAML)</span>
<span id="cb1-437"><a href="#cb1-437"></a>due to its simplicity. Therefore, the pre-training update for the reward</span>
<span id="cb1-438"><a href="#cb1-438"></a>model $\psi$ is $$\begin{aligned}</span>
<span id="cb1-439"><a href="#cb1-439"></a>    \psi \xleftarrow{} \psi - \beta \nabla_\psi \sum_{i = 1}^N {L} (\psi - \alpha \nabla_\psi {L}(\psi, {D}_i), {D}_i),</span>
<span id="cb1-440"><a href="#cb1-440"></a>\end{aligned}$$ where $\alpha, \beta$ are the inner and outer learning</span>
<span id="cb1-441"><a href="#cb1-441"></a>rate, respectively. We note that data $<span class="sc">\{</span>D_i<span class="sc">\}</span>_i$ of labeled preferences</span>
<span id="cb1-442"><a href="#cb1-442"></a>queries for prior tasks can come from offline datasets, simulated</span>
<span id="cb1-443"><a href="#cb1-443"></a>policies, or actual humans.</span>
<span id="cb1-444"><a href="#cb1-444"></a></span>
<span id="cb1-445"><a href="#cb1-445"></a><span class="fu">##### Method Component 2: Few-Shot Adaptation {#method-component-2-few-shot-adaptation}</span></span>
<span id="cb1-446"><a href="#cb1-446"></a></span>
<span id="cb1-447"><a href="#cb1-447"></a>With the aforementioned pre-training with meta learning, the</span>
<span id="cb1-448"><a href="#cb1-448"></a>meta-learned reward model can then be used for few-shot preference based</span>
<span id="cb1-449"><a href="#cb1-449"></a>RL during an online adaptation phase. The core procedure of the few-shot</span>
<span id="cb1-450"><a href="#cb1-450"></a>adaption is descibed as below</span>
<span id="cb1-451"><a href="#cb1-451"></a></span>
<span id="cb1-452"><a href="#cb1-452"></a><span class="ss">1.  </span>Given a pre-trained reward model $\psi$</span>
<span id="cb1-453"><a href="#cb1-453"></a></span>
<span id="cb1-454"><a href="#cb1-454"></a><span class="ss">2.  </span>For time step $t=1, 2, \dots$</span>
<span id="cb1-455"><a href="#cb1-455"></a></span>
<span id="cb1-456"><a href="#cb1-456"></a><span class="ss">    1.  </span>Find pairs of trajectories $(\sigma_1, \sigma_2)$ with</span>
<span id="cb1-457"><a href="#cb1-457"></a>        preference uncertainty based on $\psi$.</span>
<span id="cb1-458"><a href="#cb1-458"></a></span>
<span id="cb1-459"><a href="#cb1-459"></a><span class="ss">    2.  </span>Query human preference $y$ and forms a new dataset $D_{new}$</span>
<span id="cb1-460"><a href="#cb1-460"></a></span>
<span id="cb1-461"><a href="#cb1-461"></a><span class="ss">    3.  </span>Update the reward model by</span>
<span id="cb1-462"><a href="#cb1-462"></a>        $\psi'\leftarrow \psi - \alpha \nabla_\psi L(\psi, D_{new})$</span>
<span id="cb1-463"><a href="#cb1-463"></a></span>
<span id="cb1-464"><a href="#cb1-464"></a><span class="ss">    4.  </span>Update the policy with the new reward model $\psi'$</span>
<span id="cb1-465"><a href="#cb1-465"></a></span>
<span id="cb1-466"><a href="#cb1-466"></a>As mentioned in <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span>, uncertain queries are selected using</span>
<span id="cb1-467"><a href="#cb1-467"></a>the disagreement of an ensemble of reward functions over the preference</span>
<span id="cb1-468"><a href="#cb1-468"></a>predictors. Specifically, comparisons that maximize</span>
<span id="cb1-469"><a href="#cb1-469"></a>$\texttt{std}(P<span class="co">[</span><span class="ot">\sigma_1 \succ \sigma_2</span><span class="co">]</span>)$ are selected each time</span>
<span id="cb1-470"><a href="#cb1-470"></a>feedback is collected.</span>
<span id="cb1-471"><a href="#cb1-471"></a></span>
<span id="cb1-472"><a href="#cb1-472"></a>The whole pipeline of the method is outlined in @fig-few-1.</span>
<span id="cb1-473"><a href="#cb1-473"></a></span>
<span id="cb1-474"><a href="#cb1-474"></a>!<span class="co">[</span><span class="ot">An overview of the proposed method in [@hejna2023few</span><span class="co">]</span>. **Pre-training</span>
<span id="cb1-475"><a href="#cb1-475"></a>(left):** In the pre-training phase, trajectory segment comparisons are</span>
<span id="cb1-476"><a href="#cb1-476"></a>generated using data from previously learned tasks. Then, they are used</span>
<span id="cb1-477"><a href="#cb1-477"></a>to train a reward model. **Online-Adaptation (Right)**: After</span>
<span id="cb1-478"><a href="#cb1-478"></a>pre-training the reward model, it is adapted to new data from human</span>
<span id="cb1-479"><a href="#cb1-479"></a>feedback. The adapted reward model is then used to train a policy for a</span>
<span id="cb1-480"><a href="#cb1-480"></a>new task in a closed loop manner.](Figures/overview-few.png){#fig-few-1</span>
<span id="cb1-481"><a href="#cb1-481"></a>width="<span class="sc">\\</span>linewidth"}</span>
<span id="cb1-482"><a href="#cb1-482"></a></span>
<span id="cb1-483"><a href="#cb1-483"></a>We present one set of experiment from the paper, as it illustrates the</span>
<span id="cb1-484"><a href="#cb1-484"></a>effectiveness of the proposed method in a straightforward way. The</span>
<span id="cb1-485"><a href="#cb1-485"></a>experiment test the propoesed method on the Meta-World</span>
<span id="cb1-486"><a href="#cb1-486"></a>benchmark <span class="co">[</span><span class="ot">@yu2020meta</span><span class="co">]</span>. Three baselines are compared with the proposed</span>
<span id="cb1-487"><a href="#cb1-487"></a>method:</span>
<span id="cb1-488"><a href="#cb1-488"></a></span>
<span id="cb1-489"><a href="#cb1-489"></a><span class="ss">1.  </span>SAC: The Soft-Actor Critic RL algorithm trained from ground truth</span>
<span id="cb1-490"><a href="#cb1-490"></a>    rewards. This represents the standard best possible method given the</span>
<span id="cb1-491"><a href="#cb1-491"></a>    ground-truth reward.</span>
<span id="cb1-492"><a href="#cb1-492"></a></span>
<span id="cb1-493"><a href="#cb1-493"></a><span class="ss">2.  </span>PEBBLE: The PEBBLE algorithm <span class="co">[</span><span class="ot">@lee2021pebble</span><span class="co">]</span>. It does not use</span>
<span id="cb1-494"><a href="#cb1-494"></a>    information from pripor tasks.</span>
<span id="cb1-495"><a href="#cb1-495"></a></span>
<span id="cb1-496"><a href="#cb1-496"></a><span class="ss">3.  </span>Init: This method initialize the reward model with the pretained</span>
<span id="cb1-497"><a href="#cb1-497"></a>    weights from meta learning. However, instead of adapting the reward</span>
<span id="cb1-498"><a href="#cb1-498"></a>    model to the new task, it performs standard updates as in PEBBLE.</span>
<span id="cb1-499"><a href="#cb1-499"></a></span>
<span id="cb1-500"><a href="#cb1-500"></a>The results are shown in @fig-few-exp, where we can see that the proposed methord</span>
<span id="cb1-501"><a href="#cb1-501"></a>outperforms all of the baselines.</span>
<span id="cb1-502"><a href="#cb1-502"></a></span>
<span id="cb1-503"><a href="#cb1-503"></a>![Results on MetaWorld tasks. The title of each subplot indicates the</span>
<span id="cb1-504"><a href="#cb1-504"></a>task and number of artificial feedback queries used in training. Results</span>
<span id="cb1-505"><a href="#cb1-505"></a>for each method are shown across five seeds.</span>
<span id="cb1-506"><a href="#cb1-506"></a>](Figures/few-exp.png){#fig-few-exp width="<span class="sc">\\</span>linewidth"}</span>
<span id="cb1-507"><a href="#cb1-507"></a></span>
<span id="cb1-508"><a href="#cb1-508"></a>This paper <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span> shows that meta reward learning indeed reduce</span>
<span id="cb1-509"><a href="#cb1-509"></a>the number of queries of human preferences. However, as mentioned in the</span>
<span id="cb1-510"><a href="#cb1-510"></a>paper, there are still some drawbacks, as shown in the following.</span>
<span id="cb1-511"><a href="#cb1-511"></a></span>
<span id="cb1-512"><a href="#cb1-512"></a>Many of the queries the model pick for human preference elicitation are</span>
<span id="cb1-513"><a href="#cb1-513"></a>actually almost identical to human. After all, the model would pick the</span>
<span id="cb1-514"><a href="#cb1-514"></a>most uncertain pair of trajectories for human preference queries, and</span>
<span id="cb1-515"><a href="#cb1-515"></a>similar trajectories are for sure having high uncertainty in their</span>
<span id="cb1-516"><a href="#cb1-516"></a>preference. This suggest the need of new ways for designing the query</span>
<span id="cb1-517"><a href="#cb1-517"></a>selection strategy.</span>
<span id="cb1-518"><a href="#cb1-518"></a></span>
<span id="cb1-519"><a href="#cb1-519"></a>Moreover, despite the improved query complexity, it still needs an</span>
<span id="cb1-520"><a href="#cb1-520"></a>impractical amount of queries. As shown in @fig-few-exp, the "sweep into" task still needs 2500 human</span>
<span id="cb1-521"><a href="#cb1-521"></a>queries for it to work properly, which is still not ideal for what we</span>
<span id="cb1-522"><a href="#cb1-522"></a>want them to be.</span>
<span id="cb1-523"><a href="#cb1-523"></a></span>
<span id="cb1-524"><a href="#cb1-524"></a>In addition, it is mentioned in the paper that the proposed method may</span>
<span id="cb1-525"><a href="#cb1-525"></a>be even worse than training from scratch, if the new task is too</span>
<span id="cb1-526"><a href="#cb1-526"></a>out-of-distribution. Certainly, since meta-learning assumes</span>
<span id="cb1-527"><a href="#cb1-527"></a>in-distribution tasks, we cannot expect the proposed method to be good</span>
<span id="cb1-528"><a href="#cb1-528"></a>for out-of-distribution task. It is thus an interesting future direction</span>
<span id="cb1-529"><a href="#cb1-529"></a>to investigate whether one can design a method that automatically</span>
<span id="cb1-530"><a href="#cb1-530"></a>balance between using the prior information or training from scratch.</span>
<span id="cb1-531"><a href="#cb1-531"></a></span>
<span id="cb1-532"><a href="#cb1-532"></a><span class="fu">#### Watch Try Learn {#sec-watch}</span></span>
<span id="cb1-533"><a href="#cb1-533"></a></span>
<span id="cb1-534"><a href="#cb1-534"></a>Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards</span>
<span id="cb1-535"><a href="#cb1-535"></a><span class="co">[</span><span class="ot">@zhou2019watch</span><span class="co">]</span> asks the question "How can we efficiently learn both</span>
<span id="cb1-536"><a href="#cb1-536"></a>from expert demonstrations and from trials where we only get **binary**</span>
<span id="cb1-537"><a href="#cb1-537"></a>feedback from a human\". Why do we care about this question? In the</span>
<span id="cb1-538"><a href="#cb1-538"></a>context of robotics, a very compelling answer is the *cost of</span>
<span id="cb1-539"><a href="#cb1-539"></a>data-collection*. In a hypothetical world in which we have a vast number</span>
<span id="cb1-540"><a href="#cb1-540"></a>of **expert demonstrations** of robots accomplishing a large number of</span>
<span id="cb1-541"><a href="#cb1-541"></a>diverse tasks, we don't necessarily need to worry about learning from</span>
<span id="cb1-542"><a href="#cb1-542"></a>trials or from humans. We could simply learn a very capable imitation</span>
<span id="cb1-543"><a href="#cb1-543"></a>agent to perform any task. Natural Language Processing could be seen as</span>
<span id="cb1-544"><a href="#cb1-544"></a>living in this world, because internet-scale data is available.</span>
<span id="cb1-545"><a href="#cb1-545"></a>**Robots, however, are expensive**, so people generally don't have</span>
<span id="cb1-546"><a href="#cb1-546"></a>access to them, and therefore cannot use them to produce information to</span>
<span id="cb1-547"><a href="#cb1-547"></a>imitate. Similarly, **human time is expensive**, so even for large</span>
<span id="cb1-548"><a href="#cb1-548"></a>organizations that do have access to a lot of robots, it's still hard to</span>
<span id="cb1-549"><a href="#cb1-549"></a>collect a lot of expert demonstrations.</span>
<span id="cb1-550"><a href="#cb1-550"></a></span>
<span id="cb1-551"><a href="#cb1-551"></a>The largest available collection of robotics datasets today is Open</span>
<span id="cb1-552"><a href="#cb1-552"></a>X-Embodiment (<span class="co">[</span><span class="ot">@padalkar2023open</span><span class="co">]</span>), which consists of around 1M episodes</span>
<span id="cb1-553"><a href="#cb1-553"></a>from more than 300 different scenes. Even such large datastes are not</span>
<span id="cb1-554"><a href="#cb1-554"></a>enough to learn generally-capable robotic policies from imitation</span>
<span id="cb1-555"><a href="#cb1-555"></a>learning alone.</span>
<span id="cb1-556"><a href="#cb1-556"></a></span>
<span id="cb1-557"><a href="#cb1-557"></a>![Visualization of the Open X-Embodiment dataset collection. Even this</span>
<span id="cb1-558"><a href="#cb1-558"></a>large-scale dataset for robot learning is not yet enough to learn</span>
<span id="cb1-559"><a href="#cb1-559"></a>generally-capable robotic</span>
<span id="cb1-560"><a href="#cb1-560"></a>policies.](Figures/open_x_embodiment.png){#fig-open-x-embodiment}</span>
<span id="cb1-561"><a href="#cb1-561"></a></span>
<span id="cb1-562"><a href="#cb1-562"></a>**Main insight:** binary feedback is much cheaper to obtain than expert</span>
<span id="cb1-563"><a href="#cb1-563"></a>demonstrations! Instead of hiring people to act as robot operators to</span>
<span id="cb1-564"><a href="#cb1-564"></a>tell the robot exactly what to do, if there was a way of having many</span>
<span id="cb1-565"><a href="#cb1-565"></a>robots trying things in parallel, we can have humans watch videos of</span>
<span id="cb1-566"><a href="#cb1-566"></a>what the robots did and then give a success classification of whether</span>
<span id="cb1-567"><a href="#cb1-567"></a>the robot accomplished the goal. This is a much cheaper form of human</span>
<span id="cb1-568"><a href="#cb1-568"></a>supervision because the human labels don't necessarily need to be given</span>
<span id="cb1-569"><a href="#cb1-569"></a>in real time, so one human labeler can label many trajectories in</span>
<span id="cb1-570"><a href="#cb1-570"></a>parallel, and the human doesn't need to be a skilled robot operator.</span>
<span id="cb1-571"><a href="#cb1-571"></a></span>
<span id="cb1-572"><a href="#cb1-572"></a>Concretely, this paper seeks to learn new tasks with the following</span>
<span id="cb1-573"><a href="#cb1-573"></a>general problem setting:</span>
<span id="cb1-574"><a href="#cb1-574"></a></span>
<span id="cb1-575"><a href="#cb1-575"></a><span class="ss">1.  </span>We only get 1 expert demonstration of the target task</span>
<span id="cb1-576"><a href="#cb1-576"></a></span>
<span id="cb1-577"><a href="#cb1-577"></a><span class="ss">2.  </span>After seeing the expert demonstration, we have robots try to solve</span>
<span id="cb1-578"><a href="#cb1-578"></a>    the task 1 or more times.</span>
<span id="cb1-579"><a href="#cb1-579"></a></span>
<span id="cb1-580"><a href="#cb1-580"></a><span class="ss">3.  </span>The user (or some pre-defined reward function) annotates each trial</span>
<span id="cb1-581"><a href="#cb1-581"></a>    as success/failure.</span>
<span id="cb1-582"><a href="#cb1-582"></a></span>
<span id="cb1-583"><a href="#cb1-583"></a><span class="ss">4.  </span>The agent learns from both the demos and the annotated trials to</span>
<span id="cb1-584"><a href="#cb1-584"></a>    perform well on the target task.</span>
<span id="cb1-585"><a href="#cb1-585"></a></span>
<span id="cb1-586"><a href="#cb1-586"></a>Note that this work falls under the **meta-learning** umbrella, because</span>
<span id="cb1-587"><a href="#cb1-587"></a>we are learning an algorithm for quickly learning new tasks given new</span>
<span id="cb1-588"><a href="#cb1-588"></a>observations (demos, trials, and success labels.)</span>
<span id="cb1-589"><a href="#cb1-589"></a></span>
<span id="cb1-590"><a href="#cb1-590"></a>The **main contribution** of this paper is a meta-learning algorithm for</span>
<span id="cb1-591"><a href="#cb1-591"></a>incorporating demonstrations and binary feedback from trials to solve</span>
<span id="cb1-592"><a href="#cb1-592"></a>new tasks.</span>
<span id="cb1-593"><a href="#cb1-593"></a></span>
<span id="cb1-594"><a href="#cb1-594"></a>Meta-Learning deals with efficient learning of new tasks. In the context</span>
<span id="cb1-595"><a href="#cb1-595"></a>of robotics or reinforcement learning in general, **how do we define</span>
<span id="cb1-596"><a href="#cb1-596"></a>tasks**? We will use the Markov decision process (**MDP**) formalism. A</span>
<span id="cb1-597"><a href="#cb1-597"></a>task $T_i$ is described with the tuple $<span class="sc">\{</span>S, A, r_i, P_i<span class="sc">\}</span>$.</span>
<span id="cb1-598"><a href="#cb1-598"></a></span>
<span id="cb1-599"><a href="#cb1-599"></a><span class="ss">1.  </span>$S$ represents the *state-space* of the task, or all possible states</span>
<span id="cb1-600"><a href="#cb1-600"></a>    the agent could find itself in. This work uses image-observations,</span>
<span id="cb1-601"><a href="#cb1-601"></a>    so $S$ is the space of all possible RGB images.</span>
<span id="cb1-602"><a href="#cb1-602"></a></span>
<span id="cb1-603"><a href="#cb1-603"></a><span class="ss">2.  </span>$A$ is the action space, meaning the set of all possible actions the</span>
<span id="cb1-604"><a href="#cb1-604"></a>    agent could take. In robotics there are many ways of representing</span>
<span id="cb1-605"><a href="#cb1-605"></a>    action spaces, and this work considers end-effector positions,</span>
<span id="cb1-606"><a href="#cb1-606"></a>    rotations, and opening.</span>
<span id="cb1-607"><a href="#cb1-607"></a></span>
<span id="cb1-608"><a href="#cb1-608"></a><span class="ss">3.  </span>$r_i$ is the reward function for the task, with function signature</span>
<span id="cb1-609"><a href="#cb1-609"></a>    $r_i : S \times A \to \mathbb{R}$. This work assumes all reward functions</span>
<span id="cb1-610"><a href="#cb1-610"></a>    are binary.</span>
<span id="cb1-611"><a href="#cb1-611"></a></span>
<span id="cb1-612"><a href="#cb1-612"></a><span class="ss">4.  </span>$P_i$ is the transition dynamics function. It's a function that maps</span>
<span id="cb1-613"><a href="#cb1-613"></a>    state-action pairs to probability distributions over next states.</span>
<span id="cb1-614"><a href="#cb1-614"></a></span>
<span id="cb1-615"><a href="#cb1-615"></a>Notice that $S$ and $A$ are shared across tasks. Transition dynamics</span>
<span id="cb1-616"><a href="#cb1-616"></a>functions are normally also shared between tasks because they represent</span>
<span id="cb1-617"><a href="#cb1-617"></a>the laws of physics. However, this work considers environments with</span>
<span id="cb1-618"><a href="#cb1-618"></a>different objects, so they don't share the dynamics function. Given this</span>
<span id="cb1-619"><a href="#cb1-619"></a>definition for tasks, they assume that the tasks from the data that they</span>
<span id="cb1-620"><a href="#cb1-620"></a>get come from some unknown task-generating distribution $p(T)$.</span>
<span id="cb1-621"><a href="#cb1-621"></a></span>
<span id="cb1-622"><a href="#cb1-622"></a>Let's give a more precise definition of the problem statement considered</span>
<span id="cb1-623"><a href="#cb1-623"></a>by **Watch, Try, Learn**. As the paper name suggests, there are 3 phases</span>
<span id="cb1-624"><a href="#cb1-624"></a>for the problem statement.</span>
<span id="cb1-625"><a href="#cb1-625"></a></span>
<span id="cb1-626"><a href="#cb1-626"></a>**Watch:** During the *watch* phase, we give the agent $K$</span>
<span id="cb1-627"><a href="#cb1-627"></a>demonstrations of the target tasks. This paper considers the case where</span>
<span id="cb1-628"><a href="#cb1-628"></a>$K$ always equals 1, and all demonstrations are successful. That is,</span>
<span id="cb1-629"><a href="#cb1-629"></a>each demonstration consists of a trajectory</span>
<span id="cb1-630"><a href="#cb1-630"></a>$<span class="sc">\{</span>(s_0, a_0), \ldots, (s_H, a_H)<span class="sc">\}</span>$ where $H$ is the task horizon, and</span>
<span id="cb1-631"><a href="#cb1-631"></a>the final state is always successful, that is</span>
<span id="cb1-632"><a href="#cb1-632"></a>$r_i(s_H, a_H) = 1, r_i(s_j, a_j) = 0$ for every $j \neq H$.</span>
<span id="cb1-633"><a href="#cb1-633"></a></span>
<span id="cb1-634"><a href="#cb1-634"></a>Importantly, these demonstrations alone might not be sufficient for</span>
<span id="cb1-635"><a href="#cb1-635"></a>**full task specification**. As an example, consider a demonstration in</span>
<span id="cb1-636"><a href="#cb1-636"></a>which an apple is moved to the right, next to a pan. Seeing this</span>
<span id="cb1-637"><a href="#cb1-637"></a>demonstration alone, the task could be always moving the apple to the</span>
<span id="cb1-638"><a href="#cb1-638"></a>right, or it could be always moving the apple next to the pan,</span>
<span id="cb1-639"><a href="#cb1-639"></a>irrespective of where the pan is. The expected output after the Watch</span>
<span id="cb1-640"><a href="#cb1-640"></a>phase is a policy capable of gathering information about a task, given</span>
<span id="cb1-641"><a href="#cb1-641"></a>demonstrations.</span>
<span id="cb1-642"><a href="#cb1-642"></a></span>
<span id="cb1-643"><a href="#cb1-643"></a>**Try:** In the Try phase, we use the agent learned during the Watch</span>
<span id="cb1-644"><a href="#cb1-644"></a>phase to attempt the task for $L$ trials. As specified earlier, this</span>
<span id="cb1-645"><a href="#cb1-645"></a>paper considers the casae where $L$ always equals 1. After the agent</span>
<span id="cb1-646"><a href="#cb1-646"></a>completes the trials, humans (or pre-programmed reward functions)</span>
<span id="cb1-647"><a href="#cb1-647"></a>provide one binary reward for each trial, indicating whether the trial</span>
<span id="cb1-648"><a href="#cb1-648"></a>was successful. The expected output of this phase is $L$ trajectories</span>
<span id="cb1-649"><a href="#cb1-649"></a>and corresponding feedback that hopefully *disambiguate* the task.</span>
<span id="cb1-650"><a href="#cb1-650"></a></span>
<span id="cb1-651"><a href="#cb1-651"></a>**Learn:** After completing the trials, the agent must learn from both</span>
<span id="cb1-652"><a href="#cb1-652"></a>the original expert demonstrations and the trials, and become capable of</span>
<span id="cb1-653"><a href="#cb1-653"></a>solving the target task.</span>
<span id="cb1-654"><a href="#cb1-654"></a></span>
<span id="cb1-655"><a href="#cb1-655"></a>**Given Data:** To train agents that can Watch, Try, and Learn, we are</span>
<span id="cb1-656"><a href="#cb1-656"></a>given a dataset of expert demonstrations containing multiple demos for</span>
<span id="cb1-657"><a href="#cb1-657"></a>each task, and the dataset contains hundreds of tasks. Importantly, **no</span>
<span id="cb1-658"><a href="#cb1-658"></a>online interaction** is needed for training, and this method trains only</span>
<span id="cb1-659"><a href="#cb1-659"></a>with **supervised learning** and no reinforcement learning.</span>
<span id="cb1-660"><a href="#cb1-660"></a></span>
<span id="cb1-661"><a href="#cb1-661"></a>This section describes exactly how this paper trains an agent from the</span>
<span id="cb1-662"><a href="#cb1-662"></a>given expert demonstrations, and how to incorporate the trials and human</span>
<span id="cb1-663"><a href="#cb1-663"></a>feedback into the loop.</span>
<span id="cb1-664"><a href="#cb1-664"></a></span>
<span id="cb1-665"><a href="#cb1-665"></a>**Training to Watch:** We now describe the algorithm to obtain an agent</span>
<span id="cb1-666"><a href="#cb1-666"></a>conditioned on the given expert demonstration. In particular, what we</span>
<span id="cb1-667"><a href="#cb1-667"></a>want to obtain out of the Watch phase is a policy conditioned on a set</span>
<span id="cb1-668"><a href="#cb1-668"></a>of expert demonstrations. Formally, we want to obtain</span>
<span id="cb1-669"><a href="#cb1-669"></a>$\pi_\theta^{\text{watch}}(a | s, <span class="sc">\{</span>d_{i,k}<span class="sc">\}</span>)$.</span>
<span id="cb1-670"><a href="#cb1-670"></a></span>
<span id="cb1-671"><a href="#cb1-671"></a>The way we can obtain this policy is through **meta-imitation</span>
<span id="cb1-672"><a href="#cb1-672"></a>learning**. Given the demonstrations $<span class="sc">\{</span>\textbf{d}_{i,k}<span class="sc">\}</span>$ for task</span>
<span id="cb1-673"><a href="#cb1-673"></a>$i$, we sample another *different* demonstration coming from the same</span>
<span id="cb1-674"><a href="#cb1-674"></a>task $\textbf{d}_i^{\text{test}}$. The key insight here is that</span>
<span id="cb1-675"><a href="#cb1-675"></a>$\textbf{d}_i^{\text{test}}$ is an example of **optimal behavior** given</span>
<span id="cb1-676"><a href="#cb1-676"></a>the demonstrations. Therefore, to obtain</span>
<span id="cb1-677"><a href="#cb1-677"></a>$\pi_\theta^{\text{watch}}(a | s, <span class="sc">\{</span>d_{i,k}<span class="sc">\}</span>)$, we simply regress the</span>
<span id="cb1-678"><a href="#cb1-678"></a>policy to imitate actions taken on $\textbf{d}_i^{\text{test}}$.</span>
<span id="cb1-679"><a href="#cb1-679"></a>Concretely, we train policy parameters $\theta$ to minimize the</span>
<span id="cb1-680"><a href="#cb1-680"></a>following loss:</span>
<span id="cb1-681"><a href="#cb1-681"></a></span>
<span id="cb1-682"><a href="#cb1-682"></a>$\mathcal{L}^\text{watch}(\theta, \mathcal{D}_i^*) = \mathbb{E}_{\{d_{i,k}\} \sim \mathcal{D}_i^*} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^*  \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[ </span>
<span id="cb1-683"><a href="#cb1-683"></a><span class="ss">- </span>\log \pi_\theta^{\text{watch}} (a_t | s_t, \{d_{i,k}<span class="sc">\}</span>) \big]$</span>
<span id="cb1-684"><a href="#cb1-684"></a></span>
<span id="cb1-685"><a href="#cb1-685"></a>This corresponds to doing imitation learning by minimizing the negative</span>
<span id="cb1-686"><a href="#cb1-686"></a>log-likelihood of the test trajectory actions, conditioning the policy</span>
<span id="cb1-687"><a href="#cb1-687"></a>on the entire demo set. However, how is the conditioning on the demo set</span>
<span id="cb1-688"><a href="#cb1-688"></a>achieved?</span>
<span id="cb1-689"><a href="#cb1-689"></a></span>
<span id="cb1-690"><a href="#cb1-690"></a>![Vision-based policy architecture that conditions on a set of</span>
<span id="cb1-691"><a href="#cb1-691"></a>demonstrations.](Figures/watch-try-learn-architecture.png){#fig-watch-try-learn-arch}</span>
<span id="cb1-692"><a href="#cb1-692"></a></span>
<span id="cb1-693"><a href="#cb1-693"></a>@fig-watch-try-learn-arch visualizes how Watch Try Learn</span>
<span id="cb1-694"><a href="#cb1-694"></a>deals with conditioning on demonstrations. In addition to using features</span>
<span id="cb1-695"><a href="#cb1-695"></a>obtained from the images of the current state, the architecture uses</span>
<span id="cb1-696"><a href="#cb1-696"></a>features from frames sampled (in order) from the demonstration episodes,</span>
<span id="cb1-697"><a href="#cb1-697"></a>which are concatenated together.</span>
<span id="cb1-698"><a href="#cb1-698"></a></span>
<span id="cb1-699"><a href="#cb1-699"></a>**Trying:** On the **Try** phase, when the agent is given a set of</span>
<span id="cb1-700"><a href="#cb1-700"></a>demonstrations $<span class="sc">\{</span>\textbf{d}_{i,k}<span class="sc">\}</span>$, we deploy the policy</span>
<span id="cb1-701"><a href="#cb1-701"></a>$\pi_\theta^{\text{watch}}(a | s, <span class="sc">\{</span>\textbf{d}_{i,k}<span class="sc">\}</span>)$ to collect $L$</span>
<span id="cb1-702"><a href="#cb1-702"></a>trials. There is no training involved in the Try phase, we simply</span>
<span id="cb1-703"><a href="#cb1-703"></a>condition the policy on the given demonstrations</span>
<span id="cb1-704"><a href="#cb1-704"></a></span>
<span id="cb1-705"><a href="#cb1-705"></a>**Training to Learn:** During the Watch phase the objective was to train</span>
<span id="cb1-706"><a href="#cb1-706"></a>a policy conditioned on demonstrations</span>
<span id="cb1-707"><a href="#cb1-707"></a>$\pi_\theta^{\text{watch}}(a | s, <span class="sc">\{</span>\textbf{d}_{i,k}<span class="sc">\}</span>)$. The authors of</span>
<span id="cb1-708"><a href="#cb1-708"></a>Watch, Try, Learn use a similar strategy as the Watch phase for the</span>
<span id="cb1-709"><a href="#cb1-709"></a>Learn phase. We now want to train a policy that is conditioned on the</span>
<span id="cb1-710"><a href="#cb1-710"></a>demonstrations, as well as the trials and binary feedback. That is, we</span>
<span id="cb1-711"><a href="#cb1-711"></a>want to learn</span>
<span id="cb1-712"><a href="#cb1-712"></a>$\pi_\phi^{\text{watch}}(a | s, <span class="sc">\{</span>\textbf{d}_{i,k}\}, \{\mathbf{\tau}_{i, l}<span class="sc">\}</span>)$.</span>
<span id="cb1-713"><a href="#cb1-713"></a>To train the policy, we again use meta-imitation learning where we</span>
<span id="cb1-714"><a href="#cb1-714"></a>additionally sample yet another trajectory from the same task.</span>
<span id="cb1-715"><a href="#cb1-715"></a>Concretely, we train policy parameters $\phi$ to minimize the following</span>
<span id="cb1-716"><a href="#cb1-716"></a>loss:</span>
<span id="cb1-717"><a href="#cb1-717"></a></span>
<span id="cb1-718"><a href="#cb1-718"></a>$\mathcal{L}^{\text{learn}}(\phi, \mathcal{D}_i, \mathcal{D}_i^*) = \mathbb{E}_{(\{d_{i,k}\}, \{\mathbf{\tau}_{i,l}\}) \sim \mathcal{D}_i} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^* \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[ </span>
<span id="cb1-719"><a href="#cb1-719"></a><span class="ss">- </span>\log \pi_\theta^{\text{learn}} (a_t | s_t, \{d_{i,k}\}, \{\tau_{i,l}<span class="sc">\}</span>) \big]$</span>
<span id="cb1-720"><a href="#cb1-720"></a></span>
<span id="cb1-721"><a href="#cb1-721"></a>The conditioning on both the demo episodes and the trial episodes is</span>
<span id="cb1-722"><a href="#cb1-722"></a>achieved in the exact same way as in the Watch phase, and is visualized</span>
<span id="cb1-723"><a href="#cb1-723"></a>in @fig-watch-try-learn-arch. The architecture is simply</span>
<span id="cb1-724"><a href="#cb1-724"></a>adjusted to be able to take in more images fro mthe trial episodes.</span>
<span id="cb1-725"><a href="#cb1-725"></a></span>
<span id="cb1-726"><a href="#cb1-726"></a>In this section, we describe the evaluation suite for the paper,</span>
<span id="cb1-727"><a href="#cb1-727"></a>including the simulation benchmark used, the baselines considered, and</span>
<span id="cb1-728"><a href="#cb1-728"></a>the results.</span>
<span id="cb1-729"><a href="#cb1-729"></a></span>
<span id="cb1-730"><a href="#cb1-730"></a>**Gripper environment setup:**</span>
<span id="cb1-731"><a href="#cb1-731"></a></span>
<span id="cb1-732"><a href="#cb1-732"></a>![Visualization of different tasks from the simulated benchmark for</span>
<span id="cb1-733"><a href="#cb1-733"></a>Watch Try Learn.](Figures/watch-try-learn-envs.png){#fig-envs}</span>
<span id="cb1-734"><a href="#cb1-734"></a></span>
<span id="cb1-735"><a href="#cb1-735"></a>@fig-envs illustrates the different task families considered in the simulated</span>
<span id="cb1-736"><a href="#cb1-736"></a>Gripper environment. Button Pressing, Grasping, Pushing, and Pick and</span>
<span id="cb1-737"><a href="#cb1-737"></a>Place. For each task family, the environment supports hundreds of</span>
<span id="cb1-738"><a href="#cb1-738"></a>different tasks by changing the objects in the scene and the objectives</span>
<span id="cb1-739"><a href="#cb1-739"></a>(e.g. which object to pick and where to place). For each task in each</span>
<span id="cb1-740"><a href="#cb1-740"></a>task family, a handful of expert demonstrations are given in a</span>
<span id="cb1-741"><a href="#cb1-741"></a>demonstrations dataset. As mentioned previously, the environment gives</span>
<span id="cb1-742"><a href="#cb1-742"></a>the agent image observations, and take in actions as end-effector</span>
<span id="cb1-743"><a href="#cb1-743"></a>(gripper) positions, angles, and opening.</span>
<span id="cb1-744"><a href="#cb1-744"></a></span>
<span id="cb1-745"><a href="#cb1-745"></a>**Baselines:** The following three baselines are considered:</span>
<span id="cb1-746"><a href="#cb1-746"></a></span>
<span id="cb1-747"><a href="#cb1-747"></a><span class="ss">1.  </span>**Behavior Cloning**: simple imitation learning based on maximum</span>
<span id="cb1-748"><a href="#cb1-748"></a>    log-likelihood training using data from all tasks.</span>
<span id="cb1-749"><a href="#cb1-749"></a></span>
<span id="cb1-750"><a href="#cb1-750"></a><span class="ss">2.  </span>**Meta-imitation learning**: This baseline corresponds to simply</span>
<span id="cb1-751"><a href="#cb1-751"></a>    running the policy from the Watch step, without using any trial</span>
<span id="cb1-752"><a href="#cb1-752"></a>    data. That is, we only condition on the set of expert</span>
<span id="cb1-753"><a href="#cb1-753"></a>    demonstrations, but no online trials.</span>
<span id="cb1-754"><a href="#cb1-754"></a></span>
<span id="cb1-755"><a href="#cb1-755"></a><span class="ss">3.  </span>**Behavior Cloning + SAC**: Pre-train a policy with Behavior Cloning</span>
<span id="cb1-756"><a href="#cb1-756"></a>    on all data, and follow that with Reinforcement Learning fine-tuning</span>
<span id="cb1-757"><a href="#cb1-757"></a>    for the specific target task, using the maximum-entropy algorithm</span>
<span id="cb1-758"><a href="#cb1-758"></a>    SAC (<span class="co">[</span><span class="ot">@haarnoja2018soft</span><span class="co">]</span>).</span>
<span id="cb1-759"><a href="#cb1-759"></a></span>
<span id="cb1-760"><a href="#cb1-760"></a>![Results for Watch Try Learn on the gripper control environment, and</span>
<span id="cb1-761"><a href="#cb1-761"></a>comparisons with</span>
<span id="cb1-762"><a href="#cb1-762"></a>baselines.](Figures/watch-try-learn-results.png){#fig-watch-try-learn-results</span>
<span id="cb1-763"><a href="#cb1-763"></a>width="50%"}</span>
<span id="cb1-764"><a href="#cb1-764"></a></span>
<span id="cb1-765"><a href="#cb1-765"></a>::: {#tbl-watch-try-learn-table}</span>
<span id="cb1-766"><a href="#cb1-766"></a>  **METHOD**                     **SUCCESS RATE**</span>
<span id="cb1-767"><a href="#cb1-767"></a>  ----------------------------- ------------------</span>
<span id="cb1-768"><a href="#cb1-768"></a>  BC                              .09 $\pm$ .01</span>
<span id="cb1-769"><a href="#cb1-769"></a>  MIL                             .30 $\pm$ .02</span>
<span id="cb1-770"><a href="#cb1-770"></a>  WTL, 1 TRIAL (OURS)             .42 $\pm$ .02</span>
<span id="cb1-771"><a href="#cb1-771"></a>  **RL FINE-TUNING WITH SAC**   </span>
<span id="cb1-772"><a href="#cb1-772"></a>  BC + SAC, 1500 TRIALS           .11 $\pm$ .07</span>
<span id="cb1-773"><a href="#cb1-773"></a>  BC + SAC, 2000 TRIALS           .29 $\pm$ .10</span>
<span id="cb1-774"><a href="#cb1-774"></a>  BC + SAC, 2500 TRIALS           .39 $\pm$ .11</span>
<span id="cb1-775"><a href="#cb1-775"></a></span>
<span id="cb1-776"><a href="#cb1-776"></a>  : Average success rates over all tasks.</span>
<span id="cb1-777"><a href="#cb1-777"></a>:::</span>
<span id="cb1-778"><a href="#cb1-778"></a></span>
<span id="cb1-779"><a href="#cb1-779"></a>@fig-watch-try-learn-results shows average success rates for</span>
<span id="cb1-780"><a href="#cb1-780"></a>Watch Try Learn compared to baselines. Watch Try Learn significantly</span>
<span id="cb1-781"><a href="#cb1-781"></a>outperforms baselines on every task family. In particular, it is far</span>
<span id="cb1-782"><a href="#cb1-782"></a>superior to Behavior Cloning, which is a very weak baseline, and it</span>
<span id="cb1-783"><a href="#cb1-783"></a>significantly surpasses Meta-Imitation Learning on 3 out of 4 task</span>
<span id="cb1-784"><a href="#cb1-784"></a>families. @tbl-watch-try-learn-table includes comparison with BC</span>
<span id="cb1-785"><a href="#cb1-785"></a>fine-tuned with Reinforcement Learning. Even after 2500 online trials,</span>
<span id="cb1-786"><a href="#cb1-786"></a>SAC is not able to obtain the success rate that Watch Try Learn achieves</span>
<span id="cb1-787"><a href="#cb1-787"></a>after only 1 trial. Overall, Watch Try Learn exhibits very significant</span>
<span id="cb1-788"><a href="#cb1-788"></a>performance gains over prior methods.</span>
<span id="cb1-789"><a href="#cb1-789"></a></span>
<span id="cb1-790"><a href="#cb1-790"></a><span class="fu">### Direct Preference Optimization</span></span>
<span id="cb1-791"><a href="#cb1-791"></a></span>
<span id="cb1-792"><a href="#cb1-792"></a>A modern method for estimating the parameters of a human preference</span>
<span id="cb1-793"><a href="#cb1-793"></a>model is direct preference optimization <span class="co">[</span><span class="ot">@rafailov2023direct</span><span class="co">]</span>, which is</span>
<span id="cb1-794"><a href="#cb1-794"></a>used in the context of aligning language models to human preferences. A</span>
<span id="cb1-795"><a href="#cb1-795"></a>recent approach <span class="co">[</span><span class="ot">@christiano2023deep</span><span class="co">]</span> first trains a reward model that</span>
<span id="cb1-796"><a href="#cb1-796"></a>captures human preferences and then uses proximal policy optimization to</span>
<span id="cb1-797"><a href="#cb1-797"></a>train a language model-based policy to reflect those learned</span>
<span id="cb1-798"><a href="#cb1-798"></a>preferences. Direct Preference Optimization (DPO), on the other hand,</span>
<span id="cb1-799"><a href="#cb1-799"></a>removes the need for a reward model by directly using the model</span>
<span id="cb1-800"><a href="#cb1-800"></a>likelihood of two outcomes (a preferred or highly-ranked sequence and an</span>
<span id="cb1-801"><a href="#cb1-801"></a>unpreferred or low-ranked sequence) to capture the preference</span>
<span id="cb1-802"><a href="#cb1-802"></a>represented in the data. DPO provides a simpler framework than its</span>
<span id="cb1-803"><a href="#cb1-803"></a>reinforcement learning approach and results in comparable performance</span>
<span id="cb1-804"><a href="#cb1-804"></a>with improved stability. Furthermore, it obviates the need to train a</span>
<span id="cb1-805"><a href="#cb1-805"></a>reward model, instead using a language model policy and human preference</span>
<span id="cb1-806"><a href="#cb1-806"></a>dataset to align the policy directly to human preferences.</span>
<span id="cb1-807"><a href="#cb1-807"></a></span>
<span id="cb1-808"><a href="#cb1-808"></a><span class="fu">### Model Design Consideration</span></span>
<span id="cb1-809"><a href="#cb1-809"></a></span>
<span id="cb1-810"><a href="#cb1-810"></a>When designing models and learning their parameters, one must account</span>
<span id="cb1-811"><a href="#cb1-811"></a>for important tradeoffs when designing and optimizing a model to learn</span>
<span id="cb1-812"><a href="#cb1-812"></a>human preferences.</span>
<span id="cb1-813"><a href="#cb1-813"></a></span>
<span id="cb1-814"><a href="#cb1-814"></a>**Bias vs. Variance Trade-off.** In modeling human preferences, we aim</span>
<span id="cb1-815"><a href="#cb1-815"></a>to ensure that predicted utilities accurately reflect overall human</span>
<span id="cb1-816"><a href="#cb1-816"></a>preferences. One key challenge is managing the bias and variance</span>
<span id="cb1-817"><a href="#cb1-817"></a>trade-off.</span>
<span id="cb1-818"><a href="#cb1-818"></a></span>
<span id="cb1-819"><a href="#cb1-819"></a>Bias refers to assumptions made during model design and training that</span>
<span id="cb1-820"><a href="#cb1-820"></a>can skew predictions. For example, in Ideal Point Models, we make the</span>
<span id="cb1-821"><a href="#cb1-821"></a>assumption that the representations we use for individuals and choices</span>
<span id="cb1-822"><a href="#cb1-822"></a>are aligned in the embedding space, and that this representation is</span>
<span id="cb1-823"><a href="#cb1-823"></a>sufficient to capture human preferences using distance metrics. However,</span>
<span id="cb1-824"><a href="#cb1-824"></a>there are myriad cases in which this may break down, for example if the</span>
<span id="cb1-825"><a href="#cb1-825"></a>two sets of vectors follow different distributions each with their own</span>
<span id="cb1-826"><a href="#cb1-826"></a>unique biases. If the representations do not come from the same domain,</span>
<span id="cb1-827"><a href="#cb1-827"></a>one may have little visibility into how a distance metric computes the</span>
<span id="cb1-828"><a href="#cb1-828"></a>final utility value for a choice for a given individual. Some ways to</span>
<span id="cb1-829"><a href="#cb1-829"></a>mitigate bias in human preference models include increasing the number</span>
<span id="cb1-830"><a href="#cb1-830"></a>of parameters in a model (allowing for better learning of patterns in</span>
<span id="cb1-831"><a href="#cb1-831"></a>the data) or removing inductive biases based on our assumptions of the</span>
<span id="cb1-832"><a href="#cb1-832"></a>underlying data.</span>
<span id="cb1-833"><a href="#cb1-833"></a></span>
<span id="cb1-834"><a href="#cb1-834"></a>On the other hand, variance refers to the model's sensitivity to small</span>
<span id="cb1-835"><a href="#cb1-835"></a>changes in the input, which leads to significant changes in the outp ut.</span>
<span id="cb1-836"><a href="#cb1-836"></a>This phenomenon is often termed 'overfitting' or 'overparameterization.'</span>
<span id="cb1-837"><a href="#cb1-837"></a>This behavior can occur in models that have many parameters, and learn</span>
<span id="cb1-838"><a href="#cb1-838"></a>correlations in the data that do not contribute to learning human</span>
<span id="cb1-839"><a href="#cb1-839"></a>preferences, but are artifacts of noise in the dataset that one should</span>
<span id="cb1-840"><a href="#cb1-840"></a>ultimately ignore. One can address variance in models by reducing the</span>
<span id="cb1-841"><a href="#cb1-841"></a>number of parameters or incorporating biases in the model based on</span>
<span id="cb1-842"><a href="#cb1-842"></a>factors we can assume about the data.</span>
<span id="cb1-843"><a href="#cb1-843"></a></span>
<span id="cb1-844"><a href="#cb1-844"></a>**Model Scope.** One important consideration unique to human preference</span>
<span id="cb1-845"><a href="#cb1-845"></a>models is that we wish to model individual preferences, and we may</span>
<span id="cb1-846"><a href="#cb1-846"></a>choose to do so at arbitrary granularity. For example, we can fit models</span>
<span id="cb1-847"><a href="#cb1-847"></a>to a specific individual or even multiple models for an individual, each</span>
<span id="cb1-848"><a href="#cb1-848"></a>for different purposes or contexts. On the other end of the spectrum, we</span>
<span id="cb1-849"><a href="#cb1-849"></a>may create a model to capture human preferences across large populations</span>
<span id="cb1-850"><a href="#cb1-850"></a>or the world.</span>
<span id="cb1-851"><a href="#cb1-851"></a></span>
<span id="cb1-852"><a href="#cb1-852"></a>Individual models may certainly prove to be more powerful, as they do</span>
<span id="cb1-853"><a href="#cb1-853"></a>not need to generalize across multiple individuals and can dedicate all</span>
<span id="cb1-854"><a href="#cb1-854"></a>of their parameters to learning the preferences of a single user. In the</span>
<span id="cb1-855"><a href="#cb1-855"></a>context of human behavior, this can be a significant advantage as any</span>
<span id="cb1-856"><a href="#cb1-856"></a>two individuals can be arbitrarily different or even opposite in their</span>
<span id="cb1-857"><a href="#cb1-857"></a>preferences. On the other hand, models fit only one person can</span>
<span id="cb1-858"><a href="#cb1-858"></a>tremendously overfit to the training distribution and capture noise in</span>
<span id="cb1-859"><a href="#cb1-859"></a>the data, which is not truly representative of human preferences.</span>
<span id="cb1-860"><a href="#cb1-860"></a></span>
<span id="cb1-861"><a href="#cb1-861"></a>On the end of the spectrum, models fit to the entire world may be</span>
<span id="cb1-862"><a href="#cb1-862"></a>inadequate to model human preferences for arbitrary individuals,</span>
<span id="cb1-863"><a href="#cb1-863"></a>especially those whose data it has not been fit to. As such, models may</span>
<span id="cb1-864"><a href="#cb1-864"></a>underfit the given training distribution. These models aim to generalize</span>
<span id="cb1-865"><a href="#cb1-865"></a>to many people but may fail to capture the nuances of individual</span>
<span id="cb1-866"><a href="#cb1-866"></a>preferences, especially for those whose data is not represented in the</span>
<span id="cb1-867"><a href="#cb1-867"></a>training set. As a result, they may not perform well for arbitrary</span>
<span id="cb1-868"><a href="#cb1-868"></a>individuals within the target population</span>
<span id="cb1-869"><a href="#cb1-869"></a></span>
<span id="cb1-870"><a href="#cb1-870"></a>Choosing the appropriate scope for a model is crucial. ne must balance</span>
<span id="cb1-871"><a href="#cb1-871"></a>the trade-off between overfitting to noise in highly granular models and</span>
<span id="cb1-872"><a href="#cb1-872"></a>underfitting in broader models that may not capture individual nuances.</span>
<span id="cb1-873"><a href="#cb1-873"></a></span>
<span id="cb1-874"><a href="#cb1-874"></a><span class="fu">## Multimodal Preferences</span></span>
<span id="cb1-875"><a href="#cb1-875"></a></span>
<span id="cb1-876"><a href="#cb1-876"></a>One of the core assumptions about learning a reward function is that it</span>
<span id="cb1-877"><a href="#cb1-877"></a>is unimodal, meaning that it consists of data from one person with a</span>
<span id="cb1-878"><a href="#cb1-878"></a>certain set of preferences or a group of people with similar</span>
<span id="cb1-879"><a href="#cb1-879"></a>preferences. However, the model of unimodality often oversimplifies</span>
<span id="cb1-880"><a href="#cb1-880"></a>human preferences and their often conflicting nature. To accurately</span>
<span id="cb1-881"><a href="#cb1-881"></a>capture all the nuances of human preference, we examine a multi-modal</span>
<span id="cb1-882"><a href="#cb1-882"></a>distribution with some baseline assumptions. Consider a scenario where</span>
<span id="cb1-883"><a href="#cb1-883"></a>we, as regular drivers, make a left-hand turn at an intersection</span>
<span id="cb1-884"><a href="#cb1-884"></a><span class="co">[</span><span class="ot">@myers2021learning</span><span class="co">]</span>. What would we do if we saw a car speeding down the</span>
<span id="cb1-885"><a href="#cb1-885"></a>road approaching us? The figure below describes some options. Following</span>
<span id="cb1-886"><a href="#cb1-886"></a>a timid driving pattern, some vehicles would stop to let the other car</span>
<span id="cb1-887"><a href="#cb1-887"></a>go, preventing a collision. Other vehicles would be more aggressive and</span>
<span id="cb1-888"><a href="#cb1-888"></a>try to make the turn before colliding with the oncoming vehicle. Given</span>
<span id="cb1-889"><a href="#cb1-889"></a>the data of one of these driving patterns, our model (our autonomous</span>
<span id="cb1-890"><a href="#cb1-890"></a>vehicle) can make an appropriate decision. However, what if our model</span>
<span id="cb1-891"><a href="#cb1-891"></a>was given data from both aggressive and timid drivers, and we don't know</span>
<span id="cb1-892"><a href="#cb1-892"></a>which data corresponds to which type of driver? If we applied standard</span>
<span id="cb1-893"><a href="#cb1-893"></a>learning based on comparison techniques, we see, as illustrated by the</span>
<span id="cb1-894"><a href="#cb1-894"></a>figure below, that the car would have an accident trying to find a</span>
<span id="cb1-895"><a href="#cb1-895"></a>policy close enough to both driving patterns.</span>
<span id="cb1-896"><a href="#cb1-896"></a></span>
<span id="cb1-897"><a href="#cb1-897"></a>![<span class="co">[</span><span class="ot">@myers2021learning</span><span class="co">]</span> shows the possibilities of 2 different driving</span>
<span id="cb1-898"><a href="#cb1-898"></a>patterns when a car is taking a left-hand turn at an intersection and</span>
<span id="cb1-899"><a href="#cb1-899"></a>sees another car approaching</span>
<span id="cb1-900"><a href="#cb1-900"></a>head-on.](Figures/driving-patt.png){#fig-driving-patt}</span>
<span id="cb1-901"><a href="#cb1-901"></a></span>
<span id="cb1-902"><a href="#cb1-902"></a>!<span class="co">[</span><span class="ot">The figure [@myers2021learning</span><span class="co">]</span> depicts the resultant collision when we</span>
<span id="cb1-903"><a href="#cb1-903"></a>try to find a policy close enough to both the driving</span>
<span id="cb1-904"><a href="#cb1-904"></a>patterns.](Figures/driving-coll.png){#fig-driving-coll}</span>
<span id="cb1-905"><a href="#cb1-905"></a></span>
<span id="cb1-906"><a href="#cb1-906"></a>As illustrated by the driving example, we see that multi-modality for</span>
<span id="cb1-907"><a href="#cb1-907"></a>our reward function is extremely important and, in some cases, if it is</span>
<span id="cb1-908"><a href="#cb1-908"></a>not considered, can lead to fatal decisions <span class="co">[</span><span class="ot">@myers2021learning</span><span class="co">]</span>. But why</span>
<span id="cb1-909"><a href="#cb1-909"></a>can't we label the groups, which would be the timid and aggressive</span>
<span id="cb1-910"><a href="#cb1-910"></a>drivers in the driving case, and then learn separate reward functions</span>
<span id="cb1-911"><a href="#cb1-911"></a>for each driver? The first problem with this approach is that it is</span>
<span id="cb1-912"><a href="#cb1-912"></a>inefficient and time-consuming to separate the data into groups because</span>
<span id="cb1-913"><a href="#cb1-913"></a>we would have to cluster and label the data. Secondly, it would not be</span>
<span id="cb1-914"><a href="#cb1-914"></a>accurate just to split the data because a more timid driver can be</span>
<span id="cb1-915"><a href="#cb1-915"></a>aggressive when they are in a hurry.</span>
<span id="cb1-916"><a href="#cb1-916"></a></span>
<span id="cb1-917"><a href="#cb1-917"></a>To formulate this problem of learning reward functions and mixing</span>
<span id="cb1-918"><a href="#cb1-918"></a>coefficients from ranking queries in a fully observable deterministic</span>
<span id="cb1-919"><a href="#cb1-919"></a>dynamical system, we begin by describing the system as a trajectory</span>
<span id="cb1-920"><a href="#cb1-920"></a>$\xi = (s_0, a_0, ..., s_T, a_T)$, where the sequence of states and</span>
<span id="cb1-921"><a href="#cb1-921"></a>actions represents the system's evolution over time. Assume there are</span>
<span id="cb1-922"><a href="#cb1-922"></a>$M$ different reward functions, each representing an expert's</span>
<span id="cb1-923"><a href="#cb1-923"></a>preferences. Using the linearity assumption in reward learning, we model</span>
<span id="cb1-924"><a href="#cb1-924"></a>each expert's reward function as a linear combination of features in a</span>
<span id="cb1-925"><a href="#cb1-925"></a>known, fixed feature space $\phi(\xi)$. The reward for the $m$-th expert</span>
<span id="cb1-926"><a href="#cb1-926"></a>is given by: $$R_m(\xi) = \omega^T_m \phi(\xi),$$ where $\omega_m$ is a</span>
<span id="cb1-927"><a href="#cb1-927"></a>vector of parameters corresponding to the $m$-th expert's preferences.</span>
<span id="cb1-928"><a href="#cb1-928"></a>There exists an unknown distribution over the reward parameters and we</span>
<span id="cb1-929"><a href="#cb1-929"></a>can represent this distribution with mixing coefficients $\alpha_m$ such</span>
<span id="cb1-930"><a href="#cb1-930"></a>that $\sum_M^{m = 1} \alpha_m = 1$. Our goal is to learn reward</span>
<span id="cb1-931"><a href="#cb1-931"></a>functions and mixing coefficients using ranking queries.</span>
<span id="cb1-932"><a href="#cb1-932"></a></span>
<span id="cb1-933"><a href="#cb1-933"></a>To define our problem, let's consider a robot who performs the following</span>
<span id="cb1-934"><a href="#cb1-934"></a>trajectories and asks a user to rank all the trajectories.</span>
<span id="cb1-935"><a href="#cb1-935"></a></span>
<span id="cb1-936"><a href="#cb1-936"></a>!<span class="co">[</span><span class="ot">The figure [@myers2022learning</span><span class="co">]</span> depicts a few different trajectories</span>
<span id="cb1-937"><a href="#cb1-937"></a>for an example multi-modal ranking</span>
<span id="cb1-938"><a href="#cb1-938"></a>scenario.](Figures/robot-traj.png){#fig-robot-traj}</span>
<span id="cb1-939"><a href="#cb1-939"></a></span>
<span id="cb1-940"><a href="#cb1-940"></a>The robot will be given back a set of trajectory rankings, coming from M</span>
<span id="cb1-941"><a href="#cb1-941"></a>humans and the objective is to learn the underlying reward function. We</span>
<span id="cb1-942"><a href="#cb1-942"></a>can represent the response of the ranking query as</span>
<span id="cb1-943"><a href="#cb1-943"></a>$x = (\xi_{a_1},\ ...\ ,\xi_{a_K})$ where $a_1$ is the index of the</span>
<span id="cb1-944"><a href="#cb1-944"></a>expert's top choice, $a_2$ is the index of the expert's second choice,</span>
<span id="cb1-945"><a href="#cb1-945"></a><span class="sc">\.</span>.. and so on. With the response $x$, we generate a probability</span>
<span id="cb1-946"><a href="#cb1-946"></a>distribution with the softmax rule <span class="co">[</span><span class="ot">@myers2022learning</span><span class="co">]</span>:</span>
<span id="cb1-947"><a href="#cb1-947"></a>$Pr(x_1 = \xi_{a_1} | R = R_m) = \frac{e^R_m(\xi_{a_1})}{\sum_{j=1}^Ke^R_m(\xi_{a_j})}$.</span>
<span id="cb1-948"><a href="#cb1-948"></a>where $R_m(\xi_{a_i})$ denotes the reward assigned by the $m$-th expert</span>
<span id="cb1-949"><a href="#cb1-949"></a>to trajectory $\xi_{a_i}$. Then, we randomly sample our probability</span>
<span id="cb1-950"><a href="#cb1-950"></a>distribution to pick our top choice. From the remaining trajectories, we</span>
<span id="cb1-951"><a href="#cb1-951"></a>noisily choose from our distribution to rank our second-best option. We</span>
<span id="cb1-952"><a href="#cb1-952"></a>repeat this process until we have ranked all our trajectories. This</span>
<span id="cb1-953"><a href="#cb1-953"></a>follows what is known as the Plackett-Luce Ranking Model.</span>
<span id="cb1-954"><a href="#cb1-954"></a></span>
<span id="cb1-955"><a href="#cb1-955"></a>Given knowledge of the true reward function weights $\omega_m$ and</span>
<span id="cb1-956"><a href="#cb1-956"></a>mixing coefficients $\alpha_m$, we have the following joint mass over</span>
<span id="cb1-957"><a href="#cb1-957"></a>observations x from a query Q:</span>
<span id="cb1-958"><a href="#cb1-958"></a>$Pr(x\ |\ Q) = \sum_{m = 1}^M \alpha_m\prod_{i = 1}^K\frac{e^{\omega_m^T \Phi(\xi_{a_i})}}{\sum_{j = i}^K e^{\omega_m^T \Phi(\xi_{a_j})}}$.</span>
<span id="cb1-959"><a href="#cb1-959"></a></span>
<span id="cb1-960"><a href="#cb1-960"></a>With the above formulation of the joint mass distribution over</span>
<span id="cb1-961"><a href="#cb1-961"></a>observation and queries, we can now formulate an objective.</span>
<span id="cb1-962"><a href="#cb1-962"></a>Specifically, it is to present users with the best set of queries that</span>
<span id="cb1-963"><a href="#cb1-963"></a>learn reward weights, $\omega$, and mixing coefficient, $\alpha$, based</span>
<span id="cb1-964"><a href="#cb1-964"></a>upon user rankings of preferred query responses. By learning these</span>
<span id="cb1-965"><a href="#cb1-965"></a>parameters, we can have an accurate estimation of the joint mass</span>
<span id="cb1-966"><a href="#cb1-966"></a>distribution of the observations.</span>
<span id="cb1-967"><a href="#cb1-967"></a></span>
<span id="cb1-968"><a href="#cb1-968"></a>To learn these parameters, we use a Bayesian learning framework. The</span>
<span id="cb1-969"><a href="#cb1-969"></a>goal will be to learn the reward weights, $\omega_m$, and all mixing</span>
<span id="cb1-970"><a href="#cb1-970"></a>coefficients $\alpha_m$. Thus, define the parameters to be</span>
<span id="cb1-971"><a href="#cb1-971"></a>$\theta = <span class="sc">\{</span>\omega, \alpha<span class="sc">\}</span>$. We start by simplifying the posterior</span>
<span id="cb1-972"><a href="#cb1-972"></a>over the parameters.</span>
<span id="cb1-973"><a href="#cb1-973"></a></span>
<span id="cb1-974"><a href="#cb1-974"></a>$$\begin{aligned}</span>
<span id="cb1-975"><a href="#cb1-975"></a>\Pr(\Theta | Q^{(1)}, x^{(1)}, Q^{(2)}, x^{(2)}, \ldots) &amp; \propto \Pr(\Theta) \Pr(Q^{(1)} | x^{(1)}, Q^{(2)}, x^{(2)}, \ldots | \Theta) <span class="sc">\\</span></span>
<span id="cb1-976"><a href="#cb1-976"></a>&amp; = \Pr(\Theta) \prod_t \Pr(x^{(t)} | Q^{(t)}, \Theta, Q^{(1)}, x^{(1)}, \ldots, Q^{(t-1)}, x^{(t-1)}) <span class="sc">\\</span></span>
<span id="cb1-977"><a href="#cb1-977"></a>&amp; \propto \Pr(\Theta) \prod_t \Pr(x^{(t)} | \Theta, Q^{(t)})</span>
<span id="cb1-978"><a href="#cb1-978"></a>\end{aligned}$$</span>
<span id="cb1-979"><a href="#cb1-979"></a></span>
<span id="cb1-980"><a href="#cb1-980"></a>Note that the first proportionality term is directly from Bayes rule</span>
<span id="cb1-981"><a href="#cb1-981"></a>(removing normalization constant). The first equation comes directly</span>
<span id="cb1-982"><a href="#cb1-982"></a>from the assumption that the queries at timestamp $t$ are conditionally</span>
<span id="cb1-983"><a href="#cb1-983"></a>independent of the parameters given previous queries &amp; rankings. This</span>
<span id="cb1-984"><a href="#cb1-984"></a>assumption is reasonable because the previous queries &amp; rankings ideally</span>
<span id="cb1-985"><a href="#cb1-985"></a>give all the information to inform the choice of the next set of. The</span>
<span id="cb1-986"><a href="#cb1-986"></a>last proportionality term comes from the assumption that the ranked</span>
<span id="cb1-987"><a href="#cb1-987"></a>queries are conditionally independent given the parameters</span>
<span id="cb1-988"><a href="#cb1-988"></a></span>
<span id="cb1-989"><a href="#cb1-989"></a>The prior distribution is dependent on use case. For example, in the</span>
<span id="cb1-990"><a href="#cb1-990"></a>user studies conducted by the authors to verify this method, they use a</span>
<span id="cb1-991"><a href="#cb1-991"></a>standard Gaussian for the reward weights and the mixing coefficients to</span>
<span id="cb1-992"><a href="#cb1-992"></a>be uniform on a $M - 1$ simplex to ensure that they add up to 1. Then we</span>
<span id="cb1-993"><a href="#cb1-993"></a>can use maximum likelihood estimation to compute the parameters with the</span>
<span id="cb1-994"><a href="#cb1-994"></a>simplified posterior.</span>
<span id="cb1-995"><a href="#cb1-995"></a></span>
<span id="cb1-996"><a href="#cb1-996"></a></span>
<span id="cb1-997"><a href="#cb1-997"></a><span class="co">&lt;!--{{&lt; include psets/pset1.qmd &gt;}}--&gt;</span></span>
<span id="cb1-998"><a href="#cb1-998"></a></span>
<span id="cb1-999"><a href="#cb1-999"></a><span class="co">&lt;!--</span></span>
<span id="cb1-1000"><a href="#cb1-1000"></a><span class="co">Through our exploration of human preference models, we will ground ourselves in</span></span>
<span id="cb1-1001"><a href="#cb1-1001"></a><span class="co">building a health coaching system that can provide meal recommendations aligned with a user's dietary needs and preferences. Examples of scenarios which can benefit from a model of how humans make choices include:</span></span>
<span id="cb1-1002"><a href="#cb1-1002"></a></span>
<span id="cb1-1003"><a href="#cb1-1003"></a><span class="co">1.  **Health coaching:** Humans express their preferences every time</span></span>
<span id="cb1-1004"><a href="#cb1-1004"></a><span class="co">    they pick lunch for consumption. Humans may have several goals</span></span>
<span id="cb1-1005"><a href="#cb1-1005"></a><span class="co">    related to nutrition, such as weight loss and improving</span></span>
<span id="cb1-1006"><a href="#cb1-1006"></a><span class="co">    concentration. We can learn how a given individual or set of</span></span>
<span id="cb1-1007"><a href="#cb1-1007"></a><span class="co">    individuals prefer to eat to provide personalized recommendations to</span></span>
<span id="cb1-1008"><a href="#cb1-1008"></a><span class="co">    help them attain their goals. This chapter will use this use case to</span></span>
<span id="cb1-1009"><a href="#cb1-1009"></a><span class="co">    ground human preference modeling in a real-life application.</span></span>
<span id="cb1-1010"><a href="#cb1-1010"></a></span>
<span id="cb1-1011"><a href="#cb1-1011"></a><span class="co">2.  **Social media:** Platforms have a far greater amount of content</span></span>
<span id="cb1-1012"><a href="#cb1-1012"></a><span class="co">    than one can consume in a lifetime, yet such products must aim to</span></span>
<span id="cb1-1013"><a href="#cb1-1013"></a><span class="co">    maximize user engagement. To accomplish this, we can learn what</span></span>
<span id="cb1-1014"><a href="#cb1-1014"></a><span class="co">    specific things people like to see in their feeds to optimize the</span></span>
<span id="cb1-1015"><a href="#cb1-1015"></a><span class="co">    value they gain out of their time on social media. For example, the</span></span>
<span id="cb1-1016"><a href="#cb1-1016"></a><span class="co">    video feed social media platform [TikTok](https://www.tiktok.com/)</span></span>
<span id="cb1-1017"><a href="#cb1-1017"></a><span class="co">    has had viral adoption due to its notorious ability to personalize a</span></span>
<span id="cb1-1018"><a href="#cb1-1018"></a><span class="co">    feed for its users based on their preferences.</span></span>
<span id="cb1-1019"><a href="#cb1-1019"></a></span>
<span id="cb1-1020"><a href="#cb1-1020"></a><span class="co">3.  **Shopping:** Retail corporations largely aim to maximize revenue by</span></span>
<span id="cb1-1021"><a href="#cb1-1021"></a><span class="co">    making it easy for people to make purchases. Recommendation systems</span></span>
<span id="cb1-1022"><a href="#cb1-1022"></a><span class="co">    on online shopping platforms provide a mechanism for curating</span></span>
<span id="cb1-1023"><a href="#cb1-1023"></a><span class="co">    specific items based on an individual's previous purchases (or even</span></span>
<span id="cb1-1024"><a href="#cb1-1024"></a><span class="co">    browsing history) to make shoppers aware of items they may like and,</span></span>
<span id="cb1-1025"><a href="#cb1-1025"></a><span class="co">    therefore, purchase.</span></span>
<span id="cb1-1026"><a href="#cb1-1026"></a></span>
<span id="cb1-1027"><a href="#cb1-1027"></a><span class="co">Two key models used in pairwise sampling are the Thurstonian and Bradley-Terry models [@cattelan2012]. The Thurstonian model assumes each item $i$ has a true score $u_i$ following a normal distribution. The difference $d_{ij} = u_i - u_j$ is also normally distributed. The probability that item $i$ is preferred over item $j$ is given by $P(i \succ j) = \Phi \left( \frac{u_i - u_j}{\sqrt{2\sigma^2}} \right)$, where $\Phi$ is the cumulative normal distribution function. The denominator $\sqrt{2\sigma^2}$ is the standard deviation of the difference $d_{ij} = u_i - u_j$ when $u_i$ and $u_j$ are normally distributed with variance $\sigma^2$[@cattelan2012]. The Bradley-Terry model defines the probability of preference based on latent scores $\beta_i$ and $\beta_j$. The probability that item $i$ is preferred over item $j$ is $P(i \succ j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}$. This model is used to estimate relative strengths or preferences based on latent scores. [@cattelan2012].</span></span>
<span id="cb1-1028"><a href="#cb1-1028"></a></span>
<span id="cb1-1029"><a href="#cb1-1029"></a><span class="co">::: {#tbl-philosophy}</span></span>
<span id="cb1-1030"><a href="#cb1-1030"></a><span class="co">  -----------------------------------------------------------------------</span></span>
<span id="cb1-1031"><a href="#cb1-1031"></a><span class="co">  Application                         Human Preference</span></span>
<span id="cb1-1032"><a href="#cb1-1032"></a><span class="co">  ----------------------------------- -----------------------------------</span></span>
<span id="cb1-1033"><a href="#cb1-1033"></a><span class="co">  Computer vision: train a neural     This is how humans process images</span></span>
<span id="cb1-1034"><a href="#cb1-1034"></a><span class="co">  network to predict bounding boxes   by identifying the position and</span></span>
<span id="cb1-1035"><a href="#cb1-1035"></a><span class="co">  delineating all instances of dogs   geometry of the things we see in</span></span>
<span id="cb1-1036"><a href="#cb1-1036"></a><span class="co">  in an image                         them</span></span>
<span id="cb1-1037"><a href="#cb1-1037"></a></span>
<span id="cb1-1038"><a href="#cb1-1038"></a><span class="co">  Natural language processing: train  Coherent text is itself a</span></span>
<span id="cb1-1039"><a href="#cb1-1039"></a><span class="co">  a model to generate coherent text   human-created and defined concept,</span></span>
<span id="cb1-1040"><a href="#cb1-1040"></a><span class="co">                                      and we prefer that any</span></span>
<span id="cb1-1041"><a href="#cb1-1041"></a><span class="co">                                      synthetically generated text</span></span>
<span id="cb1-1042"><a href="#cb1-1042"></a><span class="co">                                      matches that of humans</span></span>
<span id="cb1-1043"><a href="#cb1-1043"></a></span>
<span id="cb1-1044"><a href="#cb1-1044"></a><span class="co">  Computer vision: train a diffusion  Humans prefer that images</span></span>
<span id="cb1-1045"><a href="#cb1-1045"></a><span class="co">  model to generate realistic images  accurately capture the world as</span></span>
<span id="cb1-1046"><a href="#cb1-1046"></a><span class="co">  of nature                           observed by humans, and this</span></span>
<span id="cb1-1047"><a href="#cb1-1047"></a><span class="co">                                      generative model should reflect the</span></span>
<span id="cb1-1048"><a href="#cb1-1048"></a><span class="co">                                      details that comprise that</span></span>
<span id="cb1-1049"><a href="#cb1-1049"></a><span class="co">                                      preference</span></span>
<span id="cb1-1050"><a href="#cb1-1050"></a><span class="co">  -----------------------------------------------------------------------</span></span>
<span id="cb1-1051"><a href="#cb1-1051"></a></span>
<span id="cb1-1052"><a href="#cb1-1052"></a><span class="co">  : Examples of machine learning tasks and their interpretation as</span></span>
<span id="cb1-1053"><a href="#cb1-1053"></a><span class="co">  modeling human preferences.</span></span>
<span id="cb1-1054"><a href="#cb1-1054"></a><span class="co">:::</span></span>
<span id="cb1-1055"><a href="#cb1-1055"></a><span class="co">--&gt;</span></span>
<span id="cb1-1056"><a href="#cb1-1056"></a></span>
<span id="cb1-1057"><a href="#cb1-1057"></a><span class="co">&lt;!--</span></span>
<span id="cb1-1058"><a href="#cb1-1058"></a><span class="co">Game theory provides a mathematical framework for analyzing strategic</span></span>
<span id="cb1-1059"><a href="#cb1-1059"></a><span class="co">interactions among rational agents. These models help in understanding</span></span>
<span id="cb1-1060"><a href="#cb1-1060"></a><span class="co">and predicting human behavior by considering multiple criteria and the</span></span>
<span id="cb1-1061"><a href="#cb1-1061"></a><span class="co">associated trade-offs. They enhance the understanding of preferences</span></span>
<span id="cb1-1062"><a href="#cb1-1062"></a><span class="co">across multiple criteria and allow for richer and more accurate feedback</span></span>
<span id="cb1-1063"><a href="#cb1-1063"></a><span class="co">through structured comparisons. Game-theory framings capture the</span></span>
<span id="cb1-1064"><a href="#cb1-1064"></a><span class="co">complexity of preferences and interactions in decision-making processes</span></span>
<span id="cb1-1065"><a href="#cb1-1065"></a><span class="co">[@bhatia2020preference].</span></span>
<span id="cb1-1066"><a href="#cb1-1066"></a></span>
<span id="cb1-1067"><a href="#cb1-1067"></a><span class="co">The most popular form of preference elicitation involves pairwise</span></span>
<span id="cb1-1068"><a href="#cb1-1068"></a><span class="co">comparisons. Users are asked to choose between two options, such as</span></span>
<span id="cb1-1069"><a href="#cb1-1069"></a><span class="co">product A or product B. This method is used in various applications like</span></span>
<span id="cb1-1070"><a href="#cb1-1070"></a><span class="co">search engines, recommender systems, and interactive robotics. Key</span></span>
<span id="cb1-1071"><a href="#cb1-1071"></a><span class="co">concepts include the Von Neumann Winner and the Blackwell Winner. The</span></span>
<span id="cb1-1072"><a href="#cb1-1072"></a><span class="co">Von Neumann Winner refers to a distribution over objects that beats or</span></span>
<span id="cb1-1073"><a href="#cb1-1073"></a><span class="co">ties every other object in the collection under the expected utility</span></span>
<span id="cb1-1074"><a href="#cb1-1074"></a><span class="co">assumption. The Blackwell Winner generalizes the Von Neumann Winner for</span></span>
<span id="cb1-1075"><a href="#cb1-1075"></a><span class="co">multi-criteria problems using a target set for acceptable payoff vectors</span></span>
<span id="cb1-1076"><a href="#cb1-1076"></a><span class="co">[@bhatia2020preference].</span></span>
<span id="cb1-1077"><a href="#cb1-1077"></a></span>
<span id="cb1-1078"><a href="#cb1-1078"></a><span class="co">Game-theory framings provide a framework for preference learning along</span></span>
<span id="cb1-1079"><a href="#cb1-1079"></a><span class="co">multiple criteria. These models use tools from vector-valued payoffs in</span></span>
<span id="cb1-1080"><a href="#cb1-1080"></a><span class="co">game theory, with Blackwell's approach being a key concept. This</span></span>
<span id="cb1-1081"><a href="#cb1-1081"></a><span class="co">approach allows for a more comprehensive understanding of preferences by</span></span>
<span id="cb1-1082"><a href="#cb1-1082"></a><span class="co">considering multiple criteria simultaneously [@bhatia2020preference].</span></span>
<span id="cb1-1083"><a href="#cb1-1083"></a></span>
<span id="cb1-1084"><a href="#cb1-1084"></a><span class="co">In game-theory framings, pairwise preferences are modeled as random</span></span>
<span id="cb1-1085"><a href="#cb1-1085"></a><span class="co">variables. Comparisons between objects along different criteria are</span></span>
<span id="cb1-1086"><a href="#cb1-1086"></a><span class="co">captured in a preference tensor $P$. This tensor models the probability</span></span>
<span id="cb1-1087"><a href="#cb1-1087"></a><span class="co">that one object is preferred over another along a specific criterion,</span></span>
<span id="cb1-1088"><a href="#cb1-1088"></a><span class="co">allowing for a detailed understanding of preferences across multiple</span></span>
<span id="cb1-1089"><a href="#cb1-1089"></a><span class="co">dimensions [@bhatia2020preference].</span></span>
<span id="cb1-1090"><a href="#cb1-1090"></a></span>
<span id="cb1-1091"><a href="#cb1-1091"></a><span class="co">The preference tensor $P$ captures object comparisons along different</span></span>
<span id="cb1-1092"><a href="#cb1-1092"></a><span class="co">criteria. It is defined as:</span></span>
<span id="cb1-1093"><a href="#cb1-1093"></a><span class="co">$$P(i_1, i_2; j) = P(i_1 \succ i_2 \text{ along criterion } j)$$ where</span></span>
<span id="cb1-1094"><a href="#cb1-1094"></a><span class="co">$P(i_2, i_1; j) = 1 - P(i_1, i_2; j)$. These values are aggregated to</span></span>
<span id="cb1-1095"><a href="#cb1-1095"></a><span class="co">form an overall preference matrix $P_{ov}$ [@bhatia2020preference].</span></span>
<span id="cb1-1096"><a href="#cb1-1096"></a></span>
<span id="cb1-1097"><a href="#cb1-1097"></a><span class="co">The Blackwell Winner is defined using a target set $S$ of acceptable</span></span>
<span id="cb1-1098"><a href="#cb1-1098"></a><span class="co">score vectors. The goal is to find a distribution $\pi^*$ such that</span></span>
<span id="cb1-1099"><a href="#cb1-1099"></a><span class="co">$P(\pi^*, \pi) \in S$ for all $\pi$. This method minimizes the maximum</span></span>
<span id="cb1-1100"><a href="#cb1-1100"></a><span class="co">distance to the target set, providing a robust solution to</span></span>
<span id="cb1-1101"><a href="#cb1-1101"></a><span class="co">multi-criteria preference problems [@bhatia2020preference].</span></span>
<span id="cb1-1102"><a href="#cb1-1102"></a></span>
<span id="cb1-1103"><a href="#cb1-1103"></a><span class="co">The optimization problem for finding the Blackwell Winner is defined as:</span></span>
<span id="cb1-1104"><a href="#cb1-1104"></a><span class="co">$$\pi(P, S, \|\cdot\|) = \arg \min_{\pi \in \Delta_d} \left[ \max_{\pi' \in \Delta_d} \rho(P(\pi, \pi'), S) \right]$$</span></span>
<span id="cb1-1105"><a href="#cb1-1105"></a><span class="co">where $\rho(u, v) = \|u - v\|$. This measures the distance to the target</span></span>
<span id="cb1-1106"><a href="#cb1-1106"></a><span class="co">set, ensuring that the selected distribution is as close as possible to</span></span>
<span id="cb1-1107"><a href="#cb1-1107"></a><span class="co">the ideal preference vector [@bhatia2020preference].</span></span>
<span id="cb1-1108"><a href="#cb1-1108"></a><span class="co">--&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
    <footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/002-reward_model.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer><script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




</body></html>