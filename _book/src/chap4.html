<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Aggregation of Preferences – Machine Learning from Human Preferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../src/chap5.html" rel="next">
<link href="../src/chap3.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-07ba0ad10f5680c660e360ac31d2f3b6.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8b864f0777c60eecff11d75b6b2e1175.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-fa3d1c749edcb96cd5cb7d620f3e5237.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-6f24586c8b15e78d85e3983c622e3e8a.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../src/chap4.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Aggregation of Preferences</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning from Human Preferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sangttruong/mlhp" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Machine-Learning-from-Human-Preferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Models of Preferences and Decisions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Learning Model of Preferences</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Learning Model of Decisions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap4.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Aggregation of Preferences</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/ack.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>license.qmd</p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#social-choice-theory-and-implications-for-ai-preference-aggregation" id="toc-social-choice-theory-and-implications-for-ai-preference-aggregation" class="nav-link active" data-scroll-target="#social-choice-theory-and-implications-for-ai-preference-aggregation"><span class="header-section-number">5.1</span> Social Choice Theory and Implications for AI Preference Aggregation</a></li>
  <li><a href="#single-item-auctions" id="toc-single-item-auctions" class="nav-link" data-scroll-target="#single-item-auctions"><span class="header-section-number">5.2</span> Mechanism Design</a>
  <ul class="collapse">
  <li><a href="#case-study-1-mechanism-for-peer-grading" id="toc-case-study-1-mechanism-for-peer-grading" class="nav-link" data-scroll-target="#case-study-1-mechanism-for-peer-grading"><span class="header-section-number">5.2.1</span> Case Study 1: Mechanism for Peer Grading</a></li>
  <li><a href="#case-study-2-incentive-compatible-online-learning" id="toc-case-study-2-incentive-compatible-online-learning" class="nav-link" data-scroll-target="#case-study-2-incentive-compatible-online-learning"><span class="header-section-number">5.2.2</span> Case Study 2: Incentive-Compatible Online Learning</a></li>
  </ul></li>
  <li><a href="#mutual-information-paradigm" id="toc-mutual-information-paradigm" class="nav-link" data-scroll-target="#mutual-information-paradigm"><span class="header-section-number">5.3</span> Mutual Information Paradigm</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">5.4</span> Exercises</a>
  <ul class="collapse">
  <li><a href="#question-1-pairwise-feedback-mechanisms-for-digital-goods" id="toc-question-1-pairwise-feedback-mechanisms-for-digital-goods" class="nav-link" data-scroll-target="#question-1-pairwise-feedback-mechanisms-for-digital-goods"><span class="header-section-number">5.4.1</span> Question 1: Pairwise Feedback Mechanisms for Digital Goods</a></li>
  <li><a href="#question-2-scalable-oversight-in-complex-decision-making" id="toc-question-2-scalable-oversight-in-complex-decision-making" class="nav-link" data-scroll-target="#question-2-scalable-oversight-in-complex-decision-making"><span class="header-section-number">5.4.2</span> Question 2: Scalable Oversight in Complex Decision-Making</a></li>
  </ul></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/chap4.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Aggregation of Preferences</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="social-choice-theory-and-implications-for-ai-preference-aggregation" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="social-choice-theory-and-implications-for-ai-preference-aggregation"><span class="header-section-number">5.1</span> Social Choice Theory and Implications for AI Preference Aggregation</h2>
<p>In many applications, human preferences must be aggregated across multiple individuals to determine a collective decision or ranking. This process is central to social choice theory, which provides a mathematical foundation for preference aggregation. Unlike individual preference modeling, which focuses on how a single person makes decisions, social choice theory addresses the challenge of combining multiple preference profiles into a single coherent outcome. A social welfare function (SWF) takes as input each individual’s preference ranking over a set of alternatives and produces a social ranking of those alternatives. A related concept is a social choice function (SCF), which selects a single winning alternative given individuals’ preferences. Many voting rules can be seen as social choice functions that aim to reflect the group’s preferences. Formally, let <span class="math inline">\(N=\{1,2,\dots,n\}\)</span> be a set of <span class="math inline">\(n\)</span> voters (agents) and <span class="math inline">\(A=\{a_1,\dots,a_m\}\)</span> a set of <span class="math inline">\(m\)</span> alternatives (with <span class="math inline">\(m \ge 3\)</span>). Each voter <span class="math inline">\(i\)</span> has a preference order <span class="math inline">\(\succ_i\)</span> over <span class="math inline">\(A\)</span>. A social choice function is a mapping <span class="math inline">\(f: (\succ_1,\dots,\succ_n)\mapsto A\)</span> that picks a winning alternative for each possible profile of individual preferences. A social welfare function is a mapping that produces a complete societal ranking <span class="math inline">\(\succ^*\)</span> of the alternatives. The central question is: can we design an aggregation rule that faithfully represents individual preferences while satisfying certain fairness or rationality axioms?</p>
<p>Many common voting rules illustrate different methods of aggregation, each with its own merits and vulnerabilities:</p>
<ul>
<li>Plurality: Each voter names their top choice; the alternative with the most votes wins.</li>
<li>Borda Count: Voters rank all alternatives, and points are assigned based on the position in each ranking. For example, with <span class="math inline">\(m\)</span> alternatives, a voter’s top-ranked alternative gets <span class="math inline">\(m-1\)</span> points, the second-ranked gets <span class="math inline">\(m-2\)</span>, and so on down to 0. The Borda score of an alternative is the sum of points from all voters, and the winner is the alternative with the highest total score.</li>
<li>Single Transferable Vote (STV): Voters rank choices, and the count proceeds in rounds. In each round, the alternative with the fewest votes is eliminated and those votes are transferred to the next preferred remaining alternative on each ballot, until one candidate has a majority.</li>
<li>Condorcet Methods: These look for a candidate that wins in all pairwise majority contests against other alternatives (the Condorcet winner), if such an alternative exists.</li>
</ul>
<p>However, preference aggregation is not always straightforward. The Condorcet paradox illustrates that majority preferences can be cyclic (rock-paper-scissors style), so that no single alternative is majority-preferred to all others, violating transitivity. Different voting rules can yield different winners on the same profile, highlighting how the choice of rule influences the outcome. To guide the design of social choice functions, several desirable properties or axioms have been proposed. Three classical fairness criteria are:</p>
<ol type="1">
<li>Unanimity (Pareto efficiency): If all individuals strictly prefer one alternative <span class="math inline">\(x\)</span> over another <span class="math inline">\(y\)</span> (i.e.&nbsp;<span class="math inline">\(x \succ_i y\)</span> for every voter <span class="math inline">\(i\)</span>), then the group ranking should prefer <span class="math inline">\(x\)</span> over <span class="math inline">\(y\)</span> as well (<span class="math inline">\(x \succ^* y\)</span>).</li>
<li>Independence of Irrelevant Alternatives (IIA): The social preference between any two alternatives <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> should depend only on the individual preferences between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. In other words, if we change individuals’ rankings of other “irrelevant” alternatives (not <span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span>) in any way, the group’s relative ordering of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> should remain unchanged.</li>
<li>Non-dictatorship: The aggregation should not simply follow a single individual’s preference regardless of others. There is no voter <span class="math inline">\(i\)</span> who always gets their top choice as the social choice (or whose rankings always become the social ranking), irrespective of other voters’ preferences.</li>
</ol>
<p>Additionally, we assume an unrestricted domain (universal admissibility): individuals may have any transitive preference ordering over the <span class="math inline">\(m\)</span> alternatives (no restrictions like single-peaked preferences unless explicitly imposed). One might hope that a fair voting rule exists that satisfies all the above properties for three or more alternatives. Surprisingly, a seminal negative result shows this is impossible.</p>
<p>Arrow’s Impossibility Theorem <span class="citation" data-cites="arrow1951">(<a href="#ref-arrow1951" role="doc-biblioref">Arrow 1951</a>)</span> is a cornerstone of social choice theory. It states that when there are three or more alternatives (<span class="math inline">\(m\ge 3\)</span>), no social welfare function can simultaneously satisfy Unanimity, IIA, and Non-dictatorship – unless it is a trivial dictatorial rule. In other words, any aggregation mechanism that is not dictatorial will inevitably violate at least one of the fairness criteria. The theorem is usually proven by contradiction: assuming a social welfare function satisfies all conditions, one can show that one voter’s preferences always decide the outcome, hence the rule is dictatorial. Intuitively, Arrow’s theorem is driven by the possibility of preference cycles in majority voting. Even if individual preferences are transitive, aggregated majorities can prefer <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span>, <span class="math inline">\(B\)</span> to <span class="math inline">\(C\)</span>, and <span class="math inline">\(C\)</span> to <span class="math inline">\(A\)</span> in a cycle, as in the Condorcet paradox. Under Unanimity and IIA, the social ranking must locally match these pairwise preferences, but this produces a contradiction with transitivity unless one voter’s ranking is given overriding authority. A sketch of Arrow’s proof is as follows: one shows that under the axioms, the social ranking between any two alternatives <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> must agree with some particular voter’s preference (the “pivotal” voter for that pair). With IIA, the identity of the pivotal voter must be the same across all pairs of alternatives, otherwise by cleverly constructing profiles one can derive a conflict. This single pivotal voter then effectively dictates the entire social order, violating Non-dictatorship. Hence, the axioms are incompatible.</p>
<p>Arrow’s Impossibility Theorem has profound implications: it formalizes the inherent trade-offs in designing any fair aggregation scheme. In practice, different voting rules relax one or more of Arrow’s conditions. For instance, Borda count violates IIA (since introducing or removing an irrelevant alternative can change the point totals), while a dictatorship violates fairness blatantly. The theorem suggests that every practical voting system must sacrifice at least one of the ideal fairness criteria. It also motivated the exploration of alternative frameworks (such as allowing interpersonal comparisons of utility or cardinal preference aggregation) to escape the impossibility by weakening assumptions.</p>
<p>Complementing Arrow’s theorem, the Gibbard–Satterthwaite theorem focuses on incentives and strategic manipulation in voting systems <span class="citation" data-cites="gibbard1973 satterthwaite1975">(<a href="#ref-gibbard1973" role="doc-biblioref">Gibbard 1973</a>; <a href="#ref-satterthwaite1975" role="doc-biblioref">Satterthwaite 1975</a>)</span>. It considers any deterministic social choice function <span class="math inline">\(f\)</span> that chooses a single winner from the set of <span class="math inline">\(m\ge 3\)</span> alternatives. The theorem states that if <span class="math inline">\(f\)</span> is strategy-proof (incentive compatible) and onto (its range of outcomes is the entire set of alternatives), then <span class="math inline">\(f\)</span> must be dictatorial. Strategy-proofness (also called truthfulness or dominant-strategy incentive compatibility) means that no voter can ever benefit by misrepresenting their true preferences, regardless of what others do. In other words, reporting their genuine ranking is a (weakly) dominant strategy for each voter. The theorem implies that for any realistic voting rule where every alternative can possibly win, either one voter effectively decides the outcome (a dictatorship) or else the rule is susceptible to strategic manipulation by voters. The Gibbard–Satterthwaite theorem tells us that every non-dictatorial voting rule for 3 or more alternatives is manipulable: there will exist some election scenario where a voter can gain a more preferred outcome by voting insincerely (i.e.&nbsp;not according to their true preferences). For example, in a simple plurality vote, a voter whose true favorite is a long-shot candidate might vote for a more viable candidate to avoid a worst-case outcome (“lesser of two evils” voting). In a Borda count election, voters might strategically raise a competitor in their ranking to push down an even stronger rival. The only way to avoid all such strategic voting incentives is to have a dictatorship or limit the choice set to at most two alternatives.</p>
<p>The proof of Gibbard–Satterthwaite is non-trivial, but one can outline the idea: Given a non-dictatorial and onto rule <span class="math inline">\(f\)</span>, one shows there exist at least three distinct outcomes that can result from some preference profiles. By carefully constructing profiles and using the onto property, one finds a situation where a single voter can change the outcome by switching their order of two candidates, demonstrating manipulability. The theorem is robust – even if we allow ties or weaker conditions, similar impossibility results hold (Gibbard’s 1978 extension handles randomized rules). The practical takeaway is that all meaningful voting protocols encourage tactical voting in some situations. Nonetheless, certain systems are considered “harder to manipulate” or more resistant due to complexity or uncertainty. For instance, while STV (ranked-choice voting) can be manipulated in theory, determining a beneficial strategic vote can be NP-hard in worst cases, which arguably provides some practical deterrence to manipulation <span class="citation" data-cites="bartholdi1989">(<a href="#ref-bartholdi1989" role="doc-biblioref">Bartholdi, Tovey, and Trick 1989</a>)</span>.</p>
<p>Arrow’s and Gibbard–Satterthwaite’s theorems highlight the limitations any preference aggregation method must face. In domains like reinforcement learning from human feedback (RLHF) and AI alignment, we also aggregate preferences – often preferences of multiple human evaluators or preferences revealed in pairwise comparisons – to guide machine learning systems. While these settings sometimes use cardinal scores or learned reward functions (escaping the strict ordinal framework of Arrow’s theorem), the spirit of these impossibility results still applies: there is no perfect way to aggregate human opinions without trade-offs.</p>
<p>For example, aggregating human feedback to train a model may run into inconsistencies analogous to preference cycles, especially when feedback comes from diverse individuals with different values. A simple majority vote over preferences might yield unstable or unfair outcomes if some annotators are systematically in the minority. Weighting votes by some credibility or expertise (weighted voting) can improve outcomes but raises the question of how to set the weights without introducing dictator-like influence. Recent research has proposed methods like jury learning – which integrates dissenting voices by having a panel (“jury”) of models or human subgroups whose aggregated judgment guides the learning <span class="citation" data-cites="gordon2022jury">(<a href="#ref-gordon2022jury" role="doc-biblioref">Gordon et al. 2022</a>)</span> – to ensure minority preferences are not entirely ignored. Another perspective is social choice in AI alignment, which suggests using social choice theory to design AI systems that respect a plurality of human values instead of collapsing everything into a single objective. In pluralistic value alignment, instead of forcing a single “best” solution, an AI might present a diverse set of options or behave in a way that reflects a distribution of values. This approach aims to preserve the diversity of human preferences rather than always aggregating to one monolithic preference. For instance, a conversational AI might be designed to recognize multiple acceptable responses (each aligning with different value systems) rather than one canonical “aligned” response for a given query.</p>
<p>These considerations are especially relevant in generative AI and large language models, where training involves human preference data. If we aggregate feedback naively, we might overfit to the majority preference and lose minority perspectives (a form of tyranny of the majority). On the other hand, trying to satisfy everyone can lead to indecision or an incoherent objective. The impossibility results remind us there is no free lunch: we must carefully decide which properties to prioritize (e.g.&nbsp;giving more weight to expert annotators versus preserving broader fairness, or balancing consistency vs inclusivity). Designing aggregation mechanisms for AI that reflect collective human values is an ongoing challenge. It often involves insights from traditional voting theory (to understand trade-offs and failure modes) combined with machine learning techniques (to model and learn from preference data). In summary, social choice theory provides cautionary guidance as we build systems that learn from human preferences: we need to be conscious of which fairness criteria we relax and be transparent about the compromises being made in any preference aggregation pipeline.</p>
</section>
<section id="single-item-auctions" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="single-item-auctions"><span class="header-section-number">5.2</span> Mechanism Design</h2>
<p>While voting rules aggregate ordinal rank preferences to select a social outcome, another class of preference aggregation occurs in economic settings like auctions and general mechanism design. Here individuals reveal their valuations (numerical utilities) for outcomes, and the mechanism chooses an outcome (such as an allocation of goods) and possibly payments. Mechanism design asks: how can we design rules so that rational agents, acting in their own interest, end up revealing information that leads to a socially desirable outcome? A central concept is incentive compatibility – the mechanism should be designed so that each participant’s best strategy is to act according to their true preferences (e.g.&nbsp;bid their true value). In this section, we focus on auctions as a prime example of preference aggregation with money, and highlight classical results including Vickrey–Clarke–Groves (VCG) mechanisms and Myerson’s optimal auction.</p>
<p>Consider a single-item auction with one item for sale and <span class="math inline">\(n\)</span> bidders. Bidder <span class="math inline">\(i\)</span> has a private valuation <span class="math inline">\(v_i\)</span> for the item (how much the item is worth to them). Each bidder’s goal is to maximize their own utility, defined as <span class="math inline">\(v_i - p_i\)</span> if they win and pay price <span class="math inline">\(p_i\)</span>, or <span class="math inline">\(0\)</span> if they do not win (assuming quasilinear utility where money is the transferable utility). The auction’s task is to allocate the item to one of the bidders and possibly determine payments. We can think of an auction as a mechanism that asks each bidder for a “message” (typically a bid representing how much they are willing to pay), then selects a winner and a price based on the bids. A key objective might be social welfare maximization – allocate the item to the bidder who values it most (maximizing <span class="math inline">\(v_i\)</span> of the winner). Another possible objective is revenue maximization for the seller – choose the allocation and price to maximize the seller’s expected payment.</p>
<p>A classic result in auction theory is that to maximize social welfare in a single-item private-value setting, one should award the item to the highest valuer – and this can be done in an incentive-compatible way by using a second-price auction. The Vickrey second-price auction works as follows: (1) All bidders submit sealed bids <span class="math inline">\(b_1, b_2, \ldots, b_n\)</span>. (2) The bidder with the highest bid wins the item. (3) The price paid by the winner is the second-highest bid. For example, if the bids are <span class="math inline">\((2,\, 6,\, 4,\, 1)\)</span> (in some currency units), the highest bid is <span class="math inline">\(6\)</span> (by bidder 2, say) and the second-highest is <span class="math inline">\(4\)</span>. Bidder 2 wins the item and pays <span class="math inline">\(4\)</span>.</p>
<p>Under this mechanism, it turns out that bidding truthfully <span class="math inline">\(b_i = v_i\)</span> is a dominant strategy for each bidder. In other words, the auction is dominant-strategy incentive compatible (DSIC): no matter what others do, a bidder maximizes their expected utility by reporting their true valuation. The intuition is as follows. If bidder <span class="math inline">\(i\)</span> bids lower than their true value (i.e.&nbsp;<span class="math inline">\(b_i &lt; v_i\)</span>), and if their true value was actually the highest, they risk losing the item even though they value it more than the price they would have paid – a missed opportunity for positive utility. Bidding higher than their value (<span class="math inline">\(b_i &gt; v_i\)</span>) cannot help them win in any situation where bidding truthfully wouldn’t (it could only make a difference if their true <span class="math inline">\(v_i\)</span> wasn’t the highest but they tried to win anyway); and if they do win with an inflated bid, they might end up paying the second-highest bid which could be above their true value, yielding negative utility. By bidding exactly <span class="math inline">\(v_i\)</span>, if they win, it means all other bids were lower, so <span class="math inline">\(v_i\)</span> is at least as high as the second-highest bid <span class="math inline">\(p\)</span> they pay – guaranteeing non-negative utility <span class="math inline">\(v_i - p \ge 0\)</span>. If they lose, it means someone else had a higher bid (hence higher value, if others are truthful), so bidder <span class="math inline">\(i\)</span> wouldn’t have gained anyway. This argument, made rigorous by Vickrey <span class="citation" data-cites="vickrey1961">(<a href="#ref-vickrey1961" role="doc-biblioref">Vickrey 1961</a>)</span>, establishes that truth-telling is a dominant strategy in the second-price auction. As a consequence, when everyone bids truthfully, the item is allocated to the bidder with the highest <span class="math inline">\(v_i\)</span>, achieving maximum social surplus (allocative efficiency). The second-price auction is thus an elegant mechanism that aligns individual incentives with social welfare maximization.</p>
<p>It is worth contrasting this with a first-price auction, where the winner pays their own bid. In a first-price auction, bidders have an incentive to bid below their true value (to avoid the winner’s curse of paying too much), in a Nash equilibrium that involves bid shading. The first-price auction can still allocate to the highest valuer in equilibrium, but only through strategic behavior (and it is not DSIC). By charging the second-highest bid, the Vickrey auction removes the incentive to shade bids, since the price does not directly depend on one’s own bid beyond the fact of winning or losing.</p>
<p>So far, we discussed auctions aimed at maximizing social welfare. In many cases, the auctioneer (seller) is interested in maximizing revenue. A foundational result by Roger Myerson (1981) provides a characterization of optimal auctions (those that maximize the seller’s expected revenue) for single-item settings under certain assumptions <span class="citation" data-cites="myerson1981">(<a href="#ref-myerson1981" role="doc-biblioref">Myerson 1981</a>)</span>. The problem can be formulated as follows: suppose each bidder’s private value <span class="math inline">\(v_i\)</span> is drawn independently from a known distribution <span class="math inline">\(F_i\)</span> (for simplicity, assume identical distribution <span class="math inline">\(F\)</span> for all bidders, i.i.d.). We seek a mechanism (allocation rule and payment rule) that maximizes the seller’s expected payment, subject to incentive compatibility and individual rationality (participants should not expect negative utility from truthful participation).</p>
<p>Myerson’s theorem states that the optimal auction in such a setting is a threshold auction characterized by virtual valuations. Define the virtual value for a bidder with value <span class="math inline">\(v\)</span> as <span class="math inline">\(\varphi(v) = v - \frac{1-F(v)}{f(v)}\)</span>, where <span class="math inline">\(f\)</span> is the probability density function of <span class="math inline">\(F\)</span> (assuming it is continuous). An assumption called regularity (which holds for many distributions) is that <span class="math inline">\(\varphi(v)\)</span> is non-decreasing in <span class="math inline">\(v\)</span>. Myerson showed that the revenue-maximizing strategy is: treat <span class="math inline">\(\varphi(v)\)</span> as the effective “score” of a bid, allocate the item to the bidder with the highest non-negative virtual value (if all virtual values are negative, allocate to no one), and charge them the smallest value they could have such that they would still win (the payment is essentially the critical bid where <span class="math inline">\(\varphi\)</span> of that bid equals the second-highest virtual value or the zero cutoff). In practice, for i.i.d. bidders, this reduces to: there is an optimal reserve price <span class="math inline">\(r\)</span> such that you sell to the highest bidder if and only if their bid <span class="math inline">\(b_{\max} \ge r\)</span>; if sold, the price is the max of the second-highest bid and <span class="math inline">\(r\)</span>.</p>
<p>In the case of <span class="math inline">\(n\)</span> bidders with values i.i.d. uniform on <span class="math inline">\([0,1]\)</span> (which is a regular distribution), one can compute the optimal reserve price. The virtual value function for uniform <span class="math inline">\([0,1]\)</span> is <span class="math inline">\(\varphi(v) = v - \frac{1-v}{1} = 2v - 1\)</span>. Setting <span class="math inline">\(\varphi(v)\ge 0\)</span> gives <span class="math inline">\(v \ge 0.5\)</span>. So Myerson’s mechanism says: don’t sell the item if all bids are below 0.5; otherwise, sell to the highest bidder at at least 0.5. This is exactly a second-price auction with a reserve of <span class="math inline">\(r=0.5\)</span>. Our earlier example implicitly demonstrated this: with two uniform(0,1) bidders, the optimal auction sets a reserve price of <span class="math inline">\(0.5\)</span> and yields a certain expected revenue. We can break down the cases: - With probability <span class="math inline">\(1/4\)</span>, both bidders have values below <span class="math inline">\(0.5\)</span> (each below 0.5 with probability 1/2), in which case nobody wins and revenue is 0. - With probability <span class="math inline">\(1/4\)</span>, both bidders have <span class="math inline">\(v &gt; 0.5\)</span>. In this case, the second-price auction with reserve will sell to the highest bidder at the max of the second-highest value and 0.5. Given both <span class="math inline">\(v_1, v_2 &gt; 0.5\)</span>, the expected second-highest value (conditional on both &gt;0.5) is <span class="math inline">\(\frac{2}{3}\)</span> (in fact, the order statistics of two uniforms on [0.5,1] give mean of min = 2/3). So in this case the expected price is the second-highest value (since that will exceed 0.5), about 0.667. - With probability <span class="math inline">\(1/2\)</span>, one bidder is above 0.5 and the other below. In that case, the one above 0.5 wins at price equal to the reserve 0.5 (since the second-highest bid is the reserve).</p>
<p>Taking the expectation, the seller’s expected revenue is <span class="math inline">\(0*(1/4) + (2/3)*(1/4) + (1/2*1/2) = 0 + 1/6 + 1/4 = 5/12 \approx 0.417\)</span>. This is higher than the expected revenue without a reserve. In fact, without a reserve (just a plain second-price with two bidders uniform [0,1]), one can compute the expected revenue is <span class="math inline">\(1/3 \approx 0.333\)</span> (the second order statistic’s expectation). Thus, the reserve has increased revenue. Myerson’s theory tells us that indeed the second-price auction with an optimally chosen reserve maximizes revenue among all DSIC mechanisms for this setting. A notable special case result is that when bidder distributions are i.i.d. and regular, an optimal auction is essentially “allocatively efficient with a reserve price” – i.e.&nbsp;aside from possibly excluding low-value bidders via a reserve, it allocates to the highest remaining bid.</p>
<p>Myerson’s work also highlighted the gap between revenue maximization and welfare maximization. The price of optimality (in revenue) is that the seller might sometimes forego efficient allocation (e.g.&nbsp;not selling despite a willing buyer, in order to preserve a high reserve price strategy). In contrast, Vickrey’s auction always allocates efficiently but may not maximize revenue.</p>
<p>An interesting insight in auction theory is that increasing competition can yield more revenue than fine-tuning the auction mechanism. The Bulow–Klemperer theorem <span class="citation" data-cites="bulow-klemperer1996">(<a href="#ref-bulow-klemperer1996" role="doc-biblioref">Bulow and Klemperer 1996</a>)</span> demonstrates that, under certain regularity assumptions, a simple welfare-maximizing auction with one extra bidder outperforms the optimal auction with fewer bidders. Specifically, for i.i.d. bidders with a regular distribution <span class="math inline">\(F\)</span>, the expected revenue of a second-price auction with <span class="math inline">\(n+1\)</span> bidders is at least as high as the expected revenue of the Myerson-optimal auction with <span class="math inline">\(n\)</span> bidders. In formula form:</p>
<p><span class="math display">\[
\mathbb{E}_{v_1,\ldots,v_{n+1} \sim F}[\text{Rev}^{\text{(second-price)}}(n+1 \text{ bidders})] \geq
\mathbb{E}_{v_1,\ldots,v_n \sim F}[\text{Rev}^{\text{(optimal)}}(n \text{ bidders})] \,.
\tag{4.1}\label{eq-eq3.64}
\]</span></p>
<p>This result suggests that, in practice, having more participants (competition) is often more valuable than exploiting detailed knowledge of bidder distributions. As a corollary, a policy recommendation is that a seller is usually better off using a simple auction design (like a Vickrey auction or other transparent mechanism) and putting effort into attracting more bidders, rather than using a complex optimal mechanism that might discourage participation.</p>
<p>Vickrey’s second-price auction can be generalized to multiple items and more complex outcomes by the Vickrey–Clarke–Groves (VCG) mechanism. The VCG mechanism is a cornerstone of mechanism design that provides a general solution for implementing socially efficient outcomes (maximizing total stated value) in dominant strategies, for a broad class of problems. It works for any scenario where agents have quasilinear utilities and we want to maximize the sum of valuations.</p>
<p>In a general mechanism design setting, let <span class="math inline">\(\Omega\)</span> be the set of possible outcomes. Each agent <span class="math inline">\(i\)</span> has a private valuation function <span class="math inline">\(v_i(\omega)\)</span> for outcomes <span class="math inline">\(\omega \in \Omega\)</span> (the amount of utility, in money terms, that <span class="math inline">\(i\)</span> gets from outcome <span class="math inline">\(\omega\)</span>). Agents report bids <span class="math inline">\(b_i(\omega)\)</span> (which we hope equal <span class="math inline">\(v_i(\omega)\)</span> if they are truthful). The mechanism then chooses an outcome <span class="math inline">\(\omega^* \in \Omega\)</span> to maximize the reported total value:</p>
<p><span class="math display">\[
\omega^* = \arg\max_{\omega \in \Omega} \sum_{i=1}^n b_i(\omega) \,,
\]</span></p>
<p>i.e.&nbsp;<span class="math inline">\(\omega^*\)</span> is the outcome that would be socially optimal if the <span class="math inline">\(b_i\)</span> were true values. To induce truth-telling, VCG sets payments such that each agent pays the externality they impose on others by their presence. Specifically, one convenient form of the VCG payment for agent <span class="math inline">\(i\)</span> is:</p>
<p><span class="math display">\[
p_i(b) = \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega)\;-\;\sum_{j \neq i} b_j(\omega^*) \,,
\]</span></p>
<p>which can be interpreted as: what would the total value of others be if <span class="math inline">\(i\)</span> were not present (first term, maximizing without <span class="math inline">\(i\)</span>) minus the total value others actually get in the chosen outcome <span class="math inline">\(\omega^*\)</span>. Equivalently, we can write the payment as the agent’s bid for the chosen outcome minus a rebate term:</p>
<p><span class="math display">\[
p_i(b) = b_i(\omega^*) \;-\; \Big[\sum_{j=1}^n b_j(\omega^*) - \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega)\Big] \,. \tag{4.2}\label{eq-eq3.67}
\]</span></p>
<p>This formula (which in single-item auction reduces to second-price logic) ensures that each agent’s net payoff is <span class="math inline">\(v_i(\omega^*) - p_i = \max_{\omega} \sum_{j\neq i} v_j(\omega) + v_i(\omega^*) - \sum_{j\neq i} v_j(\omega^*)\)</span>. All terms except <span class="math inline">\(v_i(\omega^*)\)</span> cancel out, meaning each agent’s utility equals the max total welfare of others plus their own value for the chosen outcome minus others’ welfare in the chosen outcome – which does not depend on <span class="math inline">\(v_i(\omega^*)\)</span> except through the decision of <span class="math inline">\(\omega^*\)</span>. By construction, an agent cannot influence <span class="math inline">\(\omega^*\)</span> in a way that improves this expression unless it genuinely increases total welfare, so misreporting <span class="math inline">\(v_i\)</span> cannot increase their utility. Thus truthful reporting is a dominant strategy. VCG is dominant-strategy incentive compatible (DSIC) and produces an outcome that maximizes <span class="math inline">\(\sum_i v_i(\omega)\)</span>, achieving social welfare maximization.</p>
<p>VCG provides a powerful existence result: under broad conditions, there is a mechanism that achieves efficient allocation with truth-telling (in fact, VCG is essentially the unique one, aside from adding harmless constant transfers). However, implementing VCG in practice can be difficult. One challenge is computational: finding <span class="math inline">\(\arg\max_{\omega}\sum_i b_i(\omega)\)</span> can be NP-hard if <span class="math inline">\(\Omega\)</span> is a combinatorially large space (as in many combinatorial auctions). Another issue is budget balance and revenue: VCG payments might not yield any revenue to the mechanism designer in some cases (or even require subsidies in complex settings), and they can be low or zero in certain environments, which is problematic if the seller needs revenue. VCG is also vulnerable to collusion or the presence of fake identities (sybil attacks) – the mechanism assumes each participant is a separate entity; if one bidder can split into two identities, they might game the outcome.</p>
<p>Nonetheless, for many domains, VCG or variants have been successfully used or at least studied. Notably, combinatorial auctions (where multiple items are up for sale and bidders have valuations for bundles of items) can in theory be handled by VCG: just let <span class="math inline">\(\Omega\)</span> be all possible allocations of items to bidders, and have bidders report <span class="math inline">\(b_i(S)\)</span> for each bundle <span class="math inline">\(S\)</span> of items. VCG would allocate the items in the way that maximizes total reported value and charge each bidder the opportunity cost their presence imposes on others. In practice, as mentioned, combinatorial auctions face exponential complexity in preference reporting (each bidder potentially has to specify a value for every subset of items) and winner determination (solving an NP-hard combinatorial optimization). Heuristic or restricted approaches (like limiting the kinds of bundles or using iterative bidding with query learning of preferences) are used to make the problem tractable. Additionally, pure VCG in combinatorial settings can have undesirable properties: for example, in some cases adding more bidders can cause VCG prices to drop to zero (the so-called “threshold problem” or revenue monotonicity failure), and bidders may collude to manipulate their bids collectively.</p>
<p>One high-stakes application of combinatorial auctions is spectrum auctions for selling licenses of electromagnetic spectrum to telecom companies. Governments have used multi-round combinatorial auctions to allocate spectrum, with billions of dollars at stake. Designing these auctions requires balancing efficiency with simplicity and robustness to strategic behavior. Early spectrum auctions that used simpler formats (like sequential auctions or one-shot sealed bids for each license) ran into problems like the exposure problem – a bidder valuing a combination of items (say complementary licenses in adjacent regions) risks winning only part of the combination at a high price, which could be bad for them if the items are worth much less separately. The simultaneous multi-round auction (SMRA) was an innovation that allowed bidding on all items at once in rounds, giving bidders some price discovery to mitigate the exposure problem. Even so, strategic issues like demand reduction (bidders deliberately not bidding on too many items to keep prices low) and tacit collusion through signaling bids have been observed. These practical complications underscore that while VCG is a beautiful theoretical ideal, real-world mechanism design often involves compromises and tweaks.</p>
<section id="case-study-1-mechanism-for-peer-grading" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="case-study-1-mechanism-for-peer-grading"><span class="header-section-number">5.2.1</span> Case Study 1: Mechanism for Peer Grading</h3>
<p>To illustrate an application of mechanism design beyond auctions, consider a classroom setting where students grade each other’s work (peer assessment). The goal is to design a system (a “mechanism”) that produces fair and accurate grades while incentivizing students to put effort into grading. Jason Hartline and colleagues (2020) studied such a scenario, examining how to optimize scoring rules for peer grading <span class="citation" data-cites="jasonH2020">(<a href="#ref-jasonH2020" role="doc-biblioref">Hartline et al. 2020</a>)</span>. In this setting, students are both agents (who might strategize to maximize their own grade or minimize their effort) and graders. The “outcome” we want is a set of final grades for students, ideally reflecting the true quality of their work.</p>
<p>One idea is to use proper scoring rules to evaluate the peer graders. A proper scoring rule is a concept from forecast evaluation that gives highest expected score for truthful reporting of probabilities. In peer grading, one might try to reward students based on how close their grading is to some ground truth or to the TA’s grades. However, a naive application of proper scoring can backfire. Hartline et al.&nbsp;observed a “lazy peer grader” problem: if students figure out that always giving an average score (say 80%) yields a decent reward under the scoring rule, they might not bother to carefully distinguish good and bad work. In one experiment, giving all peers an 80% could yield a 96% accuracy score for the grader under a certain scoring rule <span class="citation" data-cites="jasonH2023">(<a href="#ref-jasonH2023" role="doc-biblioref">Hartline et al. 2023</a>)</span>. This clearly undermines the goal – the grader is basically cheating the system by always predicting the class average.</p>
<p>To combat this, the mechanism designers sought a scoring rule that maximizes the difference in reward between a diligent grading and a lazy strategy, thereby incentivizing effort. They formulated this as an optimization problem: design the reward function for peer graders such that truthful, careful grading yields a strictly higher expected score than any degenerate strategy like always giving the average. By analyzing data and grader behavior models, they adjusted the scoring rules to penalize obviously lazy patterns and reward variance when warranted. The resulting mechanism improved the accuracy of peer grading by aligning the incentives of student graders (who want a high score for their grading job) with the objective of accurate assessment. This case study highlights how ideas of incentive compatibility and mechanism design apply even in social/educational contexts: the “payments” are points towards one’s own grade, and the mechanism must account for strategic behavior to ensure a reliable outcome.</p>
<p>In conclusion, mechanism design provides a toolkit for aggregating preferences (or signals, like grades or bids) in a principled way, by explicitly accounting for individual incentives. Whether in auctions, peer grading, or other domains, the design of rules (allocation algorithms, payment or scoring schemes) crucially determines whether people feel encouraged to be truthful or to game the system. The theories of VCG and Myerson give us optimal baselines for efficiency and revenue in auctions, while impossibility results like Gibbard–Satterthwaite warn us of the limitations in voting. Real-world implementations often have to grapple with complexity and approximate these ideals. While learning from individual human preference is a powerful approach, it too faces aggregation challenges. If the human feedback is inconsistent or if different annotators have different preferences, the reward model may end up capturing an average that satisfies no one perfectly. There is active research on scalable oversight: techniques to gather and aggregate human feedback on tasks that are too complex for any single person to evaluate reliably. This includes approaches like recursive reward modeling, iterated amplification <span class="citation" data-cites="christiano2018supervising">(<a href="#ref-christiano2018supervising" role="doc-biblioref">Christiano, Shlegeris, and Amodei 2018</a>)</span>, and AI-assisted debate <span class="citation" data-cites="irving2018ai">(<a href="#ref-irving2018ai" role="doc-biblioref">Irving, Christiano, and Amodei 2018</a>)</span>, where AI systems help humans provide better feedback or break down tasks. The goal of scalable oversight is to leverage human preferences and principles in guiding AI even as AI systems tackle increasingly complex or open-ended tasks, while mitigating the human burden and bias in evaluation.</p>
<p>In summary, preference aggregation in machine learning spans from simple models like Bradley–Terry for pairwise comparisons to elaborate RLHF pipelines for training large models. The deep mathematical foundations – whether Arrow’s theorem or Myerson’s auction theory – remind us that whenever we aggregate preferences or signals from multiple sources, we must consider incentive effects, fairness criteria, and the possibility of inconsistency. By combining insights from social choice, economics, and statistical learning, we aim to build AI systems that not only learn from human preferences but do so in a principled, robust, and fair manner. The next chapter will delve further into aligning AI with human values, building on the mechanisms and learning algorithms discussed here to ensure AI systems remain beneficial and in line with what people truly want.</p>
</section>
<section id="case-study-2-incentive-compatible-online-learning" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="case-study-2-incentive-compatible-online-learning"><span class="header-section-number">5.2.2</span> Case Study 2: Incentive-Compatible Online Learning</h3>
<p>To address this problem, we seek to create a model. We first outline the key criteria that our model must achieve. The model revolves around repeated interactions between a planner (the system) and multiple agents (the users). Each agent, upon arrival in the system, is presented with a set of available options to choose from. These options could vary widely depending on the application of the model, such as routes in a transportation network, a selection of hotels in a travel booking system, or even entertainment choices in a streaming service. The interaction process is straightforward but crucial: agents arrive, select an action from the provided options, and then report feedback based on their experience. This feedback is vital as it forms the basis upon which the planner improves and evolves its recommendations. The agents in this model are considered strategic; they aim to maximize their reward based on the information available to them. This aspect of the model acknowledges the real-world scenario where users are typically self-interested and seek to optimize their own outcomes. The planner, on the other hand, has a broader objective. It aims to learn which alternatives are best in a given context and works to maximize the overall welfare of all agents. This involves a complex balancing act: the planner must accurately interpret feedback from a diverse set of agents, each with their own preferences and biases, and use this information to refine and improve the set of options available. The ultimate goal of the planner is to create a dynamic, responsive system that not only caters to the immediate needs of individual agents but also enhances the collective experience over time, leading to a continually improving recommendation ecosystem.</p>
<p>Here, we seek to address the inherent limitations faced by the planner, particularly in scenarios where monetary transfers are not an option, and the only tool at its disposal is the control over the flow of information between agents. This inquiry aims to understand the extent to which these limitations impact the planner’s ability to effectively guide and influence agent behavior. A critical question is whether the planner can successfully induce exploration among agents, especially in the absence of financial incentives. This involves investigating strategies to encourage users to try less obvious or popular options, thus broadening the scope of feedback and enhancing the system’s ability to learn and identify the best alternatives. Another question is understanding the rate at which the planner learns from agent interactions. This encompasses examining how different agent incentives, their willingness to explore, and their feedback impact the speed and efficiency with which the planner can identify optimal recommendations.</p>
<p>The model can be extended in several directions, each raising its own set of questions.</p>
<pre><code>1.  Multiple Agents with Interconnected Payoffs: When multiple agents arrive simultaneously, their choices and payoffs become interconnected, resembling a game. The research question here focuses on how these interdependencies affect individual and collective decision-making.

2.  Planner with Arbitrary Objective Function: Investigating scenarios where the planner operates under an arbitrary objective function, which might not align with maximizing overall welfare or learning the best alternative.

3.  Observed Heterogeneity Among Agents: This involves situations where differences among agents are observable and known, akin to contextual bandits in machine learning. The research question revolves around how these observable traits can be used to tailor recommendations more effectively.

4.  Unobserved Heterogeneity Among Agents: This aspect delves into scenarios where differences among agents are not directly observable, necessitating the use of causal inference techniques to understand and cater to diverse user needs.</code></pre>
<p>In our setup, there is a “planner,” which aims to increase exploration, and many independent “agents,” which will act selfishly (in a way that they believe will maximize their individual reward) <span class="citation" data-cites="mansour2019bayesianincentivecompatiblebanditexploration mansour2021bayesianexplorationincentivizingexploration">(<a href="#ref-mansour2019bayesianincentivecompatiblebanditexploration" role="doc-biblioref">Mansour, Slivkins, and Syrgkanis 2019</a>; <a href="#ref-mansour2021bayesianexplorationincentivizingexploration" role="doc-biblioref">Mansour et al. 2021</a>)</span>. Under our model shown in Figure <a href="#fig-planner-agent" data-reference-type="ref" data-reference="fig-planner-agent">1.1</a>, there are <span class="math inline">\(K\)</span> possible actions that all users can take, and each action has some mean reward <span class="math inline">\(\mu_i \in [0, 1]\)</span>. In addition, there is a common prior belief on each <span class="math inline">\(\mu_i\)</span> across all users.. The <span class="math inline">\(T\)</span> agents, or users, will arrive sequentially. As the <span class="math inline">\(t\)</span>’th user arrives, they are recommended an action <span class="math inline">\(I_t\)</span> by the planner, which they are free to follow or not follow. After taking whichever action they choose, the user experiences some realized reward <span class="math inline">\(r_i \in [0, 1]\)</span>, which is stochastic i.i.d. with mean <span class="math inline">\(\mu_i\)</span>, and reports this reward back to the planner.</p>
<p>So far, the model we have defined is equivalent to a multi-armed bandit model, which we have seen earlier in this chapter (<a href="#4optim" data-reference-type="ref" data-reference="4optim">1</a>). Under this model, well-known results in economics, operations research and computer science show that <span class="math inline">\(O(\sqrt{T})\)</span> regret is achievable <span class="citation" data-cites="russo2015informationtheoreticanalysisthompsonsampling auer_cesa-bianchi_fischer_2002 LAI19854">(<a href="#ref-russo2015informationtheoreticanalysisthompsonsampling" role="doc-biblioref">Russo and Roy 2015</a>; <a href="#ref-auer_cesa-bianchi_fischer_2002" role="doc-biblioref">Auer, Cesa-Bianchi, and Fischer 2002</a>; <a href="#ref-LAI19854" role="doc-biblioref">Lai and Robbins 1985</a>)</span> with algorithms such as Thompson sampling and UCB. However, our agents are strategic and aim to maximize their own rewards. If they observe the rewards gained from actions taken by other previous users, they will simply take the action they believe will yield the highest reward given the previous actions; they would prefer to benefit from exploration done by other users rather than take the risk of exploring themselves. Therefore, exploration on an individual level, which the planner would like to facilitate, is not guaranteed under this paradigm.</p>
<p>In light of this, we also require that our model satisfy incentive compatibility, or that taking the action recommended by the planner has an expected utility that is as high as any other action the agent could take. Formally, <span class="math inline">\(\forall i : \, E[\mu_i | I_t = i] \geq E[\mu_{i'} | I_t = i].\)</span> Note that this incentivizes the agents to actually take the actions recommended by the planner; if incentive compatibility is not satisfied, agents would simply ignore the planner and take whatever action they think will lead to the highest reward.</p>
<p>At a high level, the key to achieving incentive compatibility while still creating a policy for the planner that facilitates exploration is information asymmetry. Under this paradigm, the users only have access to their previous recommendations, actions, and rewards, and not to the recommendations, actions, and rewards of other users. Therefore, they are unsure of whether, after other users take certain actions and receive certain rewards, arms that they might have initially considered worse in practice outperform arms that they initially considered better. Only the planner has access to the previous actions and rewards of all users; the user only has access to their own recommendations and overall knowledge of the planner’s policy. The main question we aim to answer for the rest of this section is, given this new constraint of incentive compatibility, is <span class="math inline">\(O(\sqrt{T})\)</span> regret still achievable? We illustrate such an algorithm in the following.</p>
<p>The main result here is a black-box reduction algorithm to turn any bandit algorithm into an incentive compatible one, with only a constant increase in Bayesian regret. Since, as mentioned earlier, there are bandit algorithms with <span class="math inline">\(O(\sqrt{T})\)</span> Bayesian regret, black-box reduction will also allow us to get incentive-compatible algorithms with <span class="math inline">\(O(\sqrt{T})\)</span> regret. The idea of black-box reduction will be to simulate <span class="math inline">\(T\)</span> steps of any bandit algorithm in an incentive-compatible way in <span class="math inline">\(c T\)</span> steps. This allows us to design incentive-compatible recommendation systems by using any bandit algorithm and then adapting it. Consider the following setting: there are two possible actions, <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span>. Assume the setting of deterministic rewards, where action 1 has reward <span class="math inline">\(\mu_1\)</span> with prior <span class="math inline">\(U[1/3, 1]\)</span> and mean <span class="math inline">\(\mathbb{E}[\mu_1] = 2/3\)</span>, and action 2 has reward <span class="math inline">\(\mu_2\)</span> with prior <span class="math inline">\(U[0, 1]\)</span> and mean <span class="math inline">\(\mathbb{E}[\mu_2] = 1/2\)</span>. Without the planner intervention and with full observability, users would simply always pick <span class="math inline">\(A_1\)</span>, so how can the planner incentivize users to play <span class="math inline">\(A_2\)</span>?</p>
<p>The key insight is going to be to hide exploration in a pool of exploitation. The users are only going to receive a recommendation from the planner, and no other observations. After deterministically recommending the action with the highest expected reward (<span class="math inline">\(A_1\)</span>), the planner will pick one guinea pig to recommend the exploratory action of <span class="math inline">\(A_2\)</span>. The users don’t know whether they are the guinea pig, so intuitively, as long as the planner picks guinea pigs uniformly at random and at low enough frequencies, the optimal decision for the users is still to follow the planner’s recommendation, even if it might go against their interest. The planner will pick the user who will be recommended the exploratory action uniformly at random from the <span class="math inline">\(L\)</span> users that come after the first one (which deterministically gets recommended the exploitation action). Under this setting (illustrated in Figure <a href="#fig-deterministic-guinea-pig" data-reference-type="ref" data-reference="fig-deterministic-guinea-pig">1.2</a>), it is optimal for users to always follow the option that is recommended for them. More formally, if <span class="math inline">\(I_t\)</span> is the recommendation that a user receives at time <span class="math inline">\(t\)</span>, then we have that:</p>
<p><span class="math display">\[
\begin{split}
    \mathbb{E}[\mu_1 - \mu_2 | I_t = 2] Pr[I_t = 2] &amp;= \frac{1}{L} (\mu_1 - \mu_2) \quad \text{(Gains if you are the unlucky guinea pig)}\\
    &amp;+ (1 - \frac{1}{L}) \mathbb{E}[\mu_1 - \mu_2 | \mu_1 &lt; \mu_2] \times p[\mu_1 &lt; \mu_2] \quad \text{(Loss if you are not and $\mu_1 &lt; \mu_2$)}\\
    &amp;\leq 0
\end{split}
\]</span></p>
<p>This holds when <span class="math inline">\(L \geq 12\)</span>. It means that the gains from not taking the recommended action are negative, which implies that users should always take the recommendation. So far we have considered the case where rewards are deterministic, but what about stochastic rewards? We are now going to consider the case where rewards are independent and identically distributed from some distribution, and where each action <span class="math inline">\(A_i\)</span> has some reward distribution <span class="math inline">\(r_i^t \sim D_i, \mathbb{E}[r_i^t] = \mu_i\)</span>. Back to the case where there are only two actions, we are going to adapt the prior algorithm of guinea pig-picking to the stochastic reward setting. Since one reward observation is not enough to fully know <span class="math inline">\(\mu_1\)</span> anymore, we’ll instead observe the outcome of the first action <span class="math inline">\(M\)</span> times to form a strong posterior <span class="math inline">\(\mathbb{E}[\mu_1 | r_1^1, \ldots r_1^M]\)</span>. We can use with stochastic rewards when there are two actions. Similarly, as before, we pick one guinea pig uniformly at random from the next <span class="math inline">\(L\)</span> users and use the reward we get as the exploratory signal.&nbsp;In a very similar manner, we can generalize this algorithm from always having two actions to the general multi-armed bandit problem. Now suppose we have a general multi-armed bandit algorithm <span class="math inline">\(A\)</span>. We will wrap this algorithm around our black box reduction algorithm to make it incentive-compatible. We wrap every decision that <span class="math inline">\(A\)</span> would make by exactly <span class="math inline">\(L-1\)</span> recommendations of the action believed to be the best so far. This guarantees that the expected rewards for the users that are not chosen as guinea pigs are at least as good as <span class="math inline">\(A\)</span>’s reward at phase <span class="math inline">\(n\)</span>.</p>
</section>
</section>
<section id="mutual-information-paradigm" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="mutual-information-paradigm"><span class="header-section-number">5.3</span> Mutual Information Paradigm</h2>
<p>In this section we discuss an influential new framework for designing peer prediction mechanisms, the Mutual Information Paradigm (MIP) introduced by Kong and Schoenebeck <span class="citation" data-cites="kongschoenebeck2019">(<a href="#ref-kongschoenebeck2019" role="doc-biblioref">Kong and Schoenebeck 2019</a>)</span>. Traditional peer prediction approaches typically rely on scoring rules and correlation between agents’ signals. However, these methods often struggle with issues like uninformed equilibria, where agents can coordinate on uninformative strategies that yield higher payoffs than truth-telling. The core idea is to reward agents based on the mutual information between their report and the reports of other agents. We consider a setting with <span class="math inline">\(n\)</span> agents, each possessing a private signal <span class="math inline">\(\Psi_i\)</span> drawn from some set <span class="math inline">\(\Sigma\)</span>. The mechanism asks each agent to report their signal, which we denote as <span class="math inline">\(\hat{\Psi}_i\)</span>. For each agent <span class="math inline">\(i\)</span>, the mechanism randomly selects a reference agent <span class="math inline">\(j \neq i\)</span>. Agent <span class="math inline">\(i\)</span>’s payment is then calculated as <span class="math inline">\(MI(\hat{\Psi}_i; \hat{\Psi}_j)\)</span> where <span class="math inline">\(MI\)</span> is an information-monotone mutual information measure. An information-monotone <span class="math inline">\(MI\)</span> measure must satisfy the following properties:</p>
<ul>
<li><p>Symmetry: <span class="math inline">\(MI(X; Y) = MI(Y; X)\)</span>.</p></li>
<li><p>Non-negativity: <span class="math inline">\(MI(X; Y) \geq 0\)</span>, with equality if and only if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</p></li>
<li><p>Data processing inequality: For any transition probability <span class="math inline">\(M\)</span>, if <span class="math inline">\(Y\)</span> is independent of <span class="math inline">\(M(X)\)</span> conditioned on <span class="math inline">\(X\)</span>, then <span class="math inline">\(MI(M(X); Y) \leq MI(X; Y)\)</span>.</p></li>
</ul>
<p>Two important families of mutual information measures that satisfy these properties are <span class="math inline">\(f\)</span>-mutual information and Bregman mutual information. The <span class="math inline">\(f\)</span>-mutual information is defined as <span class="math inline">\(MI_f(X; Y) = D_f(U_{X,Y}, V_{X,Y})\)</span>, where <span class="math inline">\(D_f\)</span> is an <span class="math inline">\(f\)</span>-divergence, <span class="math inline">\(U_{X,Y}\)</span> is the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and <span class="math inline">\(V_{X,Y}\)</span> is the product of their marginal distributions. The Bregman mutual information is defined as: <span class="math inline">\(BMI_{PS}(X; Y) = \mathbb{E}_{X} [D{PS}(U_{Y|X}, U_Y)]\)</span>, where <span class="math inline">\(D_{PS}\)</span> is a Bregman divergence based on a proper scoring rule <span class="math inline">\(PS\)</span>, <span class="math inline">\(U_{Y|X}\)</span> is the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, and <span class="math inline">\(U_Y\)</span> is the marginal distribution of <span class="math inline">\(Y\)</span>. The MIP framework can be applied in both single-question and multi-question settings. In the multi-question setting, the mechanism can estimate the mutual information empirically from multiple questions. In the single-question setting, additional techniques like asking for predictions about other agents’ reports are used to estimate the mutual information. A key theoretical result of the MIP framework is that when the chosen mutual information measure is strictly information-monotone with respect to agents’ priors, the resulting mechanism is both dominantly truthful and strongly truthful. This means that truth-telling is a dominant strategy for each agent and that the truth-telling equilibrium yields strictly higher payoffs than any other non-permutation strategy profile. As research continues to address practical implementation challenges of designing truthful mechanisms, MIP-based approaches have significant potential to improve preference elicitation and aggregation in real-world applications lacking verifiable ground truth.</p>
</section>
<section id="exercises" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="exercises"><span class="header-section-number">5.4</span> Exercises</h2>
<section id="question-1-pairwise-feedback-mechanisms-for-digital-goods" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="question-1-pairwise-feedback-mechanisms-for-digital-goods"><span class="header-section-number">5.4.1</span> Question 1: Pairwise Feedback Mechanisms for Digital Goods</h3>
<p>Consider a marketplace for digital goods (such as personalized articles, artwork, or AI-generated data), where the exact utility derived from these goods is only revealed to buyers after the goods have been generated and delivered. To elicit truthful preferences from buyers who find it difficult to precisely quantify their valuations beforehand, the marketplace implements a pairwise feedback mechanism, inspired by the work of Robertson and Koyejo (2023).</p>
<p>Formally, each buyer requests a personalized digital good and, upon receiving the good, provides feedback by indicating whether their realized utility is higher or lower than a randomly chosen reference price <span class="math inline">\(c \in [0,1]\)</span>. The mechanism utilizes this binary feedback to estimate valuations and allocate future goods accordingly.</p>
<p>Answer the following:</p>
<ol type="a">
<li><p><strong>Formalize the Problem:</strong> Let <span class="math inline">\(u_i\)</span> denote the true valuation of buyer <span class="math inline">\(i\)</span>, and let <span class="math inline">\(r_i(c)\)</span> denote the buyer’s reported feedback (<span class="math inline">\(r_i(c) = 1\)</span> if <span class="math inline">\(u_i \geq c\)</span>, 0 otherwise). Prove that, under uniform random selection of the reference price <span class="math inline">\(c\)</span>, the expected value <span class="math inline">\(\mathbb{E}[r_i(c)]\)</span> is equal to the true valuation <span class="math inline">\(u_i\)</span>.</p></li>
<li><p><strong>Incentive Compatibility Analysis:</strong> Discuss conditions under which this feedback-based mechanism is incentive compatible, i.e., buyers have no incentive to misreport their preferences. Specifically, analyze why strategic misreporting (reporting <span class="math inline">\(r_i(c)\)</span> incorrectly for some reference prices) would not increase a buyer’s expected payoff.</p></li>
<li><p><strong>Regret Analysis:</strong> Suppose the mechanism estimates buyers’ utilities from past feedback and allocates future goods using an epsilon-greedy strategy (exploration rate <span class="math inline">\(\eta_t\)</span>). Provide an informal discussion of the trade-off involved in choosing the exploration rate, and how it affects the social welfare and revenue of the marketplace over time.</p></li>
<li><p><strong>Practical Implications:</strong> Suggest one practical scenario outside the digital-goods marketplace where such a feedback-driven, pairwise comparison approach would be beneficial. Briefly justify your choice, mentioning challenges and benefits.</p></li>
</ol>
</section>
<section id="question-2-scalable-oversight-in-complex-decision-making" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="question-2-scalable-oversight-in-complex-decision-making"><span class="header-section-number">5.4.2</span> Question 2: Scalable Oversight in Complex Decision-Making</h3>
<p>In scenarios involving complex or high-dimensional outcomes (such as summarizing lengthy texts, assessing the quality of detailed AI-generated reports, or reviewing scientific papers), evaluating the quality of outputs can become infeasible for a single human overseer. One practical solution is scalable oversight, where the evaluation task is decomposed and distributed among multiple human evaluators or even assisted by AI agents. Consider a scalable oversight scenario inspired by recursive reward modeling, where complex evaluations are hierarchically decomposed into simpler tasks. Specifically, suppose you want to evaluate a lengthy report generated by an AI system. Answer the following:</p>
<p><strong>(a) Decomposition of the Task:</strong> Propose a formal recursive decomposition strategy to evaluate a long AI-generated report of length (N) paragraphs. Specifically, describe a hierarchical evaluation method that decomposes the original evaluation into simpler subtasks at multiple hierarchical levels. Clearly describe how many subtasks you have at each level and how the final aggregated evaluation is computed.</p>
<p><strong>(b) Statistical Aggregation Method:</strong> Suppose each evaluation subtask yields a binary score (s_i {0,1}), where (1) indicates acceptable quality and (0) indicates unacceptable quality. Propose a simple statistical aggregation method (e.g., majority voting, threshold voting, weighted aggregation, etc.) to combine subtask evaluations into a single global quality assessment at the top level. Justify your choice mathematically.</p>
<p><strong>(c) Computational Simulation:</strong> Implement a Python simulation of your hierarchical decomposition and aggregation method described in parts (a) and (b). Assume each subtask is evaluated with some fixed probability (p) of being correct (representing human evaluators with bounded accuracy).</p>
<p>Specifically, your simulation should: - Implement a hierarchical evaluation scheme (e.g., binary-tree decomposition). - Assume evaluators have accuracy (p = 0.8) (i.e., probability of correctly identifying paragraph quality). - Simulate how evaluator accuracy at the leaf nodes affects the reliability of the global evaluation at the root node. - Plot how the reliability of the top-level evaluation (accuracy at the root) varies as you increase the depth of hierarchy for a report of fixed length (e.g., (N = 64) paragraphs).</p>
<p><strong>(d) Practical Discussion:</strong> Briefly discuss advantages and potential drawbacks of scalable oversight approaches such as recursive decomposition in the context of AI alignment. Include considerations such as evaluator fatigue, consistency, cost, and vulnerability to manipulation or collusion.</p>
<!--
## Social Choice Theory {#sec-choices-aggregation}
In many applications, human preferences must be aggregated across multiple individuals to determine a collective decision or ranking. This process is central to social choice theory, which provides a mathematical foundation for preference aggregation. Unlike individual preference modeling, which focuses on understanding how a single person makes decisions, social choice theory addresses the challenge of combining multiple preference profiles into a single, coherent outcome. One of the most widely used approaches to aggregating preferences is voting. A voting rule is a function that maps a set of individual preference rankings to a collective decision. The outcome of a vote is determined by a social choice function (SCF), which selects a winner based on the aggregated preferences. Several voting rules exist, each with different properties:

- Plurality Rule: Each voter assigns one point to their top choice, and the alternative with the most points wins.
- Borda Count: Voters rank all alternatives, and points are assigned based on the position in each ranking.
- Single Transferable Vote (STV): Voters rank choices, and rounds of elimination occur until a candidate has a majority.
- Condorcet Methods: The Condorcet winner is the item that would win in all pairwise comparisons against other alternatives (if one exists).

However, preference aggregation is not always straightforward. The Condorcet Paradox illustrates that no single alternative may be a clear winner due to cycles in majority preferences, violating transitivity. Additionally, different voting rules can yield different winners, highlighting the importance of selecting an appropriate aggregation method. A fundamental result in social choice theory is Arrow’s Impossibility Theorem, which states that when there are three or more alternatives, no voting system can simultaneously satisfy the following fairness criteria:

1. Unanimity (Pareto efficiency): If all individuals prefer one item over another, the group ranking should reflect this.
2. Independence of Irrelevant Alternatives: The relative ranking of two items should not be influenced by another unrelated item.
3. Non-dictatorship: No single individual's preference should always determine the group's outcome.

Arrow’s theorem suggests that every fair aggregation method must compromise on at least one of these desirable properties. Additionally, the Gibbard-Satterthwaite Theorem proves that any deterministic voting rule that selects a single winner is either dictatorial (one person always determines the result) or manipulable (voters can strategically misrepresent their preferences to achieve a better outcome). While manipulation is theoretically possible, certain voting rules, such as STV, introduce computational complexity that makes strategic voting impractical in real-world scenarios.

Preference aggregation is also critical in RLHF, where human judgments guide model training. Aggregating human preferences in RLHF faces challenges similar to traditional voting, such as inconsistencies in preferences and strategic bias. Several approaches address these challenges. For example, Majority Voting simply aggregates by selecting the most preferred response. Weighted Voting adjusts vote weights based on expertise or trustworthiness. Jury Learning is a method that integrates dissenting opinions, ensuring that minority perspectives are not entirely disregarded. Lastly, Social Choice in AI Alignment incorporates diverse human feedback to align AI behavior with a broad spectrum of human values. These approaches highlight the interplay between human preference modeling and machine learning. Designing aggregation mechanisms that reflect collective human values is an ongoing research challenge. While traditional social choice methods focus on aggregation, recent work in pluralistic alignment suggests alternative frameworks that preserve the diversity of human preferences rather than collapsing them into a single decision. Pluralistic AI systems aim to:

1. Present a spectrum of reasonable responses instead of forcing a single choice.
2. Allow steering towards specific perspectives while maintaining fairness.
3. Ensure distributional pluralism, calibrating AI systems to diverse human viewpoints.

This perspective is particularly relevant in generative AI, where models trained on aggregated preferences may fail to capture the nuances of diverse human values. Aggregating human preferences is a complex task influenced by mathematical constraints and strategic considerations. Voting-based methods provide well-studied mechanisms for aggregation, but they face fundamental limitations, as Arrow’s and Gibbard-Satterthwaite’s theorems outlined. Beyond traditional aggregation, emerging approaches in RL and AI alignment seek to balance fairness, robustness, and pluralism. As machine learning systems increasingly interact with human preferences, designing aggregation frameworks that capture the richness of human decision-making remains an active and critical area of research.

## Auction Theory {#single-item-auctions}

The first problem within auction theory we will consider is the single-item auction. The premise of this problem is that there is a single item to sell, $n$ bidders (with unknown private valuations of the item $v_1$, \..., $v_n$). The bidder's individual objective is to maximize utility: the value $v_i$ of the item subtracted by the price paid for the item. The auction procedure is standard in the sense that bids are solicited and the highest bid will win the auction. While the objective of the individual bidder is clear, there could be a plethora of different objectives for the auction as a whole. One option could be to maximize social surplus, meaning the goal is to maximize the value of the winner. Another objective could be to maximize seller profit which is the payment of the winner. For simplicity, we can focus on the first objective where the goal is to maximize social surplus. If we want to maximize social surplus it turns out that a great way to do this is the "second-price auction".

In the second-price auction, we will operate under slightly different conditions. In the second-price auction we (1) solicit sealed bids, (2) have the winner be the highest bidder, and (3) charger winner the second-highest bid price. As an example, if the solicited bids are $b = (2, 6, 4, 1)$ the winner will be that who bid $6$, but will pay a price of $4$. From here, we can do some equilibrium analysis to try and learn what the optimal bidding strategy is for each bidder. Let the amount bidder $i$ bids to be $b_i$, so we have bids $b_1, b_2, ..., b_n$. How much should bidder $i$ bid? To analyze this, let us define $t_i = max_{j \neq i} b_j$ which represents the max of the bids that is not from bidder $i$. There are now two cases to consider: if $b_i$ \> $t_i$ and if $b_i$ \< $t_i$. In the first case the bidder $i$ wins, and if the bidder bid $b_i = v_i$, they are guaranteed to have a positive return on bid. In the other case, they lose the bid and the net loss is 0 because they don't have to pay. From this we can conclude that bidder $i$'s dominant strategy is to just bid $b_i = v_i$. Rigorously proving this is a little bit trickier, but it was shown from Vickrey in 1961 \[cite\] that truthful bidding is the dominant strategy in second-price auctions. A corollary of this is that we are maximizing social surplus since bids are values and the winner is the bidder with highest valuation.

If we want to look at things from the perspective of a seller trying to maximize their profit we need to treat the bidder's bids as uniform random variables. Consider the example scenario where we have two bidders each bidding uniformly between 0 and 1. What is the seller's expected profit? (in this case profit and revenue for the seller are the same because we assume the seller throws away the item if it doesn't sell/has no valuation for it). From there the question now becomes, can we get more expected profit from the seller's perspective? It turns out there is a design where we can add a reserve price of $r$ to the second-price auction. The way this works is we can (1) Insert seller-bid at $r$, (2) solicit bids, (3) pick the highest bidder, and 3) charge the 2nd-highest bid. In effect, this is just the second-price auction but with a bid from the seller as well, at a price of $r$. A lemma, that we won't prove here, is that the second-price auction with reserve price $r$ still has a dominant strategy of just being truthful. Let's now consider what the profit of a second-price auction would be with two bidders that uniformly bid between 0 and 1 -- but this time we have a reserve price of $1/2$. To calculate the expected profit we break down the situation into 3 cases:

-   Case 1: $1/2 > v_1 > v_2 \rightarrow 1/4 \text{ probability} \rightarrow  \mathbb{E}[\text{profit}] = 0$

-   Case 2: $v_1 > v_2 > 1/2 \rightarrow 1/4 \text{probability} \rightarrow \mathbb{E}[v_2 | \text{case 2}] = 2/3$

-   Case 3: $v_1 > 1/2 > v_2 \rightarrow 1/2 \text{ probability} \rightarrow 1/2$

Why is $E[v2 | case 2] = 2/3$? If $v_1$ and $v_2$ are greater than $1/2$, they are evenly spread across the interval, meaning the expectation will be 1/2 + 1/6 = 2/3. Adding up all these cases we get $E[profit] = 5/12$. It turns out that second-price auctions with reserve actually maximize profit in general (for symmetric bidders)! In the previous section we conclude that second-price auctions with reserve maximize profit for the seller. In order to prove this, we now move to the more general topic of asking how should a monopolist divide good across separate markets. We can make the assumption that the demand model is a concave revenue $R(q)$ in quantity $q$. Under this assumption, we can just divide supply into $q = q_a + q_b$ such that $R'_a(q_a) = R'_b(q_b)$. The idea from here is a theorem from Myerson in 1981 that states an optimal action maximizes "marginal revenue". Consider an example where we have two bidders bidding a uniform value between 0 and 1. Our revenue curve can now be derived from the offering price $V(q) = 1 - q$ like so: $R(q) = qV(q) = q - q^2$. Taking the derivative gives us the marginal revenue $R'(q) = 1-2q$. This means two things: 1) we want to sell to bidder $i$ with the highest $R'(q_i)$ and 2) we want to sell to bidder $i$ with value at least $1/2$ (if we want a positive $R'(q_i)$. But this is just a second-price auction with reserve $1/2$! This means that for symmetric bidders, a second price with reserve is the optimal auction.

An interesting topic to discuss is what benefits auctions bring to the table as opposed to just standard pricing. Online auctions used to be a lot more popular in the early 2000s and have been completely replaced by standard online pricing, even on sites like e-bay. While auctions are slower and have added inherent complexities, they are actually optimal on paper. Standard pricing on the other is non-optimal; although it is fast and simpler for buyers. There is actually a way to quantify this: for pricing $k$ units, the loss is at most $1 / \sqrt{2\pi k}$ of optimal profit. Let's consider applications in duopoly platform design. We know that the optimal auction is second-price with reserve, but what happens when we introduce competition between two auction platforms? Some important details related to the revenue of a second-price auction is that a second-price auction with no reserve and n bidders leads to larger revenue having an optimal reserve and n - 1 bidders [@bulow-klemperer1996]. Additionally, with an entry cost, no reserve is the optimal strategy for maximizing revenue  [@mcafee-87]. Let's consider an example of a competing auction system which is Google ads vs Bing ads. How should an advertiser divide the budget between Google and Bing? They should give the same budget to both companies. What happens if Bing raises their prices? Then, the advertising company moves more of its budget to Google from Bing.

The Bulow-Klemperer theorem demonstrates that increased competition can be more valuable than perfect knowledge of bidders' valuation distributions. This result provides insight into the potential of simple, prior-independent auctions to approach the performance of optimal auctions. The theorem states that for a single-item auction with bidders' valuations drawn independently from a regular distribution $F$. Let $F$ be a regular distribution and $n$ a positive integer. Then:
$$E_{v_1,\ldots,v_{n+1} \sim F}[\text{Rev(VA)}(n+1 \text{ bidders})] \geq E_{v_1,\ldots,v_n \sim F}[\text{Rev(OPT}_F)(n \text{ bidders})]$$  {#eq-eq3.64}
where VA denotes the Vickrey auction and $\text{OPT}_F$ denotes the
optimal auction for $F$. This shows that running a simple Vickrey auction with one extra bidder outperforms the revenue-optimal auction that requires precise knowledge of the distribution. It suggests that in practice, effort spent on recruiting additional bidders may be more fruitful than fine-tuning auction parameters.

The VCG mechanism is a cornerstone of mechanism design, providing a general solution for welfare maximization in multi-parameter environments. The key result is that in every general mechanism design environment, there is a dominant-strategy incentive-compatible (DSIC) welfare-maximizing mechanism. According to VCG, given bids $b_1, \ldots, b_n$, where each $b_i$ is indexed by the outcome set $\Omega$, the allocation rule is $x(b) = \arg \max_{\omega \in \Omega} \sum_{i=1}^n b_i(\omega)$. The payment rule for each agent $i$ is $p_i(b) = \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega) - \sum_{j \neq i} b_j(\omega^*)$ where $\omega^* = x(b)$ is the chosen outcome. The key insight is to charge each agent its "externality" - the welfare loss inflicted on other agents by its presence. This payment rule, coupled with the welfare-maximizing allocation rule, yields a DSIC mechanism. The VCG mechanism can be interpreted as having each agent pay its bid minus a "rebate" equal to the increase in welfare attributable to its presence:

$$p_i(b) = b_i(\omega^*) - \left[ \sum_{j=1}^n b_j(\omega^*) - \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega) \right]$$  {#eq-eq3.67}

While the VCG mechanism provides a theoretical solution for DSIC welfare-maximization in general environments, it can be challenging to implement in practice due to computational and communication complexities.

Combinatorial auctions are an important class of multi-parameter mechanism design problems, with applications ranging from spectrum auctions to airport slot allocation. In a combinatorial auction, there are $n$ bidders and a set $M$ of $m$ items. The outcome set $\Omega$ consists of allocations $(S_1, \ldots, S_n)$, where $S_i$ is the bundle allocated to bidder $i$. Each bidder $i$ has a private valuation $v_i(S)$ for each bundle $S \subseteq M$. While the VCG mechanism theoretically solves the welfare-maximization problem, combinatorial auctions face several major challenges in practice. First, each bidder has $2^m - 1$ private parameters, making direct revelation infeasible for even moderate numbers of items. This necessitates the use of indirect mechanisms that elicit information on a "need-to-know" basis. In addition, even when preference elicitation is not an issue, welfare-maximization can be an intractable problem. In practice, approximations are often used, hoping to achieve reasonably good welfare. The VCG mechanism can exhibit bad revenue and incentive properties in combinatorial settings. For example, adding bidders can sometimes decrease revenue to zero, and the mechanism can be vulnerable to collusion and false-name bids. Last but not least, strategic Behavior in Iterative Auctions: Most practical combinatorial auctions are iterative, comprising multiple rounds. This introduces new opportunities for strategic behavior, such as using bids to signal intentions to other bidders. These challenges make combinatorial auctions a rich and complex area of study, requiring careful design to balance theoretical guarantees with practical considerations.

Spectrum auctions represent a complex application of combinatorial auction theory. With n bidders and m non-identical items, each bidder has a private valuation for every possible bundle of items, making it impractical to directly elicit all preferences. This necessitates the use of indirect, iterative mechanisms that query bidders for valuation information on a "need-to-know" basis, sacrificing some of the desirable properties of direct mechanisms like dominant strategy incentive compatibility (DSIC) and full welfare maximization. The fundamental challenge in spectrum auctions lies in the nature of the items being sold. There is a dichotomy between items that are substitutes (where $v(AB) \leq v(A) + v(B))$ and those that are complements (where $v(AB) > v(A) + v(B))$. Substitute items, such as licenses for the same area with equal-sized frequency ranges, are generally easier to handle. When items are substitutes, welfare maximization is computationally tractable, and the VCG mechanism avoids many undesirable properties. However, complementary items, which arise naturally in spectrum auctions when bidders want adjacent licenses, present significant challenges. Early attempts at spectrum auctions revealed the pitfalls of naive approaches. Sequential auctions, where items are sold one after another, proved problematic as demonstrated by a Swiss auction in 2000. Bidders struggled to bid intelligently without knowing future prices, leading to unpredictable outcomes and potential revenue loss. Similarly, simultaneous sealed-bid auctions, as used in New Zealand in 1990, created difficulties for bidders in coordinating their bids across multiple items, resulting in severely suboptimal outcomes.

The Simultaneous Ascending Auction (SAA) emerged as a solution to these issues and has formed the basis of most spectrum auctions over the past two decades. In an SAA, multiple items are auctioned simultaneously in rounds, with bidders placing bids on any subset of items subject to an activity rule. This format facilitates price discovery, allowing bidders to adjust their strategies as they learn about others' valuations. It also allows bidders to determine valuations on a need-to-know basis, reducing the cognitive burden compared to direct-revelation auctions. Despite its advantages, the SAA is not without vulnerabilities. Demand reduction, where bidders strategically reduce their demand to lower prices, can lead to inefficient outcomes even when items are substitutes. The exposure problem arises with complementary items, where bidders risk winning only a subset of desired items at unfavorable prices. These issues highlight the ongoing challenges in designing effective spectrum auctions, balancing theoretical guarantees with practical considerations.

Case study: Classroom Peer Grading. This section discusses work by Jason Hartline, Yingkai Li, Liren Shan, and Yifan Wu at Northwestern University, where researchers examined mechanism design for the classroom, specifically in terms of the optimization of scoring rules. They explored peer grading in the classroom and how to construct a peer grading system that optimizes the objectives for each stakeholder in the system, including those being graded, the peer graders, the TAs of the class, and the professor. Firstly, let's think of the classroom like a computer. We can think of students as local optimizers; their incentive is to minimize the amount of work they need to do and maximize the grades that they receive. The graders are imprecise operators, which means that there is some uncertainty in their ability to grade the work completed by the students. The syllabus can be thought of as the rules that map the actions of the students to the grade they end up receiving in the class. Our overall goals for this classroom based on these definitions is to minimize work, maximize learning, and fairly assess the students for the work that they do [@jasonH2020]. One basic question that we can examine, is what is the best syllabus that maximizes our objectives for our classroom design. Some components of this could include grading randomized exams, grading with partial credit, group projects, and finally, peer grading, which is the component that we will be taking a deeper dive into. The general situation of the peer grading problem is that proper scoring rules make peer grades horrible [@jasonH2020]. So we want to be able to optimize scoring rules and make sure that we are optimizing each component of the peer grading pipeline.

The main algorithms focused on in this peer grading design paper were matching peers and TAs to submissions and the grading of those submissions from the TAs and the peer reviews  [@jasonH2020]. There are quite a number of advantages to peer grading including that peers are able to learn from reviewing other people's work, it reduces the work for the teacher, and improves the turnaround time for assignment feedback (which are all part of our overarching goals for our mechanism design for the classroom). But, it is also important to acknowledge the potential disadvantages of the peer grading system: it is possible that the peer graders present inaccurate grades and there is student unrest. This presents us with a challenge: being able to incentivize accurate peer reviews.

One problem that we run into, when we use the proper scoring rule to score peer reviews, if the peer graders use the lazy peer strategy, which means that they always report 80$\%$ for their peer reviews, they get graded very well using the proper scoring rule algorithm. In fact, the proper scoring rule says that their peer review is 96$\%$ accurate [@jasonH2023]. So how do we incentivize effort in reviews from peer graders? We use a scoring rule that maximizes the difference in score between effort or no effort reviews as indicated by the peer reviewers [@jasonH2023]. So overall, the analysis of datasets leads to decision optimizations and, eventually, payoff from those decisions.

In conclusion, scoring rules are essential in being able to understand and analyze data thoroughly, and optimal scoring rules for binary effort allow us to understand the setting independent of the dataset [@jasonH2023].
-->


<!-- -->

</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-arrow1951" class="csl-entry" role="listitem">
Arrow, Kenneth J. 1951. <em>Social Choice and Individual Values</em>. John Wiley; Sons.
</div>
<div id="ref-auer_cesa-bianchi_fischer_2002" class="csl-entry" role="listitem">
Auer, Peter, Nicolò Cesa-Bianchi, and Paul Fischer. 2002. <span>“Finite-Time Analysis of the Multiarmed Bandit Problem.”</span> <em>Machine Learning</em> 47 (2). <a href="https://doi.org/10.1023/A:1013689704352">https://doi.org/10.1023/A:1013689704352</a>.
</div>
<div id="ref-bartholdi1989" class="csl-entry" role="listitem">
Bartholdi, John J., Craig A. Tovey, and Michael A. Trick. 1989. <span>“The Computational Difficulty of Manipulating an Election.”</span> <em>Social Choice and Welfare</em> 6 (3): 227–41.
</div>
<div id="ref-bulow-klemperer1996" class="csl-entry" role="listitem">
Bulow, Jeremy, and Paul Klemperer. 1996. <span>“Auctions Versus Negotiations.”</span> <em>The American Economic Review</em> 86 (1): 180–94. <a href="http://www.jstor.org/stable/2118262">http://www.jstor.org/stable/2118262</a>.
</div>
<div id="ref-christiano2018supervising" class="csl-entry" role="listitem">
Christiano, Paul, Buck Shlegeris, and Dario Amodei. 2018. <span>“Supervising Strong Learners by Amplifying Weak Experts.”</span> <em>arXiv Preprint arXiv:1810.08575</em>.
</div>
<div id="ref-gibbard1973" class="csl-entry" role="listitem">
Gibbard, Allan. 1973. <span>“Manipulation of Voting Schemes: A General Result.”</span> <em>Econometrica</em> 41 (4): 587–601.
</div>
<div id="ref-gordon2022jury" class="csl-entry" role="listitem">
Gordon, Noah J., Vaishnavh Nagarajan Shankar, Shi Feng, Yejin Choi, and Noah A. Smith. 2022. <span>“Jury Learning: Integrating Dissenting Voices into Machine Learning Models.”</span> In <em>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2658–73. Association for Computational Linguistics.
</div>
<div id="ref-jasonH2020" class="csl-entry" role="listitem">
Hartline, Jason D., Yingkai Li, Liren Shan, and Yifan Wu. 2020. <span>“Optimization of Scoring Rules.”</span> <em>CoRR</em> abs/2007.02905. <a href="https://arxiv.org/abs/2007.02905">https://arxiv.org/abs/2007.02905</a>.
</div>
<div id="ref-jasonH2023" class="csl-entry" role="listitem">
Hartline, Jason D., Liren Shan, Yingkai Li, and Yifan Wu. 2023. <span>“Optimal Scoring Rules for Multi-Dimensional Effort.”</span> In <em>Proceedings of Thirty Sixth Conference on Learning Theory</em>, edited by Gergely Neu and Lorenzo Rosasco, 195:2624–50. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v195/hartline23a.html">https://proceedings.mlr.press/v195/hartline23a.html</a>.
</div>
<div id="ref-irving2018ai" class="csl-entry" role="listitem">
Irving, Geoffrey, Paul Christiano, and Dario Amodei. 2018. <span>“AI Safety via Debate.”</span> <em>arXiv Preprint arXiv:1805.00899</em>.
</div>
<div id="ref-kongschoenebeck2019" class="csl-entry" role="listitem">
Kong, Yuqing, and Grant Schoenebeck. 2019. <span>“An Information Theoretic Framework for Designing Information Elicitation Mechanisms That Reward Truth-Telling.”</span> <em>ACM Trans. Econ. Comput.</em> 7 (1). <a href="https://doi.org/10.1145/3296670">https://doi.org/10.1145/3296670</a>.
</div>
<div id="ref-LAI19854" class="csl-entry" role="listitem">
Lai, T. L, and Herbert Robbins. 1985. <span>“Asymptotically Efficient Adaptive Allocation Rules.”</span> <em>Advances in Applied Mathematics</em> 6 (1): 4–22. https://doi.org/<a href="https://doi.org/10.1016/0196-8858(85)90002-8">https://doi.org/10.1016/0196-8858(85)90002-8</a>.
</div>
<div id="ref-mansour2019bayesianincentivecompatiblebanditexploration" class="csl-entry" role="listitem">
Mansour, Yishay, Aleksandrs Slivkins, and Vasilis Syrgkanis. 2019. <span>“Bayesian Incentive-Compatible Bandit Exploration.”</span> <a href="https://arxiv.org/abs/1502.04147">https://arxiv.org/abs/1502.04147</a>.
</div>
<div id="ref-mansour2021bayesianexplorationincentivizingexploration" class="csl-entry" role="listitem">
Mansour, Yishay, Aleksandrs Slivkins, Vasilis Syrgkanis, and Zhiwei Steven Wu. 2021. <span>“Bayesian Exploration: Incentivizing Exploration in Bayesian Games.”</span> <a href="https://arxiv.org/abs/1602.07570">https://arxiv.org/abs/1602.07570</a>.
</div>
<div id="ref-myerson1981" class="csl-entry" role="listitem">
Myerson, Roger B. 1981. <span>“Optimal Auction Design.”</span> <em>Mathematics of Operations Research</em> 6 (1): 58–73.
</div>
<div id="ref-russo2015informationtheoreticanalysisthompsonsampling" class="csl-entry" role="listitem">
Russo, Daniel, and Benjamin Van Roy. 2015. <span>“An Information-Theoretic Analysis of Thompson Sampling.”</span> <a href="https://arxiv.org/abs/1403.5341">https://arxiv.org/abs/1403.5341</a>.
</div>
<div id="ref-satterthwaite1975" class="csl-entry" role="listitem">
Satterthwaite, Mark Allen. 1975. <span>“Strategy-Proofness and Arrow’s Conditions: Existence and Correspondence Theorems for Voting Procedures and Social Welfare Functions.”</span> <em>Journal of Economic Theory</em> 10 (2): 187–217.
</div>
<div id="ref-vickrey1961" class="csl-entry" role="listitem">
Vickrey, William. 1961. <span>“Counterspeculation, Auctions, and Competitive Sealed Tenders.”</span> <em>Journal of Finance</em> 16 (1): 8–37.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../src/chap3.html" class="pagination-link" aria-label="Learning Model of Decisions">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Learning Model of Decisions</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../src/chap5.html" class="pagination-link" aria-label="Human Values and AI Alignment">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="an">title:</span><span class="co"> Aggregation of Preferences</span></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="an">format:</span><span class="co"> html</span></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="an">filters:</span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="co">  - pyodide</span></span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="an">execute:</span></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="co">  engine: pyodide</span></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="co">  pyodide:</span></span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="co">    auto: true</span></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="co">---</span></span>
<span id="cb2-11"><a href="#cb2-11"></a></span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="fu">## Social Choice Theory and Implications for AI Preference Aggregation</span></span>
<span id="cb2-13"><a href="#cb2-13"></a></span>
<span id="cb2-14"><a href="#cb2-14"></a>In many applications, human preferences must be aggregated across multiple individuals to determine a collective decision or ranking. This process is central to social choice theory, which provides a mathematical foundation for preference aggregation. Unlike individual preference modeling, which focuses on how a single person makes decisions, social choice theory addresses the challenge of combining multiple preference profiles into a single coherent outcome. A social welfare function (SWF) takes as input each individual's preference ranking over a set of alternatives and produces a social ranking of those alternatives. A related concept is a social choice function (SCF), which selects a single winning alternative given individuals’ preferences. Many voting rules can be seen as social choice functions that aim to reflect the group’s preferences. Formally, let $N=<span class="sc">\{</span>1,2,\dots,n<span class="sc">\}</span>$ be a set of $n$ voters (agents) and $A=<span class="sc">\{</span>a_1,\dots,a_m<span class="sc">\}</span>$ a set of $m$ alternatives (with $m \ge 3$). Each voter $i$ has a preference order $\succ_i$ over $A$. A social choice function is a mapping $f: (\succ_1,\dots,\succ_n)\mapsto A$ that picks a winning alternative for each possible profile of individual preferences. A social welfare function is a mapping that produces a complete societal ranking $\succ^*$ of the alternatives. The central question is: can we design an aggregation rule that faithfully represents individual preferences while satisfying certain fairness or rationality axioms?</span>
<span id="cb2-15"><a href="#cb2-15"></a></span>
<span id="cb2-16"><a href="#cb2-16"></a>Many common voting rules illustrate different methods of aggregation, each with its own merits and vulnerabilities:</span>
<span id="cb2-17"><a href="#cb2-17"></a></span>
<span id="cb2-18"><a href="#cb2-18"></a><span class="ss">- </span>Plurality: Each voter names their top choice; the alternative with the most votes wins.</span>
<span id="cb2-19"><a href="#cb2-19"></a><span class="ss">- </span>Borda Count: Voters rank all alternatives, and points are assigned based on the position in each ranking. For example, with $m$ alternatives, a voter’s top-ranked alternative gets $m-1$ points, the second-ranked gets $m-2$, and so on down to 0. The Borda score of an alternative is the sum of points from all voters, and the winner is the alternative with the highest total score.</span>
<span id="cb2-20"><a href="#cb2-20"></a><span class="ss">- </span>Single Transferable Vote (STV): Voters rank choices, and the count proceeds in rounds. In each round, the alternative with the fewest votes is eliminated and those votes are transferred to the next preferred remaining alternative on each ballot, until one candidate has a majority.</span>
<span id="cb2-21"><a href="#cb2-21"></a><span class="ss">- </span>Condorcet Methods: These look for a candidate that wins in all pairwise majority contests against other alternatives (the Condorcet winner), if such an alternative exists.</span>
<span id="cb2-22"><a href="#cb2-22"></a></span>
<span id="cb2-23"><a href="#cb2-23"></a>However, preference aggregation is not always straightforward. The Condorcet paradox illustrates that majority preferences can be cyclic (rock-paper-scissors style), so that no single alternative is majority-preferred to all others, violating transitivity. Different voting rules can yield different winners on the same profile, highlighting how the choice of rule influences the outcome. To guide the design of social choice functions, several desirable properties or axioms have been proposed. Three classical fairness criteria are:</span>
<span id="cb2-24"><a href="#cb2-24"></a></span>
<span id="cb2-25"><a href="#cb2-25"></a><span class="ss">1. </span>Unanimity (Pareto efficiency): If all individuals strictly prefer one alternative $x$ over another $y$ (i.e. $x \succ_i y$ for every voter $i$), then the group ranking should prefer $x$ over $y$ as well ($x \succ^* y$).</span>
<span id="cb2-26"><a href="#cb2-26"></a><span class="ss">2. </span>Independence of Irrelevant Alternatives (IIA): The social preference between any two alternatives $x$ and $y$ should depend only on the individual preferences between $x$ and $y$. In other words, if we change individuals’ rankings of other “irrelevant” alternatives (not $x$ or $y$) in any way, the group’s relative ordering of $x$ and $y$ should remain unchanged.</span>
<span id="cb2-27"><a href="#cb2-27"></a><span class="ss">3. </span>Non-dictatorship: The aggregation should not simply follow a single individual’s preference regardless of others. There is no voter $i$ who always gets their top choice as the social choice (or whose rankings always become the social ranking), irrespective of other voters’ preferences.</span>
<span id="cb2-28"><a href="#cb2-28"></a></span>
<span id="cb2-29"><a href="#cb2-29"></a>Additionally, we assume an unrestricted domain (universal admissibility): individuals may have any transitive preference ordering over the $m$ alternatives (no restrictions like single-peaked preferences unless explicitly imposed). One might hope that a fair voting rule exists that satisfies all the above properties for three or more alternatives. Surprisingly, a seminal negative result shows this is impossible.</span>
<span id="cb2-30"><a href="#cb2-30"></a></span>
<span id="cb2-31"><a href="#cb2-31"></a>Arrow’s Impossibility Theorem <span class="co">[</span><span class="ot">@arrow1951</span><span class="co">]</span> is a cornerstone of social choice theory. It states that when there are three or more alternatives ($m\ge 3$), no social welfare function can simultaneously satisfy Unanimity, IIA, and Non-dictatorship – unless it is a trivial dictatorial rule. In other words, any aggregation mechanism that is not dictatorial will inevitably violate at least one of the fairness criteria. The theorem is usually proven by contradiction: assuming a social welfare function satisfies all conditions, one can show that one voter’s preferences always decide the outcome, hence the rule is dictatorial. Intuitively, Arrow’s theorem is driven by the possibility of preference cycles in majority voting. Even if individual preferences are transitive, aggregated majorities can prefer $A$ to $B$, $B$ to $C$, and $C$ to $A$ in a cycle, as in the Condorcet paradox. Under Unanimity and IIA, the social ranking must locally match these pairwise preferences, but this produces a contradiction with transitivity unless one voter’s ranking is given overriding authority. A sketch of Arrow’s proof is as follows: one shows that under the axioms, the social ranking between any two alternatives $x$ and $y$ must agree with some particular voter’s preference (the “pivotal” voter for that pair). With IIA, the identity of the pivotal voter must be the same across all pairs of alternatives, otherwise by cleverly constructing profiles one can derive a conflict. This single pivotal voter then effectively dictates the entire social order, violating Non-dictatorship. Hence, the axioms are incompatible.</span>
<span id="cb2-32"><a href="#cb2-32"></a></span>
<span id="cb2-33"><a href="#cb2-33"></a>Arrow’s Impossibility Theorem has profound implications: it formalizes the inherent trade-offs in designing any fair aggregation scheme. In practice, different voting rules relax one or more of Arrow’s conditions. For instance, Borda count violates IIA (since introducing or removing an irrelevant alternative can change the point totals), while a dictatorship violates fairness blatantly. The theorem suggests that every practical voting system must sacrifice at least one of the ideal fairness criteria. It also motivated the exploration of alternative frameworks (such as allowing interpersonal comparisons of utility or cardinal preference aggregation) to escape the impossibility by weakening assumptions.</span>
<span id="cb2-34"><a href="#cb2-34"></a></span>
<span id="cb2-35"><a href="#cb2-35"></a>Complementing Arrow’s theorem, the Gibbard–Satterthwaite theorem focuses on incentives and strategic manipulation in voting systems <span class="co">[</span><span class="ot">@gibbard1973; @satterthwaite1975</span><span class="co">]</span>. It considers any deterministic social choice function $f$ that chooses a single winner from the set of $m\ge 3$ alternatives. The theorem states that if $f$ is strategy-proof (incentive compatible) and onto (its range of outcomes is the entire set of alternatives), then $f$ must be dictatorial. Strategy-proofness (also called truthfulness or dominant-strategy incentive compatibility) means that no voter can ever benefit by misrepresenting their true preferences, regardless of what others do. In other words, reporting their genuine ranking is a (weakly) dominant strategy for each voter. The theorem implies that for any realistic voting rule where every alternative can possibly win, either one voter effectively decides the outcome (a dictatorship) or else the rule is susceptible to strategic manipulation by voters. The Gibbard–Satterthwaite theorem tells us that every non-dictatorial voting rule for 3 or more alternatives is manipulable: there will exist some election scenario where a voter can gain a more preferred outcome by voting insincerely (i.e. not according to their true preferences). For example, in a simple plurality vote, a voter whose true favorite is a long-shot candidate might vote for a more viable candidate to avoid a worst-case outcome (“lesser of two evils” voting). In a Borda count election, voters might strategically raise a competitor in their ranking to push down an even stronger rival. The only way to avoid all such strategic voting incentives is to have a dictatorship or limit the choice set to at most two alternatives.</span>
<span id="cb2-36"><a href="#cb2-36"></a></span>
<span id="cb2-37"><a href="#cb2-37"></a>The proof of Gibbard–Satterthwaite is non-trivial, but one can outline the idea: Given a non-dictatorial and onto rule $f$, one shows there exist at least three distinct outcomes that can result from some preference profiles. By carefully constructing profiles and using the onto property, one finds a situation where a single voter can change the outcome by switching their order of two candidates, demonstrating manipulability. The theorem is robust – even if we allow ties or weaker conditions, similar impossibility results hold (Gibbard’s 1978 extension handles randomized rules). The practical takeaway is that all meaningful voting protocols encourage tactical voting in some situations. Nonetheless, certain systems are considered “harder to manipulate” or more resistant due to complexity or uncertainty. For instance, while STV (ranked-choice voting) can be manipulated in theory, determining a beneficial strategic vote can be NP-hard in worst cases, which arguably provides some practical deterrence to manipulation <span class="co">[</span><span class="ot">@bartholdi1989</span><span class="co">]</span>.</span>
<span id="cb2-38"><a href="#cb2-38"></a></span>
<span id="cb2-39"><a href="#cb2-39"></a>Arrow’s and Gibbard–Satterthwaite’s theorems highlight the limitations any preference aggregation method must face. In domains like reinforcement learning from human feedback (RLHF) and AI alignment, we also aggregate preferences – often preferences of multiple human evaluators or preferences revealed in pairwise comparisons – to guide machine learning systems. While these settings sometimes use cardinal scores or learned reward functions (escaping the strict ordinal framework of Arrow’s theorem), the spirit of these impossibility results still applies: there is no perfect way to aggregate human opinions without trade-offs.</span>
<span id="cb2-40"><a href="#cb2-40"></a></span>
<span id="cb2-41"><a href="#cb2-41"></a>For example, aggregating human feedback to train a model may run into inconsistencies analogous to preference cycles, especially when feedback comes from diverse individuals with different values. A simple majority vote over preferences might yield unstable or unfair outcomes if some annotators are systematically in the minority. Weighting votes by some credibility or expertise (weighted voting) can improve outcomes but raises the question of how to set the weights without introducing dictator-like influence. Recent research has proposed methods like jury learning – which integrates dissenting voices by having a panel (“jury”) of models or human subgroups whose aggregated judgment guides the learning <span class="co">[</span><span class="ot">@gordon2022jury</span><span class="co">]</span> – to ensure minority preferences are not entirely ignored. Another perspective is social choice in AI alignment, which suggests using social choice theory to design AI systems that respect a plurality of human values instead of collapsing everything into a single objective. In pluralistic value alignment, instead of forcing a single “best” solution, an AI might present a diverse set of options or behave in a way that reflects a distribution of values. This approach aims to preserve the diversity of human preferences rather than always aggregating to one monolithic preference. For instance, a conversational AI might be designed to recognize multiple acceptable responses (each aligning with different value systems) rather than one canonical “aligned” response for a given query.</span>
<span id="cb2-42"><a href="#cb2-42"></a></span>
<span id="cb2-43"><a href="#cb2-43"></a>These considerations are especially relevant in generative AI and large language models, where training involves human preference data. If we aggregate feedback naively, we might overfit to the majority preference and lose minority perspectives (a form of tyranny of the majority). On the other hand, trying to satisfy everyone can lead to indecision or an incoherent objective. The impossibility results remind us there is no free lunch: we must carefully decide which properties to prioritize (e.g. giving more weight to expert annotators versus preserving broader fairness, or balancing consistency vs inclusivity). Designing aggregation mechanisms for AI that reflect collective human values is an ongoing challenge. It often involves insights from traditional voting theory (to understand trade-offs and failure modes) combined with machine learning techniques (to model and learn from preference data). In summary, social choice theory provides cautionary guidance as we build systems that learn from human preferences: we need to be conscious of which fairness criteria we relax and be transparent about the compromises being made in any preference aggregation pipeline.</span>
<span id="cb2-44"><a href="#cb2-44"></a></span>
<span id="cb2-45"><a href="#cb2-45"></a><span class="fu">## Mechanism Design {#single-item-auctions}</span></span>
<span id="cb2-46"><a href="#cb2-46"></a></span>
<span id="cb2-47"><a href="#cb2-47"></a>While voting rules aggregate ordinal rank preferences to select a social outcome, another class of preference aggregation occurs in economic settings like auctions and general mechanism design. Here individuals reveal their valuations (numerical utilities) for outcomes, and the mechanism chooses an outcome (such as an allocation of goods) and possibly payments. Mechanism design asks: how can we design rules so that rational agents, acting in their own interest, end up revealing information that leads to a socially desirable outcome? A central concept is incentive compatibility – the mechanism should be designed so that each participant’s best strategy is to act according to their true preferences (e.g. bid their true value). In this section, we focus on auctions as a prime example of preference aggregation with money, and highlight classical results including Vickrey–Clarke–Groves (VCG) mechanisms and Myerson’s optimal auction.</span>
<span id="cb2-48"><a href="#cb2-48"></a></span>
<span id="cb2-49"><a href="#cb2-49"></a>Consider a single-item auction with one item for sale and $n$ bidders. Bidder $i$ has a private valuation $v_i$ for the item (how much the item is worth to them). Each bidder’s goal is to maximize their own utility, defined as $v_i - p_i$ if they win and pay price $p_i$, or $0$ if they do not win (assuming quasilinear utility where money is the transferable utility). The auction’s task is to allocate the item to one of the bidders and possibly determine payments. We can think of an auction as a mechanism that asks each bidder for a “message” (typically a bid representing how much they are willing to pay), then selects a winner and a price based on the bids. A key objective might be social welfare maximization – allocate the item to the bidder who values it most (maximizing $v_i$ of the winner). Another possible objective is revenue maximization for the seller – choose the allocation and price to maximize the seller’s expected payment.</span>
<span id="cb2-50"><a href="#cb2-50"></a></span>
<span id="cb2-51"><a href="#cb2-51"></a>A classic result in auction theory is that to maximize social welfare in a single-item private-value setting, one should award the item to the highest valuer – and this can be done in an incentive-compatible way by using a second-price auction. The Vickrey second-price auction works as follows: (1) All bidders submit sealed bids $b_1, b_2, \ldots, b_n$. (2) The bidder with the highest bid wins the item. (3) The price paid by the winner is the second-highest bid. For example, if the bids are $(2,\, 6,\, 4,\, 1)$ (in some currency units), the highest bid is $6$ (by bidder 2, say) and the second-highest is $4$. Bidder 2 wins the item and pays $4$. </span>
<span id="cb2-52"><a href="#cb2-52"></a></span>
<span id="cb2-53"><a href="#cb2-53"></a>Under this mechanism, it turns out that bidding truthfully $b_i = v_i$ is a dominant strategy for each bidder. In other words, the auction is dominant-strategy incentive compatible (DSIC): no matter what others do, a bidder maximizes their expected utility by reporting their true valuation. The intuition is as follows. If bidder $i$ bids lower than their true value (i.e. $b_i &lt; v_i$), and if their true value was actually the highest, they risk losing the item even though they value it more than the price they would have paid – a missed opportunity for positive utility. Bidding higher than their value ($b_i &gt; v_i$) cannot help them win in any situation where bidding truthfully wouldn’t (it could only make a difference if their true $v_i$ wasn’t the highest but they tried to win anyway); and if they do win with an inflated bid, they might end up paying the second-highest bid which could be above their true value, yielding negative utility. By bidding exactly $v_i$, if they win, it means all other bids were lower, so $v_i$ is at least as high as the second-highest bid $p$ they pay – guaranteeing non-negative utility $v_i - p \ge 0$. If they lose, it means someone else had a higher bid (hence higher value, if others are truthful), so bidder $i$ wouldn’t have gained anyway. This argument, made rigorous by Vickrey <span class="co">[</span><span class="ot">@vickrey1961</span><span class="co">]</span>, establishes that truth-telling is a dominant strategy in the second-price auction. As a consequence, when everyone bids truthfully, the item is allocated to the bidder with the highest $v_i$, achieving maximum social surplus (allocative efficiency). The second-price auction is thus an elegant mechanism that aligns individual incentives with social welfare maximization.</span>
<span id="cb2-54"><a href="#cb2-54"></a></span>
<span id="cb2-55"><a href="#cb2-55"></a>It is worth contrasting this with a first-price auction, where the winner pays their own bid. In a first-price auction, bidders have an incentive to bid below their true value (to avoid the winner’s curse of paying too much), in a Nash equilibrium that involves bid shading. The first-price auction can still allocate to the highest valuer in equilibrium, but only through strategic behavior (and it is not DSIC). By charging the second-highest bid, the Vickrey auction removes the incentive to shade bids, since the price does not directly depend on one’s own bid beyond the fact of winning or losing.</span>
<span id="cb2-56"><a href="#cb2-56"></a></span>
<span id="cb2-57"><a href="#cb2-57"></a>So far, we discussed auctions aimed at maximizing social welfare. In many cases, the auctioneer (seller) is interested in maximizing revenue. A foundational result by Roger Myerson (1981) provides a characterization of optimal auctions (those that maximize the seller’s expected revenue) for single-item settings under certain assumptions <span class="co">[</span><span class="ot">@myerson1981</span><span class="co">]</span>. The problem can be formulated as follows: suppose each bidder’s private value $v_i$ is drawn independently from a known distribution $F_i$ (for simplicity, assume identical distribution $F$ for all bidders, i.i.d.). We seek a mechanism (allocation rule and payment rule) that maximizes the seller’s expected payment, subject to incentive compatibility and individual rationality (participants should not expect negative utility from truthful participation).</span>
<span id="cb2-58"><a href="#cb2-58"></a></span>
<span id="cb2-59"><a href="#cb2-59"></a>Myerson’s theorem states that the optimal auction in such a setting is a threshold auction characterized by virtual valuations. Define the virtual value for a bidder with value $v$ as $\varphi(v) = v - \frac{1-F(v)}{f(v)}$, where $f$ is the probability density function of $F$ (assuming it is continuous). An assumption called regularity (which holds for many distributions) is that $\varphi(v)$ is non-decreasing in $v$. Myerson showed that the revenue-maximizing strategy is: treat $\varphi(v)$ as the effective “score” of a bid, allocate the item to the bidder with the highest non-negative virtual value (if all virtual values are negative, allocate to no one), and charge them the smallest value they could have such that they would still win (the payment is essentially the critical bid where $\varphi$ of that bid equals the second-highest virtual value or the zero cutoff). In practice, for i.i.d. bidders, this reduces to: there is an optimal reserve price $r$ such that you sell to the highest bidder if and only if their bid $b_{\max} \ge r$; if sold, the price is the max of the second-highest bid and $r$.</span>
<span id="cb2-60"><a href="#cb2-60"></a></span>
<span id="cb2-61"><a href="#cb2-61"></a>In the case of $n$ bidders with values i.i.d. uniform on $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ (which is a regular distribution), one can compute the optimal reserve price. The virtual value function for uniform $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ is $\varphi(v) = v - \frac{1-v}{1} = 2v - 1$. Setting $\varphi(v)\ge 0$ gives $v \ge 0.5$. So Myerson’s mechanism says: don’t sell the item if all bids are below 0.5; otherwise, sell to the highest bidder at at least 0.5. This is exactly a second-price auction with a reserve of $r=0.5$. Our earlier example implicitly demonstrated this: with two uniform(0,1) bidders, the optimal auction sets a reserve price of $0.5$ and yields a certain expected revenue. We can break down the cases:</span>
<span id="cb2-62"><a href="#cb2-62"></a><span class="ss">- </span>With probability $1/4$, both bidders have values below $0.5$ (each below 0.5 with probability 1/2), in which case nobody wins and revenue is 0.</span>
<span id="cb2-63"><a href="#cb2-63"></a><span class="ss">- </span>With probability $1/4$, both bidders have $v &gt; 0.5$. In this case, the second-price auction with reserve will sell to the highest bidder at the max of the second-highest value and 0.5. Given both $v_1, v_2 &gt; 0.5$, the expected second-highest value (conditional on both &gt;0.5) is $\frac{2}{3}$ (in fact, the order statistics of two uniforms on <span class="co">[</span><span class="ot">0.5,1</span><span class="co">]</span> give mean of min = 2/3). So in this case the expected price is the second-highest value (since that will exceed 0.5), about 0.667.</span>
<span id="cb2-64"><a href="#cb2-64"></a><span class="ss">- </span>With probability $1/2$, one bidder is above 0.5 and the other below. In that case, the one above 0.5 wins at price equal to the reserve 0.5 (since the second-highest bid is the reserve). </span>
<span id="cb2-65"><a href="#cb2-65"></a></span>
<span id="cb2-66"><a href="#cb2-66"></a>Taking the expectation, the seller’s expected revenue is $0*(1/4) + (2/3)*(1/4) + (1/2*1/2) = 0 + 1/6 + 1/4 = 5/12 \approx 0.417$. This is higher than the expected revenue without a reserve. In fact, without a reserve (just a plain second-price with two bidders uniform <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>), one can compute the expected revenue is $1/3 \approx 0.333$ (the second order statistic’s expectation). Thus, the reserve has increased revenue. Myerson’s theory tells us that indeed the second-price auction with an optimally chosen reserve maximizes revenue among all DSIC mechanisms for this setting. A notable special case result is that when bidder distributions are i.i.d. and regular, an optimal auction is essentially “allocatively efficient with a reserve price” – i.e. aside from possibly excluding low-value bidders via a reserve, it allocates to the highest remaining bid.</span>
<span id="cb2-67"><a href="#cb2-67"></a></span>
<span id="cb2-68"><a href="#cb2-68"></a>Myerson’s work also highlighted the gap between revenue maximization and welfare maximization. The price of optimality (in revenue) is that the seller might sometimes forego efficient allocation (e.g. not selling despite a willing buyer, in order to preserve a high reserve price strategy). In contrast, Vickrey’s auction always allocates efficiently but may not maximize revenue.</span>
<span id="cb2-69"><a href="#cb2-69"></a></span>
<span id="cb2-70"><a href="#cb2-70"></a>An interesting insight in auction theory is that increasing competition can yield more revenue than fine-tuning the auction mechanism. The Bulow–Klemperer theorem <span class="co">[</span><span class="ot">@bulow-klemperer1996</span><span class="co">]</span> demonstrates that, under certain regularity assumptions, a simple welfare-maximizing auction with one extra bidder outperforms the optimal auction with fewer bidders. Specifically, for i.i.d. bidders with a regular distribution $F$, the expected revenue of a second-price auction with $n+1$ bidders is at least as high as the expected revenue of the Myerson-optimal auction with $n$ bidders. In formula form:</span>
<span id="cb2-71"><a href="#cb2-71"></a></span>
<span id="cb2-72"><a href="#cb2-72"></a>$$</span>
<span id="cb2-73"><a href="#cb2-73"></a>\mathbb{E}_{v_1,\ldots,v_{n+1} \sim F}<span class="co">[</span><span class="ot">\text{Rev}^{\text{(second-price)}}(n+1 \text{ bidders})</span><span class="co">]</span> \geq </span>
<span id="cb2-74"><a href="#cb2-74"></a>\mathbb{E}_{v_1,\ldots,v_n \sim F}<span class="co">[</span><span class="ot">\text{Rev}^{\text{(optimal)}}(n \text{ bidders})</span><span class="co">]</span> \,. </span>
<span id="cb2-75"><a href="#cb2-75"></a>\tag{4.1}\label{eq-eq3.64}</span>
<span id="cb2-76"><a href="#cb2-76"></a>$$</span>
<span id="cb2-77"><a href="#cb2-77"></a></span>
<span id="cb2-78"><a href="#cb2-78"></a>This result suggests that, in practice, having more participants (competition) is often more valuable than exploiting detailed knowledge of bidder distributions. As a corollary, a policy recommendation is that a seller is usually better off using a simple auction design (like a Vickrey auction or other transparent mechanism) and putting effort into attracting more bidders, rather than using a complex optimal mechanism that might discourage participation.</span>
<span id="cb2-79"><a href="#cb2-79"></a></span>
<span id="cb2-80"><a href="#cb2-80"></a>Vickrey’s second-price auction can be generalized to multiple items and more complex outcomes by the Vickrey–Clarke–Groves (VCG) mechanism. The VCG mechanism is a cornerstone of mechanism design that provides a general solution for implementing socially efficient outcomes (maximizing total stated value) in dominant strategies, for a broad class of problems. It works for any scenario where agents have quasilinear utilities and we want to maximize the sum of valuations.</span>
<span id="cb2-81"><a href="#cb2-81"></a></span>
<span id="cb2-82"><a href="#cb2-82"></a>In a general mechanism design setting, let $\Omega$ be the set of possible outcomes. Each agent $i$ has a private valuation function $v_i(\omega)$ for outcomes $\omega \in \Omega$ (the amount of utility, in money terms, that $i$ gets from outcome $\omega$). Agents report bids $b_i(\omega)$ (which we hope equal $v_i(\omega)$ if they are truthful). The mechanism then chooses an outcome $\omega^* \in \Omega$ to maximize the reported total value: </span>
<span id="cb2-83"><a href="#cb2-83"></a></span>
<span id="cb2-84"><a href="#cb2-84"></a>$$</span>
<span id="cb2-85"><a href="#cb2-85"></a>\omega^* = \arg\max_{\omega \in \Omega} \sum_{i=1}^n b_i(\omega) \,,</span>
<span id="cb2-86"><a href="#cb2-86"></a>$$ </span>
<span id="cb2-87"><a href="#cb2-87"></a></span>
<span id="cb2-88"><a href="#cb2-88"></a>i.e. $\omega^*$ is the outcome that would be socially optimal if the $b_i$ were true values. To induce truth-telling, VCG sets payments such that each agent pays the externality they impose on others by their presence. Specifically, one convenient form of the VCG payment for agent $i$ is: </span>
<span id="cb2-89"><a href="#cb2-89"></a></span>
<span id="cb2-90"><a href="#cb2-90"></a>$$</span>
<span id="cb2-91"><a href="#cb2-91"></a>p_i(b) = \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega)\;-\;\sum_{j \neq i} b_j(\omega^*) \,,</span>
<span id="cb2-92"><a href="#cb2-92"></a>$$ </span>
<span id="cb2-93"><a href="#cb2-93"></a></span>
<span id="cb2-94"><a href="#cb2-94"></a>which can be interpreted as: what would the total value of others be if $i$ were not present (first term, maximizing without $i$) minus the total value others actually get in the chosen outcome $\omega^*$. Equivalently, we can write the payment as the agent’s bid for the chosen outcome minus a rebate term: </span>
<span id="cb2-95"><a href="#cb2-95"></a></span>
<span id="cb2-96"><a href="#cb2-96"></a>$$</span>
<span id="cb2-97"><a href="#cb2-97"></a>p_i(b) = b_i(\omega^*) \;-\; \Big[\sum_{j=1}^n b_j(\omega^*) - \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega)\Big] \,. \tag{4.2}\label{eq-eq3.67}</span>
<span id="cb2-98"><a href="#cb2-98"></a>$$</span>
<span id="cb2-99"><a href="#cb2-99"></a></span>
<span id="cb2-100"><a href="#cb2-100"></a>This formula (which in single-item auction reduces to second-price logic) ensures that each agent’s net payoff is $v_i(\omega^*) - p_i = \max_{\omega} \sum_{j\neq i} v_j(\omega) + v_i(\omega^*) - \sum_{j\neq i} v_j(\omega^*)$. All terms except $v_i(\omega^*)$ cancel out, meaning each agent’s utility equals the max total welfare of others plus their own value for the chosen outcome minus others’ welfare in the chosen outcome – which does not depend on $v_i(\omega^*)$ except through the decision of $\omega^*$. By construction, an agent cannot influence $\omega^*$ in a way that improves this expression unless it genuinely increases total welfare, so misreporting $v_i$ cannot increase their utility. Thus truthful reporting is a dominant strategy. VCG is dominant-strategy incentive compatible (DSIC) and produces an outcome that maximizes $\sum_i v_i(\omega)$, achieving social welfare maximization.</span>
<span id="cb2-101"><a href="#cb2-101"></a></span>
<span id="cb2-102"><a href="#cb2-102"></a>VCG provides a powerful existence result: under broad conditions, there is a mechanism that achieves efficient allocation with truth-telling (in fact, VCG is essentially the unique one, aside from adding harmless constant transfers). However, implementing VCG in practice can be difficult. One challenge is computational: finding $\arg\max_{\omega}\sum_i b_i(\omega)$ can be NP-hard if $\Omega$ is a combinatorially large space (as in many combinatorial auctions). Another issue is budget balance and revenue: VCG payments might not yield any revenue to the mechanism designer in some cases (or even require subsidies in complex settings), and they can be low or zero in certain environments, which is problematic if the seller needs revenue. VCG is also vulnerable to collusion or the presence of fake identities (sybil attacks) – the mechanism assumes each participant is a separate entity; if one bidder can split into two identities, they might game the outcome.</span>
<span id="cb2-103"><a href="#cb2-103"></a></span>
<span id="cb2-104"><a href="#cb2-104"></a>Nonetheless, for many domains, VCG or variants have been successfully used or at least studied. Notably, combinatorial auctions (where multiple items are up for sale and bidders have valuations for bundles of items) can in theory be handled by VCG: just let $\Omega$ be all possible allocations of items to bidders, and have bidders report $b_i(S)$ for each bundle $S$ of items. VCG would allocate the items in the way that maximizes total reported value and charge each bidder the opportunity cost their presence imposes on others. In practice, as mentioned, combinatorial auctions face exponential complexity in preference reporting (each bidder potentially has to specify a value for every subset of items) and winner determination (solving an NP-hard combinatorial optimization). Heuristic or restricted approaches (like limiting the kinds of bundles or using iterative bidding with query learning of preferences) are used to make the problem tractable. Additionally, pure VCG in combinatorial settings can have undesirable properties: for example, in some cases adding more bidders can cause VCG prices to drop to zero (the so-called “threshold problem” or revenue monotonicity failure), and bidders may collude to manipulate their bids collectively.</span>
<span id="cb2-105"><a href="#cb2-105"></a></span>
<span id="cb2-106"><a href="#cb2-106"></a>One high-stakes application of combinatorial auctions is spectrum auctions for selling licenses of electromagnetic spectrum to telecom companies. Governments have used multi-round combinatorial auctions to allocate spectrum, with billions of dollars at stake. Designing these auctions requires balancing efficiency with simplicity and robustness to strategic behavior. Early spectrum auctions that used simpler formats (like sequential auctions or one-shot sealed bids for each license) ran into problems like the exposure problem – a bidder valuing a combination of items (say complementary licenses in adjacent regions) risks winning only part of the combination at a high price, which could be bad for them if the items are worth much less separately. The simultaneous multi-round auction (SMRA) was an innovation that allowed bidding on all items at once in rounds, giving bidders some price discovery to mitigate the exposure problem. Even so, strategic issues like demand reduction (bidders deliberately not bidding on too many items to keep prices low) and tacit collusion through signaling bids have been observed. These practical complications underscore that while VCG is a beautiful theoretical ideal, real-world mechanism design often involves compromises and tweaks.</span>
<span id="cb2-107"><a href="#cb2-107"></a></span>
<span id="cb2-108"><a href="#cb2-108"></a><span class="fu">### Case Study 1: Mechanism for Peer Grading</span></span>
<span id="cb2-109"><a href="#cb2-109"></a></span>
<span id="cb2-110"><a href="#cb2-110"></a>To illustrate an application of mechanism design beyond auctions, consider a classroom setting where students grade each other’s work (peer assessment). The goal is to design a system (a “mechanism”) that produces fair and accurate grades while incentivizing students to put effort into grading. Jason Hartline and colleagues (2020) studied such a scenario, examining how to optimize scoring rules for peer grading <span class="co">[</span><span class="ot">@jasonH2020</span><span class="co">]</span>. In this setting, students are both agents (who might strategize to maximize their own grade or minimize their effort) and graders. The "outcome" we want is a set of final grades for students, ideally reflecting the true quality of their work.</span>
<span id="cb2-111"><a href="#cb2-111"></a></span>
<span id="cb2-112"><a href="#cb2-112"></a>One idea is to use proper scoring rules to evaluate the peer graders. A proper scoring rule is a concept from forecast evaluation that gives highest expected score for truthful reporting of probabilities. In peer grading, one might try to reward students based on how close their grading is to some ground truth or to the TA’s grades. However, a naive application of proper scoring can backfire. Hartline et al. observed a “lazy peer grader” problem: if students figure out that always giving an average score (say 80%) yields a decent reward under the scoring rule, they might not bother to carefully distinguish good and bad work. In one experiment, giving all peers an 80% could yield a 96% accuracy score for the grader under a certain scoring rule <span class="co">[</span><span class="ot">@jasonH2023</span><span class="co">]</span>. This clearly undermines the goal – the grader is basically cheating the system by always predicting the class average.</span>
<span id="cb2-113"><a href="#cb2-113"></a></span>
<span id="cb2-114"><a href="#cb2-114"></a>To combat this, the mechanism designers sought a scoring rule that maximizes the difference in reward between a diligent grading and a lazy strategy, thereby incentivizing effort. They formulated this as an optimization problem: design the reward function for peer graders such that truthful, careful grading yields a strictly higher expected score than any degenerate strategy like always giving the average. By analyzing data and grader behavior models, they adjusted the scoring rules to penalize obviously lazy patterns and reward variance when warranted. The resulting mechanism improved the accuracy of peer grading by aligning the incentives of student graders (who want a high score for their grading job) with the objective of accurate assessment. This case study highlights how ideas of incentive compatibility and mechanism design apply even in social/educational contexts: the “payments” are points towards one’s own grade, and the mechanism must account for strategic behavior to ensure a reliable outcome.</span>
<span id="cb2-115"><a href="#cb2-115"></a></span>
<span id="cb2-116"><a href="#cb2-116"></a>In conclusion, mechanism design provides a toolkit for aggregating preferences (or signals, like grades or bids) in a principled way, by explicitly accounting for individual incentives. Whether in auctions, peer grading, or other domains, the design of rules (allocation algorithms, payment or scoring schemes) crucially determines whether people feel encouraged to be truthful or to game the system. The theories of VCG and Myerson give us optimal baselines for efficiency and revenue in auctions, while impossibility results like Gibbard–Satterthwaite warn us of the limitations in voting. Real-world implementations often have to grapple with complexity and approximate these ideals. While learning from individual human preference is a powerful approach, it too faces aggregation challenges. If the human feedback is inconsistent or if different annotators have different preferences, the reward model may end up capturing an average that satisfies no one perfectly. There is active research on scalable oversight: techniques to gather and aggregate human feedback on tasks that are too complex for any single person to evaluate reliably. This includes approaches like recursive reward modeling, iterated amplification <span class="co">[</span><span class="ot">@christiano2018supervising</span><span class="co">]</span>, and AI-assisted debate <span class="co">[</span><span class="ot">@irving2018ai</span><span class="co">]</span>, where AI systems help humans provide better feedback or break down tasks. The goal of scalable oversight is to leverage human preferences and principles in guiding AI even as AI systems tackle increasingly complex or open-ended tasks, while mitigating the human burden and bias in evaluation.</span>
<span id="cb2-117"><a href="#cb2-117"></a></span>
<span id="cb2-118"><a href="#cb2-118"></a>In summary, preference aggregation in machine learning spans from simple models like Bradley–Terry for pairwise comparisons to elaborate RLHF pipelines for training large models. The deep mathematical foundations – whether Arrow’s theorem or Myerson’s auction theory – remind us that whenever we aggregate preferences or signals from multiple sources, we must consider incentive effects, fairness criteria, and the possibility of inconsistency. By combining insights from social choice, economics, and statistical learning, we aim to build AI systems that not only learn from human preferences but do so in a principled, robust, and fair manner. The next chapter will delve further into aligning AI with human values, building on the mechanisms and learning algorithms discussed here to ensure AI systems remain beneficial and in line with what people truly want.</span>
<span id="cb2-119"><a href="#cb2-119"></a></span>
<span id="cb2-120"><a href="#cb2-120"></a><span class="fu">### Case Study 2: Incentive-Compatible Online Learning</span></span>
<span id="cb2-121"><a href="#cb2-121"></a></span>
<span id="cb2-122"><a href="#cb2-122"></a>To address this problem, we seek to create a model. We first outline the key criteria that our model must achieve. The model revolves around repeated interactions between a planner (the system) and multiple agents (the users). Each agent, upon arrival in the system, is presented with a set of available options to choose from. These options could vary widely depending on the application of the model, such as routes in a transportation network, a selection of hotels in a travel booking system, or even entertainment choices in a streaming service. The interaction process is straightforward but crucial: agents arrive, select an action from the provided options, and then report feedback based on their experience. This feedback is vital as it forms the basis upon which the planner improves and evolves its recommendations. The agents in this model are considered strategic; they aim to maximize their reward based on the information available to them. This aspect of the model acknowledges the real-world scenario where users are typically self-interested and seek to optimize their own outcomes. The planner, on the other hand, has a broader objective. It aims to learn which alternatives are best in a given context and works to maximize the overall welfare of all agents. This involves a complex balancing act: the planner must accurately interpret feedback from a diverse set of agents, each with their own preferences and biases, and use this information to refine and improve the set of options available. The ultimate goal of the planner is to create a dynamic, responsive system that not only caters to the immediate needs of individual agents but also enhances the collective experience over time, leading to a continually improving recommendation ecosystem.</span>
<span id="cb2-123"><a href="#cb2-123"></a></span>
<span id="cb2-124"><a href="#cb2-124"></a>Here, we seek to address the inherent limitations faced by the planner, particularly in scenarios where monetary transfers are not an option, and the only tool at its disposal is the control over the flow of information between agents. This inquiry aims to understand the extent to which these limitations impact the planner's ability to effectively guide and influence agent behavior. A critical question is whether the planner can successfully induce exploration among agents, especially in the absence of financial incentives. This involves investigating strategies to encourage users to try less obvious or popular options, thus broadening the scope of feedback and enhancing the system's ability to learn and identify the best alternatives. Another question is understanding the rate at which the planner learns from agent interactions. This encompasses examining how different agent incentives, their willingness to explore, and their feedback impact the speed and efficiency with which the planner can identify optimal recommendations.</span>
<span id="cb2-125"><a href="#cb2-125"></a></span>
<span id="cb2-126"><a href="#cb2-126"></a>The model can be extended in several directions, each raising its own set of questions.</span>
<span id="cb2-127"><a href="#cb2-127"></a>    </span>
<span id="cb2-128"><a href="#cb2-128"></a><span class="in">    1.  Multiple Agents with Interconnected Payoffs: When multiple agents arrive simultaneously, their choices and payoffs become interconnected, resembling a game. The research question here focuses on how these interdependencies affect individual and collective decision-making.</span></span>
<span id="cb2-129"><a href="#cb2-129"></a></span>
<span id="cb2-130"><a href="#cb2-130"></a><span class="in">    2.  Planner with Arbitrary Objective Function: Investigating scenarios where the planner operates under an arbitrary objective function, which might not align with maximizing overall welfare or learning the best alternative.</span></span>
<span id="cb2-131"><a href="#cb2-131"></a></span>
<span id="cb2-132"><a href="#cb2-132"></a><span class="in">    3.  Observed Heterogeneity Among Agents: This involves situations where differences among agents are observable and known, akin to contextual bandits in machine learning. The research question revolves around how these observable traits can be used to tailor recommendations more effectively.</span></span>
<span id="cb2-133"><a href="#cb2-133"></a></span>
<span id="cb2-134"><a href="#cb2-134"></a><span class="in">    4.  Unobserved Heterogeneity Among Agents: This aspect delves into scenarios where differences among agents are not directly observable, necessitating the use of causal inference techniques to understand and cater to diverse user needs.</span></span>
<span id="cb2-135"><a href="#cb2-135"></a></span>
<span id="cb2-136"><a href="#cb2-136"></a>In our setup, there is a "planner," which aims to increase exploration, and many independent "agents," which will act selfishly (in a way that they believe will maximize their individual reward) <span class="co">[</span><span class="ot">@mansour2019bayesianincentivecompatiblebanditexploration; @mansour2021bayesianexplorationincentivizingexploration</span><span class="co">]</span>. Under our model shown in Figure <span class="co">[</span><span class="ot">1.1</span><span class="co">](#fig-planner-agent)</span>{reference-type="ref" reference="fig-planner-agent"}, there are $K$ possible actions that all users can take, and each action has some mean reward $\mu_i \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$. In addition, there is a common prior belief on each $\mu_i$ across all users.. The $T$ agents, or users, will arrive sequentially. As the $t$'th user arrives, they are recommended an action $I_t$ by the planner, which they are free to follow or not follow. After taking whichever action they choose, the user experiences some realized reward $r_i \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$, which is stochastic i.i.d. with mean $\mu_i$, and reports this reward back to the planner.</span>
<span id="cb2-137"><a href="#cb2-137"></a></span>
<span id="cb2-138"><a href="#cb2-138"></a>So far, the model we have defined is equivalent to a multi-armed bandit model, which we have seen earlier in this chapter (<span class="co">[</span><span class="ot">1</span><span class="co">](#4optim)</span>{reference-type="ref" reference="4optim"}). Under this model, well-known results in economics, operations research and computer science show that $O(\sqrt{T})$ regret is achievable <span class="co">[</span><span class="ot">@russo2015informationtheoreticanalysisthompsonsampling; @auer_cesa-bianchi_fischer_2002; @LAI19854</span><span class="co">]</span> with algorithms such as Thompson sampling and UCB. However, our agents are strategic and aim to maximize their own rewards. If they observe the rewards gained from actions taken by other previous users, they will simply take the action they believe will yield the highest reward given the previous actions; they would prefer to benefit from exploration done by other users rather than take the risk of exploring themselves. Therefore, exploration on an individual level, which the planner would like to facilitate, is not guaranteed under this paradigm.</span>
<span id="cb2-139"><a href="#cb2-139"></a></span>
<span id="cb2-140"><a href="#cb2-140"></a>In light of this, we also require that our model satisfy incentive compatibility, or that taking the action recommended by the planner has an expected utility that is as high as any other action the agent could take. Formally, $\forall i : \, E<span class="co">[</span><span class="ot">\mu_i | I_t = i</span><span class="co">]</span> \geq E<span class="co">[</span><span class="ot">\mu_{i'} | I_t = i</span><span class="co">]</span>.$ Note that this incentivizes the agents to actually take the actions recommended by the planner; if incentive compatibility is not satisfied, agents would simply ignore the planner and take whatever action they think will lead to the highest reward.</span>
<span id="cb2-141"><a href="#cb2-141"></a></span>
<span id="cb2-142"><a href="#cb2-142"></a>At a high level, the key to achieving incentive compatibility while still creating a policy for the planner that facilitates exploration is information asymmetry. Under this paradigm, the users only have access to their previous recommendations, actions, and rewards, and not to the recommendations, actions, and rewards of other users. Therefore, they are unsure of whether, after other users take certain actions and receive certain rewards, arms that they might have initially considered worse in practice outperform arms that they initially considered better. Only the planner has access to the previous actions and rewards of all users; the user only has access to their own recommendations and overall knowledge of the planner's policy. The main question we aim to answer for the rest of this section is, given this new constraint of incentive compatibility, is $O(\sqrt{T})$ regret still achievable? We illustrate such an algorithm in the following.</span>
<span id="cb2-143"><a href="#cb2-143"></a></span>
<span id="cb2-144"><a href="#cb2-144"></a>The main result here is a black-box reduction algorithm to turn any bandit algorithm into an incentive compatible one, with only a constant increase in Bayesian regret. Since, as mentioned earlier, there are bandit algorithms with $O(\sqrt{T})$ Bayesian regret, black-box reduction will also allow us to get incentive-compatible algorithms with $O(\sqrt{T})$ regret. The idea of black-box reduction will be to simulate $T$ steps of any bandit algorithm in an incentive-compatible way in $c T$ steps. This allows us to design incentive-compatible recommendation systems by using any bandit algorithm and then adapting it. Consider the following setting: there are two possible actions, $A_1$ and $A_2$. Assume the setting of deterministic rewards, where action 1 has reward $\mu_1$ with prior $U<span class="co">[</span><span class="ot">1/3, 1</span><span class="co">]</span>$ and mean $\mathbb{E}<span class="co">[</span><span class="ot">\mu_1</span><span class="co">]</span> = 2/3$, and action 2 has reward $\mu_2$ with prior $U<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$ and mean $\mathbb{E}<span class="co">[</span><span class="ot">\mu_2</span><span class="co">]</span> = 1/2$. Without the planner intervention and with full observability, users would simply always pick $A_1$, so how can the planner incentivize users to play $A_2$?</span>
<span id="cb2-145"><a href="#cb2-145"></a></span>
<span id="cb2-146"><a href="#cb2-146"></a>The key insight is going to be to hide exploration in a pool of exploitation. The users are only going to receive a recommendation from the planner, and no other observations. After deterministically recommending the action with the highest expected reward ($A_1$), the planner will pick one guinea pig to recommend the exploratory action of $A_2$. The users don't know whether they are the guinea pig, so intuitively, as long as the planner picks guinea pigs uniformly at random and at low enough frequencies, the optimal decision for the users is still to follow the planner's recommendation, even if it might go against their interest. The planner will pick the user who will be recommended the exploratory action uniformly at random from the $L$ users that come after the first one (which deterministically gets recommended the exploitation action). Under this setting (illustrated in Figure <span class="co">[</span><span class="ot">1.2</span><span class="co">](#fig-deterministic-guinea-pig)</span>{reference-type="ref" reference="fig-deterministic-guinea-pig"}), it is optimal for users to always follow the option that is recommended for them. More formally, if $I_t$ is the recommendation that a user receives at time $t$, then we have that:</span>
<span id="cb2-147"><a href="#cb2-147"></a></span>
<span id="cb2-148"><a href="#cb2-148"></a>$$</span>
<span id="cb2-149"><a href="#cb2-149"></a>\begin{split}</span>
<span id="cb2-150"><a href="#cb2-150"></a>    \mathbb{E}<span class="co">[</span><span class="ot">\mu_1 - \mu_2 | I_t = 2</span><span class="co">]</span> Pr<span class="co">[</span><span class="ot">I_t = 2</span><span class="co">]</span> &amp;= \frac{1}{L} (\mu_1 - \mu_2) \quad \text{(Gains if you are the unlucky guinea pig)}<span class="sc">\\</span></span>
<span id="cb2-151"><a href="#cb2-151"></a>    &amp;+ (1 - \frac{1}{L}) \mathbb{E}<span class="co">[</span><span class="ot">\mu_1 - \mu_2 | \mu_1 &lt; \mu_2</span><span class="co">]</span> \times p<span class="co">[</span><span class="ot">\mu_1 &lt; \mu_2</span><span class="co">]</span> \quad \text{(Loss if you are not and $\mu_1 &lt; \mu_2$)}<span class="sc">\\</span></span>
<span id="cb2-152"><a href="#cb2-152"></a>    &amp;\leq 0</span>
<span id="cb2-153"><a href="#cb2-153"></a>\end{split}</span>
<span id="cb2-154"><a href="#cb2-154"></a>$$</span>
<span id="cb2-155"><a href="#cb2-155"></a></span>
<span id="cb2-156"><a href="#cb2-156"></a>This holds when $L \geq 12$. It means that the gains from not taking the recommended action are negative, which implies that users should always take the recommendation. So far we have considered the case where rewards are deterministic, but what about stochastic rewards? We are now going to consider the case where rewards are independent and identically distributed from some distribution, and where each action $A_i$ has some reward distribution $r_i^t \sim D_i, \mathbb{E}<span class="co">[</span><span class="ot">r_i^t</span><span class="co">]</span> = \mu_i$. Back to the case where there are only two actions, we are going to adapt the prior algorithm of guinea pig-picking to the stochastic reward setting. Since one reward observation is not enough to fully know $\mu_1$ anymore, we'll instead observe the outcome of the first action $M$ times to form a strong posterior $\mathbb{E}<span class="co">[</span><span class="ot">\mu_1 | r_1^1, \ldots r_1^M</span><span class="co">]</span>$. We can use with stochastic rewards when there are two actions. Similarly, as before, we pick one guinea pig uniformly at random from the next $L$ users and use the reward we get as the exploratory signal.\ In a very similar manner, we can generalize this algorithm from always having two actions to the general multi-armed bandit problem. Now suppose we have a general multi-armed bandit algorithm $A$. We will wrap this algorithm around our black box reduction algorithm to make it incentive-compatible. We wrap every decision that $A$ would make by exactly $L-1$ recommendations of the action believed to be the best so far. This guarantees that the expected rewards for the users that are not chosen as guinea pigs are at least as good as $A$'s reward at phase $n$.</span>
<span id="cb2-157"><a href="#cb2-157"></a></span>
<span id="cb2-158"><a href="#cb2-158"></a><span class="fu">## Mutual Information Paradigm {#mutual-information-paradigm}</span></span>
<span id="cb2-159"><a href="#cb2-159"></a>In this section we discuss an influential new framework for designing peer prediction mechanisms, the Mutual Information Paradigm (MIP) introduced by Kong and Schoenebeck <span class="co">[</span><span class="ot">@kongschoenebeck2019</span><span class="co">]</span>. Traditional peer prediction approaches typically rely on scoring rules and correlation between agents' signals. However, these methods often struggle with issues like uninformed equilibria, where agents can coordinate on uninformative strategies that yield higher payoffs than truth-telling. The core idea is to reward agents based on the mutual information between their report and the reports of other agents. We consider a setting with $n$ agents, each possessing a private signal $\Psi_i$ drawn from some set $\Sigma$. The mechanism asks each agent to report their signal, which we denote as $\hat{\Psi}_i$. For each agent $i$, the mechanism randomly selects a reference agent $j \neq i$. Agent $i$'s payment is then calculated as $MI(\hat{\Psi}_i; \hat{\Psi}_j)$ where $MI$ is an information-monotone mutual information measure. An information-monotone $MI$ measure must satisfy the following properties:</span>
<span id="cb2-160"><a href="#cb2-160"></a></span>
<span id="cb2-161"><a href="#cb2-161"></a><span class="ss">-   </span>Symmetry: $MI(X; Y) = MI(Y; X)$.</span>
<span id="cb2-162"><a href="#cb2-162"></a></span>
<span id="cb2-163"><a href="#cb2-163"></a><span class="ss">-   </span>Non-negativity: $MI(X; Y) \geq 0$, with equality if and only if $X$ and $Y$ are independent.</span>
<span id="cb2-164"><a href="#cb2-164"></a></span>
<span id="cb2-165"><a href="#cb2-165"></a><span class="ss">-   </span>Data processing inequality: For any transition probability $M$, if $Y$ is independent of $M(X)$ conditioned on $X$, then $MI(M(X); Y) \leq MI(X; Y)$.</span>
<span id="cb2-166"><a href="#cb2-166"></a></span>
<span id="cb2-167"><a href="#cb2-167"></a>Two important families of mutual information measures that satisfy these properties are $f$-mutual information and Bregman mutual information. The $f$-mutual information is defined as $MI_f(X; Y) = D_f(U_{X,Y}, V_{X,Y})$, where $D_f$ is an $f$-divergence, $U_{X,Y}$ is the joint distribution of $X$ and $Y$, and $V_{X,Y}$ is the product of their marginal distributions. The Bregman mutual information is defined as: $BMI_{PS}(X; Y) = \mathbb{E}_{X} [D{PS}(U_{Y|X}, U_Y)]$, where $D_{PS}$ is a Bregman divergence based on a proper scoring rule $PS$, $U_{Y|X}$ is the conditional distribution of $Y$ given $X$, and $U_Y$ is the marginal distribution of $Y$. The MIP framework can be applied in both single-question and multi-question settings. In the multi-question setting, the mechanism can estimate the mutual information empirically from multiple questions. In the single-question setting, additional techniques like asking for predictions about other agents' reports are used to estimate the mutual information. A key theoretical result of the MIP framework is that when the chosen mutual information measure is strictly information-monotone with respect to agents' priors, the resulting mechanism is both dominantly truthful and strongly truthful. This means that truth-telling is a dominant strategy for each agent and that the truth-telling equilibrium yields strictly higher payoffs than any other non-permutation strategy profile. As research continues to address practical implementation challenges of designing truthful mechanisms, MIP-based approaches have significant potential to improve preference elicitation and aggregation in real-world applications lacking verifiable ground truth.</span>
<span id="cb2-168"><a href="#cb2-168"></a></span>
<span id="cb2-169"><a href="#cb2-169"></a><span class="fu">## Exercises</span></span>
<span id="cb2-170"><a href="#cb2-170"></a><span class="fu">### Question 1: Pairwise Feedback Mechanisms for Digital Goods</span></span>
<span id="cb2-171"><a href="#cb2-171"></a></span>
<span id="cb2-172"><a href="#cb2-172"></a>Consider a marketplace for digital goods (such as personalized articles, artwork, or AI-generated data), where the exact utility derived from these goods is only revealed to buyers after the goods have been generated and delivered. To elicit truthful preferences from buyers who find it difficult to precisely quantify their valuations beforehand, the marketplace implements a pairwise feedback mechanism, inspired by the work of Robertson and Koyejo (2023).</span>
<span id="cb2-173"><a href="#cb2-173"></a></span>
<span id="cb2-174"><a href="#cb2-174"></a>Formally, each buyer requests a personalized digital good and, upon receiving the good, provides feedback by indicating whether their realized utility is higher or lower than a randomly chosen reference price $c \in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$. The mechanism utilizes this binary feedback to estimate valuations and allocate future goods accordingly.</span>
<span id="cb2-175"><a href="#cb2-175"></a></span>
<span id="cb2-176"><a href="#cb2-176"></a>Answer the following:</span>
<span id="cb2-177"><a href="#cb2-177"></a></span>
<span id="cb2-178"><a href="#cb2-178"></a>(a) **Formalize the Problem:** Let $u_i$ denote the true valuation of buyer $i$, and let $r_i(c)$ denote the buyer's reported feedback ($r_i(c) = 1$ if $u_i \geq c$, 0 otherwise). Prove that, under uniform random selection of the reference price $c$, the expected value $\mathbb{E}<span class="co">[</span><span class="ot">r_i(c)</span><span class="co">]</span>$ is equal to the true valuation $u_i$.</span>
<span id="cb2-179"><a href="#cb2-179"></a></span>
<span id="cb2-180"><a href="#cb2-180"></a>(b) **Incentive Compatibility Analysis:** Discuss conditions under which this feedback-based mechanism is incentive compatible, i.e., buyers have no incentive to misreport their preferences. Specifically, analyze why strategic misreporting (reporting $r_i(c)$ incorrectly for some reference prices) would not increase a buyer's expected payoff.</span>
<span id="cb2-181"><a href="#cb2-181"></a></span>
<span id="cb2-182"><a href="#cb2-182"></a>(c) **Regret Analysis:** Suppose the mechanism estimates buyers' utilities from past feedback and allocates future goods using an epsilon-greedy strategy (exploration rate $\eta_t$). Provide an informal discussion of the trade-off involved in choosing the exploration rate, and how it affects the social welfare and revenue of the marketplace over time.</span>
<span id="cb2-183"><a href="#cb2-183"></a></span>
<span id="cb2-184"><a href="#cb2-184"></a>(d) **Practical Implications:** Suggest one practical scenario outside the digital-goods marketplace where such a feedback-driven, pairwise comparison approach would be beneficial. Briefly justify your choice, mentioning challenges and benefits.</span>
<span id="cb2-185"><a href="#cb2-185"></a></span>
<span id="cb2-186"><a href="#cb2-186"></a><span class="fu">### Question 2: Scalable Oversight in Complex Decision-Making</span></span>
<span id="cb2-187"><a href="#cb2-187"></a></span>
<span id="cb2-188"><a href="#cb2-188"></a>In scenarios involving complex or high-dimensional outcomes (such as summarizing lengthy texts, assessing the quality of detailed AI-generated reports, or reviewing scientific papers), evaluating the quality of outputs can become infeasible for a single human overseer. One practical solution is scalable oversight, where the evaluation task is decomposed and distributed among multiple human evaluators or even assisted by AI agents. Consider a scalable oversight scenario inspired by recursive reward modeling, where complex evaluations are hierarchically decomposed into simpler tasks. Specifically, suppose you want to evaluate a lengthy report generated by an AI system. Answer the following:</span>
<span id="cb2-189"><a href="#cb2-189"></a></span>
<span id="cb2-190"><a href="#cb2-190"></a>**(a) Decomposition of the Task:** Propose a formal recursive decomposition strategy to evaluate a long AI-generated report of length <span class="sc">\(</span>N<span class="sc">\)</span> paragraphs. Specifically, describe a hierarchical evaluation method that decomposes the original evaluation into simpler subtasks at multiple hierarchical levels. Clearly describe how many subtasks you have at each level and how the final aggregated evaluation is computed.</span>
<span id="cb2-191"><a href="#cb2-191"></a></span>
<span id="cb2-192"><a href="#cb2-192"></a>**(b) Statistical Aggregation Method:** Suppose each evaluation subtask yields a binary score <span class="sc">\(</span>s_i \in <span class="sc">\{</span>0,1<span class="sc">\}\)</span>, where <span class="sc">\(</span>1<span class="sc">\)</span> indicates acceptable quality and <span class="sc">\(</span>0<span class="sc">\)</span> indicates unacceptable quality. Propose a simple statistical aggregation method (e.g., majority voting, threshold voting, weighted aggregation, etc.) to combine subtask evaluations into a single global quality assessment at the top level. Justify your choice mathematically.</span>
<span id="cb2-193"><a href="#cb2-193"></a></span>
<span id="cb2-194"><a href="#cb2-194"></a>**(c) Computational Simulation:** Implement a Python simulation of your hierarchical decomposition and aggregation method described in parts (a) and (b). Assume each subtask is evaluated with some fixed probability <span class="sc">\(</span>p<span class="sc">\)</span> of being correct (representing human evaluators with bounded accuracy).</span>
<span id="cb2-195"><a href="#cb2-195"></a></span>
<span id="cb2-196"><a href="#cb2-196"></a>Specifically, your simulation should:</span>
<span id="cb2-197"><a href="#cb2-197"></a><span class="ss">- </span>Implement a hierarchical evaluation scheme (e.g., binary-tree decomposition).</span>
<span id="cb2-198"><a href="#cb2-198"></a><span class="ss">- </span>Assume evaluators have accuracy <span class="sc">\(</span>p = 0.8<span class="sc">\)</span> (i.e., probability of correctly identifying paragraph quality).</span>
<span id="cb2-199"><a href="#cb2-199"></a><span class="ss">- </span>Simulate how evaluator accuracy at the leaf nodes affects the reliability of the global evaluation at the root node.</span>
<span id="cb2-200"><a href="#cb2-200"></a><span class="ss">- </span>Plot how the reliability of the top-level evaluation (accuracy at the root) varies as you increase the depth of hierarchy for a report of fixed length (e.g., <span class="sc">\(</span>N = 64<span class="sc">\)</span> paragraphs).</span>
<span id="cb2-201"><a href="#cb2-201"></a></span>
<span id="cb2-202"><a href="#cb2-202"></a>**(d) Practical Discussion:** Briefly discuss advantages and potential drawbacks of scalable oversight approaches such as recursive decomposition in the context of AI alignment. Include considerations such as evaluator fatigue, consistency, cost, and vulnerability to manipulation or collusion.</span>
<span id="cb2-203"><a href="#cb2-203"></a></span>
<span id="cb2-204"><a href="#cb2-204"></a></span>
<span id="cb2-205"><a href="#cb2-205"></a><span class="co">&lt;!--</span></span>
<span id="cb2-206"><a href="#cb2-206"></a><span class="co">## Social Choice Theory {#sec-choices-aggregation}</span></span>
<span id="cb2-207"><a href="#cb2-207"></a><span class="co">In many applications, human preferences must be aggregated across multiple individuals to determine a collective decision or ranking. This process is central to social choice theory, which provides a mathematical foundation for preference aggregation. Unlike individual preference modeling, which focuses on understanding how a single person makes decisions, social choice theory addresses the challenge of combining multiple preference profiles into a single, coherent outcome. One of the most widely used approaches to aggregating preferences is voting. A voting rule is a function that maps a set of individual preference rankings to a collective decision. The outcome of a vote is determined by a social choice function (SCF), which selects a winner based on the aggregated preferences. Several voting rules exist, each with different properties:</span></span>
<span id="cb2-208"><a href="#cb2-208"></a></span>
<span id="cb2-209"><a href="#cb2-209"></a><span class="co">- Plurality Rule: Each voter assigns one point to their top choice, and the alternative with the most points wins.</span></span>
<span id="cb2-210"><a href="#cb2-210"></a><span class="co">- Borda Count: Voters rank all alternatives, and points are assigned based on the position in each ranking.</span></span>
<span id="cb2-211"><a href="#cb2-211"></a><span class="co">- Single Transferable Vote (STV): Voters rank choices, and rounds of elimination occur until a candidate has a majority.</span></span>
<span id="cb2-212"><a href="#cb2-212"></a><span class="co">- Condorcet Methods: The Condorcet winner is the item that would win in all pairwise comparisons against other alternatives (if one exists).</span></span>
<span id="cb2-213"><a href="#cb2-213"></a></span>
<span id="cb2-214"><a href="#cb2-214"></a><span class="co">However, preference aggregation is not always straightforward. The Condorcet Paradox illustrates that no single alternative may be a clear winner due to cycles in majority preferences, violating transitivity. Additionally, different voting rules can yield different winners, highlighting the importance of selecting an appropriate aggregation method. A fundamental result in social choice theory is Arrow’s Impossibility Theorem, which states that when there are three or more alternatives, no voting system can simultaneously satisfy the following fairness criteria:</span></span>
<span id="cb2-215"><a href="#cb2-215"></a></span>
<span id="cb2-216"><a href="#cb2-216"></a><span class="co">1. Unanimity (Pareto efficiency): If all individuals prefer one item over another, the group ranking should reflect this.</span></span>
<span id="cb2-217"><a href="#cb2-217"></a><span class="co">2. Independence of Irrelevant Alternatives: The relative ranking of two items should not be influenced by another unrelated item.</span></span>
<span id="cb2-218"><a href="#cb2-218"></a><span class="co">3. Non-dictatorship: No single individual's preference should always determine the group's outcome.</span></span>
<span id="cb2-219"><a href="#cb2-219"></a></span>
<span id="cb2-220"><a href="#cb2-220"></a><span class="co">Arrow’s theorem suggests that every fair aggregation method must compromise on at least one of these desirable properties. Additionally, the Gibbard-Satterthwaite Theorem proves that any deterministic voting rule that selects a single winner is either dictatorial (one person always determines the result) or manipulable (voters can strategically misrepresent their preferences to achieve a better outcome). While manipulation is theoretically possible, certain voting rules, such as STV, introduce computational complexity that makes strategic voting impractical in real-world scenarios.</span></span>
<span id="cb2-221"><a href="#cb2-221"></a></span>
<span id="cb2-222"><a href="#cb2-222"></a><span class="co">Preference aggregation is also critical in RLHF, where human judgments guide model training. Aggregating human preferences in RLHF faces challenges similar to traditional voting, such as inconsistencies in preferences and strategic bias. Several approaches address these challenges. For example, Majority Voting simply aggregates by selecting the most preferred response. Weighted Voting adjusts vote weights based on expertise or trustworthiness. Jury Learning is a method that integrates dissenting opinions, ensuring that minority perspectives are not entirely disregarded. Lastly, Social Choice in AI Alignment incorporates diverse human feedback to align AI behavior with a broad spectrum of human values. These approaches highlight the interplay between human preference modeling and machine learning. Designing aggregation mechanisms that reflect collective human values is an ongoing research challenge. While traditional social choice methods focus on aggregation, recent work in pluralistic alignment suggests alternative frameworks that preserve the diversity of human preferences rather than collapsing them into a single decision. Pluralistic AI systems aim to:</span></span>
<span id="cb2-223"><a href="#cb2-223"></a></span>
<span id="cb2-224"><a href="#cb2-224"></a><span class="co">1. Present a spectrum of reasonable responses instead of forcing a single choice.</span></span>
<span id="cb2-225"><a href="#cb2-225"></a><span class="co">2. Allow steering towards specific perspectives while maintaining fairness.</span></span>
<span id="cb2-226"><a href="#cb2-226"></a><span class="co">3. Ensure distributional pluralism, calibrating AI systems to diverse human viewpoints.</span></span>
<span id="cb2-227"><a href="#cb2-227"></a></span>
<span id="cb2-228"><a href="#cb2-228"></a><span class="co">This perspective is particularly relevant in generative AI, where models trained on aggregated preferences may fail to capture the nuances of diverse human values. Aggregating human preferences is a complex task influenced by mathematical constraints and strategic considerations. Voting-based methods provide well-studied mechanisms for aggregation, but they face fundamental limitations, as Arrow’s and Gibbard-Satterthwaite’s theorems outlined. Beyond traditional aggregation, emerging approaches in RL and AI alignment seek to balance fairness, robustness, and pluralism. As machine learning systems increasingly interact with human preferences, designing aggregation frameworks that capture the richness of human decision-making remains an active and critical area of research.</span></span>
<span id="cb2-229"><a href="#cb2-229"></a></span>
<span id="cb2-230"><a href="#cb2-230"></a><span class="co">## Auction Theory {#single-item-auctions}</span></span>
<span id="cb2-231"><a href="#cb2-231"></a></span>
<span id="cb2-232"><a href="#cb2-232"></a><span class="co">The first problem within auction theory we will consider is the single-item auction. The premise of this problem is that there is a single item to sell, $n$ bidders (with unknown private valuations of the item $v_1$, \..., $v_n$). The bidder's individual objective is to maximize utility: the value $v_i$ of the item subtracted by the price paid for the item. The auction procedure is standard in the sense that bids are solicited and the highest bid will win the auction. While the objective of the individual bidder is clear, there could be a plethora of different objectives for the auction as a whole. One option could be to maximize social surplus, meaning the goal is to maximize the value of the winner. Another objective could be to maximize seller profit which is the payment of the winner. For simplicity, we can focus on the first objective where the goal is to maximize social surplus. If we want to maximize social surplus it turns out that a great way to do this is the "second-price auction".</span></span>
<span id="cb2-233"><a href="#cb2-233"></a></span>
<span id="cb2-234"><a href="#cb2-234"></a><span class="co">In the second-price auction, we will operate under slightly different conditions. In the second-price auction we (1) solicit sealed bids, (2) have the winner be the highest bidder, and (3) charger winner the second-highest bid price. As an example, if the solicited bids are $b = (2, 6, 4, 1)$ the winner will be that who bid $6$, but will pay a price of $4$. From here, we can do some equilibrium analysis to try and learn what the optimal bidding strategy is for each bidder. Let the amount bidder $i$ bids to be $b_i$, so we have bids $b_1, b_2, ..., b_n$. How much should bidder $i$ bid? To analyze this, let us define $t_i = max_{j \neq i} b_j$ which represents the max of the bids that is not from bidder $i$. There are now two cases to consider: if $b_i$ \&gt; $t_i$ and if $b_i$ \&lt; $t_i$. In the first case the bidder $i$ wins, and if the bidder bid $b_i = v_i$, they are guaranteed to have a positive return on bid. In the other case, they lose the bid and the net loss is 0 because they don't have to pay. From this we can conclude that bidder $i$'s dominant strategy is to just bid $b_i = v_i$. Rigorously proving this is a little bit trickier, but it was shown from Vickrey in 1961 \[cite\] that truthful bidding is the dominant strategy in second-price auctions. A corollary of this is that we are maximizing social surplus since bids are values and the winner is the bidder with highest valuation.</span></span>
<span id="cb2-235"><a href="#cb2-235"></a></span>
<span id="cb2-236"><a href="#cb2-236"></a><span class="co">If we want to look at things from the perspective of a seller trying to maximize their profit we need to treat the bidder's bids as uniform random variables. Consider the example scenario where we have two bidders each bidding uniformly between 0 and 1. What is the seller's expected profit? (in this case profit and revenue for the seller are the same because we assume the seller throws away the item if it doesn't sell/has no valuation for it). From there the question now becomes, can we get more expected profit from the seller's perspective? It turns out there is a design where we can add a reserve price of $r$ to the second-price auction. The way this works is we can (1) Insert seller-bid at $r$, (2) solicit bids, (3) pick the highest bidder, and 3) charge the 2nd-highest bid. In effect, this is just the second-price auction but with a bid from the seller as well, at a price of $r$. A lemma, that we won't prove here, is that the second-price auction with reserve price $r$ still has a dominant strategy of just being truthful. Let's now consider what the profit of a second-price auction would be with two bidders that uniformly bid between 0 and 1 -- but this time we have a reserve price of $1/2$. To calculate the expected profit we break down the situation into 3 cases:</span></span>
<span id="cb2-237"><a href="#cb2-237"></a></span>
<span id="cb2-238"><a href="#cb2-238"></a><span class="co">-   Case 1: $1/2 &gt; v_1 &gt; v_2 \rightarrow 1/4 \text{ probability} \rightarrow  \mathbb{E}[\text{profit}] = 0$</span></span>
<span id="cb2-239"><a href="#cb2-239"></a></span>
<span id="cb2-240"><a href="#cb2-240"></a><span class="co">-   Case 2: $v_1 &gt; v_2 &gt; 1/2 \rightarrow 1/4 \text{probability} \rightarrow \mathbb{E}[v_2 | \text{case 2}] = 2/3$</span></span>
<span id="cb2-241"><a href="#cb2-241"></a></span>
<span id="cb2-242"><a href="#cb2-242"></a><span class="co">-   Case 3: $v_1 &gt; 1/2 &gt; v_2 \rightarrow 1/2 \text{ probability} \rightarrow 1/2$</span></span>
<span id="cb2-243"><a href="#cb2-243"></a></span>
<span id="cb2-244"><a href="#cb2-244"></a><span class="co">Why is $E[v2 | case 2] = 2/3$? If $v_1$ and $v_2$ are greater than $1/2$, they are evenly spread across the interval, meaning the expectation will be 1/2 + 1/6 = 2/3. Adding up all these cases we get $E[profit] = 5/12$. It turns out that second-price auctions with reserve actually maximize profit in general (for symmetric bidders)! In the previous section we conclude that second-price auctions with reserve maximize profit for the seller. In order to prove this, we now move to the more general topic of asking how should a monopolist divide good across separate markets. We can make the assumption that the demand model is a concave revenue $R(q)$ in quantity $q$. Under this assumption, we can just divide supply into $q = q_a + q_b$ such that $R'_a(q_a) = R'_b(q_b)$. The idea from here is a theorem from Myerson in 1981 that states an optimal action maximizes "marginal revenue". Consider an example where we have two bidders bidding a uniform value between 0 and 1. Our revenue curve can now be derived from the offering price $V(q) = 1 - q$ like so: $R(q) = qV(q) = q - q^2$. Taking the derivative gives us the marginal revenue $R'(q) = 1-2q$. This means two things: 1) we want to sell to bidder $i$ with the highest $R'(q_i)$ and 2) we want to sell to bidder $i$ with value at least $1/2$ (if we want a positive $R'(q_i)$. But this is just a second-price auction with reserve $1/2$! This means that for symmetric bidders, a second price with reserve is the optimal auction.</span></span>
<span id="cb2-245"><a href="#cb2-245"></a></span>
<span id="cb2-246"><a href="#cb2-246"></a><span class="co">An interesting topic to discuss is what benefits auctions bring to the table as opposed to just standard pricing. Online auctions used to be a lot more popular in the early 2000s and have been completely replaced by standard online pricing, even on sites like e-bay. While auctions are slower and have added inherent complexities, they are actually optimal on paper. Standard pricing on the other is non-optimal; although it is fast and simpler for buyers. There is actually a way to quantify this: for pricing $k$ units, the loss is at most $1 / \sqrt{2\pi k}$ of optimal profit. Let's consider applications in duopoly platform design. We know that the optimal auction is second-price with reserve, but what happens when we introduce competition between two auction platforms? Some important details related to the revenue of a second-price auction is that a second-price auction with no reserve and n bidders leads to larger revenue having an optimal reserve and n - 1 bidders [@bulow-klemperer1996]. Additionally, with an entry cost, no reserve is the optimal strategy for maximizing revenue  [@mcafee-87]. Let's consider an example of a competing auction system which is Google ads vs Bing ads. How should an advertiser divide the budget between Google and Bing? They should give the same budget to both companies. What happens if Bing raises their prices? Then, the advertising company moves more of its budget to Google from Bing.</span></span>
<span id="cb2-247"><a href="#cb2-247"></a></span>
<span id="cb2-248"><a href="#cb2-248"></a><span class="co">The Bulow-Klemperer theorem demonstrates that increased competition can be more valuable than perfect knowledge of bidders' valuation distributions. This result provides insight into the potential of simple, prior-independent auctions to approach the performance of optimal auctions. The theorem states that for a single-item auction with bidders' valuations drawn independently from a regular distribution $F$. Let $F$ be a regular distribution and $n$ a positive integer. Then:</span></span>
<span id="cb2-249"><a href="#cb2-249"></a><span class="co">$$E_{v_1,\ldots,v_{n+1} \sim F}[\text{Rev(VA)}(n+1 \text{ bidders})] \geq E_{v_1,\ldots,v_n \sim F}[\text{Rev(OPT}_F)(n \text{ bidders})]$$  {#eq-eq3.64}</span></span>
<span id="cb2-250"><a href="#cb2-250"></a><span class="co">where VA denotes the Vickrey auction and $\text{OPT}_F$ denotes the</span></span>
<span id="cb2-251"><a href="#cb2-251"></a><span class="co">optimal auction for $F$. This shows that running a simple Vickrey auction with one extra bidder outperforms the revenue-optimal auction that requires precise knowledge of the distribution. It suggests that in practice, effort spent on recruiting additional bidders may be more fruitful than fine-tuning auction parameters.</span></span>
<span id="cb2-252"><a href="#cb2-252"></a></span>
<span id="cb2-253"><a href="#cb2-253"></a><span class="co">The VCG mechanism is a cornerstone of mechanism design, providing a general solution for welfare maximization in multi-parameter environments. The key result is that in every general mechanism design environment, there is a dominant-strategy incentive-compatible (DSIC) welfare-maximizing mechanism. According to VCG, given bids $b_1, \ldots, b_n$, where each $b_i$ is indexed by the outcome set $\Omega$, the allocation rule is $x(b) = \arg \max_{\omega \in \Omega} \sum_{i=1}^n b_i(\omega)$. The payment rule for each agent $i$ is $p_i(b) = \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega) - \sum_{j \neq i} b_j(\omega^*)$ where $\omega^* = x(b)$ is the chosen outcome. The key insight is to charge each agent its "externality" - the welfare loss inflicted on other agents by its presence. This payment rule, coupled with the welfare-maximizing allocation rule, yields a DSIC mechanism. The VCG mechanism can be interpreted as having each agent pay its bid minus a "rebate" equal to the increase in welfare attributable to its presence:</span></span>
<span id="cb2-254"><a href="#cb2-254"></a></span>
<span id="cb2-255"><a href="#cb2-255"></a><span class="co">$$p_i(b) = b_i(\omega^*) - \left[ \sum_{j=1}^n b_j(\omega^*) - \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega) \right]$$  {#eq-eq3.67}</span></span>
<span id="cb2-256"><a href="#cb2-256"></a></span>
<span id="cb2-257"><a href="#cb2-257"></a><span class="co">While the VCG mechanism provides a theoretical solution for DSIC welfare-maximization in general environments, it can be challenging to implement in practice due to computational and communication complexities.</span></span>
<span id="cb2-258"><a href="#cb2-258"></a></span>
<span id="cb2-259"><a href="#cb2-259"></a><span class="co">Combinatorial auctions are an important class of multi-parameter mechanism design problems, with applications ranging from spectrum auctions to airport slot allocation. In a combinatorial auction, there are $n$ bidders and a set $M$ of $m$ items. The outcome set $\Omega$ consists of allocations $(S_1, \ldots, S_n)$, where $S_i$ is the bundle allocated to bidder $i$. Each bidder $i$ has a private valuation $v_i(S)$ for each bundle $S \subseteq M$. While the VCG mechanism theoretically solves the welfare-maximization problem, combinatorial auctions face several major challenges in practice. First, each bidder has $2^m - 1$ private parameters, making direct revelation infeasible for even moderate numbers of items. This necessitates the use of indirect mechanisms that elicit information on a "need-to-know" basis. In addition, even when preference elicitation is not an issue, welfare-maximization can be an intractable problem. In practice, approximations are often used, hoping to achieve reasonably good welfare. The VCG mechanism can exhibit bad revenue and incentive properties in combinatorial settings. For example, adding bidders can sometimes decrease revenue to zero, and the mechanism can be vulnerable to collusion and false-name bids. Last but not least, strategic Behavior in Iterative Auctions: Most practical combinatorial auctions are iterative, comprising multiple rounds. This introduces new opportunities for strategic behavior, such as using bids to signal intentions to other bidders. These challenges make combinatorial auctions a rich and complex area of study, requiring careful design to balance theoretical guarantees with practical considerations.</span></span>
<span id="cb2-260"><a href="#cb2-260"></a></span>
<span id="cb2-261"><a href="#cb2-261"></a><span class="co">Spectrum auctions represent a complex application of combinatorial auction theory. With n bidders and m non-identical items, each bidder has a private valuation for every possible bundle of items, making it impractical to directly elicit all preferences. This necessitates the use of indirect, iterative mechanisms that query bidders for valuation information on a "need-to-know" basis, sacrificing some of the desirable properties of direct mechanisms like dominant strategy incentive compatibility (DSIC) and full welfare maximization. The fundamental challenge in spectrum auctions lies in the nature of the items being sold. There is a dichotomy between items that are substitutes (where $v(AB) \leq v(A) + v(B))$ and those that are complements (where $v(AB) &gt; v(A) + v(B))$. Substitute items, such as licenses for the same area with equal-sized frequency ranges, are generally easier to handle. When items are substitutes, welfare maximization is computationally tractable, and the VCG mechanism avoids many undesirable properties. However, complementary items, which arise naturally in spectrum auctions when bidders want adjacent licenses, present significant challenges. Early attempts at spectrum auctions revealed the pitfalls of naive approaches. Sequential auctions, where items are sold one after another, proved problematic as demonstrated by a Swiss auction in 2000. Bidders struggled to bid intelligently without knowing future prices, leading to unpredictable outcomes and potential revenue loss. Similarly, simultaneous sealed-bid auctions, as used in New Zealand in 1990, created difficulties for bidders in coordinating their bids across multiple items, resulting in severely suboptimal outcomes.</span></span>
<span id="cb2-262"><a href="#cb2-262"></a></span>
<span id="cb2-263"><a href="#cb2-263"></a><span class="co">The Simultaneous Ascending Auction (SAA) emerged as a solution to these issues and has formed the basis of most spectrum auctions over the past two decades. In an SAA, multiple items are auctioned simultaneously in rounds, with bidders placing bids on any subset of items subject to an activity rule. This format facilitates price discovery, allowing bidders to adjust their strategies as they learn about others' valuations. It also allows bidders to determine valuations on a need-to-know basis, reducing the cognitive burden compared to direct-revelation auctions. Despite its advantages, the SAA is not without vulnerabilities. Demand reduction, where bidders strategically reduce their demand to lower prices, can lead to inefficient outcomes even when items are substitutes. The exposure problem arises with complementary items, where bidders risk winning only a subset of desired items at unfavorable prices. These issues highlight the ongoing challenges in designing effective spectrum auctions, balancing theoretical guarantees with practical considerations.</span></span>
<span id="cb2-264"><a href="#cb2-264"></a></span>
<span id="cb2-265"><a href="#cb2-265"></a><span class="co">Case study: Classroom Peer Grading. This section discusses work by Jason Hartline, Yingkai Li, Liren Shan, and Yifan Wu at Northwestern University, where researchers examined mechanism design for the classroom, specifically in terms of the optimization of scoring rules. They explored peer grading in the classroom and how to construct a peer grading system that optimizes the objectives for each stakeholder in the system, including those being graded, the peer graders, the TAs of the class, and the professor. Firstly, let's think of the classroom like a computer. We can think of students as local optimizers; their incentive is to minimize the amount of work they need to do and maximize the grades that they receive. The graders are imprecise operators, which means that there is some uncertainty in their ability to grade the work completed by the students. The syllabus can be thought of as the rules that map the actions of the students to the grade they end up receiving in the class. Our overall goals for this classroom based on these definitions is to minimize work, maximize learning, and fairly assess the students for the work that they do [@jasonH2020]. One basic question that we can examine, is what is the best syllabus that maximizes our objectives for our classroom design. Some components of this could include grading randomized exams, grading with partial credit, group projects, and finally, peer grading, which is the component that we will be taking a deeper dive into. The general situation of the peer grading problem is that proper scoring rules make peer grades horrible [@jasonH2020]. So we want to be able to optimize scoring rules and make sure that we are optimizing each component of the peer grading pipeline.</span></span>
<span id="cb2-266"><a href="#cb2-266"></a></span>
<span id="cb2-267"><a href="#cb2-267"></a><span class="co">The main algorithms focused on in this peer grading design paper were matching peers and TAs to submissions and the grading of those submissions from the TAs and the peer reviews  [@jasonH2020]. There are quite a number of advantages to peer grading including that peers are able to learn from reviewing other people's work, it reduces the work for the teacher, and improves the turnaround time for assignment feedback (which are all part of our overarching goals for our mechanism design for the classroom). But, it is also important to acknowledge the potential disadvantages of the peer grading system: it is possible that the peer graders present inaccurate grades and there is student unrest. This presents us with a challenge: being able to incentivize accurate peer reviews.</span></span>
<span id="cb2-268"><a href="#cb2-268"></a></span>
<span id="cb2-269"><a href="#cb2-269"></a><span class="co">One problem that we run into, when we use the proper scoring rule to score peer reviews, if the peer graders use the lazy peer strategy, which means that they always report 80$\%$ for their peer reviews, they get graded very well using the proper scoring rule algorithm. In fact, the proper scoring rule says that their peer review is 96$\%$ accurate [@jasonH2023]. So how do we incentivize effort in reviews from peer graders? We use a scoring rule that maximizes the difference in score between effort or no effort reviews as indicated by the peer reviewers [@jasonH2023]. So overall, the analysis of datasets leads to decision optimizations and, eventually, payoff from those decisions.</span></span>
<span id="cb2-270"><a href="#cb2-270"></a></span>
<span id="cb2-271"><a href="#cb2-271"></a><span class="co">In conclusion, scoring rules are essential in being able to understand and analyze data thoroughly, and optimal scoring rules for binary effort allow us to understand the setting independent of the dataset [@jasonH2023].</span></span>
<span id="cb2-272"><a href="#cb2-272"></a><span class="co">--&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
    <footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/chap4.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer><script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




</body></html>