<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Elicitation – Machine Learning from Human Preferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../src/chap5.html" rel="next">
<link href="../src/chap3.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-07ba0ad10f5680c660e360ac31d2f3b6.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8b864f0777c60eecff11d75b6b2e1175.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-fa3d1c749edcb96cd5cb7d620f3e5237.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-6f24586c8b15e78d85e3983c622e3e8a.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/monaco-editor@0.46.0/min/vs/editor/editor.main.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous" referrerpolicy="no-referrer">
  
<style type="text/css">
.monaco-editor pre {
  background-color: unset !important;
}

.qpyodide-editor-toolbar {
  width: 100%;
  display: flex;
  justify-content: space-between;
  box-sizing: border-box;
}

.qpyodide-editor-toolbar-left-buttons, .qpyodide-editor-toolbar-right-buttons {
  display: flex;
}

.qpyodide-non-interactive-loading-container.qpyodide-cell-needs-evaluation, .qpyodide-non-interactive-loading-container.qpyodide-cell-evaluated {
  justify-content: center;
  display: flex;
  background-color: rgba(250, 250, 250, 0.65);
  border: 1px solid rgba(233, 236, 239, 0.65);
  border-radius: 0.5rem;
  margin-top: 15px;
  margin-bottom: 15px;
}

.qpyodide-r-project-logo {
  color: #2767B0; /* R Project's blue color */
}

.qpyodide-icon-status-spinner {
  color: #7894c4;
}

.qpyodide-icon-run-code {
  color: #0d9c29
}

.qpyodide-output-code-stdout {
  color: #111;
}

.qpyodide-output-code-stderr {
  color: #db4133;
}

.qpyodide-editor {
  border: 1px solid #EEEEEE;
}

.qpyodide-editor-toolbar {
  background-color: #EEEEEE;
  padding: 0.2rem 0.5rem;
}

.qpyodide-button {
  background-color: #EEEEEE;
  display: inline-block;
  font-weight: 400;
  line-height: 1;
  text-decoration: none;
  text-align: center;
  color: #000;
  border-color: #dee2e6;
  border: 1px solid rgba(0,0,0,0);
  padding: 0.375rem 0.75rem;
  font-size: .9rem;
  border-radius: 0.25rem;
  transition: color .15s ease-in-out,background-color .15s ease-in-out,border-color .15s ease-in-out,box-shadow .15s ease-in-out;
}

.qpyodide-button:hover {
  color: #000;
  background-color: #d9dce0;
  border-color: #c8ccd0;
}

.qpyodide-button:disabled,.qpyodide-button.disabled,fieldset:disabled .qpyodide-button {
  pointer-events: none;
  opacity: .65
}

.qpyodide-button-reset {
  color: #696969; /*#4682b4;*/
}

.qpyodide-button-copy {
  color: #696969;
}


/* Custom styling for RevealJS Presentations*/

/* Reset the style of the interactive area */
.reveal div.qpyodide-interactive-area {
  display: block;
  box-shadow: none;
  max-width: 100%;
  max-height: 100%;
  margin: 0;
  padding: 0;
} 

/* Provide space to entries */
.reveal div.qpyodide-output-code-area pre div {
  margin: 1px 2px 1px 10px;
}

/* Collapse the inside code tags to avoid extra space between line outputs */
.reveal pre div code.qpyodide-output-code-stdout, .reveal pre div code.qpyodide-output-code-stderr {
  padding: 0;
  display: contents;
}

.reveal pre div code.qpyodide-output-code-stdout {
  color: #111;
}

.reveal pre div code.qpyodide-output-code-stderr {
  color: #db4133;
}


/* Create a border around console and output (does not effect graphs) */
.reveal div.qpyodide-console-area {
  border: 1px solid #EEEEEE;
  box-shadow: 2px 2px 10px #EEEEEE;
}

/* Cap output height and allow text to scroll */
/* TODO: Is there a better way to fit contents/max it parallel to the monaco editor size? */
.reveal div.qpyodide-output-code-area pre {
  max-height: 400px;
  overflow: scroll;
}
</style>
<script type="module">
// Document level settings ----

// Determine if we need to install python packages
globalThis.qpyodideInstallPythonPackagesList = [''];

// Check to see if we have an empty array, if we do set to skip the installation.
globalThis.qpyodideSetupPythonPackages = !(qpyodideInstallPythonPackagesList.indexOf("") !== -1);

// Display a startup message?
globalThis.qpyodideShowStartupMessage = true;

// Describe the webR settings that should be used
globalThis.qpyodideCustomizedPyodideOptions = {
  "indexURL": "https://cdn.jsdelivr.net/pyodide/v0.27.2/full/",
  "env": {
    "HOME": "/home/pyodide",
  }, 
  stdout: (text) => {qpyodideAddToOutputArray(text, "out");},
  stderr: (text) => {qpyodideAddToOutputArray(text, "error");}
}

// Store cell data
globalThis.qpyodideCellDetails = [{"code":"import numpy as np\n\n# Predictive distributions\nprobs = np.array([\n    [0.6, 0.4],  # p(y1|x1), p(y2|x1)\n    [0.3, 0.7],  # p(y1|x2), p(y2|x2)\n    [0.8, 0.2],  # p(y1|x3), p(y2|x3)\n])\n\n# Entropy Sampling\nentropy = -np.sum(probs * np.log(probs), axis=1)\nfor i, e in enumerate(entropy, start=1):\n    print(f\"Entropy(x_{i}) = {e:.2f}\")\n# Find the index with the highest entropy\nselected_index = np.argmax(entropy)\nprint(f\"\\nSelect x_{selected_index + 1} for labeling (highest entropy = {entropy[selected_index]:.2f})\")\n\n# Sort each row in descending order to get the top two class probabilities\nsorted_probs = np.sort(probs, axis=1)[:, ::-1]\nmargin = sorted_probs[:, 0] - sorted_probs[:, 1]\nfor i, m in enumerate(margin, start=1):\n    print(f\"Margin(x_{i}) = {m:.1f}\")\n\n# Select the index with the smallest margin (most uncertain)\nselected_index = np.argmin(margin)\nprint(f\"\\nSelect x_{selected_index + 1} for labeling (smallest margin = {margin[selected_index]:.1f})\")\n\n# Least confidence sampling\n# Get the highest predicted probability for each sample\nmax_probs = np.max(probs, axis=1)\nleast_confidence = 1 - max_probs\nfor i, lc in enumerate(least_confidence, start=1):\n    print(f\"alpha(x_{i}) = {lc:.1f}\")\n\n# Select the index with the highest least confidence score (i.e., most uncertain)\nselected_index = np.argmax(least_confidence)\nprint(f\"\\nSelect x_{selected_index + 1} for labeling (highest least confidence = {least_confidence[selected_index]:.1f})\")","id":1,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"import numpy as np\n\n# Predictive distributions from 3 committee members for 3 samples\n# Shape: (num_samples, num_committee_members, num_classes)\nprobs = np.array([\n    [[0.6, 0.4], [0.7, 0.3], [0.3, 0.7]],  # x1\n    [[0.3, 0.7], [0.4, 0.6], [0.4, 0.6]],  # x2\n    [[0.8, 0.2], [0.9, 0.1], [0.7, 0.3]],  # x3\n])\n\n# Vote entropy\npredicted_labels = np.argmax(probs, axis=2)  # Shape: (num_samples, num_committee_members)\nnum_classes = probs.shape[2]\nvote_counts = np.array([\n    [np.sum(predicted_labels[i] == c) for c in range(num_classes)]\n    for i in range(predicted_labels.shape[0])\n])\nvote_distributions = vote_counts / vote_counts.sum(axis=1, keepdims=True)\nvote_entropy = -np.sum(vote_distributions * np.log(vote_distributions + 1e-12), axis=1)  # add epsilon to avoid log(0)\nfor i, ve in enumerate(vote_entropy, start=1):\n    print(f\"alpha(x_{i}) = {ve:.2f}\")\nselected_index = np.argmax(vote_entropy)\nprint(f\"\\nSelect x_{selected_index + 1} for labeling (highest vote entropy = {vote_entropy[selected_index]:.2f})\")\n\n# Consensus Entropy\nconsensus_probs = np.mean(probs, axis=1)  # Shape: (num_samples, num_classes)\nconsensus_entropy = -np.sum(consensus_probs * np.log(consensus_probs + 1e-12), axis=1)  # add epsilon to avoid log(0)\nfor i, ce in enumerate(consensus_entropy, start=1):\n    print(f\"alpha(x_{i}) = {ce:.2f}\")\nselected_index = np.argmax(consensus_entropy)\nprint(f\"\\nSelect x_{selected_index + 1} for labeling (highest consensus entropy = {consensus_entropy[selected_index]:.2f})\")","id":2,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}},{"code":"import numpy as np\n\n# First and second time inferences for each sample\n# Shape: (num_samples, num_draws, num_classes)\n# theta_1 and theta_2 samples\nprobs = np.array([\n    [[0.6, 0.4], [0.8, 0.2]],  # x1\n    [[0.4, 0.6], [0.5, 0.5]],  # x2\n])\n\n# Step 1: Compute the average predictive distribution (consensus probs)\nmean_probs = np.mean(probs, axis=1)  # shape: (num_samples, num_classes)\n\n# Step 1 continued: Compute entropy of the consensus distribution\nconsensus_entropy = -np.sum(mean_probs * np.log(mean_probs + 1e-12), axis=1)\n\n# Step 2: Compute entropy for each model draw\nindividual_entropies = -np.sum(probs * np.log(probs + 1e-12), axis=2)\n\n# Step 2 continued: Average the entropies across model draws\nexpected_entropy = np.mean(individual_entropies, axis=1)\n\n# Step 3: Compute BALD = entropy of mean - mean of entropies\nbald_scores = consensus_entropy - expected_entropy\n\n# Print results\nfor i, (h, eh, b) in enumerate(zip(consensus_entropy, expected_entropy, bald_scores), start=1):\n    print(f\"x_{i}:\")\n    print(f\"  Predictive Entropy = {h:.3f}\")\n    print(f\"  Expected Entropy   = {eh:.3f}\")\n    print(f\"  BALD Score         = {b:.3f}\")\n\n# Select sample with highest BALD score\nselected_index = np.argmax(bald_scores)\nprint(f\"\\nSelect x_{selected_index + 1} for labeling (highest BALD = {bald_scores[selected_index]:.3f})\")","id":3,"options":{"autorun":"","classes":"","comment":"","context":"interactive","dpi":72,"fig-cap":"","fig-height":5,"fig-width":7,"label":"","message":"true","out-height":"","out-width":"700px","output":"true","read-only":"false","results":"markup","warning":"true"}}];


</script>
<script type="module">
// Declare startupMessageqpyodide globally
globalThis.qpyodideStartupMessage = document.createElement("p");

// Function to set the button text
globalThis.qpyodideSetInteractiveButtonState = function(buttonText, enableCodeButton = true) {
  document.querySelectorAll(".qpyodide-button-run").forEach((btn) => {
    btn.innerHTML = buttonText;
    btn.disabled = !enableCodeButton;
  });
}

// Function to update the status message in non-interactive cells
globalThis.qpyodideUpdateStatusMessage = function(message) {
  document.querySelectorAll(".qpyodide-status-text.qpyodide-cell-needs-evaluation").forEach((elem) => {
    elem.innerText = message;
  });
}

// Function to update the status message
globalThis.qpyodideUpdateStatusHeader = function(message) {

  if (!qpyodideShowStartupMessage) return;

  qpyodideStartupMessage.innerHTML = message;
}

// Status header update with customized spinner message
globalThis.qpyodideUpdateStatusHeaderSpinner = function(message) {

  qpyodideUpdateStatusHeader(`
    <i class="fa-solid fa-spinner fa-spin qpyodide-icon-status-spinner"></i>
    <span>${message}</span>
  `);
}


// Function that attaches the document status message
function qpyodideDisplayStartupMessage(showStartupMessage) {
  if (!showStartupMessage) {
    return;
  }

  // Get references to header elements
  const headerHTML = document.getElementById("title-block-header");
  const headerRevealJS = document.getElementById("title-slide");

  // Create the outermost div element for metadata
  const quartoTitleMeta = document.createElement("div");
  quartoTitleMeta.classList.add("quarto-title-meta");

  // Create the first inner div element
  const firstInnerDiv = document.createElement("div");
  firstInnerDiv.setAttribute("id", "qpyodide-status-message-area");

  // Create the second inner div element for "Pyodide Status" heading and contents
  const secondInnerDiv = document.createElement("div");
  secondInnerDiv.setAttribute("id", "qpyodide-status-message-title");
  secondInnerDiv.classList.add("quarto-title-meta-heading");
  secondInnerDiv.innerText = "Pyodide Status";

  // Create another inner div for contents
  const secondInnerDivContents = document.createElement("div");
  secondInnerDivContents.setAttribute("id", "qpyodide-status-message-body");
  secondInnerDivContents.classList.add("quarto-title-meta-contents");

  // Describe the Pyodide state
  qpyodideStartupMessage.innerText = "🟡 Loading...";
  qpyodideStartupMessage.setAttribute("id", "qpyodide-status-message-text");
  // Add `aria-live` to auto-announce the startup status to screen readers
  qpyodideStartupMessage.setAttribute("aria-live", "assertive");

  // Append the startup message to the contents
  secondInnerDivContents.appendChild(qpyodideStartupMessage);

  // Combine the inner divs and contents
  firstInnerDiv.appendChild(secondInnerDiv);
  firstInnerDiv.appendChild(secondInnerDivContents);
  quartoTitleMeta.appendChild(firstInnerDiv);

  // Determine where to insert the quartoTitleMeta element
  if (headerHTML || headerRevealJS) {
    // Append to the existing "title-block-header" element or "title-slide" div
    (headerHTML || headerRevealJS).appendChild(quartoTitleMeta);
  } else {
    // If neither headerHTML nor headerRevealJS is found, insert after "Pyodide-monaco-editor-init" script
    const monacoScript = document.getElementById("qpyodide-monaco-editor-init");
    const header = document.createElement("header");
    header.setAttribute("id", "title-block-header");
    header.appendChild(quartoTitleMeta);
    monacoScript.after(header);
  }
}

qpyodideDisplayStartupMessage(qpyodideShowStartupMessage);
</script>
<script type="module">
// Create a logging setup
globalThis.qpyodideMessageArray = []

// Add messages to array
globalThis.qpyodideAddToOutputArray = function(message, type) {
  qpyodideMessageArray.push({ message, type });
}

// Function to reset the output array
globalThis.qpyodideResetOutputArray = function() {
  qpyodideMessageArray = [];
}

globalThis.qpyodideRetrieveOutput = function() {
  return qpyodideMessageArray.map(entry => entry.message).join('\n');
}

// Start a timer
const initializePyodideTimerStart = performance.now();

// Encase with a dynamic import statement
globalThis.qpyodideInstance = await import(
  qpyodideCustomizedPyodideOptions.indexURL + "pyodide.mjs").then(
   async({ loadPyodide }) => {

    console.log("Start loading Pyodide");
    
    // Populate Pyodide options with defaults or new values based on `pyodide`` meta
    let mainPyodide = await loadPyodide(
      qpyodideCustomizedPyodideOptions
    );
    
    // Setup a namespace for global scoping
    // await loadedPyodide.runPythonAsync("globalScope = {}"); 
    
    // Update status to reflect the next stage of the procedure
    qpyodideUpdateStatusHeaderSpinner("Initializing Python Packages");

    // Load the `micropip` package to allow installation of packages.
    await mainPyodide.loadPackage("micropip");
    await mainPyodide.runPythonAsync(`import micropip`);

    // Load the `pyodide_http` package to shim uses of `requests` and `urllib3`.
    // This allows for `pd.read_csv(url)` to work flawlessly.
    // Details: https://github.com/coatless-quarto/pyodide/issues/9
    await mainPyodide.loadPackage("pyodide_http");
    await mainPyodide.runPythonAsync(`
    import pyodide_http
    pyodide_http.patch_all()  # Patch all libraries
    `);

    // Load the `matplotlib` package with necessary environment hook
    await mainPyodide.loadPackage("matplotlib");

    // Set the backend for matplotlib to be interactive.
    await mainPyodide.runPythonAsync(`
    import matplotlib
    matplotlib.use("module://matplotlib_pyodide.html5_canvas_backend")
    from matplotlib import pyplot as plt
    `);

    // Unlock interactive buttons
    qpyodideSetInteractiveButtonState(
      `<i class="fa-solid fa-play qpyodide-icon-run-code"></i> <span>Run Code</span>`, 
      true
    );

    // Set document status to viable
    qpyodideUpdateStatusHeader(
      "🟢 Ready!"
    );

    // Assign Pyodide into the global environment
    globalThis.mainPyodide = mainPyodide;

    console.log("Completed loading Pyodide");
    return mainPyodide;
  }
);

// Stop timer
const initializePyodideTimerEnd = performance.now();

// Create a function to retrieve the promise object.
globalThis._qpyodideGetInstance = function() {
    return qpyodideInstance;
}

</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../src/chap4.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Elicitation</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning from Human Preferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/stair-lab/mlhp" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Machine-Learning-from-Human-Preferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap4.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Elicitation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Decisions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Aggregation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Alternatives</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/chap8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/ack.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>license.qmd</p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-active-learning-problem" id="toc-the-active-learning-problem" class="nav-link active" data-scroll-target="#the-active-learning-problem"><span class="header-section-number">3.1</span> The Active Learning Problem</a></li>
  <li><a href="#estimating-the-value-of-additional-data-with-acquisition-function" id="toc-estimating-the-value-of-additional-data-with-acquisition-function" class="nav-link" data-scroll-target="#estimating-the-value-of-additional-data-with-acquisition-function"><span class="header-section-number">3.2</span> Estimating the Value of Additional Data with Acquisition Function</a></li>
  <li><a href="#active-preference-learning-with-ideal-point-model" id="toc-active-preference-learning-with-ideal-point-model" class="nav-link" data-scroll-target="#active-preference-learning-with-ideal-point-model"><span class="header-section-number">3.3</span> Active Preference Learning with Ideal Point Model</a></li>
  <li><a href="#sec-metric-elicitation" id="toc-sec-metric-elicitation" class="nav-link" data-scroll-target="#sec-metric-elicitation"><span class="header-section-number">3.4</span> Case Study 2: Performance Metric Elicitation</a>
  <ul class="collapse">
  <li><a href="#sec-orgb6dac4e" id="toc-sec-orgb6dac4e" class="nav-link" data-scroll-target="#sec-orgb6dac4e"><span class="header-section-number">3.4.1</span> Linear Performance Metric Elicitation</a></li>
  <li><a href="#sec-lfpm-elicitation" id="toc-sec-lfpm-elicitation" class="nav-link" data-scroll-target="#sec-lfpm-elicitation"><span class="header-section-number">3.4.2</span> Linear-Fractional Performance Metric Elicitation</a></li>
  <li><a href="#multiclass-performance-metric-elicitation" id="toc-multiclass-performance-metric-elicitation" class="nav-link" data-scroll-target="#multiclass-performance-metric-elicitation"><span class="header-section-number">3.4.3</span> Multiclass Performance Metric Elicitation</a></li>
  </ul></li>
  <li><a href="#case-study-3-active-preference-learning-in-robotics" id="toc-case-study-3-active-preference-learning-in-robotics" class="nav-link" data-scroll-target="#case-study-3-active-preference-learning-in-robotics"><span class="header-section-number">3.5</span> Case Study 3: Active Preference Learning in Robotics</a>
  <ul class="collapse">
  <li><a href="#application-guiding-human-demonstrations-in-robotics" id="toc-application-guiding-human-demonstrations-in-robotics" class="nav-link" data-scroll-target="#application-guiding-human-demonstrations-in-robotics"><span class="header-section-number">3.5.1</span> Application: Guiding Human Demonstrations in Robotics</a></li>
  </ul></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/stair-lab/mlhp/blob/main/src/chap4.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/stair-lab/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
<script src="https://cdn.jsdelivr.net/npm/monaco-editor@0.46.0/min/vs/loader.js"></script>
<script type="module" id="qpyodide-monaco-editor-init">

  // Configure the Monaco Editor's loader
  require.config({
    paths: {
      'vs': 'https://cdn.jsdelivr.net/npm/monaco-editor@0.46.0/min/vs'
    }
  });
</script>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Elicitation</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<iframe src="https://web.stanford.edu/class/cs329h/slides/3.1.active_learning/#/" style="width:45%; height:225px;">
</iframe>
<iframe src="https://web.stanford.edu/class/cs329h/slides/3.2.metric_elicitation/#/" style="width:45%; height:225px;">
</iframe>
<p><a href="https://web.stanford.edu/class/cs329h/slides/3.1.active_learning/#/" class="btn btn-outline-primary" role="button">Fullscreen - AL</a> <a href="https://web.stanford.edu/class/cs329h/slides/3.2.metric_elicitation/#/" class="btn btn-outline-primary" role="button">Fullscreen - ME</a></p>
<section id="the-active-learning-problem" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="the-active-learning-problem"><span class="header-section-number">3.1</span> The Active Learning Problem</h2>
<p>Acquiring labeled data is expensive. Active learning (AL) is a learning paradigm that aims to reduce the amount of labeled data required to train a model to achieve high accuracy. AL algorithms iteratively select an input datapoint for an oracle (e.g., a human annotator) to label such that when the label is observed, the model improves the most. Two primary setups in AL is pool-based and stream-based. In pool-based AL, the model selects samples from a large unlabeled pool of data. For example, a model for text classification selects the most uncertain texts from a large pool to ask a human annotator to label. In stream-based AL, the model receives samples sequentially (one sample at a time) and decides whether to label them. The data is gone if the decision maker decides not to label it. In AL, a model is trained on the current dataset, and a set of candidate points is evaluated for potential inclusion. AL selects one of these points to add to the dataset based on an “acquisition function” defined with respect to the current model to estimate the value of each candidate point for improving model performance. The dataset is updated with the newly queried point, and the cycle repeats until the budget is exhausted or a predefined reliability criterion is met.</p>
<p>AL has successfully enhance various real-world systems. For example, AL can improve the computer vision models used in autonomous vehicles <span class="citation" data-cites="AL_app_autonomous">(<a href="#ref-AL_app_autonomous" role="doc-biblioref">Jarl et al. 2021</a>)</span>. Probing a model to understand what type of data it would benefit from is more practical. In robotics, autonomous agents may query humans when unsure how to act when facing new situations <span class="citation" data-cites="AL_app_robotics">(<a href="#ref-AL_app_robotics" role="doc-biblioref">Taylor, Berrueta, and Murphey 2021</a>)</span>. Here, collecting data often incurs significant financial and time costs because physical robot arm worns out over time. In meteorology, AL can help decide where to place additional sensors for weather predictions <span class="citation" data-cites="AL_app_sensors">(<a href="#ref-AL_app_sensors" role="doc-biblioref">Singh, Nowak, and Ramanathan 2006</a>)</span>. Sensor placement involves deploying teams to remote locations and expensive construction for an extra data point. Choosing these locations and allocating resources wisely is of interest to governments and businesses. AL could also be employed to select data for fine-tuning large language models (LLMs) for specific downstream tasks <span class="citation" data-cites="AL_app_LLMs">(<a href="#ref-AL_app_LLMs" role="doc-biblioref">Margatina et al. 2023</a>)</span>. Here, it might be difficult to fully describe a targeted NLP task. Often, instead of defining a task via a dataset of examples, it may be easier for a human to interact with the LLM for a specific use case, identify gaps in the model, and address those using AL.</p>
<p>Typically, in robotic, robots learn by observing human demonstrations. However, expert demonstrations are often limited, and training a supervised learning model would require vast amounts of demonstration data, which is difficult to obtain at scale. Demonstrations tend to be variable, reflecting the actions of individual humans, making the data collection process inconsistent. To address these limitations, alternative approaches have been proposed, such as using pairwise comparisons, where humans evaluate two action trajectories to determine the superior one, or employing physical corrections, in which reward functions are learned through human-robot interactions, with humans guiding the robot’s actions during the task. AL algorithms can be employed in preference learning tasks, where the objective is to develop a model that aligns with human preferences while minimizing the need for extensive labeled data or reducing the high cost of annotations.</p>
<p>Motivating by the pairwise preference setting, we consider a binary classification problem. The model is trained on a small labeled dataset <span class="math inline">\(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N\)</span>, where <span class="math inline">\(x_i\)</span> represents the input data and <span class="math inline">\(y_i\)</span> is the corresponding label. The model is uncertain about the class labels of some data points and can query an oracle to obtain the true labels of these data points. The goal is to minimize the number of queries to the oracle while maximizing the model’s performance. Here, the value of a datapoint is in how much it helps identify the underlying model, and this notion of informativeness is often quantify with uncertainty. Two primary types of uncertainty are often considered: epistemic and aleatoric uncertainty. Epistemic uncertainty, or model uncertainty, arises from a lack of knowledge and can be reduced by acquiring more data. This type of uncertainty is especially significant when the model lacks confidence due to insufficient or incomplete information in its training set. On the other hand, aleatoric uncertainty, or data uncertainty, stems from the inherent randomness within the data itself. Unlike epistemic uncertainty, aleatoric uncertainty cannot be reduced, even with additional data, as it reflects noise or unpredictability in the real data-generating process. AL often focuses on selecting data that reduce the epistemic uncertainty.</p>
<p>There are several method for quantify model uncertainty. Bayesian methods, such as Bayesian neural networks and Gaussian processes, offer a principled way of estimating uncertainty of parameter posterior distribution by iteratively updating a prior distribution over model. Exact posterior computation can become computationally prohibitive, especially for complex likelihood function, and approximated Bayesian computation is proposed to address this. For example, ensemble methods involve training multiple models and combining their predictions to provide an estimate of uncertainty. Ensemble methods are relatively easy to implement, but they are noisy and still somewhat expensive. Conformal prediction methods also provide a framework for estimating uncertainty by offering a measure of confidence in predictions based on the conformity of a given instance with the training data.</p>
</section>
<section id="estimating-the-value-of-additional-data-with-acquisition-function" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="estimating-the-value-of-additional-data-with-acquisition-function"><span class="header-section-number">3.2</span> Estimating the Value of Additional Data with Acquisition Function</h2>
<p>Uncertainty quantification plays a vital role in acquisition functions, which are central to AL strategies. These functions determine which samples are most valuable to label by evaluating their utility based on the model’s current uncertainty estimates. Common acquisition functions include uncertainty sampling <span class="citation" data-cites="AL_uncertainty">(<a href="#ref-AL_uncertainty" role="doc-biblioref">Zhu et al. 2010</a>)</span>, which selects samples the model is least confident about, query-by-committee <span class="citation" data-cites="AL_committee">(<a href="#ref-AL_committee" role="doc-biblioref">Beluch et al. 2018</a>)</span>, which utilizes a set of models to choose the most uncertain samples, and Bayesian AL by Disagreement (BALD) <span class="citation" data-cites="AL_BALD">(<a href="#ref-AL_BALD" role="doc-biblioref">Houlsby et al. 2011</a>)</span>, which selects samples that maximize information gain by reducing model uncertainty. Through careful uncertainty quantification, acquisition functions guide the AL process, improving the model’s efficiency in learning from limited data. Other acquisition functions that can be employed include:</p>
<ul>
<li><p>Expected model change <span class="citation" data-cites="AL_expmodelchange">(<a href="#ref-AL_expmodelchange" role="doc-biblioref">Cai, Zhang, and Zhou 2013</a>)</span>: This approach focuses on labeling points that would have the most impact on changing the current model parameters.</p></li>
<li><p>Expected error reduction <span class="citation" data-cites="AL_experrorredn">(<a href="#ref-AL_experrorredn" role="doc-biblioref">Mussmann et al. 2022</a>)</span>: Points that would most effectively reduce the model’s generalization error are labeled using this strategy.</p></li>
<li><p>Variance reduction <span class="citation" data-cites="AL_variance">(<a href="#ref-AL_variance" role="doc-biblioref">Cohn, Ghahramani, and Jordan 1996</a>)</span>: This approach labels points that would minimize output variance, which is one component of error. By selecting points that reduce variability in the model’s predictions, it aims to improve overall performance.</p></li>
</ul>
<p>Uncertainty sampling <span class="citation" data-cites="AL_uncertainty">(<a href="#ref-AL_uncertainty" role="doc-biblioref">Zhu et al. 2010</a>)</span> selects data points for which the model exhibits the greatest uncertainty, focusing labeling efforts on ambiguous samples where additional information is likely to yield the greatest benefit. Several acquisition strategies fall under uncertainty sampling, including entropy sampling, margin sampling, and least confidence sampling. Entropy sampling measures value of addition data by the entropy of the predicted probability distribution: <span class="math inline">\(\alpha(x) = - \sum_{y} p(y|x) \log p(y|x)\)</span>. Margin sampling focuses on the difference between the two highest predicted probabilities for a sample: <span class="math inline">\(\alpha(x) = p(y_1|x) - p(y_2|x)\)</span>, where <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> are two most likely classes. Least confidence sampling measures value of additional data by the lowest predicted probability for its most likely class: <span class="math inline">\(\alpha(x) = 1 - p(y_{\text{max}}|x)\)</span>, where <span class="math inline">\(y_{\text{max}}\)</span> is the class with the highest probability. Consider a binary classification problem with three candidate <span class="math inline">\(x_1, x_2, x_3\)</span>. The code below demonstrate that uncertainty sampling methods yield the same conclusion of selecting <span class="math inline">\(x_1\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
</div>
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-1"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
<p>Query-by-Committee <span class="citation" data-cites="AL_committee">(<a href="#ref-AL_committee" role="doc-biblioref">Beluch et al. 2018</a>)</span> is selects samples for labeling based on the level of disagreement among members of a committee. Several acquisition functions can be employed under this framework to quantify the disagreement. The vote entropy measures the uncertainty based on how often the committee members vote for each class. The acquisition function is defined as <span class="math inline">\(\alpha(x) = \mathbb{H}\left[V(y)/C\right]\)</span>, where <span class="math inline">\(V(y)\)</span> is the number of votes for class <span class="math inline">\(y\)</span> and <span class="math inline">\(C\)</span> is the number of committee members. Consensus Entropy measures the entropy of the average probability distribution across committee members. It is given by <span class="math inline">\(\alpha(x) = \mathbb{H}[p_C(y|x)]\)</span>, where <span class="math inline">\(p_C(y|x)\)</span> is the average probability distribution for sample <span class="math inline">\(x\)</span> across all committee members. The KL divergence quantifies the disagreement by comparing the probability distribution of each committee member to the average distribution. The acquisition function is given by <span class="math inline">\(\alpha(x) = \frac{1}{C} \sum_{c=1}^{C} D_{KL}[p_C(y|x) || p_C(y|x)]\)</span>, where <span class="math inline">\(p_C(y|x)\)</span> is the probability distribution of committee member <span class="math inline">\(c\)</span> and <span class="math inline">\(p_C(y|x)\)</span> is the average distribution across the committee. As an example, consider a binary classification problem with three candidate <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span> and three committee members. Numerical result below show that all acquisition functions selects <span class="math inline">\(x_1\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
</div>
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-2"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
<p>Bayesian AL by Disagreement (BALD) <span class="citation" data-cites="AL_BALD">(<a href="#ref-AL_BALD" role="doc-biblioref">Houlsby et al. 2011</a>)</span> selects the samples for which the model expects to gain the most Shannon information when corresponding labels are observed:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{I}(\theta; y|x, \mathcal{D}) = \mathbb{H}[p(y|x, \mathcal{D})] - \mathbb{E}_{p(\theta | \mathcal{D})} [\mathbb{H}[p(y|x, \theta, \mathcal{D})]] \\
&amp;\mathbb{H}[p(y|x, \mathcal{D})] = \mathbb{H}\left[\int_{\theta} p(y|x, \theta, \mathcal{D}) p(\theta | \mathcal{D}) d\theta\right] \approx \mathbb{H}\left[\frac{1}{N}\sum_{i=1}^{N} p(y|x, \theta_i, \mathcal{D})\right] = \mathbb{H}\left[\overline{p}(y|x, \mathcal{D})\right] \\
&amp;\mathbb{E}_{p(\theta|\mathcal{D})} [\mathbb{H}[p(y|x, \theta, \mathcal{D})]] = \mathbb{E}_{p(\theta|\mathcal{D})} \left[ - \sum_{y} p(y|x, \theta, \mathcal{D}) \log p(y|x, \theta, \mathcal{D}) \right] \approx - \frac{1}{N} \sum_{i=1}^{N} \left( \sum_{y} p(y|x, \theta_i, \mathcal{D}) \log p(y|x, \theta_i, \mathcal{D}) \right)
\end{aligned}
\]</span></p>
<p>When there is significant disagreement among models, the predictive entropy (the first term) will be large, while the expected entropy (the second term) will be smaller. This difference represents the degree to which the models disagree. BALD selects points where this disagreement is maximized. As an example, consider a binary classification problem with two classes, <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span>. We have two samples, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. BALD selects <span class="math inline">\(x_1\)</span> for labeling.</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
</div>
<div class="callout-body-container callout-body">
<div id="qpyodide-insertion-location-3"></div>
<noscript>Please enable JavaScript to experience the dynamic code cell content on this page.</noscript>
</div>
</div>
<p>AL by Variance Reduction <span class="citation" data-cites="AL_variance">(<a href="#ref-AL_variance" role="doc-biblioref">Cohn, Ghahramani, and Jordan 1996</a>)</span> is an algorithm designed to select the next data point for labeling based on the anticipated reduction in the model’s variance. The objective is to identify the point <span class="math inline">\(x \sim p(x)\)</span> that, when labeled <span class="math inline">\(y_x\)</span>, will most effectively decrease the model’s variance. The expected error at a given input <span class="math inline">\(x\)</span> is <span class="math inline">\(\mathbb{E}_{\hat{y} \sim p(\hat{y} | \mathcal{D}; x), y \sim p(y|x)} (\hat{y} - y)^2\)</span>. <span class="math inline">\(\hat{y}\)</span> represents the model’s prediction, and <span class="math inline">\(y\)</span> denotes the true label at <span class="math inline">\(x\)</span>. Using bias-variance decomposition <span class="citation" data-cites="bias_variance_orig_paper">(<a href="#ref-bias_variance_orig_paper" role="doc-biblioref">Geman, Bienenstock, and Doursat 1992</a>)</span>, the expected error is decomposed as <span class="math display">\[\begin{aligned}
\mathbb{E} (\hat{y} - y)^2 = \mathbb{E}[(\hat{y} - \mathbb{E}[y|x]) + (\mathbb{E}[y|x] - y)]^2 = \mathbb{E} [(y - \mathbb{E}[y|x])^2] + 2\mathbb{E} [(\hat{y} - \mathbb{E}[y|x])(\mathbb{E}[y|x] - y)] + \mathbb{E}(\hat{y} - \mathbb{E}[y|x])^2
\end{aligned}\]</span> where the expectation is taken over <span class="math inline">\(\hat{y} \sim p(\hat{y} | \mathcal{D}; x), y \sim p(y|x)\)</span>. The first term represents the variance of the true label <span class="math inline">\(y\)</span>, the second term evaluates to zero since <span class="math inline">\(\mathbb{E}_{\hat{y}, y}[\mathbb{E}[y|x] - y] = 0\)</span>, and the third term accounts for the variance of the model’s prediction <span class="math inline">\(\hat{y}\)</span>: <span class="math display">\[\mathbb{E}(\hat{y} - \mathbb{E}[y|x])^2 = \mathbb{E}[(\hat{y} - \mathbb{E}[\hat{y}] + \mathbb{E}[\hat{y}] - \mathbb{E}[y|x])^2] = \mathbb{E}[(\hat{y} - \mathbb{E}[\hat{y}])^2] + (\mathbb{E}[\hat{y}] - \mathbb{E}[y|x])^2\]</span></p>
<p>Hence, <span class="math display">\[\mathbb{E} (\hat{y} - y)^2 = \mathbb{E}_{y} [(y - \mathbb{E}[y|x])^2] + (\mathbb{E}_{\hat{y}} [\hat{y} - \mathbb{E}[y|x]] )^2 + \mathbb{E}_{\hat{y}} [(\hat{y} - \mathbb{E}_{\hat{y}}[\hat{y}])^2]\]</span></p>
<p>Here, the first term signifies the variance of the true label, which remains constant for a given <span class="math inline">\(x\)</span>. The second term captures how much the average model prediction deviates from the expected true label. The third term quantifies the model’s uncertainty at <span class="math inline">\(x\)</span>. <span class="citation" data-cites="AL_variance">Cohn, Ghahramani, and Jordan (<a href="#ref-AL_variance" role="doc-biblioref">1996</a>)</span> denotes the uncertainty term as <span class="math inline">\(\sigma^2_{\hat{y}} (x | \mathcal{D}) = \mathbb{E}_{\hat{y}} [(\hat{y} - \mathbb{E}_{\hat{y}}[\hat{y}])^2]\)</span>. The acquisition function is <span class="math inline">\(\mathbb{E}_{p(x)} [\sigma^2_{\hat{y}} (x | \tilde{\mathcal{D}})]\)</span>. One could rely on empirical measure like a loss on test labelled data to gauge model improvement, which can help decide the termination of data acquisition. The size of the data set and its relationship to the loss is tied to the model complexity. To evaluate the performance of variance reduction strategy, <span class="citation" data-cites="AL_variance">Cohn, Ghahramani, and Jordan (<a href="#ref-AL_variance" role="doc-biblioref">1996</a>)</span> studies the Arm2D problem. Arm2D is a kinematics problem where learner has to predict the tip position of a robotic arm given a set of joint angles <span class="math inline">\(\mathbf{\theta_1}, \mathbf{\theta_2}\)</span>. In this analysis, the two models are the Gaussian mixture model and locally-weighted regression (LOESS). The results shown that the variance of the learner decreases because the authors selected points to minimize expected variance. Additionally, we observe a related decrease in the mean square error (MSE) of both models as the dataset size increases. This is a notable outcome because the expected learner variance for these models can be computed accurately and efficiently relative to a new point. When integrated into the general AL loop, this significantly enhances model performance. In the case of the locally-weighted regression model (<span class="quarto-unresolved-ref">?fig-empirical:regress</span>), it is surprising that if points were chosen randomly, the MSE would be highly unstable, with sharp fluctuations. However, when AL by variance reduction is applied, using expected learner variance as a proxy, the MSE decreases almost smoothly, aside from some initial instabilities.</p>
</section>
<section id="active-preference-learning-with-ideal-point-model" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="active-preference-learning-with-ideal-point-model"><span class="header-section-number">3.3</span> Active Preference Learning with Ideal Point Model</h2>
<p>For any <span class="math inline">\(n\)</span> elements to be ranked, there are <span class="math inline">\(n!\)</span> possible orderings that can result in the correct complete ranking. Given that a lower bound on sorting is <span class="math inline">\(n\log n\)</span>, obtaining a guaranteed true rating over <span class="math inline">\(n\)</span> items requires <span class="math inline">\(n\log n\)</span> pairwise comparisons if those comparisons are chosen at random. This number can be quite high and costly in many applications, especially since most ranking information comes from humans. The more comparisons they have to make, the more money and time is spent. This process can also be inefficient, as some comparisons provide more value to the learning process than others, making some comparisons a waste. This inefficiency can be detrimental in fields like psychology and market research, where comparisons are heavily utilized, and a faster process could offer significant benefits. The reason the lower bound on the number of comparisons is <span class="math inline">\(n\log n\)</span> is that it assumes no prior information about the underlying space and field, so comparisons are chosen at random. However, leveraging the structures within the comparison space can provide more information about which comparisons are most valuable. For example, <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span> discusses how eye doctors have a wide range of options when assigning prescriptions for glasses, yet patients do not see them making many comparisons before deciding on the best option. This is because eye doctors incorporate domain knowledge into the process and only ask clients for comparisons when necessary. Applying similar knowledge in the ranking field leads to an AL approach that selects data based on the relevance of a comparison query toward finding the final <span class="math inline">\(\sigma(\Theta)\)</span>.</p>
<p><span class="citation" data-cites="geo_paper">G. and Nowak (<a href="#ref-geo_paper" role="doc-biblioref">2011</a>)</span> explores AL within data that can be embedded in a <span class="math inline">\(d\)</span>-dimensional embedding space, where comparisons between two different items divide the space into halves, with one object being superior in each half. By leveraging such geometry, the paper develops a geometric AL approach. Let <span class="math inline">\(\theta\)</span> be the item representation in the embedding space. For each ranking <span class="math inline">\(\sigma\)</span>, there is a reference point <span class="math inline">\(r_{\sigma} \in \mathbb{R}^d\)</span>, such that if <span class="math inline">\(\theta_{i} \succ \theta_{j}\)</span>, <span class="math inline">\(||\theta_i - r_{\sigma}|| &lt; ||\theta_j - r_{\sigma}||\)</span>. In other words, object <span class="math inline">\(i\)</span> is closer to the reference point <span class="math inline">\(r_{\sigma}\)</span> than object <span class="math inline">\(j\)</span>. <span class="math inline">\(\Sigma_{n,d}\)</span> is the set of all possible rankings of the <span class="math inline">\(n\)</span> items that satisfy the above embedding distances condition. Not all rankings will satisfy the embedding conditions, but multiple rankings might satisfy all those conditions. For every ranking <span class="math inline">\(\sigma\)</span>, there is <span class="math inline">\(M_n(\sigma)\)</span>, the number of pairwise comparisons needed to identify the ranking. When comparisons are done at random, <span class="math inline">\(\mathbb{E}[M_n(\sigma)] = n\log n\)</span>, and it can be reduced by incorporating geometry. <span class="math inline">\(q_{i,j}\)</span> is the query of comparison between items <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</p>
<p>As an example, <span class="citation" data-cites="geo_paper">G. and Nowak (<a href="#ref-geo_paper" role="doc-biblioref">2011</a>)</span> studies a 2D space with three items: <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_2\)</span>, and <span class="math inline">\(\theta_3\)</span>. There are pairwise queries <span class="math inline">\(q_{1,3}\)</span>, <span class="math inline">\(q_{2,3}\)</span>, and <span class="math inline">\(q_{1,2}\)</span> between them, denoted by solid lines equidistant from the two items they compare. These lines split the <span class="math inline">\(R^2\)</span> space into halves, with each half closer to one of the two items. The paper colors the side of the worse object for each query in dark grey and takes the intersection of these halves, resulting in the dark grey region in the image. This region indicates <span class="math inline">\(\Sigma_{n,2}\)</span> since all points follow the embedding conditions. Specifically, for every point <span class="math inline">\(r\)</span> in the dark grey area, <span class="math inline">\(||\theta_3 - r|| &lt; ||\theta_2 - r|| &lt; ||\theta_1 - r||\)</span>, meaning <span class="math inline">\(\theta_3 &lt; \theta_2 &lt; \theta_1\)</span>. Thus, every point <span class="math inline">\(r\)</span> is one of the <span class="math inline">\(r_\sigma\)</span> representing their respective rankings <span class="math inline">\(\sigma \in \Sigma_{n,2}\)</span>. In other words, the paper aims to have the reference points and dark grey region closest to the worst object and furthest from the best object.</p>
<p>The authors also denote the label for each query <span class="math inline">\(q_{i,j}\)</span>, such as label <span class="math inline">\(y_{i,j} = 1\{q_{i,j}\}\)</span> (for example, <span class="math inline">\(y_{1,2} = 0, y_{3,2} = 1\)</span>). This allows for deciding how to label new queries represented by dashed and dotted lines, depending on which items each query compares. Focusing on the dotted line, called <span class="math inline">\(q_{i,4}\)</span>, where <span class="math inline">\(i={1,2,3}\)</span>, and considering potential locations of <span class="math inline">\(\theta_4\)</span>, the line must be equidistant from one of the three items in the picture and <span class="math inline">\(\theta_4\)</span>, meaning <span class="math inline">\(\theta_4\)</span> can be placed in three different locations. If the query performed is <span class="math inline">\(q_{2,4}\)</span>, then <span class="math inline">\(\theta_4\)</span> will be closer to the dark grey area than <span class="math inline">\(\theta_2\)</span>, thus <span class="math inline">\(y_{2,4} = 0\)</span>. However, if <span class="math inline">\(q_{1,4}\)</span> or <span class="math inline">\(q_{3,4}\)</span> are performed, <span class="math inline">\(\theta_4\)</span> will be further from the dark grey area than <span class="math inline">\(\theta_1\)</span> or <span class="math inline">\(\theta_3\)</span>, meaning <span class="math inline">\(y_{1,4} = y_{3,4} = 1\)</span>. In this case, the labels are contradictory and depend on which object they are compared with, making such a query <span class="math inline">\(q_{i,4}\)</span> ambiguous.</p>
<p>In contrast, the authors analyze the dashed line, called <span class="math inline">\(q_{i,5}\)</span>, where <span class="math inline">\(i={1,2,3}\)</span>, and consider potential locations of <span class="math inline">\(\theta_5\)</span>. Since the line must be equidistant from one of the three items in the picture and <span class="math inline">\(\theta_5\)</span>, it can be placed in three different locations. If one of the three potential queries is performed, <span class="math inline">\(\theta_5\)</span> will be closer to the dark grey area than <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_2\)</span>, and <span class="math inline">\(\theta_3\)</span>, meaning <span class="math inline">\(y_{1,5} = y_{2,5} = y_{3,5} = 0\)</span>. In this case, all labels are the same regardless of which object is used, meaning such a query will not be contradictory, as all agree on the label. The goal is to perform as many ambiguous queries as possible and skip non-ambiguous queries to decrease the total <span class="math inline">\(M_n(\sigma)\)</span>. Intuitively, if there is contradictory information about a query, it needs to be erformed so that a human can clarify its direction. Conversely, if all sources of information from the domain space agree on the query’s label, that information can be used without asking a human, incorporating the knowledge of the embedding distances. Lastly, to consider the general case of the <span class="math inline">\(R^d\)</span> space, rather than discussing halves of the image, it is essential to discuss half-spaces. Similarly, consider the half-space that assigns a label of <span class="math inline">\(1\)</span> to the query and the half-space assigning a label of <span class="math inline">\(0\)</span>. If both half-spaces exist, they have conflicting information on the query, making the query ambiguous. However, if one of the half-spaces does not exist, it means the other is the full space, representing consistency in the label assignment and a non-ambiguous query.</p>
<p>It is important to demonstrate that the number of comparisons decreases. Specifically, <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span> shows that this algorithm has <span class="math inline">\(E[M_n(\sigma)] = O(d\log n)\)</span>, where <span class="math inline">\(d\)</span> is the dimension of the space and <span class="math inline">\(d &lt; n\)</span>, which improves on the <span class="math inline">\(O(n\log n)\)</span> baseline. The proof can be studied in detail in the paper itself, but at a high level, it starts by reasoning about the probability of a query being ambiguous and a comparison being requested from a human, thus representing <span class="math inline">\(M_n = \Sigma_{k=1}^{n-1}\Sigma_{i=1}^k 1\{Requestq_{i,k+1}\}\)</span>. For that, the authors define <span class="math inline">\(Q(i,j)\)</span>, which represents the number of different rankings that exist for <span class="math inline">\(i\)</span> elements in <span class="math inline">\(j\)</span>-dimensional space (e.g., <span class="math inline">\(Q(1,d) = 1, Q(n,0) = 1, Q(n,1) = n!\)</span>). In that case, <span class="math inline">\(|\Sigma_{n,d}| = Q(n,d)\)</span>. Further, using recurrence relations for <span class="math inline">\(Q(i,j)\)</span>, the authors derive that <span class="math inline">\(|\Sigma_{n,d}| = Q(n,d) = O(n^{2d})\)</span>, which is omitted here. Analogously, the authors define <span class="math inline">\(P(i,j)\)</span>, which represents the number of rankings in <span class="math inline">\(\Sigma_{n,d}\)</span> that will still be possible with the addition of a new element <span class="math inline">\(i+1\)</span> to the ranking items. <span class="math inline">\(P(i,j)\)</span> estimates how much of the dark grey area will still exist after making a query for <span class="math inline">\(i+1\)</span>. As indicated there, the dotted line ambiguous query did not change the dark grey a rea at all (<span class="math inline">\(P(n,d) = Q(n,d)\)</span>), whereas the dashed non-ambiguous query would cut a piece from it (<span class="math inline">\(P(n,d) &lt; Q(n,d)\)</span>). Thus, <span class="math inline">\(Request q_{i,k+1} = P(k,d) / Q(k,d)\)</span>, so a higher value indicates more possible rankings and an ambiguous query that needs to be requested to obtain more useful information. With this in mind, the authors derive that <span class="math inline">\(E[M_n(\sigma)] = O(d\log n)\)</span>, showing that fewer queries are needed for effective ranking.</p>
<p>The issue with this algorithm is that only one human provides the answers to the requested queries, which means it does not account for their biases. An alternative approach is a Robust Query Selection Algorithm (RQSA) <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span>, which uses majority voting for every query to indicate the ground truth of the query’s label. However, the authors consider that a group of people can still give incorrect or divided responses. If the votes for each answer are almost equal in number, the authors push that query to the end of the algorithm to see if it can become a non-ambiguous query with more information learned. If it does not, an odd number of voters is used to determine the final ranking.</p>
<div id="tbl-geo_acc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-geo_acc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Statistics for the Robust Query Selection Algorithm (RQSA) <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span> and the baseline of conducting all comparisons. <span class="math inline">\(y\)</span> serves as a noisy ground truth, <span class="math inline">\(\tilde{y}\)</span> is the result of all comparisons, and <span class="math inline">\(\hat{y}\)</span> is the output of the RQSA.
</figcaption>
<div aria-describedby="tbl-geo_acc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Dimension</th>
<th></th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">% of queries</td>
<td>mean</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">18.5</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td>std</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Average error</td>
<td><span class="math inline">\(d(\bar{y}, y)\)</span></td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.21</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td><span class="math inline">\(d(\bar{y}, y)\)</span></td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.29</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>With regard to the accuracy and performance of the method, the authors did a ranking experiment on 100 different audio signals, results of which can be seen in <a href="#tbl-geo_acc" class="quarto-xref">Table&nbsp;<span>3.1</span></a>. The ground truth labels came from humans, indicated by <span class="math inline">\(y\)</span> in the table. That resulted in the existence of noise and potential errors in the ground truth, which could influence the performance of both the baseline algorithm that does all comparisons (<span class="math inline">\(\tilde{y}\)</span>) and the Robust Query Selection Algorithm (RQSA) (<span class="math inline">\(\hat{y}\)</span>). As can be seen in both 2 and 3-dimensional spaces RQSA performed worse by <span class="math inline">\(8\%\)</span> compared to the baseline, which indicates that AL that uses the domain information can still be erroneous due to the inference of certain comparisons that sometimes may not be entirely correct. However, as can be seen by the upper part of <a href="#tbl-geo_acc" class="quarto-xref">Table&nbsp;<span>3.1</span></a>, significantly less queries were requested compared to the baseline, which means that the approach can have a significant benefit at a cost of slight loss in accuracy.</p>
<section id="sec-geo_app" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="sec-geo_app">User Information as Domain Knowledge for Active Learning</h4>
<p>An alternative source of domain knowledge could be users themselves, who can indicate their uncertainty when it comes to comparing two items. Prior studies have shown <span class="citation" data-cites="unnoisy_humans">(<a href="#ref-unnoisy_humans" role="doc-biblioref">Amershi et al. 2014</a>)</span> that when presented with only two options when selecting which object is better, but not being able to properly decide, users would get frustrated and tend to respond more faultyly, creating noise and incorrect responses in the data. Through feedback and other studies <span class="citation" data-cites="noisy_humans">(<a href="#ref-noisy_humans" role="doc-biblioref">Guillory and Bilmes 2011</a>)</span> it was determined that presenting users with an option of indifference between the two items can remove those problems. Moreover, in connection to AL, the authors show that such an option helps to select more informative queries since it provides more domain knowledge that can be used, resulting in a decrease in the number of queries required. For this problem, the following terms are defined:</p>
<ol type="1">
<li><p><span class="math inline">\(c\)</span> - a cost function that represents user preferences, and the result the model has to determine at the end of training. The preferred items will have lower costs, and less preferred ones will have higher costs. The goal is to determine this function with the fewest possible number of queries using AL.</p></li>
<li><p><span class="math inline">\(H\)</span> - a set of hypotheses over the possible cost functions, where for each <span class="math inline">\(h \in H\)</span> there is a cost function <span class="math inline">\(c_h\)</span> associated with it.</p></li>
<li><p><span class="math inline">\(h^*\)</span> - a true hypothesis that the model needs to determine, which has cost <span class="math inline">\(c_{h^*}\)</span> associated with it</p></li>
<li><p><span class="math inline">\(t(x,y)\)</span> - a test performed to compare items <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> (the user is being asked to provide a response to which item is better). Those tests result in changes and adjustments to <span class="math inline">\(H\)</span> as more information is learned.</p></li>
<li><p><span class="math inline">\(o(x,y)\)</span> - observation or result of <span class="math inline">\(t(x,y)\)</span>, where <span class="math inline">\(o(x,y) \in \{x&lt;y, x&gt;y\}\)</span></p></li>
<li><p><span class="math inline">\(S = \{(t_1, o_1), (t_2, o_2),...,(t_m, o_m)\}\)</span> - a sequence of <span class="math inline">\(m\)</span> pairs of tests and observations</p></li>
<li><p><span class="math inline">\(w(H|S)\)</span> - probability mass of all hypotheses that are still consistent with the observations (similar to the dark grey area and <span class="math inline">\(Q(i,j)\)</span>). This means that if <span class="math inline">\(h \in H\)</span> is inconsistent with user responses received, it is removed from <span class="math inline">\(H\)</span>.</p></li>
</ol>
<p>With the key terms defined, let’s consider the noiseless base setting where users only have two options for response. Those components will also later be translated to the setting with the third option so the true cost function can be determined there. <span class="math inline">\(w(H|S)\)</span> is the sum of the weights of all hypotheses that are still consistent with the evidence: <span class="math inline">\(w(H|S) = \sum_{h \in H} w(h | S)\)</span>. Each <span class="math inline">\(w(h|S)\)</span> is a probability of the evidence’s existence given such hypothesis: <span class="math inline">\(w(h|S) = p(S|h)\)</span>. Such probability comes from the test-observation pairs since they compose the set <span class="math inline">\(S\)</span>. Moreover, each test is independent of other tests, which gives <span class="math inline">\(p(S|h) = \prod_{(t,o) \in S} p((t,o) | h)\)</span>. In the noiseless setting, users will select an option that minimizes their cost function (selecting more preferred items), mathematically defined as: <span class="math display">\[\begin{aligned}
    p((t, o = x) | h) =
    \begin{cases}
        1 &amp; c_h(x) &lt; c_h(y)\\
        0 &amp; else
    \end{cases}
\end{aligned}\]</span></p>
<p>Users are not perfect evaluators. Prior work <span class="citation" data-cites="unnoisy_humans">(<a href="#ref-unnoisy_humans" role="doc-biblioref">Amershi et al. 2014</a>)</span> has shown that treating users as perfect can lead to poor performance. That gave rise to accounting for noise in users’ responses, but a majority of such work applies the same noise to all queries and all responses. While those led to great performance results <span class="citation" data-cites="noisy_humans">(<a href="#ref-noisy_humans" role="doc-biblioref">Guillory and Bilmes 2011</a>)</span>, they don’t accurately reflect the real world, which gave rise to the idea of creating query-based noise. Effectively, for some of the queries it is important to incorporate the fact that the user is unsure and noisy, but for others, if the user is confident, noise in the response is not needed at all. For comparison-based learning, this means that the noise is related to the costs of the two items compared. Specifically for items <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, if <span class="math inline">\(c_{h^*}(x) \simeq c_{h^*}(y)\)</span> then the items are hard to distinguish for the user, so here it is preferred to incorporate user uncertainty and noise. But if <span class="math inline">\(c_{h^*}(x) &gt;&gt; c_{h^*}(y)\)</span>, the user will certainly select <span class="math inline">\(y\)</span> and the other way around, which is where the noise is not needed. Query-dependent noise is also supported in the psychology literature, which means that such an approach is more related to the real world. In particular, psychologists talk about the Luce-Sheppard Choice rule <span class="citation" data-cites="lus-shep">(<a href="#ref-lus-shep" role="doc-biblioref">Shepard 1957</a>)</span> when talking about comparisons. This rule previously gave rise to a logistic model based on the noise <span class="citation" data-cites="lus-log">(<a href="#ref-lus-log" role="doc-biblioref">Viappiani and Boutilier 2010</a>)</span> where the probability of observation for a given test is <span class="math inline">\(p((t, o = x) | h) \propto exp(-\gamma * c_h(x))\)</span></p>
<div id="fig-noiseless_1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-noiseless_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/Noiseless probs.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-noiseless_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: User response model in the noiseless setting
</figcaption>
</figure>
</div>
<div id="fig-noiseless_2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-noiseless_2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/Noise probs.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-noiseless_2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: User response with Luce Sheppard noise model
</figcaption>
</figure>
</div>
<p><a href="#fig-noiseless_1" class="quarto-xref">Figure&nbsp;<span>3.1</span></a>, <a href="#fig-noiseless_2" class="quarto-xref">Figure&nbsp;<span>3.2</span></a> demonstrate the difference between the noiseless setting and incorporating the Luce-Sheppard Choice rule. GBS is the baseline model with only 2 response options, and CLAUS is the model with the uncertainty option added. The figures show how incorporating such noise influences and smoothes the probability distribution of the user’s response.</p>
<p>We will now discuss the functionality of CLAUS, which is an algorithm designed by <span class="citation" data-cites="claus">(<a href="#ref-claus" role="doc-biblioref">Holladay et al. 2016</a>)</span> that allows users to select an uncertain response about the two options that they need to rank. The authors model such uncertainty as <span class="math inline">\(\epsilon\)</span> and it is associated with each <span class="math inline">\(c_h\)</span>, so now every hypothesis <span class="math inline">\(h\)</span> is defined over a pair of <span class="math inline">\((c_h, \epsilon_h)\)</span>. It is important to note that the goal is to still learn and maintain our objective on <span class="math inline">\(c\)</span>, <span class="math inline">\(\epsilon\)</span> is only necessary to model the users’ responses. The uncertainty relates to the cost function as <span class="math inline">\(|c_h(x) - c_h(y)| &lt; \epsilon_h\)</span>. This means that the user is uncertain between items <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> and their cost difference is negligible such that the user is not able to select which item is better. This in turn gives more information about the real value of the two items, as a binary response would indicate the user’s preference towards one item, which will not be real and will skew the cost functions. This causes modifications of the problem set-up:</p>
<ol type="1">
<li><p>For test <span class="math inline">\(t(x,y)\)</span> the observation will be <span class="math inline">\(o(x,y) \in \{x&lt;y, x&gt;y, \tilde{xy}\}\)</span>, where <span class="math inline">\(\tilde{xy}\)</span> is the uncertain response.</p></li>
<li><p>The probability distribution over the user’s response (<span class="quarto-unresolved-ref">?eq-prob_base</span>) will now be defined as:</p></li>
</ol>
<p><span class="math display">\[\begin{aligned}
    p((t, o = x) | h) =
    \begin{cases}
        1 &amp; c_h(x) &lt; c_h(y) - \epsilon_h\\
        0 &amp; else
    \end{cases}, \quad
    p((t, o = \tilde{xy}) | h) =
    \begin{cases}
        1 &amp; |c_h(x) - c_h(y)|^2 &lt; \epsilon_h^2\\
        0 &amp; else
    \end{cases}
\end{aligned}\]</span></p>
<p>This means the user confidently selects <span class="math inline">\(x\)</span> when it is better than <span class="math inline">\(y\)</span> by more than <span class="math inline">\(\epsilon\)</span>, but if the squared difference of the cost functions of two items is negligible by <span class="math inline">\(\epsilon\)</span> user will choose the indifferent option.</p>
<ol start="3" type="1">
<li>Finally this also updates the noise model: <span class="math display">\[\begin{aligned}
&amp;p((t, o = x) | h) \propto \exp(-\gamma * [c_h(x) - c_h(y)]) \\
&amp;p((t, o = \tilde{xy}) | h) \propto exp(-1/\epsilon_h^2 * [c_h(x) - c_h(y)]^2)
\end{aligned}\]</span></li>
</ol>
<p>Rather than predicting a specific pair <span class="math inline">\((c_h, \epsilon_h)\)</span>, the algorithm focuses on predicting a group of pairs that are similar to one another, otherwise called equivalence class (<span class="quarto-unresolved-ref">?fig-equiv_c</span>), which indicates not essentially different hypothesis for the cost function and uncertainty. That information is learned through each new test, as the algorithm updates the information about <span class="math inline">\(c\)</span> and <span class="math inline">\(\epsilon\)</span> that distinguishes between the distinct <span class="math inline">\(h\)</span>, finding the equivalence groups among them. Moreover, the authors tweaked the parameter responsible for the size of the equivalence class (how many hypotheses can be grouped together at a time).</p>
<div id="tbl-claus_tab" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-claus_tab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.2: Performance of GBS and CLAUS with different labels for the uncertainty
</figcaption>
<div aria-describedby="tbl-claus_tab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Category</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Query Count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">GBS - About Equal</td>
<td style="text-align: center;"><span class="math inline">\(94.15 \pm 0.52\)</span></td>
<td style="text-align: center;"><span class="math inline">\(36.02 \pm 0.03\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">GBS - Not Sure</td>
<td style="text-align: center;"><span class="math inline">\(\textbf{94.66} \pm \textbf{0.55}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(35.95 \pm 0.04\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">CLAUS - About Equal</td>
<td style="text-align: center;"><span class="math inline">\(91.56 \pm 0.84\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\textbf{25.93} \pm \textbf{0.41}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">CLAUS - Not Sure</td>
<td style="text-align: center;"><span class="math inline">\(90.86 \pm 0.74\)</span></td>
<td style="text-align: center;"><span class="math inline">\(26.98 \pm 0.47\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The first performance evaluation is done on the number of queries and confirms that it decreases. The GBS model serves as the baseline, as it will do all of the comparison queries using the binary response options. The CLAUS model is measured over different values of <span class="math inline">\(\epsilon\)</span> on the x-axis and over different sizes of the equivalence sets indicated by different shades of blue. Figure shows that all variants of CLAUS use approximately 10 fewer queries on average compared to GBS. Moreover, using bigger-sized equivalence classes can further decrease the number of needed queries. The most optimal <span class="math inline">\(\epsilon \simeq 0.07\)</span>, after which higher <span class="math inline">\(\epsilon\)</span> does not provide any benefit.</p>
<p>Lastly, the authors considered the performance difference, which is indicated in <a href="#tbl-claus_tab" class="quarto-xref">Table&nbsp;<span>3.2</span></a>. For that authors used two different labels for the uncertainty button in CLAUS, it was either labeled as “About Equal” or “Not Sure” as those can provoke different responses and feelings in users. Moreover, GBS and CLAUS-type responses were mixed in the same set of questions to the user, which splits the metrics for both in two as can be seen in <a href="#tbl-claus_tab" class="quarto-xref">Table&nbsp;<span>3.2</span></a>. The performance of CLAUS is lower by <span class="math inline">\(3\%\)</span> on average, showing that a smaller number of queries can still lead to a performance loss. However, the second column of <a href="#tbl-claus_tab" class="quarto-xref">Table&nbsp;<span>3.2</span></a> supports the information, as it also shows that 10 fewer queries were conducted on average.</p>
<p>AL can be essential in learning within dynamic systems and environments. Say we have an agent in an environment, and we want it to conform to a certain behavior as set by a human. How exactly do we go about doing this? In a traditional RL setting, this is solved by a class of algorithms under Inverse Reinforcement Learning. Techniques such as VICE and GAIL attempt to learn a reward function that can distinguish between states visited by the agent and states desired to be visited as defined by a human. In effect, a human will demonstrate what it would like the agent to do in the environment, and from there, learning is done. However, what if humans do not precisely know how an agent should optimally behave in an environment but still have some opinion on what trajectories would be better than others? This is where a paper like Active Preference-Based Learning of Reward Functions comes into the picture. The paper aims to use human preferences to aid an agent’s learning within a dynamic system.</p>
<p>A dynamic system contains human input, robotic input, and an environment state. The transitions between states is defined by <span class="math inline">\(f_{HR}\)</span>, so that we have <span class="math inline">\(x^{t+1} = f_{HR}(x^t, u_R, u_H)\)</span>. At a given time step <span class="math inline">\(t\)</span>, we have <span class="math inline">\(x_t\)</span>, <span class="math inline">\(u_R^t\)</span>, and <span class="math inline">\(u_H^t\)</span>. This can be encapsulated into a single <span class="math inline">\(d\)</span> dimensional feature vector that the authors denote as <span class="math inline">\(\phi\)</span>. The paper then assumes that the underlying reward model we are trying to learn can be represented linearly. If we have our human reward preference function defined as <span class="math inline">\(r_H\)</span>, this means we can write <span class="math inline">\(r_H\)</span> as <span class="math inline">\(r_H(x^t, u_R^t, u_H^t) = w^{\intercal}\phi(x^t, u_R^t, u_H^t)\)</span>. Because the reward function is linear, we can take the weight vector out of the summation if we want to calculate the reward over an entire trajectory:</p>
<p><span class="math display">\[R_{H}(x^0, u_R, u_H) = \sum_{t=0}^{N} r_{H}(x^t, u^t, u_H^t) \quad \Phi = \sum \phi(x^t, u_R^t, u_H^t) \quad R_H(traj) = w\cdot\Phi(traj)\]</span></p>
<p>First, the scale of <span class="math inline">\(w\)</span> does not matter because we only care about the relative rewards produced with <span class="math inline">\(w\)</span> (given two different trajectories, we want to answer the question of which trajectory a human would prefer, i.e.&nbsp;which one has a higher preference reward). This means we can constrain <span class="math inline">\(||w|| &lt;= 1\)</span>, so the initial prior is uniform over a unit ball. From here, we can determine a probabilistic expression to assess whether we should prefer trajectory A or B (because it can be noisy with human input). Let <span class="math inline">\(I_t = +1\)</span> if the human prefers trajectory <span class="math inline">\(A\)</span>. According to Bradley-Terry model, <span class="math inline">\(p(A \succ B|w) = \sigma(R_H(traj_A) - R_H(traj_B))\)</span>. Let <span class="math inline">\(\psi = \Phi(traj_a) - \Phi(traj_b). Then f_{\psi} (w) = p(I_t|w) = \sigma(I_t w^{\intercal}\psi)\)</span>. We can update <span class="math inline">\(p(w)\)</span> everytime we get a result from a human preference query using Bayes’ rule: <span class="math inline">\(p(w|I_t) &lt;- p(w) \cdot p(I_t|w)\)</span> via Markov chain Monte Carlo method. This paper synthetically generates queries through an optimization process and then presents them to a human to pick between. The idea is that we want to generate a query that maximizes the conditional entropy <span class="math inline">\(H(I|w)\)</span>. We want to pick a query that we are most uncertain about given our current weights (thus having the highest conditional entropy given the weights): <span class="math display">\[\max_{x^0, u_R, u_H^A, u_H^B} \min\{\mathbb{E}[1-f_{\psi}(w)], \mathbb{E}[1 - f_{-\psi}(w)]\}\]</span></p>
<p>To do so, we sample <span class="math inline">\(w_1, ... w_m\)</span> from <span class="math inline">\(p(w)\)</span>, approximating the distribution <span class="math inline">\(p(w)\)</span> as <span class="math inline">\(p(w) = \frac{1}{M} \sum \delta (w_i).\)</span> We can now approximate the expectation expression as <span class="math inline">\(E[1 - f_{\psi}(w)] = \frac{1}{M} (\sum 1 - f_{\psi}(w_i))\)</span>, and now we can optimize the expression to generate a synthetic query. The algorithm itself works well, however there ends up being a bottle neck that each query needs to be synthesized before being sent to the human – one at a time. There is no room for parallelization and so the authors proposed a second algorithm in a separate paper that allows for the batching of queries:</p>
<p><span class="math display">\[\max_{\xi_{ib+1_A}, \xi_{ib+1_B}, ... , \xi_{ib+b_A}, \xi_{ib+b_B}} \mathbb{H}(I_{ib+1}, I_{ib+2}, .., I_{ib+b} | w)\]</span></p>
<p>We could consider optimizing this in the greedy fashion. This would mean just synthetically generating <span class="math inline">\(b\)</span> independent queries. The drawback of this method would be that the queries would likely be very similar to each other. The authors propose a few other heuristics that would help guide the algorithm away from generating very similar queries, such as Medioid Selection where we have to cluster <span class="math inline">\(B\)</span> greedy vectors into <span class="math inline">\(b &lt; B\)</span> groups and pick one vector from each group (the medioid). The authors also propose two other methods rooted in providing different queries: boundary medioids selection and successive elimination. The authors test both the non-batched and variety of batched learning algorithms on multiple environments. When graphed over <span class="math inline">\(N\)</span> the non-batched AL approach does in the same ball-park of performance as the batched approaches. However, over time, we see that learning is a much slower process when not-batched.</p>
</section>
</section>
<section id="sec-metric-elicitation" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-metric-elicitation"><span class="header-section-number">3.4</span> Case Study 2: Performance Metric Elicitation</h2>
<p>In binary classification problems, selecting an appropriate performance metric that aligns with the real-world task is crucial. The problem of <em>metric elicitation</em> aims to characterize and discover the performance metric of a practitioner, reflecting the rewards or costs associated with correct or incorrect classification. For instance, in medical contexts such as diagnosing a disease or determining the appropriateness of a treatment, trade-offs are made for incorrect decisions. Not administering a treatment could lead to the worsening of a disease (a false negative), whereas delivering the wrong treatment could cause adverse side effects worse than not treating the condition (a false positive). Rather than choosing from a limited set of default choices like the F1-score or weighted accuracy, metric elicitation considers the process of devising a metric that best matches the preferences of practitioners or users. This is achieved by querying an “oracle” who provides feedback on proposed potential metrics through pairwise comparisons. Since queries to humans are often expensive, the goal is to minimize the number of comparisons needed.</p>
<p>The motivation for the pairwise comparison aspect of metric elicitation <span class="citation" data-cites="pmlr-v89-hiranandani19a">(<a href="#ref-pmlr-v89-hiranandani19a" role="doc-biblioref">Hiranandani et al. 2019a</a>)</span> stems from a rich history of literature in psychology, economics, and computer science <span class="citation" data-cites="pref1 pref2 pref3 pref4 ab">(<a href="#ref-pref1" role="doc-biblioref">Samuelson 1938</a>; <a href="#ref-pref2" role="doc-biblioref">Mas-Colell 1977</a>; <a href="#ref-pref3" role="doc-biblioref">Varian 2006</a>; <a href="#ref-pref4" role="doc-biblioref">Braziunas and Boutilier 2012</a>; <a href="#ref-ab" role="doc-biblioref">Tamburrelli and Margara 2014</a>)</span>, demonstrating that humans are often ineffective at providing absolute feedback on aspects such as potential prices, user interfaces, or even ML model outputs (hence the comparison-based structure of RLHF, for instance). Additionally, confusion matrices accurately capture binary metrics such as accuracy, <span class="math inline">\(F_\beta\)</span>, and Jaccard similarity by recording the number of false positives, true positives, false negatives, and true negatives obtained by a classifier. The main goal of this chapter is to introduce two binary-search procedures that can approximate the oracle’s performance metric for two types of metrics (linear and linear-fractional performance metrics) by presenting the oracle with confusion matrices generated by various classifiers. Essentially, we are learning an optimal threshold for classification given a decision boundary for a binary classification problem.</p>
<p>First, we introduce some relevant notation that will later be used to formalize notions of oracle queries, classifiers, and metrics. In this context, <span class="math inline">\(X \in \mathcal{X}\)</span> represents an input random variable, while <span class="math inline">\(Y \in \{0, 1\}\)</span> denotes the output random variable. We learn from a dataset of size <span class="math inline">\(n\)</span>, denoted by <span class="math inline">\(\{(x, y)_i\}^n_{i=1}\)</span>, which is generated independently and identically distributed (i.i.d.) from some distribution <span class="math inline">\(\mathbb{P}(X, Y)\)</span>. The conditional probability of the positive class, given some sample <span class="math inline">\(x\)</span>, is denoted by <span class="math inline">\(\eta(\vec{x}) = \mathbb{P}(Y=1 | X=x)\)</span>. The marginal probability of the positive class is represented by <span class="math inline">\(\zeta = \mathbb{P}(Y=1)\)</span>. The set of all potential classifiers is <span class="math inline">\(\mathcal{H} = \{h : \mathcal{X} \rightarrow \{0,1\}\}\)</span>. The confusion matrix for a classifier <span class="math inline">\(h\)</span> is <span class="math inline">\(C(h, \mathbb{P}) \in \mathbb{R}^{2 \times 2}\)</span>, where <span class="math inline">\(C_{ij}(h, \mathbb{P}) = \mathbb{P}(Y=i, h=j)\)</span> for <span class="math inline">\(i, j \in \{0,1\}\)</span>. These entries represent the false positives, true positives, false negatives, and true negatives, ensuring that <span class="math inline">\(\sum_{i,j}C_{ij}=1\)</span>. The set of all confusion matrices is denoted by <span class="math inline">\(\mathcal{C}\)</span>. Since <span class="math inline">\(FN(h, \mathbb{P}) = \zeta - TP(h, \mathbb{P})\)</span> and <span class="math inline">\(FP(h, \mathbb{P}) = 1 - \zeta - TN(h, \mathbb{P})\)</span>, <span class="math inline">\(\mathcal{C}\)</span> is actually a 2-dimensional space, not a 4-dimensional space.</p>
<p>Any hyperplane in the <span class="math inline">\((tp, tn)\)</span> space is given by <span class="math inline">\(\ell := a \cdot tp + b \cdot tn = c\)</span>, where <span class="math inline">\(a, b, c \in \mathbb{R}\)</span>. Given a classifier <span class="math inline">\(h\)</span>, we define a performance metric <span class="math inline">\(\phi : [0, 1]^{2 \times 2} \rightarrow \mathbb{R}\)</span>. The value <span class="math inline">\(\phi(C(h))\)</span>, which represents the performance of a classifier with respect to a certain metric, is referred to as the <em>utility</em> of the classifier <span class="math inline">\(h\)</span>. We assume, without loss of generality, that a higher value of <span class="math inline">\(\phi\)</span> indicates a better performance metric for <span class="math inline">\(h\)</span>. Our focus is to recover some metric <span class="math inline">\(\phi\)</span> using comparisons between confusion matrices <span class="math inline">\(C(h)\)</span>, determined by classifiers <span class="math inline">\(h\)</span>, which approximates the oracle’s “ground-truth” metric <span class="math inline">\(\phi^*\)</span>. Next, we introduce two classes of performance metrics—<em>Linear Performance Metrics (LPM)</em> and <em>Linear-Fractional Performance Metrics (LFPM)</em>—for which we will present two elicitation algorithms.</p>
<p>An LPM, given constants <span class="math inline">\(\{a_{11}, a_{01}, a_{10}, a_{00}\} \in \mathbb{R}^{4}\)</span>, is defined as <span class="math inline">\(\phi(C) = a_{11} TP + a_{01} FP + a_{10} FN + a_{00} TN = m_{11} TP + m_{00} TN + m_{0}\)</span>, where <span class="math inline">\(m_{11} = (a_{11} - a_{10})\)</span>, <span class="math inline">\(m_{00} = (a_{00} - a_{01})\)</span>, and <span class="math inline">\(m_{0} = a_{10} \zeta + a_{01} (1 - \zeta)\)</span>. This reparametrization simplifies the metric by reducing dimensionality, making it more tractable for elicitation. One example of an LPM is <em>weighted accuracy</em>, defined as <span class="math inline">\(WA = w_1TP + w_2TN\)</span>, where adjusting <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> controls the relative importance of different types of misclassification. An LFPM, defined by constants <span class="math inline">\(\{a_{11}, a_{01}, a_{10}, a_{00}, b_{11}, b_{01}, b_{10}, b_{00}\} \in \mathbb{R}^{8}\)</span>, is given by: <span class="math display">\[\phi(C) = \frac{a_{11} TP + a_{01} FP + a_{10} FN + a_{00} TN}{b_{11} TP + b_{01} FP + b_{10} FN + b_{00} TN} = \frac{p_{11} TP + p_{00} TN + p_{0}}{q_{11} TP + q_{00} TN + q_{0}},\]</span> where <span class="math inline">\(p_{11} = (a_{11} - a_{10})\)</span>, <span class="math inline">\(p_{00} = (a_{00} - a_{01})\)</span>, <span class="math inline">\(q_{11} = (b_{11} - b_{10})\)</span>, <span class="math inline">\(q_{00} = (b_{00} - b_{01})\)</span>, <span class="math inline">\(p_{0} = a_{10} \zeta + a_{01} (1 - \zeta)\)</span>, and <span class="math inline">\(q_{0} = b_{10} \zeta + b_{01} (1 - \zeta)\)</span>. This parametrization also simplifies the elicitation process by reducing the number of variables. Common LFPMs include the <span class="math inline">\(F_\beta\)</span> score and Jaccard similarity, defined as:</p>
<p><span id="eq-lfpm_metrics"><span class="math display">\[F_{\beta} = \frac{TP}{\frac{TP}{1+\beta^{2}} - \frac{TN}{1+\beta^{2}} + \frac{\beta^{2} \zeta + 1 - \zeta}{1+\beta^{2}}}, \quad JAC = \frac{TP}{1 - TN}. \tag{3.1}\]</span></span></p>
<p>Setting <span class="math inline">\(\beta = 1\)</span> gives the F1 score, which is widely used as a classification metric. Since we are considering all possible metrics in the LPM and LFPM families, we need to make certain assumptions about <span class="math inline">\(\mathcal{C}\)</span>. Particularly, we will assume that <span class="math inline">\(g(t) = \mathbb{P}[\eta(X) \geq t]\)</span> is continuous and strictly decreasing for <span class="math inline">\(t \in [0, 1]\)</span>; essentially, <span class="math inline">\(\eta\)</span> has positive density and zero probability.</p>
<p>Additionally, <span class="math inline">\(\mathcal{C}\)</span> is convex, closed, and contained within the rectangle <span class="math inline">\([0, \zeta] \times [0, 1-\zeta]\)</span>, and is rotationally symmetric around its center, <span class="math inline">\((\frac{\zeta}{2}, \frac{1-\zeta}{2})\)</span>, where the axes represent the proportion of true positives and negatives. The only vertices of <span class="math inline">\(\mathcal{C}\)</span> are <span class="math inline">\((0, 1-\zeta)\)</span> and <span class="math inline">\((\zeta, 0)\)</span>, corresponding to predicting all <span class="math inline">\(0\)</span>’s or all <span class="math inline">\(1\)</span>’s on a given dataset. Therefore, <span class="math inline">\(\mathcal{C}\)</span> is strictly convex, and any line tangent to it is tangent at exactly one point, corresponding to one particular confusion matrix. Next, recall that an LPM is represented in terms of three parameters (<span class="math inline">\(\phi = m_{11}TP + m_{00}TN + m_0\)</span>). We have just seen that this LPM and its corresponding confusion matrix correspond to a certain point on the boundary of <span class="math inline">\(\mathcal{C}\)</span>. We first note that this point is independent of <span class="math inline">\(m_0\)</span>. Additionally, we only care about the relative weightings of <span class="math inline">\(m_{11}\)</span> and <span class="math inline">\(m_{00}\)</span>, not their actual values—they are scale invariant. Therefore, we can parametrize the space of LPMs as <span class="math inline">\(\varphi_{LPM} = \{\mathbf{m} = (\cos \theta, \sin \theta) : \theta \in [0, 2\pi]\}\)</span>, where <span class="math inline">\(\cos \theta\)</span> corresponds to <span class="math inline">\(m_{00}\)</span> and <span class="math inline">\(\sin \theta\)</span> corresponds to <span class="math inline">\(m_{11}\)</span>. As we already know, we can recover the Bayes classifier given <span class="math inline">\(\mathbf{m}\)</span>, and it is unique, corresponding to one point on the boundary of <span class="math inline">\(\mathcal{C}\)</span> due to its convexity. The supporting hyperplane at this point is defined as <span class="math inline">\(\bar{\ell}_{\mathbf{m}} := m_{11} \cdot tp + m_{00} \cdot tn = m_{11} \overline{TP}_{\mathbf{m}} + m_{00} \overline{TN}_{\mathbf{m}}\)</span>. We note that if <span class="math inline">\(m_{00}\)</span> and <span class="math inline">\(m_{11}\)</span> have opposite signs, then <span class="math inline">\(\bar{h}_m\)</span> is the trivial classifier predicting all 1’s or all 0’s, since either predicting true positives or true negatives results in negative reward. This corresponds to a supporting hyperplane with a positive slope, so it can only be tangent at the vertices. Additionally, the boundary <span class="math inline">\(\partial \mathcal{C}\)</span> can be split into upper and lower boundaries (<span class="math inline">\(\partial \mathcal{C}_{+}, \partial \mathcal{C}_{-}\)</span>), corresponding to <span class="math inline">\(\theta \in (0, \pi/2)\)</span> and <span class="math inline">\(\theta \in (\pi, 3\pi/2)\)</span> respectively (and whether <span class="math inline">\(m_{00}, m_{11}\)</span> are positive or negative). We also define the notions of Bayes optimal and inverse-optimal classifiers. Given a performance metric <span class="math inline">\(\phi\)</span>, we define:</p>
<ul>
<li>The <em>Bayes utility</em> as <span class="math inline">\(\bar{\tau} := \sup_{h \in \mathcal{H}} \phi(C(h)) = \sup_{C \in \mathcal{C}} \phi(C)\)</span>; this is the highest achievable utility (using the metric <span class="math inline">\(\phi\)</span>) over all classifiers $h <span class="math inline">\(\mathcal{H}\)</span> for a given problem.</li>
<li>The <em>Bayes classifier</em> as <span class="math inline">\(\bar{h} := \arg \max_{h \in \mathcal{H}} \phi(C(h))\)</span>; this is the classifier <span class="math inline">\(h\)</span> corresponding to the Bayes utility.</li>
<li>The <em>Bayes confusion matrix</em> as <span class="math inline">\(\bar{C} := \arg \max_{C \in \mathcal{C}} \phi(C)\)</span>; this is the confusion matrix corresponding to the Bayes utility and classifier.</li>
</ul>
<p>Similarly, the inverse Bayes utility, classifier, and confusion matrix can be defined by replacing “<span class="math inline">\(\sup\)</span>” with “<span class="math inline">\(\inf\)</span>”; they represent the classifier and confusion matrix corresponding to the lower bound on utility for a given problem. We also have the following useful proposition:</p>
<div class="callout callout-style-default callout-note callout-titled" title="proposition">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
proposition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="prp-prp3.1" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 3.1</strong></span> Let <span class="math inline">\(\phi \in \varphi_{LPM}\)</span>. Then</p>
<p><span id="eq-eq3.45"><span class="math display">\[\bar{h}(x) = \left\{\begin{array}{lr}
\unicode{x1D7D9} \left[\eta(x) \geq \frac{m_{00}}{m_{11} + m_{00}}\right], &amp; m_{11} + m_{00} \geq 0 \\
\unicode{x1D7D9} \left[\frac{m_{00}}{m_{11} + m_{00}} \geq \eta(x)\right], &amp; \text { o.w. }
\end{array}\right\} \tag{3.2}\]</span></span></p>
<p>is a Bayes optimal classifier with respect to <span class="math inline">\(\phi\)</span>. The inverse Bayes classifier is given by <span class="math inline">\(\underline{h} = 1 - \bar{h}\)</span>.</p>
</div>
</div>
</div>
<p>This is a simple derivation based on the fact that we only get rewards from true positives and true negatives. Essentially, if we recover an LPM, we can use it to determine the best-performing classifier, obtained by placing a threshold on the conditional probability of a given sample, that corresponds to a confusion matrix. Therefore, the three notions of Bayes utility, classifier, and confusion matrix are functionally equivalent in our setting.</p>
<p>We will now formalize the problem of metric elicitation. Given two classifiers <span class="math inline">\(h\)</span> and <span class="math inline">\(h'\)</span> (or equivalently, two confusion matrices <span class="math inline">\(C\)</span> and <span class="math inline">\(C'\)</span>), we define an <em>oracle query</em> as the function:</p>
<p><span id="eq-oracle"><span class="math display">\[\Gamma\left(h, h^{\prime}\right)=\Omega\left(C, C^{\prime}\right)=\unicode{x1D7D9}\left[\phi(C)&gt;\phi\left(C^{\prime}\right)\right]=: \unicode{x1D7D9} \left[C \succ C^{\prime}\right], \tag{3.3}\]</span></span></p>
<p>which represents the classifier preferred by the practitioner. We can then define the metric elicitation problem for populations:</p>
<div class="callout callout-style-default callout-note callout-titled" title="definition">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
definition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-def3.1" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.1</strong></span> Suppose the true (oracle) performance metric is <span class="math inline">\(\phi\)</span>. The goal is to recover a metric <span class="math inline">\(\hat{\phi}\)</span> by querying the oracle for as few pairwise comparisons of the form <span class="math inline">\(\Omega\left(C, C^{\prime}\right)\)</span> so that <span class="math inline">\(\|\phi - \hat{\phi}\|_{--} &lt; \kappa\)</span> for a sufficiently small <span class="math inline">\(\kappa &gt; 0\)</span> and for any suitable norm <span class="math inline">\(\|\cdot\|_{--}\)</span>.</p>
</div>
</div>
</div>
<p>In practice, we do not have access to the true probability distribution or the population, which would provide the true values of <span class="math inline">\(C\)</span> and <span class="math inline">\(C'\)</span>. However, we can subtly alter this problem description to use <span class="math inline">\(\hat{C}\)</span> and <span class="math inline">\(\hat{C}^{\prime}\)</span>, which are derived from our dataset of <span class="math inline">\(n\)</span> samples:</p>
<div class="callout callout-style-default callout-note callout-titled" title="definition">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
definition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-def3.2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.2</strong></span> Suppose the true (oracle) performance metric is <span class="math inline">\(\phi\)</span>. The aim is to recover a metric <span class="math inline">\(\hat{\phi}\)</span> by querying the oracle for as few pairwise comparisons of the form <span class="math inline">\(\Omega\left(\hat{C}, \hat{C}^{\prime}\right)\)</span> so that <span class="math inline">\(\|\phi - \hat{\phi}\|_{--} &lt; \kappa\)</span> for a sufficiently small <span class="math inline">\(\kappa &gt; 0\)</span> and for any suitable norm <span class="math inline">\(\|\cdot\|_{--}\)</span>.</p>
</div>
</div>
</div>
<p>As is common in theoretical ML research, we solve the population problem and then consider ways to extend this to practical settings where we only have limited datasets of samples. In our case, this corresponds to calculating the confusion matrices from a portion of the dataset we have access to.</p>
<section id="sec-orgb6dac4e" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="sec-orgb6dac4e"><span class="header-section-number">3.4.1</span> Linear Performance Metric Elicitation</h3>
<p>For LPM elicitation, we need one more proposition.</p>
<div class="callout callout-style-default callout-note callout-titled" title="proposition">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
proposition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="prp-prp3.2" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 3.2</strong></span> For a metric <span class="math inline">\(\psi\)</span> (quasiconvex and monotone increasing in TP/TN) or <span class="math inline">\(\phi\)</span> (quasiconcave and monotone increasing), and parametrization <span class="math inline">\(\rho^+\)</span>/<span class="math inline">\(\rho^-\)</span> of upper/lower boundary, composition <span class="math inline">\(\psi \circ \rho^-\)</span> is quasiconvex and unimodal on [0, 1], and <span class="math inline">\(\phi \circ \rho^+\)</span> is quasiconcave and unimodal on [0, 1].</p>
</div>
</div>
</div>
<p>Quasiconcavity and quasiconvexity are slightly more general variations on concavity and convexity. Their main useful property in our setting is that they are unimodal (they have a singular extremum), so we can devise a binary-search-style algorithm for eliciting the Bayes optimal and inverse-optimal confusion matrices for a given setting, as well as the corresponding <span class="math inline">\(\phi\)</span>’s. We first note that to maximize a quasiconcave metric, in which <span class="math inline">\(\phi\)</span> is monotonically increasing in <span class="math inline">\(TP\)</span> and <span class="math inline">\(TN\)</span>, we note that the resulting maximizer (and supporting hyperplane) will occur on the upper boundary of <span class="math inline">\(\mathcal{C}\)</span>. We thus set our initial search range to be <span class="math inline">\([0, \pi/2]\)</span> and repeatedly divide it into four regions. Then, we calculate the resulting confusion matrix on the 5 resulting boundaries of these regions and query the oracle <span class="math inline">\(4\)</span> times. We repeat this in each iteration of the binary search until a maximizer is found.</p>
<div class="callout callout-style-default callout-note callout-titled" title="remark">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
remark
</div>
</div>
<div class="callout-body-container callout-body">
<div id="rem-explaination_binary_search" class="proof remark">
<p><span class="proof-title"><em>Remark 3.1</em>. </span>In the case of quasiconcave and quasiconvex search ranges, a slightly more sophisticated variation on typical binary search must be used. To illustrate this, consider the two distributions in <a href="#fig-bsearch" class="quarto-xref">Figure&nbsp;<span>3.3</span></a>:</p>
<div id="fig-bsearch" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bsearch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<img src="Figures/normaldistribution.png" id="fig-normal-distribution" class="img-fluid figure-img" style="width:45.0%">
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-bsearch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3
</figcaption>
</figure>
</div>
<p>For both the symmetric and skewed distributions, if we were to divide the search range into two portions and compare <span class="math inline">\(A\)</span>, <span class="math inline">\(C\)</span>, and <span class="math inline">\(E\)</span>, we would find that <span class="math inline">\(C &gt; A\)</span> and <span class="math inline">\(C &gt; E\)</span>. In both cases, this does not help us reduce our search range, since the true maximum could lie on either of the two intervals (as in the second case), or at <span class="math inline">\(C\)</span> itself (as in the first case). Therefore, we must make comparisons between all five points <span class="math inline">\(A, B, C, D, and E\)</span>. This allows us to correctly restrict our search range to <span class="math inline">\([B, D]\)</span> in the first case and <span class="math inline">\([C, E]\)</span> in the second. These extra search requirements are due to the quasiconcavity of the search space we are considering, in which there exists a maximum but we need to make several comparisons at various points throughout the search space to be able to reduce its size in each iteration.</p>
</div>
</div>
</div>
<div id="alg-lpm" class="pseudocode-container quarto-float" data-caption-prefix="Algorithm" data-pseudocode-number="1">
<div class="pseudocode">
\begin{algorithm} \caption{Quasiconcave Metric Maximization} \begin{algorithmic} \State \textbf{input:} $\epsilon &gt; 0$ and oracle $\Omega$ \State \textbf{initialize:} $\theta_a = 0, \theta_b = \frac{\pi}{2}$ \While{$|\theta_b - \theta_a| &gt; \epsilon$} \State set $\theta_c = \frac{3\theta_a+\theta_b}{4}$, $\theta_d = \frac{\theta_a+\theta_b}{2}$, and $\theta_e = \frac{\theta_a+3\theta_b}{4}$ \State obtain $h\theta_a, h\theta_c, h\theta_d, h\theta_e, h\theta_b$ using Proposition 1 \State Compute $C\theta_a, C\theta_c, C\theta_d, C\theta_e, C\theta_b$ using (1) \State Query $\Omega(C\theta_c, C\theta_a), \Omega(C\theta_d, C\theta_c), \Omega(C\theta_e, C\theta_d)$, and $\Omega(C\theta_b, C\theta_e)$ \If{$q_{i,j}$ is ambiguous} \State request $q_{i,j}$'s label from reference \Else \State impute $q_{i,j}$'s label from previously labeled queries \EndIf \If{$C\theta' \succ C\theta'' \succ C\theta'''$ for consecutive $\theta &lt; \theta' &lt; \theta''$} \State assume the default order $C\theta \prec C\theta' \prec C\theta''$ \EndIf \If{$C\theta' \succ C\theta'' \succ C\theta'''$ for consecutive $\theta &lt; \theta' &lt; \theta''$} \State assume the default order $C\theta \prec C\theta' \prec C\theta''$ \EndIf \If{$C\theta_a \succ C\theta_c$} \State Set $\theta_b = \theta_d$ \ElsIf{$C\theta_a \prec C\theta_c \succ C\theta_d$} \State Set $\theta_b = \theta_d$ \ElsIf{$C\theta_c \prec C\theta_d \succ C\theta_e$} \State Set $\theta_a = \theta_c$ \State Set $\theta_b = \theta_e$ \ElsIf{$C\theta_d \prec C\theta_e \succ C\theta_b$} \State Set $\theta_a = \theta_d$ \Else \State Set $\theta_a = \theta_d$ \EndIf \EndWhile \State \textbf{output:} $\vec{m}, C$, and $\vec{l}$, where $\vec{m} = m_l(\theta_d), C = C\theta_d$, and $\vec{l} := (\vec{m}, (tp, tn)) = (\vec{m}, C)$ \end{algorithmic} \end{algorithm}
</div>
</div>
<p>To elicit LPMs, we run <a href="#alg-lpm" class="quarto-xref">Algorithm 1</a>, querying the oracle in each iteration, and set the elicited metric <span class="math inline">\(\hat{m}\)</span> (which is the maximizer on <span class="math inline">\(\mathcal{C}\)</span>) to be the slope of the resulting hyperplane, since the metric is linear.</p>
<div class="callout callout-style-default callout-note callout-titled" title="remark">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
remark
</div>
</div>
<div class="callout-body-container callout-body">
<div id="rem-explaination_lpm" class="proof remark">
<p><span class="proof-title"><em>Remark 3.2</em>. </span>To find the minimum of a quasiconvex metric, we flip all instances of <span class="math inline">\(\prec\)</span> and <span class="math inline">\(\succ\)</span>, and use an initial search range of <span class="math inline">\([\pi, 3\pi/2]\)</span>; we use this algorithm, which we refer to as <a href="#alg-lfpm" class="quarto-xref">Algorithm 2</a>, in our elicitation of LFPMs.</p>
</div>
</div>
</div>
<p>Next, we provide a Python implementation of <a href="#alg-lpm" class="quarto-xref">Algorithm 1</a>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
</div>
<div class="callout-body-container callout-body">
<div id="6c1a45be" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">def</span> get_m(theta):</span>
<span id="cb1-2"><a href="#cb1-2"></a>    <span class="co">"""</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co">    Inputs: </span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co">    - theta: the value that parametrizes m</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co">    Outputs:</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co">    - m_0 and m_1 for the LPM</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co">    """</span></span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a>    <span class="cf">return</span> (math.cos(theta), math.sin(theta))</span>
<span id="cb1-10"><a href="#cb1-10"></a></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="kw">def</span> lpm_elicitation(epsilon, oracle):</span>
<span id="cb1-12"><a href="#cb1-12"></a>    <span class="co">"""</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="co">    Inputs:</span></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="co">    - epsilon: some epsilon &gt; 0 representing threshold of error</span></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="co">    - oracle: some function that accepts 2 confusion matrices and</span></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="co">        returns true if the first is preferred and false otherwise</span></span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="co">    Outputs:</span></span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="co">    - estimate for m, which is used to compute the LPM as described above</span></span>
<span id="cb1-19"><a href="#cb1-19"></a><span class="co">    """</span></span>
<span id="cb1-20"><a href="#cb1-20"></a></span>
<span id="cb1-21"><a href="#cb1-21"></a>    a <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-22"><a href="#cb1-22"></a>    b <span class="op">=</span> math.pi<span class="op">/</span><span class="dv">2</span></span>
<span id="cb1-23"><a href="#cb1-23"></a>    <span class="cf">while</span> (b <span class="op">-</span> a <span class="op">&gt;</span> epsilon):</span>
<span id="cb1-24"><a href="#cb1-24"></a>        c <span class="op">=</span> (<span class="dv">3</span> <span class="op">*</span> a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb1-25"><a href="#cb1-25"></a>        d <span class="op">=</span> (a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb1-26"><a href="#cb1-26"></a>        e <span class="op">=</span> (a <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb1-27"><a href="#cb1-27"></a></span>
<span id="cb1-28"><a href="#cb1-28"></a>        m_a, m_b, m_c, m_d, m_e <span class="op">=</span> (get_m(x) <span class="cf">for</span> x <span class="kw">in</span> [a,b,c,d,e]) <span class="co"># using definition of m</span></span>
<span id="cb1-29"><a href="#cb1-29"></a>        c_a, c_b, c_c, c_d, c_e <span class="op">=</span> (get_c(x) <span class="cf">for</span> x <span class="kw">in</span> [m_a, m_b, m_c, m_d, m_e]) <span class="co"># compute classifier from m's then calculate confusion matrices</span></span>
<span id="cb1-30"><a href="#cb1-30"></a>        </span>
<span id="cb1-31"><a href="#cb1-31"></a>        response_ac <span class="op">=</span> oracle(c_a, c_c)</span>
<span id="cb1-32"><a href="#cb1-32"></a>        response_cd <span class="op">=</span> oracle(c_c, c_d)</span>
<span id="cb1-33"><a href="#cb1-33"></a>        response_de <span class="op">=</span> oracle(c_d, c_e)</span>
<span id="cb1-34"><a href="#cb1-34"></a>        response_eb <span class="op">=</span> oracle(c_e, c_b)</span>
<span id="cb1-35"><a href="#cb1-35"></a></span>
<span id="cb1-36"><a href="#cb1-36"></a>        <span class="co"># update ranges to keep the peak</span></span>
<span id="cb1-37"><a href="#cb1-37"></a>        <span class="cf">if</span> response_ac:</span>
<span id="cb1-38"><a href="#cb1-38"></a>            b <span class="op">=</span> d</span>
<span id="cb1-39"><a href="#cb1-39"></a>        <span class="cf">elif</span> response_cd:</span>
<span id="cb1-40"><a href="#cb1-40"></a>            b <span class="op">=</span> d</span>
<span id="cb1-41"><a href="#cb1-41"></a>        <span class="cf">elif</span> response_de:</span>
<span id="cb1-42"><a href="#cb1-42"></a>            a <span class="op">=</span> c</span>
<span id="cb1-43"><a href="#cb1-43"></a>            b <span class="op">=</span> e</span>
<span id="cb1-44"><a href="#cb1-44"></a>        <span class="cf">elif</span> response_eb:</span>
<span id="cb1-45"><a href="#cb1-45"></a>            a <span class="op">=</span> d</span>
<span id="cb1-46"><a href="#cb1-46"></a>        <span class="cf">else</span>:</span>
<span id="cb1-47"><a href="#cb1-47"></a>            a <span class="op">=</span> d</span>
<span id="cb1-48"><a href="#cb1-48"></a>    <span class="cf">return</span> get_m(d), get_c(d)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="sec-lfpm-elicitation" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="sec-lfpm-elicitation"><span class="header-section-number">3.4.2</span> Linear-Fractional Performance Metric Elicitation</h3>
<p>Now, we present the next main result, which is an algorithm to elicit linear-fractional performance metrics. For this task, we will need the following assumption: Let <span class="math inline">\(\phi \in \varphi_{L F P M}\)</span>. We assume <span class="math inline">\(p_{11}, p_{00} \geq 0, p_{11} \geq q_{11}, p_{00} \geq q_{00},\)</span> <span class="math inline">\(p_{0}=0, q_{0}=\)</span> <span class="math inline">\(\left(p_{11}-q_{11}\right) \zeta+\left(p_{00}-q_{00}\right)(1-\zeta)\)</span>, and <span class="math inline">\(p_{11}+p_{00}=1\)</span>.</p>
<p>These assumptions guarantee that the LFPM <span class="math inline">\(\phi\)</span> which we are trying to elicit is monotonically increasing in <span class="math inline">\(TP\)</span> and <span class="math inline">\(TN\)</span>, just as in the LPM elicitation case. We first provide motivation and an overview of the approach for LFPM elicitation and then present pseudocode for the algorithm.</p>
<p>The general idea of the algorithm is to use <a href="#alg-lpm" class="quarto-xref">Algorithm 1</a> to obtain a maximizer and a minimizer for the given dataset; these result in two systems of equations involving the true LFPM <span class="math inline">\(\phi^*\)</span> with 1 degree of freedom. Then, we run a grid search that is independent of oracle queries to find the point where solutions to the systems match pointwise on the resulting confusion matrices; this occurs close to where the true metric lies.</p>
<p>More formally, suppose that the true metric is <span id="eq-eq3.48"><span class="math display">\[\phi^{*}(C)=\frac{p_{11}^{*} T P+p_{00}^{*} T N}{q_{11}^{*} T P+q_{00}^{*} T N+q_{0}^{*}}. \tag{3.4}\]</span></span> Then, let <span class="math inline">\(\bar{\tau}\)</span> and <span class="math inline">\(\underline{\tau}\)</span> represent the maximizer and minimizer of <span class="math inline">\(\phi\)</span> over <span class="math inline">\(\mathcal{C}\)</span>, respectively. There exists a hyperplane <span class="math display">\[\begin{aligned}
\bar{\ell}_{f}^{*}:=\left(p_{11}^{*}-\bar{\tau}^{*} q_{11}^{*}\right) t p+\left(p_{00}^{*}-\bar{\tau}^{*} q_{00}^{*}\right) t n=\bar{\tau}^{*} q_{0}^{*},
\end{aligned}\]</span> which touches <span class="math inline">\(\mathcal{C}\)</span> at <span class="math inline">\(\left(\overline{T P}^{*}, \overline{T N}^{*}\right)\)</span> on <span class="math inline">\(\partial \mathcal{C}_{+}\)</span>. Correspondingly, there also exists a hyperplane <span class="math inline">\(\underline{\ell}_{f}^{*}:=\left(p_{11}^{*}-\underline{\tau}^{*} q_{11}^{*}\right) t p+\left(p_{00}^{*}-\underline{\tau}^{*} q_{00}^{*}\right) \operatorname{tn}=\underline{\tau}^{*} q_{0}^{*}\)</span>, which touches <span class="math inline">\(\mathcal{C}\)</span> at <span class="math inline">\(\left(\underline{TP}^{*}, \underline{T N}^{*}\right)\)</span> on <span class="math inline">\(\partial \mathcal{C}_{-}\)</span>. While we are unable to obtain <a href="#eq-eq3.48" class="quarto-xref">Equation&nbsp;<span>3.4</span></a> and <span class="quarto-unresolved-ref">?eq-eq3.49</span> directly, we can use <a href="#alg-lpm" class="quarto-xref">Algorithm 1</a> to get a hyperplane <span id="eq-eq3.51"><span class="math display">\[\bar{\ell}:=\bar{m}_{11} t p+\bar{m}_{00} t n= \bar{m}_{11} \overline{T P}^{*}+\bar{m}_{00} \overline{T N}^{*} = \bar{C}_{0}, \tag{3.5}\]</span></span> which is equivalent to <span class="math inline">\(\bar{\ell}_{f}^{*}\)</span> (<a href="#eq-eq3.48" class="quarto-xref">Equation&nbsp;<span>3.4</span></a>) up to a constant multiple. From here, we can obtain the system of equations</p>
<p><span id="eq-eq3.52"><span class="math display">\[p_{11}^{*}-\bar{\tau}^{*} q_{11}^{*}=\alpha \bar{m}_{11}, p_{00}^{*}-\bar{\tau}^{*} q_{00}^{*}=\alpha \bar{m}_{00}, \bar{\tau}^{*} q_{0}^{*}=\alpha \bar{C}_{0}, \tag{3.6}\]</span></span> where <span class="math inline">\(\alpha &gt; 0\)</span> (we know it is <span class="math inline">\(\geq0\)</span> due to our assumptions earlier and because <span class="math inline">\(\bar{m}\)</span> is positive, but if it is equal to <span class="math inline">\(0\)</span> then <span class="math inline">\(\phi^*\)</span> would be constant. So, our resulting system of equations is <span id="eq-eq3.53"><span class="math display">\[\begin{aligned}
    p_{11}^{\prime}-\bar{\tau}^{*} q_{11}^{\prime}=\bar{m}_{11}, p_{00}^{\prime}-\bar{\tau}^{*} q_{00}^{\prime}=\bar{m}_{00}, \bar{\tau}^{*} q_{0}^{\prime}=\bar{C}_{0}.
\end{aligned} \tag{3.7}\]</span></span></p>
<p>Now, similarly, we can approximate <span class="quarto-unresolved-ref">?eq-eq3.49</span> using the algorithm we defined for quasiconvex metrics (<a href="#alg-lfpm" class="quarto-xref">Algorithm 2</a>), where we altered the search range and comparisons. After finding the minimizer, we obtain the hyperplane <span id="eq-eq3.54"><span class="math display">\[\underline{\ell}:=\underline{m}_{11} t p+\underline{m}_{00} t n=\underline{m}_{11} \underline{TP}^{*}+\underline{m}_{00} \underline{TN}^{*} = \underline{C}_{0}, \tag{3.8}\]</span></span> which is equivalent to <span class="math inline">\(\underline{\ell}_{f}^{*}\)</span> (<span class="quarto-unresolved-ref">?eq-eq3.49</span>) up to a constant multiple. So then, our system of equations is <span id="eq-eq3.55"><span class="math display">\[p_{11}^{*}-\underline{\tau}^{*} q_{11}^{*}=\gamma \underline{m}_{11}, p_{00}^{*}-\underline{\tau}^{*} q_{00}^{*}=\gamma \underline{m}_{00}, \underline{\tau}^{*} q_{0}^{*}=\gamma \underline{C}_{0}, \tag{3.9}\]</span></span> where <span class="math inline">\(\gamma &lt;0\)</span> (for a reason analogous to why we have <span class="math inline">\(\alpha &gt;0\)</span>), meaning our resulting system of equations is <span id="eq-eq3.56"><span class="math display">\[\begin{aligned}
    p_{11}^{\prime \prime}-\underline{\tau}^{*} q_{11}^{\prime \prime}=\underline{m}_{11}, p_{00}^{\prime \prime}-\underline{\tau}^{*} q_{00}^{\prime \prime}=\underline{m}_{00}, \underline{\tau}^{*} q_{0}^{\prime \prime}=\underline{C}_{0}.
\end{aligned} \tag{3.10}\]</span></span></p>
<p><a href="#eq-eq3.55" class="quarto-xref">Equation&nbsp;<span>3.9</span></a> and <a href="#eq-eq3.56" class="quarto-xref">Equation&nbsp;<span>3.10</span></a> form the two systems of equations mentioned in our overview of the algorithm. Next, we demonstrate that they have only one degree of freedom. Note that if we know <span class="math inline">\(p_{11}'\)</span>, we could solve both systems of equations as follows: <span id="eq-eq3.57"><span class="math display">\[\begin{aligned}
    p_{00}^{\prime}  &amp;=1-p_{11}^{\prime}, q_{0}^{\prime}=\bar{C}_{0} \frac{P^{\prime}}{Q^{\prime}}\\
    q_{11}^{\prime}  &amp;=\left(p_{11}^{\prime}-\bar{m}_{11}\right) \frac{P^{\prime}}{Q^{\prime}} \\
    q_{00}^{\prime}&amp;=\left(p_{00}^{\prime}-\bar{m}_{00}\right) \frac{P^{\prime}}{Q^{\prime}},
\end{aligned} \tag{3.11}\]</span></span> where <span class="math inline">\(P^{\prime}=p_{11}^{\prime} \zeta+p_{00}^{\prime}(1-\zeta)\)</span> and <span class="math inline">\(Q^{\prime}=P^{\prime}+\bar{C}_{0}-\)</span> <span class="math inline">\(\bar{m}_{11} \zeta-\bar{m}_{00}(1-\zeta).\)</span></p>
<p>Now, suppose we know <span class="math inline">\(p_{11}'\)</span>. We could use this value to solve both systems <a href="#eq-eq3.55" class="quarto-xref">Equation&nbsp;<span>3.9</span></a> and <a href="#eq-eq3.56" class="quarto-xref">Equation&nbsp;<span>3.10</span></a>, yielding two metrics, <span class="math inline">\(\phi'\)</span> and <span class="math inline">\(\phi''\)</span>, from the maximizer and minimizer, respectively. Importantly, when <span class="math inline">\(p_{11}^{*} / p_{00}^{*}=p_{11}^{\prime} / p_{00}^{\prime}=p_{11}^{\prime \prime} / p_{00}^{\prime \prime}\)</span>, then <span class="math inline">\(\phi^{*}(C)=\phi^{\prime}(C) / \alpha=-\phi^{\prime \prime}(C) / \gamma\)</span>. Essentially, when we find a value of <span class="math inline">\(p_{11}'\)</span> that results in <span class="math inline">\(\phi'\)</span> and <span class="math inline">\(\phi''\)</span> h aving constant ratios at all points on the boundary of <span class="math inline">\(\mathcal{C}\)</span>, we can obtain <span class="math inline">\(\phi^*\)</span>, as it is derivable from <span class="math inline">\(\phi'\)</span> and <span class="math inline">\(\alpha\)</span> (or, alternatively, <span class="math inline">\(\phi''\)</span> and <span class="math inline">\(\gamma\)</span>).</p>
<p>We will perform a grid search for <span class="math inline">\(p_{11}'\)</span> on <span class="math inline">\([0,1]\)</span>. For each point in our search, we will compute <span class="math inline">\(\phi'\)</span> and <span class="math inline">\(\phi''\)</span>. Then, we will generate several confusion matrices on the boundaries and calculate the ratio $’’ / <span class="math inline">\(\phi'\)</span> for each. We will select the value of <span class="math inline">\(p_{11}'\)</span> for which the ratio <span class="math inline">\(\phi'' / \phi'\)</span> is closest to constant and use it to compute the elicited metric <span class="math inline">\(\hat{\phi}\)</span>. The pseudocode for LFPM elicitation is given in <a href="#alg-lfpm" class="quarto-xref">Algorithm 2</a>.</p>
<div id="alg-lfpm" class="pseudocode-container quarto-float" data-caption-prefix="Algorithm" data-pseudocode-number="2">
<div class="pseudocode">
\begin{algorithm} \caption{Grid Search for Best Ratio} \begin{algorithmic} \State \textbf{Input:} $k, \Delta$. \State \textbf{Initialize:} $\sigma_{\text{opt}} = \infty, p'_{11,\text{opt}} = 0$. \State Generate $C_1, \dots, C_k$ on $\partial C_+$ and $\partial C_-$ (Section 3). \State Generate $C_1, \dots, C_k$ on $\partial C_+$ and $\partial C_-$ (Section 3). \For{$p'_{11} = 0; \; p'_{11} \leq 1; \; p'_{11} = p'_{11} + \Delta$} \State Compute $\phi'$, $\phi''$ using Proposition 4. \State Compute array $r = \left[ \frac{\phi'(C_1)}{\phi''(C_1)}, \dots, \frac{\phi'(C_k)}{\phi''(C_k)} \right]$. \State Set $\sigma = \text{std}(r)$. \If{$\sigma &lt; \sigma_{\text{opt}}$} \State Set $\sigma_{\text{opt}} = \sigma$ and $p'_{11,\text{opt}} = p'_{11}$. \EndIf \EndFor \State \textbf{Output:} $p'_{11,\text{opt}}$. \end{algorithmic} \end{algorithm}
</div>
</div>
<p>We provide a Python implementation as below.</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
</div>
<div class="callout-body-container callout-body">
<div id="d992f9b4" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">def</span> lfpm_elicitation(k, delta):</span>
<span id="cb2-2"><a href="#cb2-2"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="co">    Inputs:</span></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="co">    - k: the number of confusion matrices to evaluate on</span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="co">    - delta: the spacing for the grid search</span></span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="co">    Outputs:</span></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="co">    - p_11', which will allow us to compute the elicited LFPM</span></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="co">    """</span></span>
<span id="cb2-9"><a href="#cb2-9"></a></span>
<span id="cb2-10"><a href="#cb2-10"></a>    sigma_opt <span class="op">=</span> np.inf</span>
<span id="cb2-11"><a href="#cb2-11"></a>    p11_opt <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-12"><a href="#cb2-12"></a>    C <span class="op">=</span> compute_confusion_matrices(k) <span class="co"># generates k confusion matrices to evaluate on</span></span>
<span id="cb2-13"><a href="#cb2-13"></a></span>
<span id="cb2-14"><a href="#cb2-14"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">int</span>(<span class="dv">1</span><span class="op">/</span>delta)):</span>
<span id="cb2-15"><a href="#cb2-15"></a>        p11 <span class="op">=</span> i <span class="op">*</span> delta</span>
<span id="cb2-16"><a href="#cb2-16"></a>        phi1 <span class="op">=</span> compute_upper_metric(p11) <span class="co"># solves the first system of equations with p11 </span></span>
<span id="cb2-17"><a href="#cb2-17"></a>        phi2 <span class="op">=</span> compute_lower_metric(p11) <span class="co"># solves the second system of equations with p11 </span></span>
<span id="cb2-18"><a href="#cb2-18"></a>        utility_1 <span class="op">=</span> [phi1(c) <span class="cf">for</span> c <span class="kw">in</span> C] <span class="co">#calculate phi for both systems of equations</span></span>
<span id="cb2-19"><a href="#cb2-19"></a>        utility_2 <span class="op">=</span> [phi2(c) <span class="cf">for</span> c <span class="kw">in</span> C]</span>
<span id="cb2-20"><a href="#cb2-20"></a></span>
<span id="cb2-21"><a href="#cb2-21"></a>        r <span class="op">=</span> []</span>
<span id="cb2-22"><a href="#cb2-22"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb2-23"><a href="#cb2-23"></a>            r.append(utility_1[i] <span class="op">/</span> utility_2[i])</span>
<span id="cb2-24"><a href="#cb2-24"></a>        sigma <span class="op">=</span> np.std(r)</span>
<span id="cb2-25"><a href="#cb2-25"></a></span>
<span id="cb2-26"><a href="#cb2-26"></a>        <span class="cf">if</span>(sigma <span class="op">&lt;</span> sigma_opt):</span>
<span id="cb2-27"><a href="#cb2-27"></a>            sigma_opt <span class="op">=</span> sigma</span>
<span id="cb2-28"><a href="#cb2-28"></a>            p11_opt <span class="op">=</span> p11</span>
<span id="cb2-29"><a href="#cb2-29"></a>    <span class="cf">return</span> p11_opt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>In summary, to elicit LFPMs, we utilize a special property of the LPM minimizer and maximizer on <span class="math inline">\(\mathcal{C}\)</span>–namely, that we can use the corresponding supporting hyperplanes to form a system of equations that can be used to approximate <span class="math inline">\(\phi^*\)</span> if one parameter (<span class="math inline">\(p_{11}'\)</span>) is found, and that this parameter can be found using an oracle-independent grid search. Importantly, these algorithms can be shown to satisfy significant theoretical guarantees. We provide formal statement and intuitive interpretation of these guarantees here, with their proofs available in the appendix of the original paper. First, we define the oracle noise <span class="math inline">\(\epsilon_{\Omega}\)</span>, which arises from the oracle potentially flipping the comparison output on two confusion matrices that are close enough in utility.</p>
<p>Given <span class="math inline">\(\epsilon, \epsilon_{\Omega} \geq 0\)</span> and a metric <span class="math inline">\(\phi\)</span> satisfying our assumptions, <a href="#alg-lpm" class="quarto-xref">Algorithm 1</a> or <a href="#alg-lfpm" class="quarto-xref">Algorithm 2</a> finds an approximate maximizer/minimizer and supporting hyperplane. Additionally, the value of <span class="math inline">\(\phi\)</span> at that point is within <span class="math inline">\(O\left(\sqrt{\epsilon_{\Omega}} + \epsilon\right)\)</span> of the optimum, and the number of queries is <span class="math inline">\(O\left(\log \frac{1}{\epsilon}\right)\)</span>. Let <span class="math inline">\(\mathbf{m}^{*}\)</span> be the true performance metric. Given <span class="math inline">\(\epsilon &gt; 0\)</span>, LPM elicitation outputs a performance metric <span class="math inline">\(\hat{\mathbf{m}}\)</span>, such that <span class="math inline">\(\left\|\mathbf{m}^{*} - \hat{\mathbf{m}}\right\|_{\infty} \leq \sqrt{2} \epsilon + \frac{2}{k_{0}} \sqrt{2 k_{1} \epsilon_{\Omega}}\)</span>. These results ensure that <a href="#alg-lpm" class="quarto-xref">Algorithm 1</a> and <a href="#alg-lfpm" class="quarto-xref">Algorithm 2</a> find an appropriate maximizer and minimizer in the search space, within a certain range of accuracy that depends on oracle and sample noise, and within a certain number of queries. Both of these statements are guaranteed by the binary search approach.</p>
<p>Let <span class="math inline">\(h_{\theta}\)</span> and <span class="math inline">\(\hat{h}_{\theta}\)</span> be two classifiers estimated using <span class="math inline">\(\eta\)</span> and <span class="math inline">\(\hat{\eta}\)</span>, respectively. Further, let <span class="math inline">\(\bar{\theta}\)</span> be such that <span class="math inline">\(h_{\bar{\theta}} = \arg \max _{\theta} \phi\left(h_{\theta}\right)\)</span>. Then <span class="math inline">\(\|C(\hat{h}_{\bar{\theta}}) - C\left(h_{\bar{\theta}}\right)\|_{\infty} = O\left(\left\|\hat{\eta}_{n} - \eta\right\|_{\infty}\right)\)</span>. This result indicates that the drop in elicited metric quality caused by using a dataset of samples rather than population confusion matrices is bounded by the drop in performance of the decision boundary <span class="math inline">\(\eta\)</span>. These three guarantees together ensure that oracle noise and sample noise do not amplify drops in performance when using metric elicitation; rather, these drops in performance are bounded by the drops that would typically occur when using the standard machine learning paradigm of training a decision boundary and using a pre-established metric. For further interesting exploration of the types of problems that can be solved using the framework of metric elicitation, we refer the reader to <span class="citation" data-cites="nips">(<a href="#ref-nips" role="doc-biblioref">Hiranandani, Narasimhan, and Koyejo 2020</a>)</span>, which performs metric elicitation to determine the oracle’s ideal tradeoff between the classifier’s overall performance and the discrepancy between its performance on certain protected groups.</p>
</section>
<section id="multiclass-performance-metric-elicitation" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="multiclass-performance-metric-elicitation"><span class="header-section-number">3.4.3</span> Multiclass Performance Metric Elicitation</h3>
<p>Although the previous section only described metric elicitation for binary classification problems, the general framework can still be applied to multiclass classification problems<span class="citation" data-cites="NEURIPS2019_1fd09c5f">(<a href="#ref-NEURIPS2019_1fd09c5f" role="doc-biblioref">Hiranandani et al. 2019b</a>)</span>. Consider the case of classifying subtypes of leukemia <span class="citation" data-cites="YangNaiman+2014+477+496">(<a href="#ref-YangNaiman+2014+477+496" role="doc-biblioref">Yang and Naiman 2014</a>)</span>. We can train a neural network to predict conditional probability of a certain leukemia subtype given certain gene expressions. However, it may not be appropriate to classify the subtype purely based on whichever one has the highest confidence. For instance, a treatment for leukemia subtype C1 may be perfect for cases of C1, but it may be ineffective or harmful for certain other subtypes. Therefore, the final response from the classifier may not be as simple as as choosing the class with the highest conditional probability, just like how the threshold for binary classification may not always be 50%. With multiclass metric elicitation, we can show confusion matrices to an oracle (like the doctor in the leukemia example) to determine which classifier has the best tradeoffs. In <span class="citation" data-cites="NEURIPS2019_1fd09c5f">(<a href="#ref-NEURIPS2019_1fd09c5f" role="doc-biblioref">Hiranandani et al. 2019b</a>)</span>, the authors focus on eliciting linear performance metrics, which is what we will describe in this chapter. Most of the notation from Binary Metric Elicitation still persists, just modified to provide categorical responses. <span class="math inline">\(X \in \mathcal{X}\)</span> is the input random variable. <span class="math inline">\(Y \in [k]\)</span> is the output random variable, where <span class="math inline">\([k]\)</span> is the index set <span class="math inline">\(\{1, 2, \dots, k\}\)</span>.</p>
<p>The dataset of size <span class="math inline">\(n\)</span> is denoted by <span class="math inline">\(\{(\vec{x}, y)\}_{i=1}^n\)</span> generated independently and identically from <span class="math inline">\(\mathbb{P}(X, Y)\)</span>. <span class="math inline">\(\eta_i(\vec{x}) = \mathbb{P}(Y=i | X=\vec{x})\)</span> gives the conditional probability of class <span class="math inline">\(i \in [k]\)</span> given an observation. <span class="math inline">\(\xi_i = \mathbb{P}(Y=i)\)</span> is the marginal probability of class <span class="math inline">\(i \in [k]\)</span>. The set of all classifiers is <span class="math inline">\(\mathcal{H} = \{h : \mathcal{X} \rightarrow \Delta_k\}\)</span>, where <span class="math inline">\(\Delta_k\)</span> is (k-1) dimensional simplex. In this case, the outputs of classifiers are 1-hot vectors of size <span class="math inline">\(k\)</span> where the only index with value 1 is the predicted class and all other positions have a value of 0. The confusion matrix for a classifier, <span class="math inline">\(h\)</span>, is <span class="math inline">\(C(h, \mathbb{P}) \in \mathbb{R}^{k \times k}\)</span>, where:</p>
<p><span id="eq-eq3.59"><span class="math display">\[C_{ij}(h, \mathbb{P}) = \mathbb{P}(Y=i, h=j) \text{\qquad for } i, j \in [k] \tag{3.12}\]</span></span></p>
<p>Note that the confusion matrices are <span class="math inline">\(k\times k\)</span> and store the joint probabilities of each type of classification for each possible class. This means that the sum of row <span class="math inline">\(i\)</span> in the confusion matrix equals <span class="math inline">\(\xi_i\)</span>, because this is equivalent to adding over all possible classifications. Since we know the sums of each row, all diagonal elements can be reconstructed from just the off-diagonal elements, so a confusion matrix <span class="math inline">\(C(h, \mathbb{P})\)</span> can be expressed as a vector of off-diagonal elements, <span class="math inline">\(\vec{c}(h, \mathbb{P}) = \textit{off-diag}(C(h, \mathbb{P}))\)</span>, and <span class="math inline">\(\vec{c} \in \mathbb{R}^q\)</span> where <span class="math inline">\(q := k^2 - k\)</span>. The vector <span class="math inline">\(\vec{c}\)</span> is called the vector of <em>‘off-diagonal confusions.’</em> The space of off-diagonal confusions is <span class="math inline">\(\mathcal{C} = \{\vec{c}(h, \mathbb{P}) : h \in \mathcal{H}\}\)</span>.</p>
<p>In cases where the oracle would care about the exact type of misclassification (i.e.&nbsp;misclassifying and object from class 1 as class 2), this off-diagonal confusion matrix is necessary. However, there are many cases where the performance of a classifier is determined by just the probability of correct prediction for each class, which just requires the diagonal elements. In these cases, we can define the vector of <em>‘diagonal confusions’</em> as <span class="math inline">\(\vec{d}(h, \mathbb{P}) = \textit{diag}(C(h, \mathbb{P})) \in \mathbb{R}^k\)</span>. The space of diagonal confusions is <span class="math inline">\(\mathcal{D} = \{\vec{d}(h, \mathbb{P}) : h \in \mathcal{H}\}\)</span>.</p>
<p>Finally, the setup for metric elicitation is identical to the one examined in the previous chapter. We still assume access to an oracle that can choose between two classifiers or confusion matrices, using notation <span class="math inline">\(\Gamma\)</span> for comparing two classifiers and <span class="math inline">\(\Omega\)</span> for comparing confusion matrices, which returns 1 if the first classifier is better and 0 otherwise. We still assume that the oracle behaves according to some unknown performance metric, and we wish to recover this metric up to some small error tolerance (based on a suitable norm). The two different types of confusion vectors result in different algorithms for metric elicitation, which we will explore in later sections.</p>
<p>A Diagonal Linear Performance Metric (DLPM) is a performance metric that only considers the diagonal elements in the confusion matrix. The metric is defined as <span class="math inline">\(\psi(\vec{d}) = \langle \vec{a}, \vec{d} \rangle\)</span>, where <span class="math inline">\(\vec{a} \in \mathbb{R}^k\)</span> such that <span class="math inline">\(||\vec{a}||_1 = 1\)</span>. It is also called weighted accuracy <span class="citation" data-cites="pmlr-v37-narasimhanb15">(<a href="#ref-pmlr-v37-narasimhanb15" role="doc-biblioref">Narasimhan et al. 2015</a>)</span>. The family of DLPMs is denoted as <span class="math inline">\(\varphi_{DLPM}\)</span>. Since these only consider the diagonal elements, which we want to maximize, we can focus on only eliciting monotonically increasing DLPMs, meaning that all elements in <span class="math inline">\(\vec{a}\)</span> are non-negative.</p>
<p>Consider the trivial classifiers that only predict a single class at all times. The diagonal confusions when only predicting class <span class="math inline">\(i\)</span> are <span class="math inline">\(\vec{v}_i \in \mathbb{R}^k\)</span> with <span class="math inline">\(\xi_i\)</span> at index <span class="math inline">\(i\)</span> and zero elsewhere. Note that this is the maximum possible value in index <span class="math inline">\(i\)</span>, because this represents perfectly classifying all points that have a true class of <span class="math inline">\(i\)</span>. We can consider the space of diagonal confusions, visualized in <a href="#fig-diag_geom" class="quarto-xref">Figure&nbsp;<span>3.4</span></a> (taken from <span class="citation" data-cites="NEURIPS2019_1fd09c5f">(<a href="#ref-NEURIPS2019_1fd09c5f" role="doc-biblioref">Hiranandani et al. 2019b</a>)</span>). The space of <span class="math inline">\(\mathcal{D}\)</span> is strictly convex, closed, and contained in the box <span class="math inline">\([0, \xi_1] \times \dots \times [0, \xi_k]\)</span>. We also know that the only vertices are <span class="math inline">\(\vec{v}_i\)</span> for each <span class="math inline">\(i \in [k]^{(k-1)}\)</span>.</p>
<div id="fig-diag_geom" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-diag_geom-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/diag_geometry.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diag_geom-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: (a) Geometry of space of diagonal confusions for <span class="math inline">\(k=3\)</span>. This is a convex region with three flat areas representing confusions when restricted to only two classes. (b) Geometry of diagonal confusions when restricted to classes <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span>. Notice how this is identical to the space of confusion matrices examined in the previous chapter.
</figcaption>
</figure>
</div>
<p>We know that this is strictly convex under the assumption that an object from any class can be misclassified as any other class. Mathematically, the assumption is that <span class="math inline">\(g_{ij}(r) = \mathbb{P} \left[\frac{\eta_i(X)}{\eta_j(X)} \geq r \right]\)</span> <span class="math inline">\(\forall i, j \in [k]\)</span> are continuous and strictly decreasing for <span class="math inline">\(r \in [0, \infty)\)</span>.</p>
<p>We can also define the space of binary classification confusion matrices confined to classes <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span>, which is the 2-D <span class="math inline">\((k_1, k_2)\)</span> axis-aligned face of <span class="math inline">\(\mathcal{D}\)</span>, denoted as <span class="math inline">\(\mathcal{D}_{k_1, k_2}\)</span>. Note that this is strictly convex, since <span class="math inline">\(\mathcal{D}\)</span> itself is strictly convex, and it has the same geometry as the space of binary confusion matrices examined in the previous chapter. Therefore, we can construct an RBO classifier for <span class="math inline">\(\psi \in \varphi_{DLPM}\)</span>, parameterized by <span class="math inline">\(\vec{a}\)</span>, as follows: <span id="eq-rbo_eq"><span class="math display">\[\begin{aligned}
\bar{h}_{k_1, k_2}(\vec{x})= \left\{
\begin{array}{ll}
      k_1, \text{ if } a_{k_1} \eta_{k_1}(\vec{x}) \geq a_{k_2} \eta_{k_2}(\vec{x})\\
k_2, \text{ o.w.}
\end{array}
\right\}.
\end{aligned} \tag{3.13}\]</span></span></p>
<p>We can parameterize the upper boundary of <span class="math inline">\(\mathcal{D}_{k_1, k_2}\)</span>, denoted as <span class="math inline">\(\partial \mathcal{D}^{+}_{k_1, k_2}\)</span>, using a single parameter <span class="math inline">\(m \in [0, 1]\)</span>. Specifically, we can construct a DLPM by setting <span class="math inline">\(a_{k_1} = m\)</span>, <span class="math inline">\(a_{k_2} = 1 - m\)</span>, and all others to 0. Using <a href="#eq-rbo_eq" class="quarto-xref">Equation&nbsp;<span>3.13</span></a>, we can get the diagonal confusions, so varying <span class="math inline">\(m\)</span> parameterizes <span class="math inline">\(\partial \mathcal{D}^{+}_{k_1, k_2}\)</span>. The parameterization is denoted as <span class="math inline">\(\nu(m; k_1, k_2)\)</span>.</p>
<section id="diagonal-linear-performance-metric-elicitation" class="level4" data-number="3.4.3.1">
<h4 data-number="3.4.3.1" class="anchored" data-anchor-id="diagonal-linear-performance-metric-elicitation"><span class="header-section-number">3.4.3.1</span> Diagonal Linear Performance Metric Elicitation</h4>
<p>Suppose the oracle follows a true metric, <span class="math inline">\(\psi\)</span>, that is linear and monotone increasing across all axes. If we consider the composition <span class="math inline">\(\psi \circ \nu(m; k_1, k_2): [0, 1] \rightarrow \mathbb{R}\)</span>, we know it must be concave and unimodal, because <span class="math inline">\(\mathcal{D}_{k_1, k_2}\)</span> is a convex set. Therefore, we can find the value of <span class="math inline">\(m\)</span> that maximizes <span class="math inline">\(\psi \circ \nu(m; k_1, k_2)\)</span> for any given <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span> using a binary search procedure.</p>
<p>Since the RBO classifier for classes <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span> only rely on the relative weights of the classes in the DLPM (see <a href="#eq-rbo_eq" class="quarto-xref">Equation&nbsp;<span>3.13</span></a>), finding the value of <span class="math inline">\(m\)</span> that maximizes <span class="math inline">\(\psi \circ \nu(m; k_1, k_2)\)</span> gives us the true relative ratio between <span class="math inline">\(a_{k_1}\)</span> and <span class="math inline">\(a_{k_2}\)</span>. Specifically, from the definition of <span class="math inline">\(\nu\)</span>, we know that <span class="math inline">\(\frac{a_{k_2}}{a_{k_1}} = \frac{1-m}{m}\)</span>. We can therefore simply calculate the ratio between <span class="math inline">\(a_1\)</span> and all other weights to reconstruct an estimate for the true metric. A python implementation of this algorithm is provided below.</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
</div>
<div class="callout-body-container callout-body">
<div id="99cfd7a5" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2"></a></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="kw">def</span> rbo_dlpm(m, k1, k2, k):</span>
<span id="cb3-4"><a href="#cb3-4"></a>    <span class="co">"""</span></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="co">    This constructs DLPM weights for the upper boundary of the</span></span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="co">    restricted diagonal confusions, given a parameter m.</span></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="co">    This is equivalent to </span><span class="ch">\n</span><span class="co">u(m; k1, k2)</span></span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="co">    </span></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="co">    Inputs:</span></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="co">    - m: parameter (between 0 and 1) for the upper boundary</span></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="co">    - k1: first axis for this  face</span></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="co">    - k2: second axis for this face</span></span>
<span id="cb3-13"><a href="#cb3-13"></a><span class="co">    - k: number of classes</span></span>
<span id="cb3-14"><a href="#cb3-14"></a><span class="co">    Outputs:</span></span>
<span id="cb3-15"><a href="#cb3-15"></a><span class="co">    - DLPM weights for this point on the upper boundary</span></span>
<span id="cb3-16"><a href="#cb3-16"></a><span class="co">    """</span></span>
<span id="cb3-17"><a href="#cb3-17"></a>    new_a <span class="op">=</span> np.zeros(k)</span>
<span id="cb3-18"><a href="#cb3-18"></a>    new_a[k1] <span class="op">=</span> m</span>
<span id="cb3-19"><a href="#cb3-19"></a>    new_a[k2] <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> m</span>
<span id="cb3-20"><a href="#cb3-20"></a>    <span class="cf">return</span> new_a</span>
<span id="cb3-21"><a href="#cb3-21"></a></span>
<span id="cb3-22"><a href="#cb3-22"></a><span class="kw">def</span> dlpm_elicitation(epsilon, oracle, get_d, k):</span>
<span id="cb3-23"><a href="#cb3-23"></a>    <span class="co">"""</span></span>
<span id="cb3-24"><a href="#cb3-24"></a><span class="co">    Inputs:</span></span>
<span id="cb3-25"><a href="#cb3-25"></a><span class="co">    - epsilon: some epsilon &gt; 0 representing threshold of error</span></span>
<span id="cb3-26"><a href="#cb3-26"></a><span class="co">    - oracle: some function that accepts 2 confusion matrices and</span></span>
<span id="cb3-27"><a href="#cb3-27"></a><span class="co">        returns true if the first is preferred and false otherwise</span></span>
<span id="cb3-28"><a href="#cb3-28"></a><span class="co">    - get_d: some function that accepts dlpm weights and returns </span></span>
<span id="cb3-29"><a href="#cb3-29"></a><span class="co">        diagonal confusions</span></span>
<span id="cb3-30"><a href="#cb3-30"></a><span class="co">    - k: number of classes</span></span>
<span id="cb3-31"><a href="#cb3-31"></a><span class="co">    Outputs:</span></span>
<span id="cb3-32"><a href="#cb3-32"></a><span class="co">    - estimate for true DLPM weights</span></span>
<span id="cb3-33"><a href="#cb3-33"></a><span class="co">    """</span></span>
<span id="cb3-34"><a href="#cb3-34"></a>    a_hat <span class="op">=</span> np.zeros(k)</span>
<span id="cb3-35"><a href="#cb3-35"></a>    a_hat[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-36"><a href="#cb3-36"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, k):</span>
<span id="cb3-37"><a href="#cb3-37"></a>        <span class="co"># iterate over each axis to find appropriate ratio</span></span>
<span id="cb3-38"><a href="#cb3-38"></a>        a <span class="op">=</span> <span class="dv">0</span>  <span class="co"># lower bound of binary search</span></span>
<span id="cb3-39"><a href="#cb3-39"></a>        b <span class="op">=</span> <span class="dv">1</span>  <span class="co"># upper bound of binary search</span></span>
<span id="cb3-40"><a href="#cb3-40"></a></span>
<span id="cb3-41"><a href="#cb3-41"></a>        <span class="cf">while</span> (b <span class="op">-</span> a <span class="op">&gt;</span> epsilon):</span>
<span id="cb3-42"><a href="#cb3-42"></a>            c <span class="op">=</span> (<span class="dv">3</span> <span class="op">*</span> a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb3-43"><a href="#cb3-43"></a>            d <span class="op">=</span> (a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb3-44"><a href="#cb3-44"></a>            e <span class="op">=</span> (a <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb3-45"><a href="#cb3-45"></a></span>
<span id="cb3-46"><a href="#cb3-46"></a>            <span class="co"># get diagonal confusions for each point</span></span>
<span id="cb3-47"><a href="#cb3-47"></a>            d_a, d_c, d_d, d_e, d_b <span class="op">=</span> (get_d(rbo_dlpm(x, <span class="dv">0</span>, i, k)) </span>
<span id="cb3-48"><a href="#cb3-48"></a>                <span class="cf">for</span> x <span class="kw">in</span> [a, c, d, e, b])</span>
<span id="cb3-49"><a href="#cb3-49"></a></span>
<span id="cb3-50"><a href="#cb3-50"></a>            <span class="co"># query oracle for each pair</span></span>
<span id="cb3-51"><a href="#cb3-51"></a>            response_ac <span class="op">=</span> oracle(d_a, d_c)</span>
<span id="cb3-52"><a href="#cb3-52"></a>            response_cd <span class="op">=</span> oracle(d_c, d_d)</span>
<span id="cb3-53"><a href="#cb3-53"></a>            response_de <span class="op">=</span> oracle(d_d, d_e)</span>
<span id="cb3-54"><a href="#cb3-54"></a>            response_eb <span class="op">=</span> oracle(d_e, d_b)</span>
<span id="cb3-55"><a href="#cb3-55"></a></span>
<span id="cb3-56"><a href="#cb3-56"></a>            <span class="co"># update ranges to keep the peak</span></span>
<span id="cb3-57"><a href="#cb3-57"></a>            <span class="cf">if</span> response_ac:</span>
<span id="cb3-58"><a href="#cb3-58"></a>                b <span class="op">=</span> d</span>
<span id="cb3-59"><a href="#cb3-59"></a>            <span class="cf">elif</span> response_cd:</span>
<span id="cb3-60"><a href="#cb3-60"></a>                b <span class="op">=</span> d</span>
<span id="cb3-61"><a href="#cb3-61"></a>            <span class="cf">elif</span> response_de:</span>
<span id="cb3-62"><a href="#cb3-62"></a>                a <span class="op">=</span> c</span>
<span id="cb3-63"><a href="#cb3-63"></a>                b <span class="op">=</span> e</span>
<span id="cb3-64"><a href="#cb3-64"></a>            <span class="cf">elif</span> response_eb:</span>
<span id="cb3-65"><a href="#cb3-65"></a>                a <span class="op">=</span> d</span>
<span id="cb3-66"><a href="#cb3-66"></a>            <span class="cf">else</span>:</span>
<span id="cb3-67"><a href="#cb3-67"></a>                a <span class="op">=</span> d</span>
<span id="cb3-68"><a href="#cb3-68"></a></span>
<span id="cb3-69"><a href="#cb3-69"></a>        midpt <span class="op">=</span> (a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb3-70"><a href="#cb3-70"></a>        a_hat[i] <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> midpt) <span class="op">/</span> midpt</span>
<span id="cb3-71"><a href="#cb3-71"></a>    <span class="cf">return</span> a_hat <span class="op">/</span> np.<span class="bu">sum</span>(a_hat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>To use this algorithm for metric elicitation on a real dataset, we need to supply the “oracle” and “get_d” functions. The oracle function is an interface to an expert who judges which of two confusion matrices is better. The get_d function will need to construct a classifier given the DLPM weights, following the principles of the RBO classifier from <a href="#eq-rbo_eq" class="quarto-xref">Equation&nbsp;<span>3.13</span></a>, and calculate the confusion matrix from a validation set.</p>
<p>Using the same oracle feedback noise model from the binary metric elicitation, we can make the following guarantees:</p>
<div class="callout callout-style-default callout-note callout-titled" title="proposition">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
proposition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="prop-prop_dlpm">
<p>Given <span class="math inline">\(\epsilon, \epsilon_\Omega \geq 0\)</span>, and a 1-Lipschitz DLPM <span class="math inline">\(\varphi^*\)</span> parameterized by <span class="math inline">\(\vec{a}^*\)</span>. Then the output <span class="math inline">\(\hat{a}\)</span> of the DLPM elicitation algorithm after <span class="math inline">\(O((k-1)\log\frac{1}{\epsilon})\)</span> queries to the oracle satisfies <span class="math inline">\(||\vec{a}^* - \hat{a}||_\infty \leq O(\epsilon + \sqrt{\epsilon_\Omega})\)</span>, which is equivalent to <span class="math inline">\(||\vec{a}^* - \hat{a}||_2 \leq O(\sqrt{k}(\epsilon + \sqrt{\epsilon_\Omega}))\)</span>.</p>
</div>
</div>
</div>
<p>In other words, the maximum difference between the estimate and true value along any component (indicated by the L-infinity norm) is linearly bounded by the sum of the epsilon specified by the algorithm and the square root of the oracle’s correctness guarantee (<span class="math inline">\(\epsilon_\Omega\)</span>).</p>
</section>
</section>
</section>
<section id="case-study-3-active-preference-learning-in-robotics" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="case-study-3-active-preference-learning-in-robotics"><span class="header-section-number">3.5</span> Case Study 3: Active Preference Learning in Robotics</h2>
<p>How exactly do robots learn human preferences from just the pairwise comparisons, if they need to learn how to act in the environment itself? The comparisons in turn help robots learn the reward function of the human, which allows them to further take actions in real settings. Let’s say there are two trajectories <span class="math inline">\(\xi_A\)</span> and <span class="math inline">\(\xi_B\)</span> that might be taken as the next course of action in any context, like choosing the next turn, or choosing the next chatGPT response. The robot is offering both to a human for comparison. To answer which of them is better, the human would ask themselves if <span class="math inline">\(R(\xi_A)\)</span> or <span class="math inline">\(R(\xi_B)\)</span> is bigger, with <span class="math inline">\(R(\xi) = w * \phi(\xi)\)</span> being the reward function. In this equation <span class="math inline">\(w\)</span> and <span class="math inline">\(\phi(\xi)\)</span> are vectors of weights and features of the trajectory, so alternatively, we can express this as:</p>
<p><span id="eq-reward_eq"><span class="math display">\[R(\xi) = \begin{bmatrix} w_1 \\ w_2 \\ ... \\ w_N \end{bmatrix} \cdot \begin{bmatrix} \phi_1(\xi) \\ \phi_2(\xi) \\ ... \\ \phi_N(\xi) \end{bmatrix} \tag{3.14}\]</span></span></p>
<p>If one says that they preferred <span class="math inline">\(\xi_2\)</span> less than <span class="math inline">\(\xi_1\)</span> then it means <span class="math inline">\(\xi_2 &lt; \xi_1 \implies R(\xi_2) &lt; R(\xi_1) \implies w * \phi(\xi_2) &lt; w * \phi(\xi_1) \implies 0 &lt; w * (\phi(\xi_1) - \phi(\xi_2)) \implies 0 &lt; w * \Phi\)</span>. Alternatively, if one preferred <span class="math inline">\(\xi_2\)</span> more than <span class="math inline">\(\xi_1\)</span>, the signs would be flipped, resulting in <span class="math inline">\(0 &gt; w * \Phi\)</span>. The two results can be represented in the N-dimensional space, where when it is split by the decision boundary, it creates half-spaces indicating preferences for each of the sides. For example we can see how a query between two items can split the plain into two halves, indicating preference towards one of the items. Such an image can be extended into bigger dimensions, where a line would become a separating hyperplane. If one is to truly believe the answers of one person, they would remove everything from the other side of the hyperplane that does not agree with the received human preference. But since humans are noisy, that approach is not optimal, thus most applications up-weight the indicated side of the plane to emphasize that points on that side are better, and down-weight the other side as they do not agree with the provided comparison.</p>
<p>How should someone choose which queries to conduct, otherwise, what is the most informative query sequence? After completing one query, the next query should be orthogonal to the previous one so that the potential space consistent with the preferences decreases in half. The intuition behind that is the potential space has all of the reward functions that agree with the provided answers, so to find a specific reward function for a human, decreasing the space narrows down the possible options. The original query created the blue space, and a new one created a red space, resulting in a purple intersection of the two which is still consistent with both of the queries’s results. The image shows that the purple portion is exactly half of the blue portion.</p>
<div id="fig-2dspace" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2dspace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/2D-space.jpg" class="img-fluid figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2dspace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.5: Creating further comparisons limits the space that agrees with answers to all of them. The blue area demonstrates a preference for object 1 over object 2. The red area demonstrates a preference for object 3 over object 4. Combination (purple area) shows the space that is consistent with both of those preferences.
</figcaption>
</figure>
</div>
<p>Mathematically, from <span class="citation" data-cites="pmlr-v87-biyik18a">(<a href="#ref-pmlr-v87-biyik18a" role="doc-biblioref">Biyik and Sadigh 2018</a>)</span> this can be expressed as set <span class="math inline">\(F\)</span> of potential queries <span class="math inline">\(\phi\)</span>, where <span class="math inline">\(F = \{\phi: \phi = \Phi(\xi_A) - \Phi(\xi_B), \xi_A, \xi_B \in \Xi\}\)</span> (defining that a query is the difference between the features of two trajectories). Using that, the authors define a human update function <span class="math inline">\(f_{\phi}(w) = \min(1, \exp(I^T\phi))\)</span> that accounts for how much of the space will still be consistent with the preferences. Finally, for a specific query, they define the minimum volume removed as <span class="math inline">\(\min\{\mathbb{E}[1 - f_{\phi}(w)], \mathbb{E}[1 - f_{-\phi}(w)]\}\)</span> (expected size of the two sides of the remaining space after it is split by a query - purple area in <a href="#fig-2dspace" class="quarto-xref">Figure&nbsp;<span>3.5</span></a>), and the final goal is to maximize that amount over all possible queries since it is optimal to get rid of as much space as possible to narrow down the options for the reward function: <span class="math inline">\(\max_{\phi} \min\{ \mathbb{E}[1 - f_{\phi}(w)], \mathbb{E}[1 - f_{-\phi}(w)]\}\)</span>. Effectively this is finding such <span class="math inline">\(\phi\)</span> that maximizes the information one can get by asking the next comparison query. While this approach uses minimum volume removed, there can be other metrics inside the <span class="math inline">\(\max\)</span> function. Some applications like movie recommendations do not require extra constraints, however in robotics one might want to add more constraints that satisfy certain rules, so that the resulting query follows the dynamics of the physical world.</p>
<p>The first real example of learning reward functions from pairwise comparisons is a 2D driving simulator from <span class="citation" data-cites="pmlr-v87-biyik18a">(<a href="#ref-pmlr-v87-biyik18a" role="doc-biblioref">Biyik and Sadigh 2018</a>)</span>. In <span class="quarto-unresolved-ref">?fig-car_direct</span> you can see the setting of a 3-lane road with the orange car being controlled by the computer. The queries conducted for this problem are two different trajectories presented to the human, and they are asked to evaluate which one of them is better. For the features that contribute to the reward function, it is important to consider that robots might not find some of the information as informative for the learning process as a human would. For this example, the underlying features included the distance between lane boundaries, distance to other cars, and the heading and speed of the controlled car. The weights toward the last feature were weighted the highest according to the authors, since it takes a lot of effort for the car to change or correct its direction.</p>
<p>At the start of the learning process, the car had no direction learned and was moving all over the road. In the middle of learning after 30 queries, the simulator learned to follow the direction of the road and go straight but still experienced collisions. After 70 queries, the simulator learned to avoid collisions, as well as keep the car within the lane without swerving.</p>
<section id="active-learning-for-pairwise-comparisons" class="level4" data-number="3.5.0.1">
<h4 data-number="3.5.0.1" class="anchored" data-anchor-id="active-learning-for-pairwise-comparisons"><span class="header-section-number">3.5.0.1</span> Active Learning for Pairwise Comparisons</h4>
<p>We have discussed that pairwise comparisons should be selected to maximize the minimum volume of remaining options removed. The question that can come out of the driving example is does it really matter to follow that goal or does random choice of queries performs as well? It turns out that indeed most AL algorithms (purposefully selecting queries) over time converge with the performance of the random query selection, so in long term the performance is similar. However, what is different is that AL achieves better performance earlier, which in time-sensitive tasks can be a critical factor. One example of such a setting can be exoskeletons for humans as part of the rehabilitation after surgery <span class="citation" data-cites="Li_2021">(<a href="#ref-Li_2021" role="doc-biblioref">Li et al. 2021</a>)</span>. Different people have significantly different walking patterns as well as rehabilitation requirements, so the exoskeleton needs to adapt to the human as soon as possible for a more successful rehabilitation. Figure <a href="#fig-robotics" class="quarto-xref">Figure&nbsp;<span>3.6</span></a> demonstrates the difference in the time needed between the two approaches. In general, in robotics, the time differences that might seem small to a human might be detrimental to the final performance.</p>
<div id="fig-robotics" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-robotics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/robo_graph.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-robotics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.6: Performance of AL and random query selection algorithms in the task of exoskeleton learning with human preferences. <span class="citation" data-cites="Li_2021">(<a href="#ref-Li_2021" role="doc-biblioref">Li et al. 2021</a>)</span>
</figcaption>
</figure>
</div>
<p>In conclusion, pairwise comparisons show to be a great way of learning linear reward functions, but at times present challenges or incapabilities that can be further improved with additional incorporations of approaches like AL. That improves many applications in terms of time spent getting to the result in case of exoskeleton adjustments, as well as getting to a middle ground between polar behaviors in applications like negotiations.</p>
</section>
<section id="application-guiding-human-demonstrations-in-robotics" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="application-guiding-human-demonstrations-in-robotics"><span class="header-section-number">3.5.1</span> Application: Guiding Human Demonstrations in Robotics</h3>
<p>A strong approach to learning policies for robotic manipulation is imitation learning, the technique of learning behaviors from human demonstrations. In particular, interactive imitation learning allows a group of humans to contribute their own demonstrations for a task, allowing for scalable learning. However, not all groups of demonstrators are equally helpful for interactive imitation learning.</p>
<p>The ideal set of demonstrations for imitation learning would follow a single, optimal method for performing the task, which a robot could learn to mimic. Conversely, <em>multimodality</em>, the presence of multiple optimal methods in the demonstration set, is challenging for imitation learning since it has to learn from contradicting information for how to accomplish a task. A common reason for multimodality is the fact that different people often subconsciously choose different paths for execution, as illustrated in <a href="#fig-multimodalexecution" class="quarto-xref">Figure&nbsp;<span>3.7</span></a>.</p>
<div id="fig-multimodalexecution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multimodalexecution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/multimodal_peg.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multimodalexecution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.7: Examples of two different ways to insert a nut onto a round peg. The orange demonstration picks up the nut from the hole while the blue demonstration picks up the nut from the side <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span>
</figcaption>
</figure>
</div>
<p>Gandhi et al. <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span> identifies whether demonstrations are compatible with one another and offer an active elicitation interface to guide humans to provide better demonstrations in interactive imitation learning. Their key motivation is to allow multiple users to contribute demonstrations over the course of data collection by guiding users towards compatible demonstrations. To identify whether a demonstration is “compatible” with a base policy trained with prior demonstrations, the researchers measure the <em>likelihood</em> of demonstrated actions under the base policy, and the <em>novelty</em> of the visited states. Intuitively, low likelihood and low novelty demonstrations should be excluded since they represent conflicting modes of behavior on states that the robot can already handle, and are therefore incompatible. This concept of compatibility is used for filtering a new set of demonstrations and actively eliciting compatible demonstrations. In the following subsections, we describe the process of estimating compatibility and active elicitation in more detal.</p>
<section id="estimating-compatiblity" class="level4" data-number="3.5.1.1">
<h4 data-number="3.5.1.1" class="anchored" data-anchor-id="estimating-compatiblity"><span class="header-section-number">3.5.1.1</span> Estimating Compatiblity</h4>
<p>We want to define a compatibility measure <span class="math inline">\(\mathcal{M}\)</span>, that estimates the performance of policy <span class="math inline">\(\pi_{base}\)</span> that is retrained on a union of <span class="math inline">\(\mathcal{D}_{base}\)</span>, the known base dataset, and <span class="math inline">\(\mathcal{D}_{new}\)</span>, the newly collected dataset. To define this compatibility measure in a way that is easy to compute, we can use two interpretable metrics: likelihood and novelty. The likelihood of actions <span class="math inline">\(a_{new}\)</span> in <span class="math inline">\(\mathcal{D}_{new}\)</span> is measured as the negative mean squared error between actions predicted by the base policy and this proposed action:</p>
<p><span id="eq-eq3.61"><span class="math display">\[likelihood(s_{new}, a_{new}) = -\mathbb{E}[|| \pi_{base}(s_{new}) - a_{new} ||^2_2]. \tag{3.15}\]</span></span></p>
<p>The novelty of the state <span class="math inline">\(s_{new}\)</span> in <span class="math inline">\(\mathcal{D}_{new}\)</span> is the standard deviation in the predicted actions under base policy:</p>
<p><span id="eq-eq3.62"><span class="math display">\[novelty(s_{new}) = \mathrm{Var}[\pi_{base}(s_{new})]. \tag{3.16}\]</span></span></p>
<p>We can plot likelihood and novelty on a 2D plane, as shown in <a href="#fig-likelihood_novelty" class="quarto-xref">Figure&nbsp;<span>3.8</span></a>, and identify thresholds on likelihood and novelty, denoted as <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\eta\)</span> respectively. Intuitively, demonstrations with low likelihood in low novelty states should be excluded, because this indicates that there is a conflict between the base behavior and the new demonstration due to multimodality. Note that in high novelty states, the likelihood should be disregarded because the base policy does not have a concrete idea for how to handle these states anyways so more data is needed.</p>
<div id="fig-likelihood_novelty" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-likelihood_novelty-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/likelihood_novelty.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-likelihood_novelty-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.8: Examples of plots of likelihood and novelty for compatible and incompatible operators <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span>
</figcaption>
</figure>
</div>
<p>The final compatibility metric, parameterized by the likelihood and novelty thresholds <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\eta\)</span>, is <span class="math inline">\(\mathcal{M}(\mathcal{D}_{base}, (s_{new}, a_{new})) \in [0, 1]\)</span>, defined as:</p>
<p><span id="eq-eq3.63"><span class="math display">\[\begin{aligned}
    \mathcal{M} = \begin{cases}
        1 - \min(\frac{\mathbb{E}[|| \pi_{base}(s_{new}) - a_{new} ||^2_2]}{\lambda}, 1) &amp; \text{ if } \text{novelty}(s_{new}) &lt; \eta \\
        1 &amp; \text{ otherwise }
       \end{cases}.
\end{aligned} \tag{3.17}\]</span></span></p>
<p>Note that <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\eta\)</span> need to be specified by hand. This is accomplished by assuming the ability to collect <em>a priori incompatible</em> demonstrations to identify reasonable thresholds that remove the most datapoints in the incompatible demonstrations while keeping the most datapoints in the compatible demonstrations.</p>
</section>
<section id="case-studies-with-fixed-sets" class="level4" data-number="3.5.1.2">
<h4 data-number="3.5.1.2" class="anchored" data-anchor-id="case-studies-with-fixed-sets"><span class="header-section-number">3.5.1.2</span> Case Studies with Fixed Sets</h4>
<p>The researchers evaluate the utility of the compatibility metric on three tasks: placing a square nut on a square peg, placing a round nut on a round peg, and opening a drawer and placing a hammer inside. For each task, they train a base policy using a “proficient” operator’s demonstration while sampling trajectories from other operators for the new set. The naive baseline is to use all datapoints while the <span class="math inline">\(\mathcal{M}\)</span>-Filtered demonstrations use the compatibility metric to filter out incompatible demonstrations. The results are presented in <a href="#tbl-m_filter_table" class="quarto-xref">Table&nbsp;<span>3.3</span></a>. As you can see, M-filtering results in equal or greater performance despite using less data than the naive baseline, demonstrating the effectiveness of compatibility-based filtering.</p>
<div id="tbl-m_filter_table" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-m_filter_table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.3: Success rates (mean/std across 3 training runs) for policies trained on <span class="math inline">\(\mathcal{D}_{new}\)</span> by using all the data (Naive) or filtering by compatibility (<span class="math inline">\(\mathcal{M}\)</span>-Filtered) <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span>
</figcaption>
<div aria-describedby="tbl-m_filter_table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;">Square Nut</td>
<td></td>
<td style="text-align: center;">Round Nut</td>
<td></td>
<td style="text-align: center;">Hammer Placement</td>
<td></td>
</tr>
<tr class="even">
<td>Operator</td>
<td style="text-align: center;">Naive</td>
<td><span class="math inline">\(\mathcal{M}\)</span>-Filtered</td>
<td style="text-align: center;">Naive</td>
<td><span class="math inline">\(\mathcal{M}\)</span>-Filtered</td>
<td style="text-align: center;">Naive</td>
<td><span class="math inline">\(\mathcal{M}\)</span>-Filtered</td>
</tr>
<tr class="odd">
<td>Base Operator</td>
<td style="text-align: center;">38.7 (2.1)</td>
<td>-</td>
<td style="text-align: center;">13.3 (2.3)</td>
<td>-</td>
<td style="text-align: center;">24.7 (6.1)</td>
<td>-</td>
</tr>
<tr class="even">
<td>Operator 1</td>
<td style="text-align: center;">54.3 (1.5)</td>
<td>61.0 (4.4)</td>
<td style="text-align: center;">26.7 (11.7)</td>
<td>32.0 (12.2)</td>
<td style="text-align: center;">38.0 (2.0)</td>
<td>39.7 (4.6)</td>
</tr>
<tr class="odd">
<td>Operator 2</td>
<td style="text-align: center;">40.3 (5.1)</td>
<td>42.0 (2.0)</td>
<td style="text-align: center;">22.0 (7.2)</td>
<td>26.7 (5.0)</td>
<td style="text-align: center;">33.3 (3.1)</td>
<td>32.7 (6.4)</td>
</tr>
<tr class="even">
<td>Operator 3</td>
<td style="text-align: center;">37.3 (2.1)</td>
<td>42.7 (0.6)</td>
<td style="text-align: center;">17.3 (4.6)</td>
<td>18.0 (13.9)</td>
<td style="text-align: center;">8.0 (0.0)</td>
<td>12.0 (0.0)</td>
</tr>
<tr class="odd">
<td>Operator 4</td>
<td style="text-align: center;">27.3 (3.5)</td>
<td>37.3 (2.1)</td>
<td style="text-align: center;">7.3 (4.6)</td>
<td>13.3 (1.2)</td>
<td style="text-align: center;">4.0 (0.0)</td>
<td>4.0 (0.0)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="fig-active_elicitation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-active_elicitation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/active_elicitation.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-active_elicitation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.9: The phases of the active elicitation interface: (a) initial prompting, (b) demonstrations with live feedback, and (c) corrective feedback <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="actively-eliciting-compatible-demonstrations" class="level4" data-number="3.5.1.3">
<h4 data-number="3.5.1.3" class="anchored" data-anchor-id="actively-eliciting-compatible-demonstrations"><span class="header-section-number">3.5.1.3</span> Actively Eliciting Compatible Demonstrations</h4>
<p>In the previous section, we assume access to a dataset that has already been collected, and we see how filtering out incompatible demonstrations helps improve performance. However, when collecting a new dataset, it would be better to ensure that operators collect compatible demonstrations from the start, allowing us to retain as much data as possible for training.</p>
<p>To actively elicit compatible demonstrations, the researchers set up a pipeline for live feedback and examples. At the start, operators are given a task specification and some episodes to practice using the robot. Then, the active elicitation process begins, as shown in <a href="#fig-active_elicitation" class="quarto-xref">Figure&nbsp;<span>3.9</span></a>. Each operator is shown some rollouts of the base policy to understand the style of the base operator. Next, the operator provides a demonstration similar to the ones they were shown. As they record their demonstrations, the interface provides online feedback, with green indicating compatible actions and red indicating incompatible actions. If the number of incompatible state-action pairs (ones where <span class="math inline">\(\mathcal{M}\)</span> is zero) exceeds 5% of the demonstration length, the demonstration is rejected. However, to provide corrective feedback, the interface shows the areas of the demonstration with the highest average incompatibility and also provides an expert demo that shows what should actually be done. Demonstrators can use this feedback to provide more compatible demonstrations moving forward.</p>
<p>This process helps improve the demonstration quality in both simulation and real experiments, as show in <a href="#tbl-active_elicitation_results" class="quarto-xref">Table&nbsp;<span>3.4</span></a>. Specifically, on the real results, active elicitation outperformed the base policy by 25% and naive data collection by 55%. Overall, active elicitation is a powerful tool to ensure that data collected for imitation learning improves the quality of the learned policy.</p>
<div id="tbl-active_elicitation_results" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-active_elicitation_results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.4: Success rates (mean/std across users) for policies trained on <span class="math inline">\(\mathcal{D}_{new}\)</span> by using all the data (Naive), filtering by compatibility (<span class="math inline">\(\mathcal{M}\)</span>-Filtered), or using informed demonstration collection <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span>
</figcaption>
<div aria-describedby="tbl-active_elicitation_results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Task Ba</th>
<th>se Naive</th>
<th style="text-align: right;">Naive + Fil</th>
<th style="text-align: left;">tered Informed</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Round Nut 13.</td>
<td>3 (2.3) 9.</td>
<td style="text-align: right;">6 (4.6)</td>
<td style="text-align: left;">9.7 (4.2) 15</td>
<td>.7 (6.0)</td>
</tr>
<tr class="even">
<td>Hammer Placement 24.</td>
<td>7 (6.1) 20.</td>
<td style="text-align: right;">8 (15.7)</td>
<td style="text-align: left;">22.0 (15.5) 31.</td>
<td>8 (16.3)</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\left[ \textup{Real} \right]\)</span> Food Plating</td>
<td>60.0 30.</td>
<td style="text-align: right;">0 (17.3)</td>
<td style="text-align: left;">- 85</td>
<td>.0 (9.6)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>A fundamental limitation of eliciting compatible demonstrations is the fact that the “base” demonstrator is considered the ground truth. When the base demonstrator specifies a preference, all other demonstrators must abide by it, even if they have strong preferences against it. For instance, when pouring milk and cereal into a bowl, different people have different preferences for what is the correct order, but active elicitation forces all demonstrators to follow the initial preference of the base operator. The researchers hope that future work can enable users to override the default demonstration set and follow a base behavior that better aligns with their preferences. This could enable multiple modes of behavior to be collected in data while only following a user’s specified preference instead of attempting to collapse all modes into a single policy.</p>
<p>Looking forward, active elicitation provides a foundation for allowing robots to query humans about the type of data needed, enabling more efficient data collection through transparency.</p>
<p>In summary, this chapter has explored the complexities and innovations in interAL as applied to large models within robotics. It begins by investigating pairwise comparisons and their role in efficiently learning linear reward functions from large datasets, overcoming limitations in supervised learning. When combined with active learning techniques, these comparisons supply timely, targeted, and context-appropriate feedback, enhancing performance in time-critical applications like exoskeleton adjustments during rehabilitation.</p>
<p>We then shift to imitation learning or inverse reward learning from demonstrations, emphasizing the difficulties introduced by multimodal demonstration sets. active elicitation approaches to compile compatible demonstrations, streamlining the learning process by guiding users to provide more valuable, steady examples are incredibly promising, however, to tackling this issue. This method shows promise in refining the interactive imitation learning data collection pipeline, enabling more capable and effective robotic training.</p>
<p>Additionally, the chapter examines the integration of foundation models into robotics, highlighting the transformative innovations of R3M and Voltron. R3M’s pre-training on diverse human activities dramatically improves robotic manipulation with minimal supervision. Meanwhile, Voltron builds on these capabilities by incorporating language-driven representation learning for remarkably adaptable and nuanced robotic task performance. These models represent significant leaps in robotics while opening new frontiers for future research and applications.</p>


<!-- -->

</section>
</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-unnoisy_humans" class="csl-entry" role="listitem">
Amershi, Saleema, Maya Cakmak, W. Bradley Knox, and Todd Kulesza. 2014. <span>“Power to the People: The Role of Humans in Interactive Machine Learning.”</span> <em>AI Magazine</em>.
</div>
<div id="ref-AL_committee" class="csl-entry" role="listitem">
Beluch, William H., Tim Genewein, A. Nürnberger, and Jan M. Köhler. 2018. <span>“The Power of Ensembles for Active Learning in Image Classification.”</span> <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 9368–77. <a href="https://api.semanticscholar.org/CorpusID:52838058">https://api.semanticscholar.org/CorpusID:52838058</a>.
</div>
<div id="ref-pmlr-v87-biyik18a" class="csl-entry" role="listitem">
Biyik, Erdem, and Dorsa Sadigh. 2018. <span>“Batch Active Preference-Based Learning of Reward Functions.”</span> In <em>Proceedings of the 2nd Conference on Robot Learning</em>, edited by Aude Billard, Anca Dragan, Jan Peters, and Jun Morimoto, 87:519–28. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v87/biyik18a.html">https://proceedings.mlr.press/v87/biyik18a.html</a>.
</div>
<div id="ref-pref4" class="csl-entry" role="listitem">
Braziunas, Darius, and Craig Boutilier. 2012. <span>“Minimax Regret Based Elicitation of Generalized Additive Utilities.”</span> <a href="https://arxiv.org/abs/1206.5255">https://arxiv.org/abs/1206.5255</a>.
</div>
<div id="ref-AL_expmodelchange" class="csl-entry" role="listitem">
Cai, Wenbin, Ya Zhang, and Jun Zhou. 2013. <span>“Maximizing Expected Model Change for Active Learning in Regression.”</span> In <em>2013 IEEE 13th International Conference on Data Mining</em>, 51–60. <a href="https://doi.org/10.1109/ICDM.2013.104">https://doi.org/10.1109/ICDM.2013.104</a>.
</div>
<div id="ref-AL_variance" class="csl-entry" role="listitem">
Cohn, David A., Zoubin Ghahramani, and Michael I. Jordan. 1996. <span>“Active Learning with Statistical Models.”</span> <em>CoRR</em> cs.AI/9603104. <a href="https://arxiv.org/abs/cs/9603104">https://arxiv.org/abs/cs/9603104</a>.
</div>
<div id="ref-geo_paper" class="csl-entry" role="listitem">
G., Jamieson Kevin, and Robert Nowak. 2011. <span>“Active Ranking Using Pairwise Comparisons.”</span> <em>Advances in Neural Information Processing Systems</em> 24.
</div>
<div id="ref-gandhi2022eliciting" class="csl-entry" role="listitem">
Gandhi, Kanishk, Siddharth Karamcheti, Madeline Liao, and Dorsa Sadigh. 2022. <span>“Eliciting Compatible Demonstrations for Multi-Human Imitation Learning.”</span> In <em>Proceedings of the 6th Conference on Robot Learning (CoRL)</em>.
</div>
<div id="ref-bias_variance_orig_paper" class="csl-entry" role="listitem">
Geman, Stuart, Elie Bienenstock, and René Doursat. 1992. <span>“Neural Networks and the Bias/Variance Dilemma.”</span> <em>Neural Computation</em> 4: 1–58. <a href="https://api.semanticscholar.org/CorpusID:14215320">https://api.semanticscholar.org/CorpusID:14215320</a>.
</div>
<div id="ref-noisy_humans" class="csl-entry" role="listitem">
Guillory, Andrew, and Jeff Bilmes. 2011. <span>“Simultaneous Learning and Covering with Adversarial Noise.”</span> <em>ICML</em>.
</div>
<div id="ref-pmlr-v89-hiranandani19a" class="csl-entry" role="listitem">
Hiranandani, Gaurush, Shant Boodaghians, Ruta Mehta, and Oluwasanmi Koyejo. 2019a. <span>“Performance Metric Elicitation from Pairwise Classifier Comparisons.”</span> In <em>Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics</em>, edited by Kamalika Chaudhuri and Masashi Sugiyama, 89:371–79. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v89/hiranandani19a.html">https://proceedings.mlr.press/v89/hiranandani19a.html</a>.
</div>
<div id="ref-NEURIPS2019_1fd09c5f" class="csl-entry" role="listitem">
Hiranandani, Gaurush, Shant Boodaghians, Ruta Mehta, and Oluwasanmi O Koyejo. 2019b. <span>“Multiclass Performance Metric Elicitation.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett. Vol. 32. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/1fd09c5f59a8ff35d499c0ee25a1d47e-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2019/file/1fd09c5f59a8ff35d499c0ee25a1d47e-Paper.pdf</a>.
</div>
<div id="ref-nips" class="csl-entry" role="listitem">
Hiranandani, Gaurush, Harikrishna Narasimhan, and Sanmi Koyejo. 2020. <span>“Fair Performance Metric Elicitation.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, 33:11083–95. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/7ec2442aa04c157590b2fa1a7d093a33-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2020/file/7ec2442aa04c157590b2fa1a7d093a33-Paper.pdf</a>.
</div>
<div id="ref-claus" class="csl-entry" role="listitem">
Holladay, Rachel, Shervin Javdani, Anca Dragan, and Siddhartha Srinivasa. 2016. <span>“Active Comparison Based Learning Incorporating User Uncertainty and Noise.”</span> <em>Proceedings of RSS ’16 Workshop on Model Learning for Human-Robot Communication</em>.
</div>
<div id="ref-AL_BALD" class="csl-entry" role="listitem">
Houlsby, Neil, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. 2011. <span>“Bayesian Active Learning for Classification and Preference Learning.”</span> <em>arXiv Preprint arXiv:1112.5745</em>.
</div>
<div id="ref-AL_app_autonomous" class="csl-entry" role="listitem">
Jarl, Sanna, Linus Aronsson, Sadegh Rahrovani, and Morteza Haghir Chehreghani. 2021. <span>“Active Learning of Driving Scenario Trajectories.”</span> <em>Eng. Appl. Artif. Intell.</em> 113: 104972. <a href="https://api.semanticscholar.org/CorpusID:249113683">https://api.semanticscholar.org/CorpusID:249113683</a>.
</div>
<div id="ref-Li_2021" class="csl-entry" role="listitem">
Li, Kejun, Maegan Tucker, Erdem Biyik, Ellen Novoseller, Joel W. Burdick, Yanan Sui, Dorsa Sadigh, Yisong Yue, and Aaron D. Ames. 2021. <span>“ROIAL: Region of Interest Active Learning for Characterizing Exoskeleton Gait Preference Landscapes.”</span> In <em>2021 IEEE International Conference on Robotics and Automation (ICRA)</em>. IEEE. <a href="https://doi.org/10.1109/icra48506.2021.9560840">https://doi.org/10.1109/icra48506.2021.9560840</a>.
</div>
<div id="ref-AL_app_LLMs" class="csl-entry" role="listitem">
Margatina, Katerina, Timo Schick, Nikolaos Aletras, and Jane Dwivedi-Yu. 2023. <span>“Active Learning Principles for in-Context Learning with Large Language Models.”</span> <em>ArXiv</em> abs/2305.14264. <a href="https://api.semanticscholar.org/CorpusID:258841313">https://api.semanticscholar.org/CorpusID:258841313</a>.
</div>
<div id="ref-pref2" class="csl-entry" role="listitem">
Mas-Colell, Andreu. 1977. <span>“The Recoverability of Consumers’ Preferences from Market Demand Behavior.”</span> <em>Econometrica</em> 45 (6): 1409–30. <a href="http://www.jstor.org/stable/1912308">http://www.jstor.org/stable/1912308</a>.
</div>
<div id="ref-AL_experrorredn" class="csl-entry" role="listitem">
Mussmann, Stephen, Julia Reisler, Daniel Tsai, Ehsan Mousavi, Shayne O’Brien, and Moises Goldszmidt. 2022. <span>“Active Learning with Expected Error Reduction.”</span> <a href="https://arxiv.org/abs/2211.09283">https://arxiv.org/abs/2211.09283</a>.
</div>
<div id="ref-pmlr-v37-narasimhanb15" class="csl-entry" role="listitem">
Narasimhan, Harikrishna, Harish Ramaswamy, Aadirupa Saha, and Shivani Agarwal. 2015. <span>“Consistent Multiclass Algorithms for Complex Performance Measures.”</span> In <em>Proceedings of the 32nd International Conference on Machine Learning</em>, edited by Francis Bach and David Blei, 37:2398–2407. Proceedings of Machine Learning Research. Lille, France: PMLR. <a href="https://proceedings.mlr.press/v37/narasimhanb15.html">https://proceedings.mlr.press/v37/narasimhanb15.html</a>.
</div>
<div id="ref-pref1" class="csl-entry" role="listitem">
Samuelson, P. A. 1938. <span>“A Note on the Pure Theory of Consumer’s Behaviour.”</span> <em>Economica</em> 5 (17): 61–71. <a href="http://www.jstor.org/stable/2548836">http://www.jstor.org/stable/2548836</a>.
</div>
<div id="ref-lus-shep" class="csl-entry" role="listitem">
Shepard, Roger N. 1957. <span>“Stimulus and Response Generalization: A Stochastic Model Relating Generalization to Distance in Psychological Space.”</span> <em>Psychometrika</em> 22(4):325–345.
</div>
<div id="ref-AL_app_sensors" class="csl-entry" role="listitem">
Singh, Aarti, Robert D. Nowak, and Parameswaran Ramanathan. 2006. <span>“Active Learning for Adaptive Mobile Sensing Networks.”</span> <em>2006 5th International Conference on Information Processing in Sensor Networks</em>, 60–68. <a href="https://api.semanticscholar.org/CorpusID:17590956">https://api.semanticscholar.org/CorpusID:17590956</a>.
</div>
<div id="ref-ab" class="csl-entry" role="listitem">
Tamburrelli, Giordano, and Alessandro Margara. 2014. <span>“Towards Automated a/b Testing.”</span> In <em>Search-Based Software Engineering</em>. <a href="https://doi.org/10.1007/978-3-319-09940-8_13">https://doi.org/10.1007/978-3-319-09940-8_13</a>.
</div>
<div id="ref-AL_app_robotics" class="csl-entry" role="listitem">
Taylor, Annalisa T., Thomas A. Berrueta, and Todd D. Murphey. 2021. <span>“Active Learning in Robotics: A Review of Control Principles.”</span> <em>ArXiv</em> abs/2106.13697. <a href="https://api.semanticscholar.org/CorpusID:235652039">https://api.semanticscholar.org/CorpusID:235652039</a>.
</div>
<div id="ref-pref3" class="csl-entry" role="listitem">
Varian, Hal R. 2006. <span>“Revealed Preference.”</span> In <em>The SAGE Encyclopedia of Business Ethics and Society</em>. <a href="https://api.semanticscholar.org/CorpusID:1632873">https://api.semanticscholar.org/CorpusID:1632873</a>.
</div>
<div id="ref-lus-log" class="csl-entry" role="listitem">
Viappiani, Paolo, and Craig Boutilier. 2010. <span>“Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets.”</span> <em>NIPS</em>, 2352–60.
</div>
<div id="ref-YangNaiman+2014+477+496" class="csl-entry" role="listitem">
Yang, Sitan, and Daniel Q. Naiman. 2014. <span>“Multiclass Cancer Classification Based on Gene Expression Comparison.”</span> <em>Statistical Applications in Genetics and Molecular Biology</em> 13 (4): 477–96. <a href="https://doi.org/doi:10.1515/sagmb-2013-0053">https://doi.org/doi:10.1515/sagmb-2013-0053</a>.
</div>
<div id="ref-AL_uncertainty" class="csl-entry" role="listitem">
Zhu, Jingbo, Huizhen Wang, Benjamin Ka-Yin T’sou, and Matthew Y. Ma. 2010. <span>“Active Learning with Sampling by Uncertainty and Density for Data Annotations.”</span> <em>IEEE Transactions on Audio, Speech, and Language Processing</em> 18: 1323–31. <a href="https://api.semanticscholar.org/CorpusID:5777911">https://api.semanticscholar.org/CorpusID:5777911</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../src/chap3.html" class="pagination-link" aria-label="Learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../src/chap5.html" class="pagination-link" aria-label="Decisions">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Decisions</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb4" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1"></a><span class="co">---</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="an">title:</span><span class="co"> Elicitation</span></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="an">format:</span><span class="co"> html</span></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="an">filters:</span></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="co">  - pyodide</span></span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="an">execute:</span></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="co">  engine: pyodide</span></span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="co">  pyodide:</span></span>
<span id="cb4-9"><a href="#cb4-9"></a><span class="co">    auto: true</span></span>
<span id="cb4-10"><a href="#cb4-10"></a></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="co">---</span></span>
<span id="cb4-12"><a href="#cb4-12"></a></span>
<span id="cb4-13"><a href="#cb4-13"></a>::: {.content-visible when-format="html"}</span>
<span id="cb4-14"><a href="#cb4-14"></a></span>
<span id="cb4-15"><a href="#cb4-15"></a>&lt;iframe</span>
<span id="cb4-16"><a href="#cb4-16"></a>  src="https://web.stanford.edu/class/cs329h/slides/3.1.active_learning/#/"</span>
<span id="cb4-17"><a href="#cb4-17"></a>  style="width:45%; height:225px;"</span>
<span id="cb4-18"><a href="#cb4-18"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb4-19"><a href="#cb4-19"></a>&lt;iframe</span>
<span id="cb4-20"><a href="#cb4-20"></a>  src="https://web.stanford.edu/class/cs329h/slides/3.2.metric_elicitation/#/"</span>
<span id="cb4-21"><a href="#cb4-21"></a>  style="width:45%; height:225px;"</span>
<span id="cb4-22"><a href="#cb4-22"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb4-23"><a href="#cb4-23"></a><span class="co">[</span><span class="ot">Fullscreen - AL</span><span class="co">](https://web.stanford.edu/class/cs329h/slides/3.1.active_learning/#/)</span>{.btn .btn-outline-primary .btn role="button"}</span>
<span id="cb4-24"><a href="#cb4-24"></a><span class="co">[</span><span class="ot">Fullscreen - ME</span><span class="co">](https://web.stanford.edu/class/cs329h/slides/3.2.metric_elicitation/#/)</span>{.btn .btn-outline-primary .btn role="button"}</span>
<span id="cb4-25"><a href="#cb4-25"></a></span>
<span id="cb4-26"><a href="#cb4-26"></a>:::</span>
<span id="cb4-27"><a href="#cb4-27"></a></span>
<span id="cb4-28"><a href="#cb4-28"></a><span class="fu">## The Active Learning Problem</span></span>
<span id="cb4-29"><a href="#cb4-29"></a></span>
<span id="cb4-30"><a href="#cb4-30"></a>Acquiring labeled data is expensive. Active learning (AL) is a learning paradigm that aims to reduce the amount of labeled data required to train a model to achieve high accuracy. AL algorithms iteratively select an input datapoint for an oracle (e.g., a human annotator) to label such that when the label is observed, the model improves the most. Two primary setups in AL is pool-based and stream-based. In pool-based AL, the model selects samples from a large unlabeled pool of data. For example, a model for text classification selects the most uncertain texts from a large pool to ask a human annotator to label. In stream-based AL, the model receives samples sequentially (one sample at a time) and decides whether to label them. The data is gone if the decision maker decides not to label it. In AL, a model is trained on the current dataset, and a set of candidate points is evaluated for potential inclusion. AL selects one of these points to add to the dataset based on an "acquisition function" defined with respect to the current model to estimate the value of each candidate point for improving model performance. The dataset is updated with the newly queried point, and the cycle repeats until the budget is exhausted or a predefined reliability criterion is met.</span>
<span id="cb4-31"><a href="#cb4-31"></a></span>
<span id="cb4-32"><a href="#cb4-32"></a>AL has successfully enhance various real-world systems. For example, AL can improve the computer vision models used in autonomous vehicles <span class="co">[</span><span class="ot">@AL_app_autonomous</span><span class="co">]</span>. Probing a model to understand what type of data it would benefit from is more practical. In robotics, autonomous agents may query humans when unsure how to act when facing new situations <span class="co">[</span><span class="ot">@AL_app_robotics</span><span class="co">]</span>. Here, collecting data often incurs significant financial and time costs because physical robot arm worns out over time. In meteorology, AL can help decide where to place additional sensors for weather predictions <span class="co">[</span><span class="ot">@AL_app_sensors</span><span class="co">]</span>. Sensor placement involves deploying teams to remote locations and expensive construction for an extra data point. Choosing these locations and allocating resources wisely is of interest to governments and businesses. AL could also be employed to select data for fine-tuning large language models (LLMs) for specific downstream tasks <span class="co">[</span><span class="ot">@AL_app_LLMs</span><span class="co">]</span>. Here, it might be difficult to fully describe a targeted NLP task. Often, instead of defining a task via a dataset of examples, it may be easier for a human to interact with the LLM for a specific use case, identify gaps in the model, and address those using AL.</span>
<span id="cb4-33"><a href="#cb4-33"></a></span>
<span id="cb4-34"><a href="#cb4-34"></a>Typically, in robotic, robots learn by observing human demonstrations. However, expert demonstrations are often limited, and training a supervised learning model would require vast amounts of demonstration data, which is difficult to obtain at scale. Demonstrations tend to be variable, reflecting the actions of individual humans, making the data collection process inconsistent. To address these limitations, alternative approaches have been proposed, such as using pairwise comparisons, where humans evaluate two action trajectories to determine the superior one, or employing physical corrections, in which reward functions are learned through human-robot interactions, with humans guiding the robot’s actions during the task. AL algorithms can be employed in preference learning tasks, where the objective is to develop a model that aligns with human preferences while minimizing the need for extensive labeled data or reducing the high cost of annotations. </span>
<span id="cb4-35"><a href="#cb4-35"></a></span>
<span id="cb4-36"><a href="#cb4-36"></a>Motivating by the pairwise preference setting, we consider a binary classification problem. The model is trained on a small labeled dataset $\mathcal{D} = <span class="sc">\{</span>(x_i, y_i)<span class="sc">\}</span>_{i=1}^N$, where $x_i$ represents the input data and $y_i$ is the corresponding label. The model is uncertain about the class labels of some data points and can query an oracle to obtain the true labels of these data points. The goal is to minimize the number of queries to the oracle while maximizing the model's performance. Here, the value of a datapoint is in how much it helps identify the underlying model, and this notion of informativeness is often quantify with uncertainty. Two primary types of uncertainty are often considered: epistemic and aleatoric uncertainty. Epistemic uncertainty, or model uncertainty, arises from a lack of knowledge and can be reduced by acquiring more data. This type of uncertainty is especially significant when the model lacks confidence due to insufficient or incomplete information in its training set. On the other hand, aleatoric uncertainty, or data uncertainty, stems from the inherent randomness within the data itself. Unlike epistemic uncertainty, aleatoric uncertainty cannot be reduced, even with additional data, as it reflects noise or unpredictability in the real data-generating process. AL often focuses on selecting data that reduce the epistemic uncertainty. </span>
<span id="cb4-37"><a href="#cb4-37"></a></span>
<span id="cb4-38"><a href="#cb4-38"></a>There are several method for quantify model uncertainty. Bayesian methods, such as Bayesian neural networks and Gaussian processes, offer a principled way of estimating uncertainty of parameter posterior distribution by iteratively updating a prior distribution over model. Exact posterior computation can become computationally prohibitive, especially for complex likelihood function, and approximated Bayesian computation is proposed to address this. For example, ensemble methods involve training multiple models and combining their predictions to provide an estimate of uncertainty. Ensemble methods are relatively easy to implement, but they are noisy and still somewhat expensive. Conformal prediction methods also provide a framework for estimating uncertainty by offering a measure of confidence in predictions based on the conformity of a given instance with the training data.</span>
<span id="cb4-39"><a href="#cb4-39"></a></span>
<span id="cb4-40"><a href="#cb4-40"></a><span class="fu">## Estimating the Value of Additional Data with Acquisition Function</span></span>
<span id="cb4-41"><a href="#cb4-41"></a>Uncertainty quantification plays a vital role in acquisition functions, which are central to AL strategies. These functions determine which samples are most valuable to label by evaluating their utility based on the model's current uncertainty estimates. Common acquisition functions include uncertainty sampling <span class="co">[</span><span class="ot">@AL_uncertainty</span><span class="co">]</span>, which selects samples the model is least confident about, query-by-committee <span class="co">[</span><span class="ot">@AL_committee</span><span class="co">]</span>, which utilizes a set of models to choose the most uncertain samples, and Bayesian AL by Disagreement (BALD) <span class="co">[</span><span class="ot">@AL_BALD</span><span class="co">]</span>, which selects samples that maximize information gain by reducing model uncertainty. Through careful uncertainty quantification, acquisition functions guide the AL process, improving the model's efficiency in learning from limited data. Other acquisition functions that can be employed include:</span>
<span id="cb4-42"><a href="#cb4-42"></a></span>
<span id="cb4-43"><a href="#cb4-43"></a><span class="ss">-   </span>Expected model change <span class="co">[</span><span class="ot">@AL_expmodelchange</span><span class="co">]</span>: This approach focuses on labeling points that would have the most impact on changing the current model parameters.</span>
<span id="cb4-44"><a href="#cb4-44"></a></span>
<span id="cb4-45"><a href="#cb4-45"></a><span class="ss">-   </span>Expected error reduction <span class="co">[</span><span class="ot">@AL_experrorredn</span><span class="co">]</span>: Points that would most effectively reduce the model's generalization error are labeled using this strategy.</span>
<span id="cb4-46"><a href="#cb4-46"></a></span>
<span id="cb4-47"><a href="#cb4-47"></a><span class="ss">-   </span>Variance reduction <span class="co">[</span><span class="ot">@AL_variance</span><span class="co">]</span>: This approach labels points that would minimize output variance, which is one component of error. By selecting points that reduce variability in the model's predictions, it aims to improve overall performance.</span>
<span id="cb4-48"><a href="#cb4-48"></a></span>
<span id="cb4-49"><a href="#cb4-49"></a>Uncertainty sampling <span class="co">[</span><span class="ot">@AL_uncertainty</span><span class="co">]</span> selects data points for which the model exhibits the greatest uncertainty, focusing labeling efforts on ambiguous samples where additional information is likely to yield the greatest benefit. Several acquisition strategies fall under uncertainty sampling, including entropy sampling, margin sampling, and least confidence sampling. Entropy sampling measures value of addition data by the entropy of the predicted probability distribution: $\alpha(x) = - \sum_{y} p(y|x) \log p(y|x)$. Margin sampling focuses on the difference between the two highest predicted probabilities for a sample: $\alpha(x) = p(y_1|x) - p(y_2|x)$, where $y_1$ and $y_2$ are two most likely classes. Least confidence sampling measures value of additional data by the lowest predicted probability for its most likely class: $\alpha(x) = 1 - p(y_{\text{max}}|x)$, where $y_{\text{max}}$ is the class with the highest probability. Consider a binary classification problem with three candidate $x_1, x_2, x_3$. The code below demonstrate that uncertainty sampling methods yield the same conclusion of selecting $x_1$.</span>
<span id="cb4-50"><a href="#cb4-50"></a></span>
<span id="cb4-51"><a href="#cb4-51"></a>::: {.callout-note title="code"}</span>
<span id="cb4-52"><a href="#cb4-52"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb4-53"><a href="#cb4-53"></a><span class="in">import numpy as np</span></span>
<span id="cb4-54"><a href="#cb4-54"></a></span>
<span id="cb4-55"><a href="#cb4-55"></a><span class="in"># Predictive distributions</span></span>
<span id="cb4-56"><a href="#cb4-56"></a><span class="in">probs = np.array([</span></span>
<span id="cb4-57"><a href="#cb4-57"></a><span class="in">    [0.6, 0.4],  # p(y1|x1), p(y2|x1)</span></span>
<span id="cb4-58"><a href="#cb4-58"></a><span class="in">    [0.3, 0.7],  # p(y1|x2), p(y2|x2)</span></span>
<span id="cb4-59"><a href="#cb4-59"></a><span class="in">    [0.8, 0.2],  # p(y1|x3), p(y2|x3)</span></span>
<span id="cb4-60"><a href="#cb4-60"></a><span class="in">])</span></span>
<span id="cb4-61"><a href="#cb4-61"></a></span>
<span id="cb4-62"><a href="#cb4-62"></a><span class="in"># Entropy Sampling</span></span>
<span id="cb4-63"><a href="#cb4-63"></a><span class="in">entropy = -np.sum(probs * np.log(probs), axis=1)</span></span>
<span id="cb4-64"><a href="#cb4-64"></a><span class="in">for i, e in enumerate(entropy, start=1):</span></span>
<span id="cb4-65"><a href="#cb4-65"></a><span class="in">    print(f"Entropy(x_{i}) = {e:.2f}")</span></span>
<span id="cb4-66"><a href="#cb4-66"></a><span class="in"># Find the index with the highest entropy</span></span>
<span id="cb4-67"><a href="#cb4-67"></a><span class="in">selected_index = np.argmax(entropy)</span></span>
<span id="cb4-68"><a href="#cb4-68"></a><span class="in">print(f"\nSelect x_{selected_index + 1} for labeling (highest entropy = {entropy[selected_index]:.2f})")</span></span>
<span id="cb4-69"><a href="#cb4-69"></a></span>
<span id="cb4-70"><a href="#cb4-70"></a><span class="in"># Sort each row in descending order to get the top two class probabilities</span></span>
<span id="cb4-71"><a href="#cb4-71"></a><span class="in">sorted_probs = np.sort(probs, axis=1)[:, ::-1]</span></span>
<span id="cb4-72"><a href="#cb4-72"></a><span class="in">margin = sorted_probs[:, 0] - sorted_probs[:, 1]</span></span>
<span id="cb4-73"><a href="#cb4-73"></a><span class="in">for i, m in enumerate(margin, start=1):</span></span>
<span id="cb4-74"><a href="#cb4-74"></a><span class="in">    print(f"Margin(x_{i}) = {m:.1f}")</span></span>
<span id="cb4-75"><a href="#cb4-75"></a></span>
<span id="cb4-76"><a href="#cb4-76"></a><span class="in"># Select the index with the smallest margin (most uncertain)</span></span>
<span id="cb4-77"><a href="#cb4-77"></a><span class="in">selected_index = np.argmin(margin)</span></span>
<span id="cb4-78"><a href="#cb4-78"></a><span class="in">print(f"\nSelect x_{selected_index + 1} for labeling (smallest margin = {margin[selected_index]:.1f})")</span></span>
<span id="cb4-79"><a href="#cb4-79"></a></span>
<span id="cb4-80"><a href="#cb4-80"></a><span class="in"># Least confidence sampling</span></span>
<span id="cb4-81"><a href="#cb4-81"></a><span class="in"># Get the highest predicted probability for each sample</span></span>
<span id="cb4-82"><a href="#cb4-82"></a><span class="in">max_probs = np.max(probs, axis=1)</span></span>
<span id="cb4-83"><a href="#cb4-83"></a><span class="in">least_confidence = 1 - max_probs</span></span>
<span id="cb4-84"><a href="#cb4-84"></a><span class="in">for i, lc in enumerate(least_confidence, start=1):</span></span>
<span id="cb4-85"><a href="#cb4-85"></a><span class="in">    print(f"alpha(x_{i}) = {lc:.1f}")</span></span>
<span id="cb4-86"><a href="#cb4-86"></a></span>
<span id="cb4-87"><a href="#cb4-87"></a><span class="in"># Select the index with the highest least confidence score (i.e., most uncertain)</span></span>
<span id="cb4-88"><a href="#cb4-88"></a><span class="in">selected_index = np.argmax(least_confidence)</span></span>
<span id="cb4-89"><a href="#cb4-89"></a><span class="in">print(f"\nSelect x_{selected_index + 1} for labeling (highest least confidence = {least_confidence[selected_index]:.1f})")</span></span>
<span id="cb4-90"><a href="#cb4-90"></a><span class="in">```</span></span>
<span id="cb4-91"><a href="#cb4-91"></a>:::</span>
<span id="cb4-92"><a href="#cb4-92"></a></span>
<span id="cb4-93"><a href="#cb4-93"></a>Query-by-Committee <span class="co">[</span><span class="ot">@AL_committee</span><span class="co">]</span> is selects samples for labeling based on the level of disagreement among members of a committee. Several acquisition functions can be employed under this framework to quantify the disagreement. The vote entropy measures the uncertainty based on how often the committee members vote for each class. The acquisition function is defined as $\alpha(x) = \mathbb{H}\left<span class="co">[</span><span class="ot">V(y)/C\right</span><span class="co">]</span>$, where $V(y)$ is the number of votes for class $y$ and $C$ is the number of committee members. Consensus Entropy measures the entropy of the average probability distribution across committee members. It is given by $\alpha(x) = \mathbb{H}<span class="co">[</span><span class="ot">p_C(y|x)</span><span class="co">]</span>$, where $p_C(y|x)$ is the average probability distribution for sample $x$ across all committee members. The KL divergence quantifies the disagreement by comparing the probability distribution of each committee member to the average distribution. The acquisition function is given by $\alpha(x) = \frac{1}{C} \sum_{c=1}^{C} D_{KL}<span class="co">[</span><span class="ot">p_C(y|x) || p_C(y|x)</span><span class="co">]</span>$, where $p_C(y|x)$ is the probability distribution of committee member $c$ and $p_C(y|x)$ is the average distribution across the committee. As an example, consider a binary classification problem with three candidate $x_1$, $x_2$, and $x_3$ and three committee members. Numerical result below show that all acquisition functions selects $x_1$.</span>
<span id="cb4-94"><a href="#cb4-94"></a></span>
<span id="cb4-95"><a href="#cb4-95"></a>::: {.callout-note title="code"}</span>
<span id="cb4-96"><a href="#cb4-96"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb4-97"><a href="#cb4-97"></a><span class="in">import numpy as np</span></span>
<span id="cb4-98"><a href="#cb4-98"></a></span>
<span id="cb4-99"><a href="#cb4-99"></a><span class="in"># Predictive distributions from 3 committee members for 3 samples</span></span>
<span id="cb4-100"><a href="#cb4-100"></a><span class="in"># Shape: (num_samples, num_committee_members, num_classes)</span></span>
<span id="cb4-101"><a href="#cb4-101"></a><span class="in">probs = np.array([</span></span>
<span id="cb4-102"><a href="#cb4-102"></a><span class="in">    [[0.6, 0.4], [0.7, 0.3], [0.3, 0.7]],  # x1</span></span>
<span id="cb4-103"><a href="#cb4-103"></a><span class="in">    [[0.3, 0.7], [0.4, 0.6], [0.4, 0.6]],  # x2</span></span>
<span id="cb4-104"><a href="#cb4-104"></a><span class="in">    [[0.8, 0.2], [0.9, 0.1], [0.7, 0.3]],  # x3</span></span>
<span id="cb4-105"><a href="#cb4-105"></a><span class="in">])</span></span>
<span id="cb4-106"><a href="#cb4-106"></a></span>
<span id="cb4-107"><a href="#cb4-107"></a><span class="in"># Vote entropy</span></span>
<span id="cb4-108"><a href="#cb4-108"></a><span class="in">predicted_labels = np.argmax(probs, axis=2)  # Shape: (num_samples, num_committee_members)</span></span>
<span id="cb4-109"><a href="#cb4-109"></a><span class="in">num_classes = probs.shape[2]</span></span>
<span id="cb4-110"><a href="#cb4-110"></a><span class="in">vote_counts = np.array([</span></span>
<span id="cb4-111"><a href="#cb4-111"></a><span class="in">    [np.sum(predicted_labels[i] == c) for c in range(num_classes)]</span></span>
<span id="cb4-112"><a href="#cb4-112"></a><span class="in">    for i in range(predicted_labels.shape[0])</span></span>
<span id="cb4-113"><a href="#cb4-113"></a><span class="in">])</span></span>
<span id="cb4-114"><a href="#cb4-114"></a><span class="in">vote_distributions = vote_counts / vote_counts.sum(axis=1, keepdims=True)</span></span>
<span id="cb4-115"><a href="#cb4-115"></a><span class="in">vote_entropy = -np.sum(vote_distributions * np.log(vote_distributions + 1e-12), axis=1)  # add epsilon to avoid log(0)</span></span>
<span id="cb4-116"><a href="#cb4-116"></a><span class="in">for i, ve in enumerate(vote_entropy, start=1):</span></span>
<span id="cb4-117"><a href="#cb4-117"></a><span class="in">    print(f"alpha(x_{i}) = {ve:.2f}")</span></span>
<span id="cb4-118"><a href="#cb4-118"></a><span class="in">selected_index = np.argmax(vote_entropy)</span></span>
<span id="cb4-119"><a href="#cb4-119"></a><span class="in">print(f"\nSelect x_{selected_index + 1} for labeling (highest vote entropy = {vote_entropy[selected_index]:.2f})")</span></span>
<span id="cb4-120"><a href="#cb4-120"></a></span>
<span id="cb4-121"><a href="#cb4-121"></a><span class="in"># Consensus Entropy</span></span>
<span id="cb4-122"><a href="#cb4-122"></a><span class="in">consensus_probs = np.mean(probs, axis=1)  # Shape: (num_samples, num_classes)</span></span>
<span id="cb4-123"><a href="#cb4-123"></a><span class="in">consensus_entropy = -np.sum(consensus_probs * np.log(consensus_probs + 1e-12), axis=1)  # add epsilon to avoid log(0)</span></span>
<span id="cb4-124"><a href="#cb4-124"></a><span class="in">for i, ce in enumerate(consensus_entropy, start=1):</span></span>
<span id="cb4-125"><a href="#cb4-125"></a><span class="in">    print(f"alpha(x_{i}) = {ce:.2f}")</span></span>
<span id="cb4-126"><a href="#cb4-126"></a><span class="in">selected_index = np.argmax(consensus_entropy)</span></span>
<span id="cb4-127"><a href="#cb4-127"></a><span class="in">print(f"\nSelect x_{selected_index + 1} for labeling (highest consensus entropy = {consensus_entropy[selected_index]:.2f})")</span></span>
<span id="cb4-128"><a href="#cb4-128"></a><span class="in">```</span></span>
<span id="cb4-129"><a href="#cb4-129"></a>:::</span>
<span id="cb4-130"><a href="#cb4-130"></a></span>
<span id="cb4-131"><a href="#cb4-131"></a>Bayesian AL by Disagreement (BALD) <span class="co">[</span><span class="ot">@AL_BALD</span><span class="co">]</span> selects the samples for which the model expects to gain the most Shannon information when corresponding labels are observed:</span>
<span id="cb4-132"><a href="#cb4-132"></a></span>
<span id="cb4-133"><a href="#cb4-133"></a>$$</span>
<span id="cb4-134"><a href="#cb4-134"></a>\begin{aligned}</span>
<span id="cb4-135"><a href="#cb4-135"></a>&amp;\mathbb{I}(\theta; y|x, \mathcal{D}) = \mathbb{H}<span class="co">[</span><span class="ot">p(y|x, \mathcal{D})</span><span class="co">]</span> - \mathbb{E}_{p(\theta | \mathcal{D})} <span class="co">[</span><span class="ot">\mathbb{H}[p(y|x, \theta, \mathcal{D})]</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb4-136"><a href="#cb4-136"></a>&amp;\mathbb{H}<span class="co">[</span><span class="ot">p(y|x, \mathcal{D})</span><span class="co">]</span> = \mathbb{H}\left<span class="co">[</span><span class="ot">\int_{\theta} p(y|x, \theta, \mathcal{D}) p(\theta | \mathcal{D}) d\theta\right</span><span class="co">]</span> \approx \mathbb{H}\left<span class="co">[</span><span class="ot">\frac{1}{N}\sum_{i=1}^{N} p(y|x, \theta_i, \mathcal{D})\right</span><span class="co">]</span> = \mathbb{H}\left<span class="co">[</span><span class="ot">\overline{p}(y|x, \mathcal{D})\right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb4-137"><a href="#cb4-137"></a>&amp;\mathbb{E}_{p(\theta|\mathcal{D})} [\mathbb{H}[p(y|x, \theta, \mathcal{D})]] = \mathbb{E}_{p(\theta|\mathcal{D})} \left[ - \sum_{y} p(y|x, \theta, \mathcal{D}) \log p(y|x, \theta, \mathcal{D}) \right] \approx - \frac{1}{N} \sum_{i=1}^{N} \left( \sum_{y} p(y|x, \theta_i, \mathcal{D}) \log p(y|x, \theta_i, \mathcal{D}) \right)</span>
<span id="cb4-138"><a href="#cb4-138"></a>\end{aligned}</span>
<span id="cb4-139"><a href="#cb4-139"></a>$$</span>
<span id="cb4-140"><a href="#cb4-140"></a></span>
<span id="cb4-141"><a href="#cb4-141"></a>When there is significant disagreement among models, the predictive entropy (the first term) will be large, while the expected entropy (the second term) will be smaller. This difference represents the degree to which the models disagree. BALD selects points where this disagreement is maximized. As an example, consider a binary classification problem with two classes, $y_1$ and $y_2$. We have two samples, $x_1$ and $x_2$. BALD selects $x_1$ for labeling.</span>
<span id="cb4-142"><a href="#cb4-142"></a></span>
<span id="cb4-143"><a href="#cb4-143"></a>::: {.callout-note title="code"}</span>
<span id="cb4-144"><a href="#cb4-144"></a><span class="in">```{pyodide-python}</span></span>
<span id="cb4-145"><a href="#cb4-145"></a><span class="in">import numpy as np</span></span>
<span id="cb4-146"><a href="#cb4-146"></a></span>
<span id="cb4-147"><a href="#cb4-147"></a><span class="in"># First and second time inferences for each sample</span></span>
<span id="cb4-148"><a href="#cb4-148"></a><span class="in"># Shape: (num_samples, num_draws, num_classes)</span></span>
<span id="cb4-149"><a href="#cb4-149"></a><span class="in"># theta_1 and theta_2 samples</span></span>
<span id="cb4-150"><a href="#cb4-150"></a><span class="in">probs = np.array([</span></span>
<span id="cb4-151"><a href="#cb4-151"></a><span class="in">    [[0.6, 0.4], [0.8, 0.2]],  # x1</span></span>
<span id="cb4-152"><a href="#cb4-152"></a><span class="in">    [[0.4, 0.6], [0.5, 0.5]],  # x2</span></span>
<span id="cb4-153"><a href="#cb4-153"></a><span class="in">])</span></span>
<span id="cb4-154"><a href="#cb4-154"></a></span>
<span id="cb4-155"><a href="#cb4-155"></a><span class="in"># Step 1: Compute the average predictive distribution (consensus probs)</span></span>
<span id="cb4-156"><a href="#cb4-156"></a><span class="in">mean_probs = np.mean(probs, axis=1)  # shape: (num_samples, num_classes)</span></span>
<span id="cb4-157"><a href="#cb4-157"></a></span>
<span id="cb4-158"><a href="#cb4-158"></a><span class="in"># Step 1 continued: Compute entropy of the consensus distribution</span></span>
<span id="cb4-159"><a href="#cb4-159"></a><span class="in">consensus_entropy = -np.sum(mean_probs * np.log(mean_probs + 1e-12), axis=1)</span></span>
<span id="cb4-160"><a href="#cb4-160"></a></span>
<span id="cb4-161"><a href="#cb4-161"></a><span class="in"># Step 2: Compute entropy for each model draw</span></span>
<span id="cb4-162"><a href="#cb4-162"></a><span class="in">individual_entropies = -np.sum(probs * np.log(probs + 1e-12), axis=2)</span></span>
<span id="cb4-163"><a href="#cb4-163"></a></span>
<span id="cb4-164"><a href="#cb4-164"></a><span class="in"># Step 2 continued: Average the entropies across model draws</span></span>
<span id="cb4-165"><a href="#cb4-165"></a><span class="in">expected_entropy = np.mean(individual_entropies, axis=1)</span></span>
<span id="cb4-166"><a href="#cb4-166"></a></span>
<span id="cb4-167"><a href="#cb4-167"></a><span class="in"># Step 3: Compute BALD = entropy of mean - mean of entropies</span></span>
<span id="cb4-168"><a href="#cb4-168"></a><span class="in">bald_scores = consensus_entropy - expected_entropy</span></span>
<span id="cb4-169"><a href="#cb4-169"></a></span>
<span id="cb4-170"><a href="#cb4-170"></a><span class="in"># Print results</span></span>
<span id="cb4-171"><a href="#cb4-171"></a><span class="in">for i, (h, eh, b) in enumerate(zip(consensus_entropy, expected_entropy, bald_scores), start=1):</span></span>
<span id="cb4-172"><a href="#cb4-172"></a><span class="in">    print(f"x_{i}:")</span></span>
<span id="cb4-173"><a href="#cb4-173"></a><span class="in">    print(f"  Predictive Entropy = {h:.3f}")</span></span>
<span id="cb4-174"><a href="#cb4-174"></a><span class="in">    print(f"  Expected Entropy   = {eh:.3f}")</span></span>
<span id="cb4-175"><a href="#cb4-175"></a><span class="in">    print(f"  BALD Score         = {b:.3f}")</span></span>
<span id="cb4-176"><a href="#cb4-176"></a></span>
<span id="cb4-177"><a href="#cb4-177"></a><span class="in"># Select sample with highest BALD score</span></span>
<span id="cb4-178"><a href="#cb4-178"></a><span class="in">selected_index = np.argmax(bald_scores)</span></span>
<span id="cb4-179"><a href="#cb4-179"></a><span class="in">print(f"\nSelect x_{selected_index + 1} for labeling (highest BALD = {bald_scores[selected_index]:.3f})")</span></span>
<span id="cb4-180"><a href="#cb4-180"></a><span class="in">```</span></span>
<span id="cb4-181"><a href="#cb4-181"></a>:::</span>
<span id="cb4-182"><a href="#cb4-182"></a></span>
<span id="cb4-183"><a href="#cb4-183"></a>AL by Variance Reduction <span class="co">[</span><span class="ot">@AL_variance</span><span class="co">]</span> is an algorithm designed to select the next data point for labeling based on the anticipated reduction in the model's variance. The objective is to identify the point $x \sim p(x)$ that, when labeled $y_x$, will most effectively decrease the model's variance. The expected error at a given input $x$ is $\mathbb{E}_{\hat{y} \sim p(\hat{y} | \mathcal{D}; x), y \sim p(y|x)} (\hat{y} - y)^2$. $\hat{y}$ represents the model's prediction, and $y$ denotes the true label at $x$. Using bias-variance decomposition <span class="co">[</span><span class="ot">@bias_variance_orig_paper</span><span class="co">]</span>, the expected error is decomposed as</span>
<span id="cb4-184"><a href="#cb4-184"></a>$$\begin{aligned}</span>
<span id="cb4-185"><a href="#cb4-185"></a>\mathbb{E} (\hat{y} - y)^2 = \mathbb{E}<span class="co">[</span><span class="ot">(\hat{y} - \mathbb{E}[y|x]) + (\mathbb{E}[y|x] - y)</span><span class="co">]</span>^2 = \mathbb{E} <span class="co">[</span><span class="ot">(y - \mathbb{E}[y|x])^2</span><span class="co">]</span> + 2\mathbb{E} <span class="co">[</span><span class="ot">(\hat{y} - \mathbb{E}[y|x])(\mathbb{E}[y|x] - y)</span><span class="co">]</span> + \mathbb{E}(\hat{y} - \mathbb{E}<span class="co">[</span><span class="ot">y|x</span><span class="co">]</span>)^2</span>
<span id="cb4-186"><a href="#cb4-186"></a>\end{aligned}$$</span>
<span id="cb4-187"><a href="#cb4-187"></a>where the expectation is taken over $\hat{y} \sim p(\hat{y} | \mathcal{D}; x), y \sim p(y|x)$. The first term represents the variance of the true label $y$, the second term evaluates to zero since $\mathbb{E}_{\hat{y}, y}<span class="co">[</span><span class="ot">\mathbb{E}[y|x] - y</span><span class="co">]</span> = 0$, and the third term accounts for the variance of the model's prediction $\hat{y}$:</span>
<span id="cb4-188"><a href="#cb4-188"></a>$$\mathbb{E}(\hat{y} - \mathbb{E}<span class="co">[</span><span class="ot">y|x</span><span class="co">]</span>)^2 = \mathbb{E}<span class="co">[</span><span class="ot">(\hat{y} - \mathbb{E}[\hat{y}] + \mathbb{E}[\hat{y}] - \mathbb{E}[y|x])^2</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">(\hat{y} - \mathbb{E}[\hat{y}])^2</span><span class="co">]</span> + (\mathbb{E}<span class="co">[</span><span class="ot">\hat{y}</span><span class="co">]</span> - \mathbb{E}<span class="co">[</span><span class="ot">y|x</span><span class="co">]</span>)^2$$</span>
<span id="cb4-189"><a href="#cb4-189"></a></span>
<span id="cb4-190"><a href="#cb4-190"></a>Hence,</span>
<span id="cb4-191"><a href="#cb4-191"></a>$$\mathbb{E} (\hat{y} - y)^2 = \mathbb{E}_{y} [(y - \mathbb{E}[y|x])^2] + (\mathbb{E}_{\hat{y}} [\hat{y} - \mathbb{E}[y|x]] )^2 + \mathbb{E}_{\hat{y}} [(\hat{y} - \mathbb{E}_{\hat{y}}<span class="co">[</span><span class="ot">\hat{y}</span><span class="co">]</span>)^2]$$</span>
<span id="cb4-192"><a href="#cb4-192"></a></span>
<span id="cb4-193"><a href="#cb4-193"></a>Here, the first term signifies the variance of the true label, which remains constant for a given $x$. The second term captures how much the average model prediction deviates from the expected true label. The third term quantifies the model's uncertainty at $x$. @AL_variance denotes the uncertainty term as $\sigma^2_{\hat{y}} (x | \mathcal{D}) = \mathbb{E}_{\hat{y}} [(\hat{y} - \mathbb{E}_{\hat{y}}[\hat{y}])^2]$. The acquisition function is $\mathbb{E}_{p(x)} [\sigma^2_{\hat{y}} (x | \tilde{\mathcal{D}})]$. One could rely on empirical measure like a loss on test labelled data to gauge model improvement, which can help decide the termination of data acquisition. The size of the data set and its relationship to the loss is tied to the model complexity. To evaluate the performance of variance reduction strategy, @AL_variance studies the Arm2D problem. Arm2D is a kinematics problem where learner has to predict the tip position of a robotic arm given a set of joint angles $\mathbf{\theta_1}, \mathbf{\theta_2}$. In this analysis, the two models are the Gaussian mixture model and locally-weighted regression (LOESS). The results shown that the variance of the learner decreases because the authors selected points to minimize expected variance. Additionally, we observe a related decrease in the mean square error (MSE) of both models as the dataset size increases. This is a notable outcome because the expected learner variance for these models can be computed accurately and efficiently relative to a new point. When integrated into the general AL loop, this significantly enhances model performance. In the case of the locally-weighted regression model (<span class="co">[</span><span class="ot">@fig-empirical:regress</span><span class="co">]</span>), it is surprising that if points were chosen randomly, the MSE would be highly unstable, with sharp fluctuations. However, when AL by variance reduction is applied, using expected learner variance as a proxy, the MSE decreases almost smoothly, aside from some initial instabilities.</span>
<span id="cb4-194"><a href="#cb4-194"></a></span>
<span id="cb4-195"><a href="#cb4-195"></a><span class="fu">## Active Preference Learning with Ideal Point Model</span></span>
<span id="cb4-196"><a href="#cb4-196"></a>For any $n$ elements to be ranked, there are $n!$ possible orderings that can result in the correct complete ranking. Given that a lower bound on sorting is $n\log n$, obtaining a guaranteed true rating over $n$ items requires $n\log n$ pairwise comparisons if those comparisons are chosen at random. This number can be quite high and costly in many applications, especially since most ranking information comes from humans. The more comparisons they have to make, the more money and time is spent. This process can also be inefficient, as some comparisons provide more value to the learning process than others, making some comparisons a waste. This inefficiency can be detrimental in fields like psychology and market research, where comparisons are heavily utilized, and a faster process could offer significant benefits. The reason the lower bound on the number of comparisons is $n\log n$ is that it assumes no prior information about the underlying space and field, so comparisons are chosen at random. However, leveraging the structures within the comparison space can provide more information about which comparisons are most valuable. For example, <span class="co">[</span><span class="ot">@geo_paper</span><span class="co">]</span> discusses how eye doctors have a wide range of options when assigning prescriptions for glasses, yet patients do not see them making many comparisons before deciding on the best option. This is because eye doctors incorporate domain knowledge into the process and only ask clients for comparisons when necessary. Applying similar knowledge in the ranking field leads to an AL approach that selects data based on the relevance of a comparison query toward finding the final $\sigma(\Theta)$.</span>
<span id="cb4-197"><a href="#cb4-197"></a></span>
<span id="cb4-198"><a href="#cb4-198"></a>@geo_paper explores AL within data that can be embedded in a $d$-dimensional embedding space, where comparisons between two different items divide the space into halves, with one object being superior in each half. By leveraging such geometry, the paper develops a geometric AL approach. Let $\theta$ be the item representation in the embedding space. For each ranking $\sigma$, there is a reference point $r_{\sigma} \in \mathbb{R}^d$, such that if $\theta_{i} \succ \theta_{j}$, $||\theta_i - r_{\sigma}|| &lt; ||\theta_j - r_{\sigma}||$. In other words, object $i$ is closer to the reference point $r_{\sigma}$ than object $j$. $\Sigma_{n,d}$ is the set of all possible rankings of the $n$ items that satisfy the above embedding distances condition. Not all rankings will satisfy the embedding conditions, but multiple rankings might satisfy all those conditions. For every ranking $\sigma$, there is $M_n(\sigma)$, the number of pairwise comparisons needed to identify the ranking. When comparisons are done at random, $\mathbb{E}<span class="co">[</span><span class="ot">M_n(\sigma)</span><span class="co">]</span> = n\log n$, and it can be reduced by incorporating geometry. $q_{i,j}$ is the query of comparison between items $i$ and $j$.</span>
<span id="cb4-199"><a href="#cb4-199"></a></span>
<span id="cb4-200"><a href="#cb4-200"></a>As an example, @geo_paper studies a 2D space with three items: $\theta_1$, $\theta_2$, and $\theta_3$. There are pairwise queries $q_{1,3}$, $q_{2,3}$, and $q_{1,2}$ between them, denoted by solid lines equidistant from the two items they compare. These lines split the $R^2$ space into halves, with each half closer to one of the two items. The paper colors the side of the worse object for each query in dark grey and takes the intersection of these halves, resulting in the dark grey region in the image. This region indicates $\Sigma_{n,2}$ since all points follow the embedding conditions. Specifically, for every point $r$ in the dark grey area, $||\theta_3 - r|| &lt; ||\theta_2 - r|| &lt; ||\theta_1 - r||$, meaning $\theta_3 &lt; \theta_2 &lt; \theta_1$. Thus, every point $r$ is one of the $r_\sigma$ representing their respective rankings $\sigma \in \Sigma_{n,2}$. In other words, the paper aims to have the reference points and dark grey region closest to the worst object and furthest from the best object.</span>
<span id="cb4-201"><a href="#cb4-201"></a></span>
<span id="cb4-202"><a href="#cb4-202"></a>The authors also denote the label for each query $q_{i,j}$, such as label $y_{i,j} = 1<span class="sc">\{</span>q_{i,j}<span class="sc">\}</span>$ (for example, $y_{1,2} = 0, y_{3,2} = 1$). This allows for deciding how to label new queries represented by dashed and dotted lines, depending on which items each query compares. Focusing on the dotted line, called $q_{i,4}$, where $i={1,2,3}$, and considering potential locations of $\theta_4$, the line must be equidistant from one of the three items in the picture and $\theta_4$, meaning $\theta_4$ can be placed in three different locations. If the query performed is $q_{2,4}$, then $\theta_4$ will be closer to the dark grey area than $\theta_2$, thus $y_{2,4} = 0$. However, if $q_{1,4}$ or $q_{3,4}$ are performed, $\theta_4$ will be further from the dark grey area than $\theta_1$ or $\theta_3$, meaning $y_{1,4} = y_{3,4} = 1$. In this case, the labels are contradictory and depend on which object they are compared with, making such a query $q_{i,4}$ ambiguous.</span>
<span id="cb4-203"><a href="#cb4-203"></a></span>
<span id="cb4-204"><a href="#cb4-204"></a>In contrast, the authors analyze the dashed line, called $q_{i,5}$, where $i={1,2,3}$, and consider potential locations of $\theta_5$. Since the line must be equidistant from one of the three items in the picture and $\theta_5$, it can be placed in three different locations. If one of the three potential queries is performed, $\theta_5$ will be closer to the dark grey area than $\theta_1$, $\theta_2$, and $\theta_3$, meaning $y_{1,5} = y_{2,5} = y_{3,5} = 0$. In this case, all labels are the same regardless of which object is used, meaning such a query will not be contradictory, as all agree on the label. The goal is to perform as many ambiguous queries as possible and skip non-ambiguous queries to decrease the total $M_n(\sigma)$. Intuitively, if there is contradictory information about a query, it needs to be erformed so that a human can clarify its direction. Conversely, if all sources of information from the domain space agree on the query's label, that information can be used without asking a human, incorporating the knowledge of the embedding distances. Lastly, to consider the general case of the $R^d$ space, rather than discussing halves of the image, it is essential to discuss half-spaces. Similarly, consider the half-space that assigns a label of $1$ to the query and the half-space assigning a label of $0$. If both half-spaces exist, they have conflicting information on the query, making the query ambiguous. However, if one of the half-spaces does not exist, it means the other is the full space, representing consistency in the label assignment and a non-ambiguous query.</span>
<span id="cb4-205"><a href="#cb4-205"></a></span>
<span id="cb4-206"><a href="#cb4-206"></a>It is important to demonstrate that the number of comparisons decreases. Specifically, <span class="co">[</span><span class="ot">@geo_paper</span><span class="co">]</span> shows that this algorithm has $E<span class="co">[</span><span class="ot">M_n(\sigma)</span><span class="co">]</span> = O(d\log n)$, where $d$ is the dimension of the space and $d &lt; n$, which improves on the $O(n\log n)$ baseline. The proof can be studied in detail in the paper itself, but at a high level, it starts by reasoning about the probability of a query being ambiguous and a comparison being requested from a human, thus representing $M_n = \Sigma_{k=1}^{n-1}\Sigma_{i=1}^k 1<span class="sc">\{</span>Requestq_{i,k+1}<span class="sc">\}</span>$. For that, the authors define $Q(i,j)$, which represents the number of different rankings that exist for $i$ elements in $j$-dimensional space (e.g., $Q(1,d) = 1, Q(n,0) = 1, Q(n,1) = n!$). In that case, $|\Sigma_{n,d}| = Q(n,d)$. Further, using recurrence relations for $Q(i,j)$, the authors derive that $|\Sigma_{n,d}| = Q(n,d) = O(n^{2d})$, which is omitted here. Analogously, the authors define $P(i,j)$, which represents the number of rankings in $\Sigma_{n,d}$ that will still be possible with the addition of a new element $i+1$ to the ranking items. $P(i,j)$ estimates how much of the dark grey area will still exist after making a query for $i+1$. As indicated there, the dotted line ambiguous query did not change the dark grey a rea at all ($P(n,d) = Q(n,d)$), whereas the dashed non-ambiguous query would cut a piece from it ($P(n,d) &lt; Q(n,d)$). Thus, $Request q_{i,k+1} = P(k,d) / Q(k,d)$, so a higher value indicates more possible rankings and an ambiguous query that needs to be requested to obtain more useful information. With this in mind, the authors derive that $E<span class="co">[</span><span class="ot">M_n(\sigma)</span><span class="co">]</span> = O(d\log n)$, showing that fewer queries are needed for effective ranking.</span>
<span id="cb4-207"><a href="#cb4-207"></a></span>
<span id="cb4-208"><a href="#cb4-208"></a>The issue with this algorithm is that only one human provides the answers to the requested queries, which means it does not account for their biases. An alternative approach is a Robust Query Selection Algorithm (RQSA) <span class="co">[</span><span class="ot">@geo_paper</span><span class="co">]</span>, which uses majority voting for every query to indicate the ground truth of the query's label. However, the authors consider that a group of people can still give incorrect or divided responses. If the votes for each answer are almost equal in number, the authors push that query to the end of the algorithm to see if it can become a non-ambiguous query with more information learned. If it does not, an odd number of voters is used to determine the final ranking.</span>
<span id="cb4-209"><a href="#cb4-209"></a></span>
<span id="cb4-210"><a href="#cb4-210"></a>::: {#tbl-geo_acc}</span>
<span id="cb4-211"><a href="#cb4-211"></a>  Dimension                           2      3</span>
<span id="cb4-212"><a href="#cb4-212"></a>  --------------- ----------------- ------ ------</span>
<span id="cb4-213"><a href="#cb4-213"></a>  \% of queries         mean         14.5   18.5</span>
<span id="cb4-214"><a href="#cb4-214"></a>                        std          5.3     6</span>
<span id="cb4-215"><a href="#cb4-215"></a>  Average error    $d(\bar{y}, y)$   0.23   0.21</span>
<span id="cb4-216"><a href="#cb4-216"></a>                   $d(\bar{y}, y)$   0.31   0.29</span>
<span id="cb4-217"><a href="#cb4-217"></a></span>
<span id="cb4-218"><a href="#cb4-218"></a>  : Statistics for the Robust Query Selection Algorithm (RQSA) <span class="co">[</span><span class="ot">@geo_paper</span><span class="co">]</span> and the baseline of conducting all comparisons. $y$ serves as a noisy ground truth, $\tilde{y}$ is the result of all comparisons, and $\hat{y}$ is the output of the RQSA.</span>
<span id="cb4-219"><a href="#cb4-219"></a>:::</span>
<span id="cb4-220"><a href="#cb4-220"></a></span>
<span id="cb4-221"><a href="#cb4-221"></a>With regard to the accuracy and performance of the method, the authors did a ranking experiment on 100 different audio signals, results of which can be seen in @tbl-geo_acc. The ground truth labels came from humans, indicated by $y$ in the table. That resulted in the existence of noise and potential errors in the ground truth, which could influence the performance of both the baseline algorithm that does all comparisons ($\tilde{y}$) and the Robust Query Selection Algorithm (RQSA) ($\hat{y}$). As can be seen in both 2 and 3-dimensional spaces RQSA performed worse by $8\%$ compared to the baseline, which indicates that AL that uses the domain information can still be erroneous due to the inference of certain comparisons that sometimes may not be entirely correct. However, as can be seen by the upper part of @tbl-geo_acc, significantly less queries were requested compared to the baseline, which means that the approach can have a significant benefit at a cost of slight loss in accuracy.</span>
<span id="cb4-222"><a href="#cb4-222"></a></span>
<span id="cb4-223"><a href="#cb4-223"></a><span class="fu">#### User Information as Domain Knowledge for Active Learning {#sec-geo_app .unnumbered}</span></span>
<span id="cb4-224"><a href="#cb4-224"></a></span>
<span id="cb4-225"><a href="#cb4-225"></a>An alternative source of domain knowledge could be users themselves, who can indicate their uncertainty when it comes to comparing two items. Prior studies have shown <span class="co">[</span><span class="ot">@unnoisy_humans</span><span class="co">]</span> that when presented with only two options when selecting which object is better, but not being able to properly decide, users would get frustrated and tend to respond more faultyly, creating noise and incorrect responses in the data. Through feedback and other studies <span class="co">[</span><span class="ot">@noisy_humans</span><span class="co">]</span> it was determined that presenting users with an option of indifference between the two items can remove those problems. Moreover, in connection to AL, the authors show that such an option helps to select more informative queries since it provides more domain knowledge that can be used, resulting in a decrease in the number of queries required. For this problem, the following terms are defined:</span>
<span id="cb4-226"><a href="#cb4-226"></a></span>
<span id="cb4-227"><a href="#cb4-227"></a><span class="ss">1.  </span>$c$ - a cost function that represents user preferences, and the result the model has to determine at the end of training. The preferred items will have lower costs, and less preferred ones will have higher costs. The goal is to determine this function with the fewest possible number of queries using AL.</span>
<span id="cb4-228"><a href="#cb4-228"></a></span>
<span id="cb4-229"><a href="#cb4-229"></a><span class="ss">2.  </span>$H$ - a set of hypotheses over the possible cost functions, where for each $h \in H$ there is a cost function $c_h$ associated with it.</span>
<span id="cb4-230"><a href="#cb4-230"></a></span>
<span id="cb4-231"><a href="#cb4-231"></a><span class="ss">3.  </span>$h^*$ - a true hypothesis that the model needs to determine, which has cost $c_{h^*}$ associated with it</span>
<span id="cb4-232"><a href="#cb4-232"></a></span>
<span id="cb4-233"><a href="#cb4-233"></a><span class="ss">4.  </span>$t(x,y)$ - a test performed to compare items $x$ and $y$ (the user is being asked to provide a response to which item is better). Those tests result in changes and adjustments to $H$ as more information is learned.</span>
<span id="cb4-234"><a href="#cb4-234"></a></span>
<span id="cb4-235"><a href="#cb4-235"></a><span class="ss">5.  </span>$o(x,y)$ - observation or result of $t(x,y)$, where $o(x,y) \in <span class="sc">\{</span>x&lt;y, x&gt;y<span class="sc">\}</span>$</span>
<span id="cb4-236"><a href="#cb4-236"></a></span>
<span id="cb4-237"><a href="#cb4-237"></a><span class="ss">6.  </span>$S = <span class="sc">\{</span>(t_1, o_1), (t_2, o_2),...,(t_m, o_m)<span class="sc">\}</span>$ - a sequence of $m$ pairs of tests and observations</span>
<span id="cb4-238"><a href="#cb4-238"></a></span>
<span id="cb4-239"><a href="#cb4-239"></a><span class="ss">7.  </span>$w(H|S)$ - probability mass of all hypotheses that are still consistent with the observations (similar to the dark grey area and $Q(i,j)$). This means that if $h \in H$ is inconsistent with user responses received, it is removed from $H$.</span>
<span id="cb4-240"><a href="#cb4-240"></a></span>
<span id="cb4-241"><a href="#cb4-241"></a>With the key terms defined, let's consider the noiseless base setting where users only have two options for response. Those components will also later be translated to the setting with the third option so the true cost function can be determined there. $w(H|S)$ is the sum of the weights of all hypotheses that are still consistent with the evidence: $w(H|S) = \sum_{h \in H} w(h | S)$. Each $w(h|S)$ is a probability of the evidence's existence given such hypothesis: $w(h|S) = p(S|h)$. Such probability comes from the test-observation pairs since they compose the set $S$. Moreover, each test is independent of other tests, which gives $p(S|h) = \prod_{(t,o) \in S} p((t,o) | h)$. In the noiseless setting, users will select an option that minimizes their cost function (selecting more preferred items), mathematically defined as:</span>
<span id="cb4-242"><a href="#cb4-242"></a>$$\begin{aligned}</span>
<span id="cb4-243"><a href="#cb4-243"></a>    p((t, o = x) | h) = </span>
<span id="cb4-244"><a href="#cb4-244"></a>    \begin{cases}</span>
<span id="cb4-245"><a href="#cb4-245"></a>        1 &amp; c_h(x) &lt; c_h(y)<span class="sc">\\</span></span>
<span id="cb4-246"><a href="#cb4-246"></a>        0 &amp; else</span>
<span id="cb4-247"><a href="#cb4-247"></a>    \end{cases}</span>
<span id="cb4-248"><a href="#cb4-248"></a>\end{aligned}$$</span>
<span id="cb4-249"><a href="#cb4-249"></a></span>
<span id="cb4-250"><a href="#cb4-250"></a>Users are not perfect evaluators. Prior work <span class="co">[</span><span class="ot">@unnoisy_humans</span><span class="co">]</span> has shown that treating users as perfect can lead to poor performance. That gave rise to accounting for noise in users' responses, but a majority of such work applies the same noise to all queries and all responses. While those led to great performance results <span class="co">[</span><span class="ot">@noisy_humans</span><span class="co">]</span>, they don't accurately reflect the real world, which gave rise to the idea of creating query-based noise. Effectively, for some of the queries it is important to incorporate the fact that the user is unsure and noisy, but for others, if the user is confident, noise in the response is not needed at all. For comparison-based learning, this means that the noise is related to the costs of the two items compared. Specifically for items $x$ and $y$, if $c_{h^*}(x) \simeq c_{h^*}(y)$ then the items are hard to distinguish for the user, so here it is preferred to incorporate user uncertainty and noise. But if $c_{h^*}(x) &gt;&gt; c_{h^*}(y)$, the user will certainly select $y$ and the other way around, which is where the noise is not needed. Query-dependent noise is also supported in the psychology literature, which means that such an approach is more related to the real world. In particular, psychologists talk about the Luce-Sheppard Choice rule <span class="co">[</span><span class="ot">@lus-shep</span><span class="co">]</span> when talking about comparisons. This rule previously gave rise to a logistic model based on the noise <span class="co">[</span><span class="ot">@lus-log</span><span class="co">]</span> where the probability of observation for a given test is $p((t, o = x) | h) \propto exp(-\gamma * c_h(x))$</span>
<span id="cb4-251"><a href="#cb4-251"></a></span>
<span id="cb4-252"><a href="#cb4-252"></a><span class="al">![User response model in the noiseless setting](Figures/Noiseless probs.png)</span>{#fig-noiseless_1 width="100%"}</span>
<span id="cb4-253"><a href="#cb4-253"></a></span>
<span id="cb4-254"><a href="#cb4-254"></a><span class="al">![User response with Luce Sheppard noise model](Figures/Noise probs.png)</span>{#fig-noiseless_2 width="100%"}</span>
<span id="cb4-255"><a href="#cb4-255"></a></span>
<span id="cb4-256"><a href="#cb4-256"></a><span class="co">[</span><span class="ot">@fig-noiseless_1; @fig-noiseless_2</span><span class="co">]</span> demonstrate the difference between the noiseless setting and incorporating the Luce-Sheppard Choice rule. GBS is the baseline model with only 2 response options, and CLAUS is the model with the uncertainty option added. The figures show how incorporating such noise influences and smoothes the probability distribution of the user's response.</span>
<span id="cb4-257"><a href="#cb4-257"></a></span>
<span id="cb4-258"><a href="#cb4-258"></a>We will now discuss the functionality of CLAUS, which is an algorithm designed by <span class="co">[</span><span class="ot">@claus</span><span class="co">]</span> that allows users to select an uncertain response about the two options that they need to rank. The authors model such uncertainty as $\epsilon$ and it is associated with each $c_h$, so now every hypothesis $h$ is defined over a pair of $(c_h, \epsilon_h)$. It is important to note that the goal is to still learn and maintain our objective on $c$, $\epsilon$ is only necessary to model the users' responses. The uncertainty relates to the cost function as $|c_h(x) - c_h(y)| &lt; \epsilon_h$. This means that the user is uncertain between items $x$ and $y$ and their cost difference is negligible such that the user is not able to select which item is better. This in turn gives more information about the real value of the two items, as a binary response would indicate the user's preference towards one item, which will not be real and will skew the cost functions. This causes modifications of the problem set-up:</span>
<span id="cb4-259"><a href="#cb4-259"></a></span>
<span id="cb4-260"><a href="#cb4-260"></a><span class="ss">1.  </span>For test $t(x,y)$ the observation will be $o(x,y) \in <span class="sc">\{</span>x&lt;y, x&gt;y, \tilde{xy}<span class="sc">\}</span>$, where $\tilde{xy}$ is the uncertain response.</span>
<span id="cb4-261"><a href="#cb4-261"></a></span>
<span id="cb4-262"><a href="#cb4-262"></a><span class="ss">2.  </span>The probability distribution over the user's response (<span class="co">[</span><span class="ot">@eq-prob_base</span><span class="co">]</span>) will now be defined as:</span>
<span id="cb4-263"><a href="#cb4-263"></a></span>
<span id="cb4-264"><a href="#cb4-264"></a>$$\begin{aligned}</span>
<span id="cb4-265"><a href="#cb4-265"></a>    p((t, o = x) | h) = </span>
<span id="cb4-266"><a href="#cb4-266"></a>    \begin{cases}</span>
<span id="cb4-267"><a href="#cb4-267"></a>        1 &amp; c_h(x) &lt; c_h(y) - \epsilon_h<span class="sc">\\</span></span>
<span id="cb4-268"><a href="#cb4-268"></a>        0 &amp; else</span>
<span id="cb4-269"><a href="#cb4-269"></a>    \end{cases}, \quad</span>
<span id="cb4-270"><a href="#cb4-270"></a>    p((t, o = \tilde{xy}) | h) = </span>
<span id="cb4-271"><a href="#cb4-271"></a>    \begin{cases}</span>
<span id="cb4-272"><a href="#cb4-272"></a>        1 &amp; |c_h(x) - c_h(y)|^2 &lt; \epsilon_h^2<span class="sc">\\</span></span>
<span id="cb4-273"><a href="#cb4-273"></a>        0 &amp; else</span>
<span id="cb4-274"><a href="#cb4-274"></a>    \end{cases}</span>
<span id="cb4-275"><a href="#cb4-275"></a>\end{aligned}$$</span>
<span id="cb4-276"><a href="#cb4-276"></a></span>
<span id="cb4-277"><a href="#cb4-277"></a>This means the user confidently selects $x$ when it is better than $y$ by more than $\epsilon$, but if the squared difference of the cost functions of two items is negligible by $\epsilon$ user will choose the indifferent option.</span>
<span id="cb4-278"><a href="#cb4-278"></a></span>
<span id="cb4-279"><a href="#cb4-279"></a><span class="ss">3.  </span>Finally this also updates the noise model:</span>
<span id="cb4-280"><a href="#cb4-280"></a>$$\begin{aligned}</span>
<span id="cb4-281"><a href="#cb4-281"></a>    &amp;p((t, o = x) | h) \propto \exp(-\gamma * <span class="co">[</span><span class="ot">c_h(x) - c_h(y)</span><span class="co">]</span>) <span class="sc">\\</span></span>
<span id="cb4-282"><a href="#cb4-282"></a>    &amp;p((t, o = \tilde{xy}) | h) \propto exp(-1/\epsilon_h^2 * <span class="co">[</span><span class="ot">c_h(x) - c_h(y)</span><span class="co">]</span>^2)</span>
<span id="cb4-283"><a href="#cb4-283"></a>\end{aligned}$$</span>
<span id="cb4-284"><a href="#cb4-284"></a></span>
<span id="cb4-285"><a href="#cb4-285"></a>Rather than predicting a specific pair $(c_h, \epsilon_h)$, the algorithm focuses on predicting a group of pairs that are similar to one another, otherwise called equivalence class (@fig-equiv_c), which indicates not essentially different hypothesis for the cost function and uncertainty. That information is learned through each new test, as the algorithm updates the information about $c$ and $\epsilon$ that distinguishes between the distinct $h$, finding the equivalence groups among them. Moreover, the authors tweaked the parameter responsible for the size of the equivalence class (how many hypotheses can be grouped together at a time).</span>
<span id="cb4-286"><a href="#cb4-286"></a></span>
<span id="cb4-287"><a href="#cb4-287"></a>::: {#tbl-claus_tab}</span>
<span id="cb4-288"><a href="#cb4-288"></a>  Category                        Accuracy                       Query Count</span>
<span id="cb4-289"><a href="#cb4-289"></a>  --------------------- ------------------------------------ ------------------------------------</span>
<span id="cb4-290"><a href="#cb4-290"></a>  GBS - About Equal               $94.15 \pm 0.52$                     $36.02 \pm 0.03$</span>
<span id="cb4-291"><a href="#cb4-291"></a>  GBS - Not Sure         $\textbf{94.66} \pm \textbf{0.55}$            $35.95 \pm 0.04$</span>
<span id="cb4-292"><a href="#cb4-292"></a>  CLAUS - About Equal             $91.56 \pm 0.84$            $\textbf{25.93} \pm \textbf{0.41}$</span>
<span id="cb4-293"><a href="#cb4-293"></a>  CLAUS - Not Sure                $90.86 \pm 0.74$                     $26.98 \pm 0.47$</span>
<span id="cb4-294"><a href="#cb4-294"></a></span>
<span id="cb4-295"><a href="#cb4-295"></a>  : Performance of GBS and CLAUS with different labels for the uncertainty</span>
<span id="cb4-296"><a href="#cb4-296"></a>:::</span>
<span id="cb4-297"><a href="#cb4-297"></a></span>
<span id="cb4-298"><a href="#cb4-298"></a>The first performance evaluation is done on the number of queries and confirms that it decreases. The GBS model serves as the baseline, as it will do all of the comparison queries using the binary response options. The CLAUS model is measured over different values of $\epsilon$ on the x-axis and over different sizes of the equivalence sets indicated by different shades of blue. Figure shows that all variants of CLAUS use approximately 10 fewer queries on average compared to GBS. Moreover, using bigger-sized equivalence classes can further decrease the number of needed queries. The most optimal $\epsilon \simeq 0.07$, after which higher $\epsilon$ does not provide any benefit.</span>
<span id="cb4-299"><a href="#cb4-299"></a></span>
<span id="cb4-300"><a href="#cb4-300"></a>Lastly, the authors considered the performance difference, which is indicated in @tbl-claus_tab. For that authors used two different labels for the uncertainty button in CLAUS, it was either labeled as "About Equal" or "Not Sure" as those can provoke different responses and feelings in users. Moreover, GBS and CLAUS-type responses were mixed in the same set of questions to the user, which splits the metrics for both in two as can be seen in @tbl-claus_tab. The performance of CLAUS is lower by $3\%$ on average, showing that a smaller number of queries can still lead to a performance loss. However, the second column of @tbl-claus_tab supports the information, as it also shows that 10 fewer queries were conducted on average.</span>
<span id="cb4-301"><a href="#cb4-301"></a></span>
<span id="cb4-302"><a href="#cb4-302"></a>AL can be essential in learning within dynamic systems and environments. Say we have an agent in an environment, and we want it to conform to a certain behavior as set by a human. How exactly do we go about doing this? In a traditional RL setting, this is solved by a class of algorithms under Inverse Reinforcement Learning. Techniques such as VICE and GAIL attempt to learn a reward function that can distinguish between states visited by the agent and states desired to be visited as defined by a human. In effect, a human will demonstrate what it would like the agent to do in the environment, and from there, learning is done. However, what if humans do not precisely know how an agent should optimally behave in an environment but still have some opinion on what trajectories would be better than others? This is where a paper like Active Preference-Based Learning of Reward Functions comes into the picture. The paper aims to use human preferences to aid an agent's learning within a dynamic system.</span>
<span id="cb4-303"><a href="#cb4-303"></a></span>
<span id="cb4-304"><a href="#cb4-304"></a>A dynamic system contains human input, robotic input, and an environment state. The transitions between states is defined by $f_{HR}$, so that we have $x^{t+1} = f_{HR}(x^t, u_R, u_H)$. At a given time step $t$, we have $x_t$, $u_R^t$, and $u_H^t$. This can be encapsulated into a single $d$ dimensional feature vector that the authors denote as $\phi$. The paper then assumes that the underlying reward model we are trying to learn can be represented linearly. If we have our human reward preference function defined as $r_H$, this means we can write $r_H$ as $r_H(x^t, u_R^t, u_H^t) = w^{\intercal}\phi(x^t, u_R^t, u_H^t)$. Because the reward function is linear, we can take the weight vector out of the summation if we want to calculate the reward over an entire trajectory:</span>
<span id="cb4-305"><a href="#cb4-305"></a></span>
<span id="cb4-306"><a href="#cb4-306"></a>$$R_{H}(x^0, u_R, u_H) = \sum_{t=0}^{N} r_{H}(x^t, u^t, u_H^t) \quad \Phi = \sum \phi(x^t, u_R^t, u_H^t) \quad R_H(traj) = w\cdot\Phi(traj)$$ </span>
<span id="cb4-307"><a href="#cb4-307"></a></span>
<span id="cb4-308"><a href="#cb4-308"></a>First, the scale of $w$ does not matter because we only care about the relative rewards produced with $w$ (given two different trajectories, we want to answer the question of which trajectory a human would prefer, i.e. which one has a higher preference reward). This means we can constrain $||w|| &lt;= 1$, so the initial prior is uniform over a unit ball. From here, we can determine a probabilistic expression to assess whether we should prefer trajectory A or B (because it can be noisy with human input). Let $I_t = +1$ if the human prefers trajectory $A$. According to Bradley-Terry model, $p(A \succ B|w) = \sigma(R_H(traj_A) - R_H(traj_B))$. Let $\psi = \Phi(traj_a) - \Phi(traj_b). Then f_{\psi} (w) = p(I_t|w) = \sigma(I_t w^{\intercal}\psi)$. We can update $p(w)$ everytime we get a result from a human preference query using Bayes' rule: $p(w|I_t) &lt;- p(w) \cdot p(I_t|w)$ via Markov chain Monte Carlo method. This paper synthetically generates queries through an optimization process and then presents them to a human to pick between. The idea is that we want to generate a query that maximizes the conditional entropy $H(I|w)$. We want to pick a query that we are most uncertain about given our current weights (thus having the highest conditional entropy given the weights):</span>
<span id="cb4-309"><a href="#cb4-309"></a>$$\max_{x^0, u_R, u_H^A, u_H^B} \min<span class="sc">\{</span>\mathbb{E}<span class="co">[</span><span class="ot">1-f_{\psi}(w)</span><span class="co">]</span>, \mathbb{E}<span class="co">[</span><span class="ot">1 - f_{-\psi}(w)</span><span class="co">]</span><span class="sc">\}</span>$$</span>
<span id="cb4-310"><a href="#cb4-310"></a></span>
<span id="cb4-311"><a href="#cb4-311"></a>To do so, we sample $w_1, ... w_m$ from $p(w)$, approximating the distribution $p(w)$ as $p(w) = \frac{1}{M} \sum \delta (w_i).$ We can now approximate the expectation expression as $E<span class="co">[</span><span class="ot">1 - f_{\psi}(w)</span><span class="co">]</span> = \frac{1}{M} (\sum 1 - f_{\psi}(w_i))$, and now we can optimize the expression to generate a synthetic query. The algorithm itself works well, however there ends up being a bottle neck that each query needs to be synthesized before being sent to the human -- one at a time. There is no room for parallelization and so the authors proposed a second algorithm in a separate paper that allows for the batching of queries:</span>
<span id="cb4-312"><a href="#cb4-312"></a></span>
<span id="cb4-313"><a href="#cb4-313"></a>$$\max_{\xi_{ib+1_A}, \xi_{ib+1_B}, ... , \xi_{ib+b_A}, \xi_{ib+b_B}} \mathbb{H}(I_{ib+1}, I_{ib+2}, .., I_{ib+b} | w)$$</span>
<span id="cb4-314"><a href="#cb4-314"></a></span>
<span id="cb4-315"><a href="#cb4-315"></a>We could consider optimizing this in the greedy fashion. This would mean just synthetically generating $b$ independent queries. The drawback of this method would be that the queries would likely be very similar to each other. The authors propose a few other heuristics that would help guide the algorithm away from generating very similar queries, such as Medioid Selection where we have to cluster $B$ greedy vectors into $b &lt; B$ groups and pick one vector from each group (the medioid). The authors also propose two other methods rooted in providing different queries: boundary medioids selection and successive elimination. The authors test both the non-batched and variety of batched learning algorithms on multiple environments. When graphed over $N$ the non-batched AL approach does in the same ball-park of performance as the batched approaches. However, over time, we see that learning is a much slower process when not-batched.</span>
<span id="cb4-316"><a href="#cb4-316"></a></span>
<span id="cb4-317"><a href="#cb4-317"></a><span class="fu">## Case Study 2: Performance Metric Elicitation {#sec-metric-elicitation}</span></span>
<span id="cb4-318"><a href="#cb4-318"></a></span>
<span id="cb4-319"><a href="#cb4-319"></a>In binary classification problems, selecting an appropriate performance metric that aligns with the real-world task is crucial. The problem of *metric elicitation* aims to characterize and discover the performance metric of a practitioner, reflecting the rewards or costs associated with correct or incorrect classification. For instance, in medical contexts such as diagnosing a disease or determining the appropriateness of a treatment, trade-offs are made for incorrect decisions. Not administering a treatment could lead to the worsening of a disease (a false negative), whereas delivering the wrong treatment could cause adverse side effects worse than not treating the condition (a false positive). Rather than choosing from a limited set of default choices like the F1-score or weighted accuracy, metric elicitation considers the process of devising a metric that best matches the preferences of practitioners or users. This is achieved by querying an "oracle" who provides feedback on proposed potential metrics through pairwise comparisons. Since queries to humans are often expensive, the goal is to minimize the number of comparisons needed.</span>
<span id="cb4-320"><a href="#cb4-320"></a></span>
<span id="cb4-321"><a href="#cb4-321"></a>The motivation for the pairwise comparison aspect of metric elicitation <span class="co">[</span><span class="ot">@pmlr-v89-hiranandani19a</span><span class="co">]</span> stems from a rich history of literature in psychology, economics, and computer science <span class="co">[</span><span class="ot">@pref1; @pref2; @pref3; @pref4; @ab</span><span class="co">]</span>, demonstrating that humans are often ineffective at providing absolute feedback on aspects such as potential prices, user interfaces, or even ML model outputs (hence the comparison-based structure of RLHF, for instance). Additionally, confusion matrices accurately capture binary metrics such as accuracy, $F_\beta$, and Jaccard similarity by recording the number of false positives, true positives, false negatives, and true negatives obtained by a classifier. The main goal of this chapter is to introduce two binary-search procedures that can approximate the oracle's performance metric for two types of metrics (linear and linear-fractional performance metrics) by presenting the oracle with confusion matrices generated by various classifiers. Essentially, we are learning an optimal threshold for classification given a decision boundary for a binary classification problem.</span>
<span id="cb4-322"><a href="#cb4-322"></a></span>
<span id="cb4-323"><a href="#cb4-323"></a>First, we introduce some relevant notation that will later be used to formalize notions of oracle queries, classifiers, and metrics. In this context, $X \in \mathcal{X}$ represents an input random variable, while $Y \in <span class="sc">\{</span>0, 1<span class="sc">\}</span>$ denotes the output random variable. We learn from a dataset of size $n$, denoted by $<span class="sc">\{</span>(x, y)_i\}^n_{i=1}$, which is generated independently and identically distributed (i.i.d.) from some distribution $\mathbb{P}(X, Y)$. The conditional probability of the positive class, given some sample $x$, is denoted by $\eta(\vec{x}) = \mathbb{P}(Y=1 | X=x)$. The marginal probability of the positive class is represented by $\zeta = \mathbb{P}(Y=1)$. The set of all potential classifiers is $\mathcal{H} = \{h : \mathcal{X} \rightarrow \{0,1\}\}$. The confusion matrix for a classifier $h$ is $C(h, \mathbb{P}) \in \mathbb{R}^{2 \times 2}$, where $C_{ij}(h, \mathbb{P}) = \mathbb{P}(Y=i, h=j)$ for $i, j \in \{0,1\}$. These entries represent the false positives, true positives, false negatives, and true negatives, ensuring that $\sum_{i,j}C_{ij}=1$. The set of all confusion matrices is denoted by $\mathcal{C}$. Since $FN(h, \mathbb{P}) = \zeta - TP(h, \mathbb{P})$ and $FP(h, \mathbb{P}) = 1 - \zeta - TN(h, \mathbb{P})$, $\mathcal{C}$ is actually a 2-dimensional space, not a 4-dimensional space.</span>
<span id="cb4-324"><a href="#cb4-324"></a></span>
<span id="cb4-325"><a href="#cb4-325"></a>Any hyperplane in the $(tp, tn)$ space is given by $\ell := a \cdot tp + b \cdot tn = c$, where $a, b, c \in \mathbb{R}$. </span>
<span id="cb4-326"><a href="#cb4-326"></a>Given a classifier $h$, we define a performance metric $\phi : <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>^{2 \times 2} \rightarrow \mathbb{R}$. The value </span>
<span id="cb4-327"><a href="#cb4-327"></a>$\phi(C(h))$, which represents the performance of a classifier with respect to a certain metric, is referred to as the </span>
<span id="cb4-328"><a href="#cb4-328"></a>*utility* of the classifier $h$. We assume, without loss of generality, that a higher value of $\phi$ indicates a better </span>
<span id="cb4-329"><a href="#cb4-329"></a>performance metric for $h$. Our focus is to recover some metric $\phi$ using comparisons between confusion matrices $C(h)$, determined by classifiers $h$, which approximates the oracle's "ground-truth" metric $\phi^*$. Next, we introduce two classes of performance metrics—*Linear Performance Metrics (LPM)* and *Linear-Fractional Performance Metrics (LFPM)*—for which we will present two elicitation algorithms. </span>
<span id="cb4-330"><a href="#cb4-330"></a></span>
<span id="cb4-331"><a href="#cb4-331"></a>An LPM, given constants $<span class="sc">\{</span>a_{11}, a_{01}, a_{10}, a_{00}<span class="sc">\}</span> \in \mathbb{R}^{4}$, is defined as $\phi(C) = a_{11} TP + a_{01} FP + a_{10} FN + a_{00} TN = m_{11} TP + m_{00} TN + m_{0}$, where $m_{11} = (a_{11} - a_{10})$, $m_{00} = (a_{00} - a_{01})$, and $m_{0} = a_{10} \zeta + a_{01} (1 - \zeta)$. This reparametrization simplifies the metric by reducing dimensionality, making it more tractable for elicitation. </span>
<span id="cb4-332"><a href="#cb4-332"></a>One example of an LPM is *weighted accuracy*, defined as $WA = w_1TP + w_2TN$, where adjusting $w_1$ and $w_2$ controls </span>
<span id="cb4-333"><a href="#cb4-333"></a>the relative importance of different types of misclassification. An LFPM, defined by constants $<span class="sc">\{</span>a_{11}, a_{01}, a_{10}, a_{00}, b_{11}, b_{01}, b_{10}, b_{00}<span class="sc">\}</span> \in \mathbb{R}^{8}$, is given by:</span>
<span id="cb4-334"><a href="#cb4-334"></a>$$\phi(C) = \frac{a_{11} TP + a_{01} FP + a_{10} FN + a_{00} TN}{b_{11} TP + b_{01} FP + b_{10} FN + b_{00} TN} = \frac{p_{11} TP + p_{00} TN + p_{0}}{q_{11} TP + q_{00} TN + q_{0}},$$</span>
<span id="cb4-335"><a href="#cb4-335"></a>where $p_{11} = (a_{11} - a_{10})$, $p_{00} = (a_{00} - a_{01})$, $q_{11} = (b_{11} - b_{10})$, $q_{00} = (b_{00} - b_{01})$, </span>
<span id="cb4-336"><a href="#cb4-336"></a>$p_{0} = a_{10} \zeta + a_{01} (1 - \zeta)$, and $q_{0} = b_{10} \zeta + b_{01} (1 - \zeta)$. This parametrization also </span>
<span id="cb4-337"><a href="#cb4-337"></a>simplifies the elicitation process by reducing the number of variables. Common LFPMs include the $F_\beta$ score and Jaccard </span>
<span id="cb4-338"><a href="#cb4-338"></a>similarity, defined as:</span>
<span id="cb4-339"><a href="#cb4-339"></a></span>
<span id="cb4-340"><a href="#cb4-340"></a>$$F_{\beta} = \frac{TP}{\frac{TP}{1+\beta^{2}} - \frac{TN}{1+\beta^{2}} + \frac{\beta^{2} \zeta + 1 - \zeta}{1+\beta^{2}}}, \quad JAC = \frac{TP}{1 - TN}.$$ {#eq-lfpm_metrics}</span>
<span id="cb4-341"><a href="#cb4-341"></a></span>
<span id="cb4-342"><a href="#cb4-342"></a>Setting $\beta = 1$ gives the F1 score, which is widely used as a classification metric. Since we are considering all possible metrics in the LPM and LFPM families, we need to make certain assumptions about $\mathcal{C}$. Particularly, we will assume that $g(t) = \mathbb{P}<span class="co">[</span><span class="ot">\eta(X) \geq t</span><span class="co">]</span>$ is continuous and strictly decreasing for $t \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$; essentially, $\eta$ has positive density and zero probability.</span>
<span id="cb4-343"><a href="#cb4-343"></a></span>
<span id="cb4-344"><a href="#cb4-344"></a>Additionally, $\mathcal{C}$ is convex, closed, and contained within the rectangle $<span class="co">[</span><span class="ot">0, \zeta</span><span class="co">]</span> \times <span class="co">[</span><span class="ot">0, 1-\zeta</span><span class="co">]</span>$, and is rotationally symmetric around its center, $(\frac{\zeta}{2}, \frac{1-\zeta}{2})$, where the axes represent the proportion of true positives and negatives. The only vertices of $\mathcal{C}$ are $(0, 1-\zeta)$ and $(\zeta, 0)$, corresponding to predicting all $0$'s or all $1$'s on a given dataset. Therefore, $\mathcal{C}$ is strictly convex, and any line tangent to it is tangent at exactly one point, corresponding to one particular confusion matrix. Next, recall that an LPM is represented in terms of three parameters ($\phi = m_{11}TP + m_{00}TN + m_0$). We have just seen that this LPM and its corresponding confusion matrix correspond to a certain point on the boundary of $\mathcal{C}$. We first note that this point is independent of $m_0$. Additionally, we only care about the relative weightings of $m_{11}$ and $m_{00}$, not their actual values—they are scale invariant. Therefore, we can parametrize the space of LPMs as $\varphi_{LPM} = <span class="sc">\{</span>\mathbf{m} = (\cos \theta, \sin \theta) : \theta \in <span class="co">[</span><span class="ot">0, 2\pi</span><span class="co">]</span><span class="sc">\}</span>$, where $\cos \theta$ corresponds to $m_{00}$ and $\sin \theta$ corresponds to $m_{11}$. As we already know, we can recover the Bayes classifier given $\mathbf{m}$, and it is unique, corresponding to one point on the boundary of $\mathcal{C}$ due to its convexity. The supporting hyperplane at this point is defined as $\bar{\ell}_{\mathbf{m}} := m_{11} \cdot tp + m_{00} \cdot tn = m_{11} \overline{TP}_{\mathbf{m}} + m_{00} \overline{TN}_{\mathbf{m}}$. We note that if $m_{00}$ and $m_{11}$ have opposite signs, then $\bar{h}_m$ is the trivial classifier predicting all 1's or all 0's, since either predicting true positives or true negatives results in negative reward. This corresponds to a supporting hyperplane with a positive slope, so it can only be tangent at the vertices. Additionally, the boundary $\partial \mathcal{C}$ can be split into upper and lower boundaries ($\partial \mathcal{C}_{+}, \partial \mathcal{C}_{-}$), corresponding to $\theta \in (0, \pi/2)$ and $\theta \in (\pi, 3\pi/2)$ respectively (and whether $m_{00}, m_{11}$ are positive or negative). We also define the notions of Bayes optimal and inverse-optimal classifiers. Given a performance metric $\phi$, we define:</span>
<span id="cb4-345"><a href="#cb4-345"></a></span>
<span id="cb4-346"><a href="#cb4-346"></a><span class="ss">-   </span>The *Bayes utility* as $\bar{\tau} := \sup_{h \in \mathcal{H}} \phi(C(h)) = \sup_{C \in \mathcal{C}} \phi(C)$; </span>
<span id="cb4-347"><a href="#cb4-347"></a>    this is the highest achievable utility (using the metric $\phi$) over all classifiers $h \in $\mathcal{H}$ for a given problem.</span>
<span id="cb4-348"><a href="#cb4-348"></a><span class="ss">-   </span>The *Bayes classifier* as $\bar{h} := \arg \max_{h \in \mathcal{H}} \phi(C(h))$; this is the classifier $h$ corresponding to the Bayes utility.</span>
<span id="cb4-349"><a href="#cb4-349"></a><span class="ss">-   </span>The *Bayes confusion matrix* as $\bar{C} := \arg \max_{C \in \mathcal{C}} \phi(C)$; this is the confusion matrix </span>
<span id="cb4-350"><a href="#cb4-350"></a>    corresponding to the Bayes utility and classifier.</span>
<span id="cb4-351"><a href="#cb4-351"></a></span>
<span id="cb4-352"><a href="#cb4-352"></a>Similarly, the inverse Bayes utility, classifier, and confusion matrix can be defined by replacing "$\sup$" with "$\inf$"; they represent the classifier and confusion matrix corresponding to the lower bound on utility for a given problem. We also have the following useful proposition:</span>
<span id="cb4-353"><a href="#cb4-353"></a></span>
<span id="cb4-354"><a href="#cb4-354"></a>::: {.callout-note title="proposition"}</span>
<span id="cb4-355"><a href="#cb4-355"></a>::: {#prp-prp3.1}</span>
<span id="cb4-356"><a href="#cb4-356"></a>Let $\phi \in \varphi_{LPM}$. Then</span>
<span id="cb4-357"><a href="#cb4-357"></a></span>
<span id="cb4-358"><a href="#cb4-358"></a>::: {.content-visible when-format="html"}</span>
<span id="cb4-359"><a href="#cb4-359"></a>$$\bar{h}(x) = \left<span class="sc">\{</span>\begin{array}{lr}</span>
<span id="cb4-360"><a href="#cb4-360"></a>\unicode{x1D7D9} \left<span class="co">[</span><span class="ot">\eta(x) \geq \frac{m_{00}}{m_{11} + m_{00}}\right</span><span class="co">]</span>, &amp; m_{11} + m_{00} \geq 0 <span class="sc">\\</span></span>
<span id="cb4-361"><a href="#cb4-361"></a>\unicode{x1D7D9} \left<span class="co">[</span><span class="ot">\frac{m_{00}}{m_{11} + m_{00}} \geq \eta(x)\right</span><span class="co">]</span>, &amp; \text { o.w. }</span>
<span id="cb4-362"><a href="#cb4-362"></a>\end{array}\right<span class="sc">\}</span>$$  {#eq-eq3.45}</span>
<span id="cb4-363"><a href="#cb4-363"></a>:::</span>
<span id="cb4-364"><a href="#cb4-364"></a></span>
<span id="cb4-365"><a href="#cb4-365"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb4-366"><a href="#cb4-366"></a>$$\bar{h}(x) = \left<span class="sc">\{</span>\begin{array}{lr}</span>
<span id="cb4-367"><a href="#cb4-367"></a>\mathbbm{1}\left<span class="co">[</span><span class="ot">\eta(x) \geq \frac{m_{00}}{m_{11} + m_{00}}\right</span><span class="co">]</span>, &amp; m_{11} + m_{00} \geq 0 <span class="sc">\\</span></span>
<span id="cb4-368"><a href="#cb4-368"></a>\mathbbm{1}\left<span class="co">[</span><span class="ot">\frac{m_{00}}{m_{11} + m_{00}} \geq \eta(x)\right</span><span class="co">]</span>, &amp; \text { o.w. }</span>
<span id="cb4-369"><a href="#cb4-369"></a>\end{array}\right<span class="sc">\}</span>$$  {#eq-eq3.46}</span>
<span id="cb4-370"><a href="#cb4-370"></a>:::</span>
<span id="cb4-371"><a href="#cb4-371"></a></span>
<span id="cb4-372"><a href="#cb4-372"></a>is a Bayes optimal classifier with respect to $\phi$. The inverse Bayes classifier is given by $\underline{h} = 1 - \bar{h}$.</span>
<span id="cb4-373"><a href="#cb4-373"></a>:::</span>
<span id="cb4-374"><a href="#cb4-374"></a>:::</span>
<span id="cb4-375"><a href="#cb4-375"></a></span>
<span id="cb4-376"><a href="#cb4-376"></a>This is a simple derivation based on the fact that we only get rewards from true positives and true negatives. </span>
<span id="cb4-377"><a href="#cb4-377"></a>Essentially, if we recover an LPM, we can use it to determine the best-performing classifier, obtained by placing </span>
<span id="cb4-378"><a href="#cb4-378"></a>a threshold on the conditional probability of a given sample, that corresponds to a confusion matrix. Therefore, </span>
<span id="cb4-379"><a href="#cb4-379"></a>the three notions of Bayes utility, classifier, and confusion matrix are functionally equivalent in our setting.</span>
<span id="cb4-380"><a href="#cb4-380"></a></span>
<span id="cb4-381"><a href="#cb4-381"></a>We will now formalize the problem of metric elicitation. Given two classifiers $h$ and $h'$ (or equivalently, </span>
<span id="cb4-382"><a href="#cb4-382"></a>two confusion matrices $C$ and $C'$), we define an *oracle query* as the function:</span>
<span id="cb4-383"><a href="#cb4-383"></a></span>
<span id="cb4-384"><a href="#cb4-384"></a>::: {.content-visible when-format="html"}</span>
<span id="cb4-385"><a href="#cb4-385"></a>$$\Gamma\left(h, h^{\prime}\right)=\Omega\left(C, C^{\prime}\right)=\unicode{x1D7D9}\left<span class="co">[</span><span class="ot">\phi(C)&gt;\phi\left(C^{\prime}\right)\right</span><span class="co">]</span>=: \unicode{x1D7D9} \left<span class="co">[</span><span class="ot">C \succ C^{\prime}\right</span><span class="co">]</span>,$$ {#eq-oracle}</span>
<span id="cb4-386"><a href="#cb4-386"></a>:::</span>
<span id="cb4-387"><a href="#cb4-387"></a></span>
<span id="cb4-388"><a href="#cb4-388"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb4-389"><a href="#cb4-389"></a>$$\Gamma\left(h, h^{\prime}\right)=\Omega\left(C, C^{\prime}\right)=\mathbbm{1}\left<span class="co">[</span><span class="ot">\phi(C)&gt;\phi\left(C^{\prime}\right)\right</span><span class="co">]</span>=: \mathbbm{1} \left<span class="co">[</span><span class="ot">C \succ C^{\prime}\right</span><span class="co">]</span>,$$ {#eq-oracle}</span>
<span id="cb4-390"><a href="#cb4-390"></a>:::</span>
<span id="cb4-391"><a href="#cb4-391"></a></span>
<span id="cb4-392"><a href="#cb4-392"></a>which represents the classifier preferred by the practitioner. We can then define the metric elicitation problem for populations:</span>
<span id="cb4-393"><a href="#cb4-393"></a></span>
<span id="cb4-394"><a href="#cb4-394"></a>::: {.callout-note title="definition"}</span>
<span id="cb4-395"><a href="#cb4-395"></a>::: {#def-def3.1}</span>
<span id="cb4-396"><a href="#cb4-396"></a>Suppose the true (oracle) performance metric is $\phi$. The goal is to recover a metric $\hat{\phi}$ by </span>
<span id="cb4-397"><a href="#cb4-397"></a>querying the oracle for as few pairwise comparisons of the form $\Omega\left(C, C^{\prime}\right)$ so that </span>
<span id="cb4-398"><a href="#cb4-398"></a>$\|\phi - \hat{\phi}\|_{--} &lt; \kappa$ for a sufficiently small $\kappa &gt; 0$ and for any suitable norm $\|\cdot\|_{--}$.</span>
<span id="cb4-399"><a href="#cb4-399"></a>:::</span>
<span id="cb4-400"><a href="#cb4-400"></a>:::</span>
<span id="cb4-401"><a href="#cb4-401"></a></span>
<span id="cb4-402"><a href="#cb4-402"></a>In practice, we do not have access to the true probability distribution or the population, which would </span>
<span id="cb4-403"><a href="#cb4-403"></a>provide the true values of $C$ and $C'$. However, we can subtly alter this problem description to use </span>
<span id="cb4-404"><a href="#cb4-404"></a>$\hat{C}$ and $\hat{C}^{\prime}$, which are derived from our dataset of $n$ samples:</span>
<span id="cb4-405"><a href="#cb4-405"></a></span>
<span id="cb4-406"><a href="#cb4-406"></a>::: {.callout-note title="definition"}</span>
<span id="cb4-407"><a href="#cb4-407"></a>::: {#def-def3.2}</span>
<span id="cb4-408"><a href="#cb4-408"></a>Suppose the true (oracle) performance metric is $\phi$. The aim is to recover a metric $\hat{\phi}$ </span>
<span id="cb4-409"><a href="#cb4-409"></a>by querying the oracle for as few pairwise comparisons of the form $\Omega\left(\hat{C}, \hat{C}^{\prime}\right)$ </span>
<span id="cb4-410"><a href="#cb4-410"></a>so that $\|\phi - \hat{\phi}\|_{--} &lt; \kappa$ for a sufficiently small $\kappa &gt; 0$ and for any suitable norm $\|\cdot\|_{--}$.</span>
<span id="cb4-411"><a href="#cb4-411"></a>:::</span>
<span id="cb4-412"><a href="#cb4-412"></a>:::</span>
<span id="cb4-413"><a href="#cb4-413"></a></span>
<span id="cb4-414"><a href="#cb4-414"></a>As is common in theoretical ML research, we solve the population problem and then consider ways to extend this to </span>
<span id="cb4-415"><a href="#cb4-415"></a>practical settings where we only have limited datasets of samples. In our case, this corresponds to calculating the </span>
<span id="cb4-416"><a href="#cb4-416"></a>confusion matrices from a portion of the dataset we have access to.</span>
<span id="cb4-417"><a href="#cb4-417"></a></span>
<span id="cb4-418"><a href="#cb4-418"></a><span class="fu">### Linear Performance Metric Elicitation {#sec-orgb6dac4e}</span></span>
<span id="cb4-419"><a href="#cb4-419"></a></span>
<span id="cb4-420"><a href="#cb4-420"></a>For LPM elicitation, we need one more proposition.</span>
<span id="cb4-421"><a href="#cb4-421"></a></span>
<span id="cb4-422"><a href="#cb4-422"></a>::: {.callout-note title="proposition"}</span>
<span id="cb4-423"><a href="#cb4-423"></a>::: {#prp-prp3.2}</span>
<span id="cb4-424"><a href="#cb4-424"></a>For a metric $\psi$ (quasiconvex and monotone increasing in TP/TN) or</span>
<span id="cb4-425"><a href="#cb4-425"></a>$\phi$ (quasiconcave and monotone increasing), and parametrization</span>
<span id="cb4-426"><a href="#cb4-426"></a>$\rho^+$/$\rho^-$ of upper/lower boundary, composition</span>
<span id="cb4-427"><a href="#cb4-427"></a>$\psi \circ \rho^-$ is quasiconvex and unimodal on <span class="sc">\[</span>0, 1<span class="sc">\]</span>, and</span>
<span id="cb4-428"><a href="#cb4-428"></a>$\phi \circ \rho^+$ is quasiconcave and unimodal on <span class="sc">\[</span>0, 1<span class="sc">\]</span>.</span>
<span id="cb4-429"><a href="#cb4-429"></a>:::</span>
<span id="cb4-430"><a href="#cb4-430"></a>:::</span>
<span id="cb4-431"><a href="#cb4-431"></a></span>
<span id="cb4-432"><a href="#cb4-432"></a>Quasiconcavity and quasiconvexity are slightly more general variations on concavity and convexity. Their main useful property in our setting is that they are unimodal (they have a singular extremum), so we can devise a binary-search-style algorithm for eliciting the Bayes optimal and inverse-optimal confusion matrices for a given setting, as well as the corresponding $\phi$'s. We first note that to maximize a quasiconcave metric, in which $\phi$ is monotonically increasing in $TP$ and $TN$, we note that the resulting maximizer (and supporting hyperplane) will occur on the upper boundary of $\mathcal{C}$. We thus set our initial search range to be $<span class="co">[</span><span class="ot">0, \pi/2</span><span class="co">]</span>$ and repeatedly divide it into four regions. Then, we calculate the resulting confusion matrix on the 5 resulting boundaries of these regions and query the oracle $4$ times. We repeat this in each iteration of the binary search until a maximizer is found.</span>
<span id="cb4-433"><a href="#cb4-433"></a></span>
<span id="cb4-434"><a href="#cb4-434"></a>::: {.callout-note title="remark"}</span>
<span id="cb4-435"><a href="#cb4-435"></a>::: {#rem-explaination_binary_search}</span>
<span id="cb4-436"><a href="#cb4-436"></a>In the case of quasiconcave and quasiconvex search ranges, a slightly more sophisticated variation on typical </span>
<span id="cb4-437"><a href="#cb4-437"></a>binary search must be used. To illustrate this, consider the two distributions in @fig-bsearch:</span>
<span id="cb4-438"><a href="#cb4-438"></a></span>
<span id="cb4-439"><a href="#cb4-439"></a>::: {#fig-bsearch layout-ncol=2}</span>
<span id="cb4-440"><a href="#cb4-440"></a><span class="al">![First distribution](Figures/normaldistribution.png)</span>{#fig-normal-distribution width="45%"}</span>
<span id="cb4-441"><a href="#cb4-441"></a><span class="al">![Second distribution](Figures/normaldistribution copy.png)</span>{#fig-normal-distribution-copy width="45%"}</span>
<span id="cb4-442"><a href="#cb4-442"></a>:::</span>
<span id="cb4-443"><a href="#cb4-443"></a></span>
<span id="cb4-444"><a href="#cb4-444"></a>For both the symmetric and skewed distributions, if we were to divide the search range into two portions and compare $A$, $C$, and $E$, we would find that $C &gt; A$ and $C &gt; E$. In both cases, this does not help us reduce our search range, since the true maximum could lie on either of the two intervals (as in the second case), or at $C$ itself (as in the first case). Therefore, we must make comparisons between all five points $A, B, C, D, and E$. This allows us to correctly restrict our search range to $<span class="co">[</span><span class="ot">B, D</span><span class="co">]</span>$ in the first case and $<span class="co">[</span><span class="ot">C, E</span><span class="co">]</span>$ in the second. These extra search requirements are due to the quasiconcavity of the search space we are considering, in which there exists a maximum but we need to make several comparisons at various points throughout the search space to be able to reduce its size in each iteration.</span>
<span id="cb4-445"><a href="#cb4-445"></a>:::</span>
<span id="cb4-446"><a href="#cb4-446"></a>:::</span>
<span id="cb4-447"><a href="#cb4-447"></a></span>
<span id="cb4-448"><a href="#cb4-448"></a><span class="in">```pseudocode</span></span>
<span id="cb4-449"><a href="#cb4-449"></a><span class="in">#| label: alg-lpm</span></span>
<span id="cb4-450"><a href="#cb4-450"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb4-451"><a href="#cb4-451"></a><span class="in">    \caption{Quasiconcave Metric Maximization}</span></span>
<span id="cb4-452"><a href="#cb4-452"></a><span class="in">    \begin{algorithmic}</span></span>
<span id="cb4-453"><a href="#cb4-453"></a><span class="in">        \State \textbf{input:} $\epsilon &gt; 0$ and oracle $\Omega$</span></span>
<span id="cb4-454"><a href="#cb4-454"></a><span class="in">        \State \textbf{initialize:} $\theta_a = 0, \theta_b = \frac{\pi}{2}$</span></span>
<span id="cb4-455"><a href="#cb4-455"></a><span class="in">        \While{$|\theta_b - \theta_a| &gt; \epsilon$}</span></span>
<span id="cb4-456"><a href="#cb4-456"></a><span class="in">            \State set $\theta_c = \frac{3\theta_a+\theta_b}{4}$, $\theta_d = \frac{\theta_a+\theta_b}{2}$, and $\theta_e = \frac{\theta_a+3\theta_b}{4}$</span></span>
<span id="cb4-457"><a href="#cb4-457"></a><span class="in">            </span></span>
<span id="cb4-458"><a href="#cb4-458"></a><span class="in">            \State obtain $h\theta_a, h\theta_c, h\theta_d, h\theta_e, h\theta_b$ using Proposition 1</span></span>
<span id="cb4-459"><a href="#cb4-459"></a><span class="in">            </span></span>
<span id="cb4-460"><a href="#cb4-460"></a><span class="in">            \State Compute $C\theta_a, C\theta_c, C\theta_d, C\theta_e, C\theta_b$ using (1)</span></span>
<span id="cb4-461"><a href="#cb4-461"></a><span class="in">            </span></span>
<span id="cb4-462"><a href="#cb4-462"></a><span class="in">            \State Query $\Omega(C\theta_c, C\theta_a), \Omega(C\theta_d, C\theta_c), \Omega(C\theta_e, C\theta_d)$, and $\Omega(C\theta_b, C\theta_e)$</span></span>
<span id="cb4-463"><a href="#cb4-463"></a></span>
<span id="cb4-464"><a href="#cb4-464"></a><span class="in">            \If{$q_{i,j}$ is ambiguous}</span></span>
<span id="cb4-465"><a href="#cb4-465"></a><span class="in">                \State request $q_{i,j}$'s label from reference</span></span>
<span id="cb4-466"><a href="#cb4-466"></a><span class="in">            \Else</span></span>
<span id="cb4-467"><a href="#cb4-467"></a><span class="in">                \State impute $q_{i,j}$'s label from previously labeled queries</span></span>
<span id="cb4-468"><a href="#cb4-468"></a><span class="in">            \EndIf</span></span>
<span id="cb4-469"><a href="#cb4-469"></a><span class="in">            </span></span>
<span id="cb4-470"><a href="#cb4-470"></a><span class="in">            \If{$C\theta' \succ C\theta'' \succ C\theta'''$ for consecutive $\theta &lt; \theta' &lt; \theta''$}</span></span>
<span id="cb4-471"><a href="#cb4-471"></a><span class="in">                \State assume the default order $C\theta \prec C\theta' \prec C\theta''$</span></span>
<span id="cb4-472"><a href="#cb4-472"></a><span class="in">            \EndIf</span></span>
<span id="cb4-473"><a href="#cb4-473"></a></span>
<span id="cb4-474"><a href="#cb4-474"></a><span class="in">            \If{$C\theta' \succ C\theta'' \succ C\theta'''$ for consecutive $\theta &lt; \theta' &lt; \theta''$}</span></span>
<span id="cb4-475"><a href="#cb4-475"></a><span class="in">                \State assume the default order $C\theta \prec C\theta' \prec C\theta''$</span></span>
<span id="cb4-476"><a href="#cb4-476"></a><span class="in">            \EndIf</span></span>
<span id="cb4-477"><a href="#cb4-477"></a><span class="in">            </span></span>
<span id="cb4-478"><a href="#cb4-478"></a><span class="in">            \If{$C\theta_a \succ C\theta_c$} </span></span>
<span id="cb4-479"><a href="#cb4-479"></a><span class="in">                \State Set $\theta_b = \theta_d$ </span></span>
<span id="cb4-480"><a href="#cb4-480"></a><span class="in">            \ElsIf{$C\theta_a \prec C\theta_c \succ C\theta_d$} </span></span>
<span id="cb4-481"><a href="#cb4-481"></a><span class="in">                \State Set $\theta_b = \theta_d$ </span></span>
<span id="cb4-482"><a href="#cb4-482"></a><span class="in">            \ElsIf{$C\theta_c \prec C\theta_d \succ C\theta_e$} </span></span>
<span id="cb4-483"><a href="#cb4-483"></a><span class="in">                \State Set $\theta_a = \theta_c$ </span></span>
<span id="cb4-484"><a href="#cb4-484"></a><span class="in">                \State Set $\theta_b = \theta_e$ </span></span>
<span id="cb4-485"><a href="#cb4-485"></a><span class="in">            \ElsIf{$C\theta_d \prec C\theta_e \succ C\theta_b$} </span></span>
<span id="cb4-486"><a href="#cb4-486"></a><span class="in">                \State Set $\theta_a = \theta_d$ </span></span>
<span id="cb4-487"><a href="#cb4-487"></a><span class="in">            \Else </span></span>
<span id="cb4-488"><a href="#cb4-488"></a><span class="in">                \State Set $\theta_a = \theta_d$ </span></span>
<span id="cb4-489"><a href="#cb4-489"></a><span class="in">            \EndIf</span></span>
<span id="cb4-490"><a href="#cb4-490"></a><span class="in">        \EndWhile</span></span>
<span id="cb4-491"><a href="#cb4-491"></a><span class="in">        \State \textbf{output:} $\vec{m}, C$, and $\vec{l}$, where $\vec{m} = m_l(\theta_d), C = C\theta_d$, and $\vec{l} := (\vec{m}, (tp, tn)) = (\vec{m}, C)$</span></span>
<span id="cb4-492"><a href="#cb4-492"></a><span class="in">    \end{algorithmic}</span></span>
<span id="cb4-493"><a href="#cb4-493"></a><span class="in">\end{algorithm}</span></span>
<span id="cb4-494"><a href="#cb4-494"></a><span class="in">```</span></span>
<span id="cb4-495"><a href="#cb4-495"></a></span>
<span id="cb4-496"><a href="#cb4-496"></a>To elicit LPMs, we run @alg-lpm, querying the oracle in each iteration, and set the elicited metric $\hat{m}$ (which is the maximizer on $\mathcal{C}$) to be the slope of the resulting hyperplane, since the metric is linear.</span>
<span id="cb4-497"><a href="#cb4-497"></a></span>
<span id="cb4-498"><a href="#cb4-498"></a>::: {.callout-note title="remark"}</span>
<span id="cb4-499"><a href="#cb4-499"></a>::: {#rem-explaination_lpm}</span>
<span id="cb4-500"><a href="#cb4-500"></a>To find the minimum of a quasiconvex metric, we flip all instances of</span>
<span id="cb4-501"><a href="#cb4-501"></a>$\prec$ and $\succ$, and use an initial search range of $<span class="co">[</span><span class="ot">\pi, 3\pi/2</span><span class="co">]</span>$;</span>
<span id="cb4-502"><a href="#cb4-502"></a>we use this algorithm, which we refer to as @alg-lfpm, in our</span>
<span id="cb4-503"><a href="#cb4-503"></a>elicitation of LFPMs.</span>
<span id="cb4-504"><a href="#cb4-504"></a>:::</span>
<span id="cb4-505"><a href="#cb4-505"></a>:::</span>
<span id="cb4-506"><a href="#cb4-506"></a></span>
<span id="cb4-507"><a href="#cb4-507"></a>Next, we provide a Python implementation of @alg-lpm.</span>
<span id="cb4-508"><a href="#cb4-508"></a></span>
<span id="cb4-509"><a href="#cb4-509"></a>::: {.callout-note title="code"}</span>
<span id="cb4-512"><a href="#cb4-512"></a><span class="in">```{python}</span></span>
<span id="cb4-513"><a href="#cb4-513"></a><span class="kw">def</span> get_m(theta):</span>
<span id="cb4-514"><a href="#cb4-514"></a>    <span class="co">"""</span></span>
<span id="cb4-515"><a href="#cb4-515"></a><span class="co">    Inputs: </span></span>
<span id="cb4-516"><a href="#cb4-516"></a><span class="co">    - theta: the value that parametrizes m</span></span>
<span id="cb4-517"><a href="#cb4-517"></a><span class="co">    Outputs:</span></span>
<span id="cb4-518"><a href="#cb4-518"></a><span class="co">    - m_0 and m_1 for the LPM</span></span>
<span id="cb4-519"><a href="#cb4-519"></a><span class="co">    """</span></span>
<span id="cb4-520"><a href="#cb4-520"></a></span>
<span id="cb4-521"><a href="#cb4-521"></a>    <span class="cf">return</span> (math.cos(theta), math.sin(theta))</span>
<span id="cb4-522"><a href="#cb4-522"></a></span>
<span id="cb4-523"><a href="#cb4-523"></a><span class="kw">def</span> lpm_elicitation(epsilon, oracle):</span>
<span id="cb4-524"><a href="#cb4-524"></a>    <span class="co">"""</span></span>
<span id="cb4-525"><a href="#cb4-525"></a><span class="co">    Inputs:</span></span>
<span id="cb4-526"><a href="#cb4-526"></a><span class="co">    - epsilon: some epsilon &gt; 0 representing threshold of error</span></span>
<span id="cb4-527"><a href="#cb4-527"></a><span class="co">    - oracle: some function that accepts 2 confusion matrices and</span></span>
<span id="cb4-528"><a href="#cb4-528"></a><span class="co">        returns true if the first is preferred and false otherwise</span></span>
<span id="cb4-529"><a href="#cb4-529"></a><span class="co">    Outputs:</span></span>
<span id="cb4-530"><a href="#cb4-530"></a><span class="co">    - estimate for m, which is used to compute the LPM as described above</span></span>
<span id="cb4-531"><a href="#cb4-531"></a><span class="co">    """</span></span>
<span id="cb4-532"><a href="#cb4-532"></a></span>
<span id="cb4-533"><a href="#cb4-533"></a>    a <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-534"><a href="#cb4-534"></a>    b <span class="op">=</span> math.pi<span class="op">/</span><span class="dv">2</span></span>
<span id="cb4-535"><a href="#cb4-535"></a>    <span class="cf">while</span> (b <span class="op">-</span> a <span class="op">&gt;</span> epsilon):</span>
<span id="cb4-536"><a href="#cb4-536"></a>        c <span class="op">=</span> (<span class="dv">3</span> <span class="op">*</span> a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb4-537"><a href="#cb4-537"></a>        d <span class="op">=</span> (a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb4-538"><a href="#cb4-538"></a>        e <span class="op">=</span> (a <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb4-539"><a href="#cb4-539"></a></span>
<span id="cb4-540"><a href="#cb4-540"></a>        m_a, m_b, m_c, m_d, m_e <span class="op">=</span> (get_m(x) <span class="cf">for</span> x <span class="kw">in</span> [a,b,c,d,e]) <span class="co"># using definition of m</span></span>
<span id="cb4-541"><a href="#cb4-541"></a>        c_a, c_b, c_c, c_d, c_e <span class="op">=</span> (get_c(x) <span class="cf">for</span> x <span class="kw">in</span> [m_a, m_b, m_c, m_d, m_e]) <span class="co"># compute classifier from m's then calculate confusion matrices</span></span>
<span id="cb4-542"><a href="#cb4-542"></a>        </span>
<span id="cb4-543"><a href="#cb4-543"></a>        response_ac <span class="op">=</span> oracle(c_a, c_c)</span>
<span id="cb4-544"><a href="#cb4-544"></a>        response_cd <span class="op">=</span> oracle(c_c, c_d)</span>
<span id="cb4-545"><a href="#cb4-545"></a>        response_de <span class="op">=</span> oracle(c_d, c_e)</span>
<span id="cb4-546"><a href="#cb4-546"></a>        response_eb <span class="op">=</span> oracle(c_e, c_b)</span>
<span id="cb4-547"><a href="#cb4-547"></a></span>
<span id="cb4-548"><a href="#cb4-548"></a>        <span class="co"># update ranges to keep the peak</span></span>
<span id="cb4-549"><a href="#cb4-549"></a>        <span class="cf">if</span> response_ac:</span>
<span id="cb4-550"><a href="#cb4-550"></a>            b <span class="op">=</span> d</span>
<span id="cb4-551"><a href="#cb4-551"></a>        <span class="cf">elif</span> response_cd:</span>
<span id="cb4-552"><a href="#cb4-552"></a>            b <span class="op">=</span> d</span>
<span id="cb4-553"><a href="#cb4-553"></a>        <span class="cf">elif</span> response_de:</span>
<span id="cb4-554"><a href="#cb4-554"></a>            a <span class="op">=</span> c</span>
<span id="cb4-555"><a href="#cb4-555"></a>            b <span class="op">=</span> e</span>
<span id="cb4-556"><a href="#cb4-556"></a>        <span class="cf">elif</span> response_eb:</span>
<span id="cb4-557"><a href="#cb4-557"></a>            a <span class="op">=</span> d</span>
<span id="cb4-558"><a href="#cb4-558"></a>        <span class="cf">else</span>:</span>
<span id="cb4-559"><a href="#cb4-559"></a>            a <span class="op">=</span> d</span>
<span id="cb4-560"><a href="#cb4-560"></a>    <span class="cf">return</span> get_m(d), get_c(d)</span>
<span id="cb4-561"><a href="#cb4-561"></a><span class="in">```</span></span>
<span id="cb4-562"><a href="#cb4-562"></a>:::</span>
<span id="cb4-563"><a href="#cb4-563"></a></span>
<span id="cb4-564"><a href="#cb4-564"></a><span class="fu">### Linear-Fractional Performance Metric Elicitation {#sec-lfpm-elicitation}</span></span>
<span id="cb4-565"><a href="#cb4-565"></a></span>
<span id="cb4-566"><a href="#cb4-566"></a>Now, we present the next main result, which is an algorithm to elicit linear-fractional performance metrics. For this task, we will need the following assumption: Let $\phi \in \varphi_{L F P M}$. We assume $p_{11}, p_{00} \geq 0, p_{11} \geq q_{11}, p_{00} \geq q_{00},$ $p_{0}=0, q_{0}=$ $\left(p_{11}-q_{11}\right) \zeta+\left(p_{00}-q_{00}\right)(1-\zeta)$, and $p_{11}+p_{00}=1$.</span>
<span id="cb4-567"><a href="#cb4-567"></a></span>
<span id="cb4-568"><a href="#cb4-568"></a>These assumptions guarantee that the LFPM $\phi$ which we are trying to elicit is monotonically increasing in $TP$ and $TN$, just as in the LPM elicitation case. We first provide motivation and an overview of the approach for LFPM elicitation and then present pseudocode for the algorithm.</span>
<span id="cb4-569"><a href="#cb4-569"></a></span>
<span id="cb4-570"><a href="#cb4-570"></a>The general idea of the algorithm is to use @alg-lpm to obtain a maximizer and a minimizer for the given dataset; these result in two systems of equations involving the true LFPM $\phi^*$ with 1 degree of freedom. Then, we run a grid search that is independent of oracle queries to find the point where solutions to the systems match pointwise on the resulting confusion matrices; this occurs close to where the true metric lies.</span>
<span id="cb4-571"><a href="#cb4-571"></a></span>
<span id="cb4-572"><a href="#cb4-572"></a>More formally, suppose that the true metric is</span>
<span id="cb4-573"><a href="#cb4-573"></a>$$\phi^{*}(C)=\frac{p_{11}^{*} T P+p_{00}^{*} T N}{q_{11}^{*} T P+q_{00}^{*} T N+q_{0}^{*}}.$$  {#eq-eq3.48}</span>
<span id="cb4-574"><a href="#cb4-574"></a>Then, let $\bar{\tau}$ and $\underline{\tau}$ represent the maximizer and minimizer of $\phi$ over $\mathcal{C}$, respectively. There exists a hyperplane </span>
<span id="cb4-575"><a href="#cb4-575"></a>$$\begin{aligned}</span>
<span id="cb4-576"><a href="#cb4-576"></a>\bar{\ell}_{f}^{*}:=\left(p_{11}^{*}-\bar{\tau}^{*} q_{11}^{*}\right) t p+\left(p_{00}^{*}-\bar{\tau}^{*} q_{00}^{*}\right) t n=\bar{\tau}^{*} q_{0}^{*},</span>
<span id="cb4-577"><a href="#cb4-577"></a>\end{aligned}$$</span>
<span id="cb4-578"><a href="#cb4-578"></a>which touches $\mathcal{C}$ at $\left(\overline{T P}^{*}, \overline{T N}^{*}\right)$ on $\partial \mathcal{C}_{+}$. Correspondingly, there also exists a hyperplane </span>
<span id="cb4-579"><a href="#cb4-579"></a>$\underline{\ell}_{f}^{*}:=\left(p_{11}^{*}-\underline{\tau}^{*} q_{11}^{*}\right) t p+\left(p_{00}^{*}-\underline{\tau}^{*} q_{00}^{*}\right) \operatorname{tn}=\underline{\tau}^{*} q_{0}^{*}$, which touches $\mathcal{C}$ at $\left(\underline{TP}^{*}, \underline{T N}^{*}\right)$ on $\partial \mathcal{C}_{-}$. While we are unable to obtain @eq-eq3.48 and @eq-eq3.49 directly, we can use @alg-lpm to get a hyperplane</span>
<span id="cb4-580"><a href="#cb4-580"></a>$$\bar{\ell}:=\bar{m}_{11} t p+\bar{m}_{00} t n= \bar{m}_{11} \overline{T P}^{*}+\bar{m}_{00} \overline{T N}^{*} = \bar{C}_{0},$$  {#eq-eq3.51}</span>
<span id="cb4-581"><a href="#cb4-581"></a>which is equivalent to $\bar{\ell}_{f}^{*}$ (@eq-eq3.48) up to a constant</span>
<span id="cb4-582"><a href="#cb4-582"></a>multiple. From here, we can obtain the system of equations</span>
<span id="cb4-583"><a href="#cb4-583"></a></span>
<span id="cb4-584"><a href="#cb4-584"></a>$$p_{11}^{*}-\bar{\tau}^{*} q_{11}^{*}=\alpha \bar{m}_{11}, p_{00}^{*}-\bar{\tau}^{*} q_{00}^{*}=\alpha \bar{m}_{00}, \bar{\tau}^{*} q_{0}^{*}=\alpha \bar{C}_{0},$$  {#eq-eq3.52}</span>
<span id="cb4-585"><a href="#cb4-585"></a>where $\alpha &gt; 0$ (we know it is $\geq0$ due to our assumptions earlier</span>
<span id="cb4-586"><a href="#cb4-586"></a>and because $\bar{m}$ is positive, but if it is equal to $0$ then</span>
<span id="cb4-587"><a href="#cb4-587"></a>$\phi^*$ would be constant. So, our resulting system of equations is</span>
<span id="cb4-588"><a href="#cb4-588"></a>$$\begin{aligned}</span>
<span id="cb4-589"><a href="#cb4-589"></a>    p_{11}^{\prime}-\bar{\tau}^{*} q_{11}^{\prime}=\bar{m}_{11}, p_{00}^{\prime}-\bar{\tau}^{*} q_{00}^{\prime}=\bar{m}_{00}, \bar{\tau}^{*} q_{0}^{\prime}=\bar{C}_{0}.</span>
<span id="cb4-590"><a href="#cb4-590"></a>\end{aligned}$$  {#eq-eq3.53}</span>
<span id="cb4-591"><a href="#cb4-591"></a></span>
<span id="cb4-592"><a href="#cb4-592"></a>Now, similarly, we can approximate @eq-eq3.49 using the algorithm we defined</span>
<span id="cb4-593"><a href="#cb4-593"></a>for quasiconvex metrics (@alg-lfpm), where we altered the search range</span>
<span id="cb4-594"><a href="#cb4-594"></a>and comparisons. After finding the minimizer, we obtain the hyperplane</span>
<span id="cb4-595"><a href="#cb4-595"></a>$$\underline{\ell}:=\underline{m}_{11} t p+\underline{m}_{00} t n=\underline{m}_{11} \underline{TP}^{*}+\underline{m}_{00} \underline{TN}^{*} = \underline{C}_{0},$$  {#eq-eq3.54}</span>
<span id="cb4-596"><a href="#cb4-596"></a>which is equivalent to $\underline{\ell}_{f}^{*}$ (@eq-eq3.49) up to a constant</span>
<span id="cb4-597"><a href="#cb4-597"></a>multiple. So then, our system of equations is</span>
<span id="cb4-598"><a href="#cb4-598"></a>$$p_{11}^{*}-\underline{\tau}^{*} q_{11}^{*}=\gamma \underline{m}_{11}, p_{00}^{*}-\underline{\tau}^{*} q_{00}^{*}=\gamma \underline{m}_{00}, \underline{\tau}^{*} q_{0}^{*}=\gamma \underline{C}_{0},$$  {#eq-eq3.55}</span>
<span id="cb4-599"><a href="#cb4-599"></a>where $\gamma &lt;0$ (for a reason analogous to why we have $\alpha &gt;0$),</span>
<span id="cb4-600"><a href="#cb4-600"></a>meaning our resulting system of equations is </span>
<span id="cb4-601"><a href="#cb4-601"></a>$$\begin{aligned}</span>
<span id="cb4-602"><a href="#cb4-602"></a>    p_{11}^{\prime \prime}-\underline{\tau}^{*} q_{11}^{\prime \prime}=\underline{m}_{11}, p_{00}^{\prime \prime}-\underline{\tau}^{*} q_{00}^{\prime \prime}=\underline{m}_{00}, \underline{\tau}^{*} q_{0}^{\prime \prime}=\underline{C}_{0}.</span>
<span id="cb4-603"><a href="#cb4-603"></a>\end{aligned}$$  {#eq-eq3.56}</span>
<span id="cb4-604"><a href="#cb4-604"></a></span>
<span id="cb4-605"><a href="#cb4-605"></a>@eq-eq3.55 and @eq-eq3.56 form the two systems of equations mentioned in our overview</span>
<span id="cb4-606"><a href="#cb4-606"></a>of the algorithm. Next, we demonstrate that they have only one degree of</span>
<span id="cb4-607"><a href="#cb4-607"></a>freedom. Note that if we know $p_{11}'$, we could solve both systems of</span>
<span id="cb4-608"><a href="#cb4-608"></a>equations as follows: </span>
<span id="cb4-609"><a href="#cb4-609"></a>$$\begin{aligned}</span>
<span id="cb4-610"><a href="#cb4-610"></a>    p_{00}^{\prime}  &amp;=1-p_{11}^{\prime}, q_{0}^{\prime}=\bar{C}_{0} \frac{P^{\prime}}{Q^{\prime}}<span class="sc">\\</span></span>
<span id="cb4-611"><a href="#cb4-611"></a>    q_{11}^{\prime}  &amp;=\left(p_{11}^{\prime}-\bar{m}_{11}\right) \frac{P^{\prime}}{Q^{\prime}} <span class="sc">\\</span></span>
<span id="cb4-612"><a href="#cb4-612"></a>    q_{00}^{\prime}&amp;=\left(p_{00}^{\prime}-\bar{m}_{00}\right) \frac{P^{\prime}}{Q^{\prime}},</span>
<span id="cb4-613"><a href="#cb4-613"></a>\end{aligned}$$   {#eq-eq3.57}</span>
<span id="cb4-614"><a href="#cb4-614"></a>where</span>
<span id="cb4-615"><a href="#cb4-615"></a>$P^{\prime}=p_{11}^{\prime} \zeta+p_{00}^{\prime}(1-\zeta)$ and</span>
<span id="cb4-616"><a href="#cb4-616"></a>$Q^{\prime}=P^{\prime}+\bar{C}_{0}-$</span>
<span id="cb4-617"><a href="#cb4-617"></a>$\bar{m}_{11} \zeta-\bar{m}_{00}(1-\zeta).$</span>
<span id="cb4-618"><a href="#cb4-618"></a></span>
<span id="cb4-619"><a href="#cb4-619"></a>Now, suppose we know $p_{11}'$. We could use this value to solve both systems @eq-eq3.55 and @eq-eq3.56, yielding two metrics, $\phi'$ and $\phi''$, from the maximizer and minimizer, respectively. Importantly, when $p_{11}^{*} / p_{00}^{*}=p_{11}^{\prime} / p_{00}^{\prime}=p_{11}^{\prime \prime} / p_{00}^{\prime \prime}$, then $\phi^{*}(C)=\phi^{\prime}(C) / \alpha=-\phi^{\prime \prime}(C) / \gamma$. Essentially, when we find a value of $p_{11}'$ that results in $\phi'$ and $\phi''$ h aving constant ratios at all points on the boundary of $\mathcal{C}$, we can obtain $\phi^*$, as it is derivable from $\phi'$ and $\alpha$ (or, alternatively, $\phi''$ and $\gamma$).</span>
<span id="cb4-620"><a href="#cb4-620"></a></span>
<span id="cb4-621"><a href="#cb4-621"></a>We will perform a grid search for $p_{11}'$ on $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$. For each point in our search, we will compute $\phi'$ and $\phi''$. Then, we will generate several confusion matrices on the boundaries and calculate the ratio $\phi'' / $\phi'$ for each. We will select the value of $p_{11}'$ for which the ratio $\phi'' / \phi'$ is closest to constant and use it to compute the elicited metric $\hat{\phi}$. The pseudocode for LFPM elicitation is given in @alg-lfpm.</span>
<span id="cb4-622"><a href="#cb4-622"></a></span>
<span id="cb4-623"><a href="#cb4-623"></a><span class="in">```pseudocode</span></span>
<span id="cb4-624"><a href="#cb4-624"></a><span class="in">#| label: alg-lfpm</span></span>
<span id="cb4-625"><a href="#cb4-625"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb4-626"><a href="#cb4-626"></a><span class="in">    \caption{Grid Search for Best Ratio}</span></span>
<span id="cb4-627"><a href="#cb4-627"></a><span class="in">    \begin{algorithmic}</span></span>
<span id="cb4-628"><a href="#cb4-628"></a><span class="in">        \State \textbf{Input:} $k, \Delta$.</span></span>
<span id="cb4-629"><a href="#cb4-629"></a><span class="in">        \State \textbf{Initialize:} $\sigma_{\text{opt}} = \infty, p'_{11,\text{opt}} = 0$.</span></span>
<span id="cb4-630"><a href="#cb4-630"></a><span class="in">        \State Generate $C_1, \dots, C_k$ on $\partial C_+$ and $\partial C_-$ (Section 3).</span></span>
<span id="cb4-631"><a href="#cb4-631"></a><span class="in">        \State Generate $C_1, \dots, C_k$ on $\partial C_+$ and $\partial C_-$ (Section 3).</span></span>
<span id="cb4-632"><a href="#cb4-632"></a><span class="in">        \For{$p'_{11} = 0; \; p'_{11} \leq 1; \; p'_{11} = p'_{11} + \Delta$}</span></span>
<span id="cb4-633"><a href="#cb4-633"></a><span class="in">            \State Compute $\phi'$, $\phi''$ using Proposition 4. </span></span>
<span id="cb4-634"><a href="#cb4-634"></a><span class="in">            \State Compute array $r = \left[ \frac{\phi'(C_1)}{\phi''(C_1)}, \dots, \frac{\phi'(C_k)}{\phi''(C_k)} \right]$.</span></span>
<span id="cb4-635"><a href="#cb4-635"></a><span class="in">            \State Set $\sigma = \text{std}(r)$.</span></span>
<span id="cb4-636"><a href="#cb4-636"></a><span class="in">            \If{$\sigma &lt; \sigma_{\text{opt}}$}</span></span>
<span id="cb4-637"><a href="#cb4-637"></a><span class="in">                \State Set $\sigma_{\text{opt}} = \sigma$ and $p'_{11,\text{opt}} = p'_{11}$.</span></span>
<span id="cb4-638"><a href="#cb4-638"></a><span class="in">            \EndIf</span></span>
<span id="cb4-639"><a href="#cb4-639"></a><span class="in">        \EndFor</span></span>
<span id="cb4-640"><a href="#cb4-640"></a><span class="in">        \State \textbf{Output:} $p'_{11,\text{opt}}$.</span></span>
<span id="cb4-641"><a href="#cb4-641"></a><span class="in">    \end{algorithmic}</span></span>
<span id="cb4-642"><a href="#cb4-642"></a><span class="in">\end{algorithm}</span></span>
<span id="cb4-643"><a href="#cb4-643"></a><span class="in">```</span></span>
<span id="cb4-644"><a href="#cb4-644"></a></span>
<span id="cb4-645"><a href="#cb4-645"></a>We provide a Python implementation as below.</span>
<span id="cb4-646"><a href="#cb4-646"></a></span>
<span id="cb4-647"><a href="#cb4-647"></a>::: {.callout-note title="code"}</span>
<span id="cb4-650"><a href="#cb4-650"></a><span class="in">```{python}</span></span>
<span id="cb4-651"><a href="#cb4-651"></a><span class="kw">def</span> lfpm_elicitation(k, delta):</span>
<span id="cb4-652"><a href="#cb4-652"></a>    <span class="co">"""</span></span>
<span id="cb4-653"><a href="#cb4-653"></a><span class="co">    Inputs:</span></span>
<span id="cb4-654"><a href="#cb4-654"></a><span class="co">    - k: the number of confusion matrices to evaluate on</span></span>
<span id="cb4-655"><a href="#cb4-655"></a><span class="co">    - delta: the spacing for the grid search</span></span>
<span id="cb4-656"><a href="#cb4-656"></a><span class="co">    Outputs:</span></span>
<span id="cb4-657"><a href="#cb4-657"></a><span class="co">    - p_11', which will allow us to compute the elicited LFPM</span></span>
<span id="cb4-658"><a href="#cb4-658"></a><span class="co">    """</span></span>
<span id="cb4-659"><a href="#cb4-659"></a></span>
<span id="cb4-660"><a href="#cb4-660"></a>    sigma_opt <span class="op">=</span> np.inf</span>
<span id="cb4-661"><a href="#cb4-661"></a>    p11_opt <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-662"><a href="#cb4-662"></a>    C <span class="op">=</span> compute_confusion_matrices(k) <span class="co"># generates k confusion matrices to evaluate on</span></span>
<span id="cb4-663"><a href="#cb4-663"></a></span>
<span id="cb4-664"><a href="#cb4-664"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">int</span>(<span class="dv">1</span><span class="op">/</span>delta)):</span>
<span id="cb4-665"><a href="#cb4-665"></a>        p11 <span class="op">=</span> i <span class="op">*</span> delta</span>
<span id="cb4-666"><a href="#cb4-666"></a>        phi1 <span class="op">=</span> compute_upper_metric(p11) <span class="co"># solves the first system of equations with p11 </span></span>
<span id="cb4-667"><a href="#cb4-667"></a>        phi2 <span class="op">=</span> compute_lower_metric(p11) <span class="co"># solves the second system of equations with p11 </span></span>
<span id="cb4-668"><a href="#cb4-668"></a>        utility_1 <span class="op">=</span> [phi1(c) <span class="cf">for</span> c <span class="kw">in</span> C] <span class="co">#calculate phi for both systems of equations</span></span>
<span id="cb4-669"><a href="#cb4-669"></a>        utility_2 <span class="op">=</span> [phi2(c) <span class="cf">for</span> c <span class="kw">in</span> C]</span>
<span id="cb4-670"><a href="#cb4-670"></a></span>
<span id="cb4-671"><a href="#cb4-671"></a>        r <span class="op">=</span> []</span>
<span id="cb4-672"><a href="#cb4-672"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb4-673"><a href="#cb4-673"></a>            r.append(utility_1[i] <span class="op">/</span> utility_2[i])</span>
<span id="cb4-674"><a href="#cb4-674"></a>        sigma <span class="op">=</span> np.std(r)</span>
<span id="cb4-675"><a href="#cb4-675"></a></span>
<span id="cb4-676"><a href="#cb4-676"></a>        <span class="cf">if</span>(sigma <span class="op">&lt;</span> sigma_opt):</span>
<span id="cb4-677"><a href="#cb4-677"></a>            sigma_opt <span class="op">=</span> sigma</span>
<span id="cb4-678"><a href="#cb4-678"></a>            p11_opt <span class="op">=</span> p11</span>
<span id="cb4-679"><a href="#cb4-679"></a>    <span class="cf">return</span> p11_opt</span>
<span id="cb4-680"><a href="#cb4-680"></a><span class="in">```</span></span>
<span id="cb4-681"><a href="#cb4-681"></a>:::</span>
<span id="cb4-682"><a href="#cb4-682"></a></span>
<span id="cb4-683"><a href="#cb4-683"></a>In summary, to elicit LFPMs, we utilize a special property of the LPM minimizer and maximizer on $\mathcal{C}$--namely, that we can use the corresponding supporting hyperplanes to form a system of equations that can be used to approximate $\phi^*$ if one parameter ($p_{11}'$) is found, and that this parameter can be found using an oracle-independent grid search. Importantly, these algorithms can be shown to satisfy significant theoretical guarantees. We provide formal statement and intuitive interpretation of these guarantees here, with their proofs available in the appendix of the original paper. First, we define the oracle noise $\epsilon_{\Omega}$, which arises from the oracle potentially flipping the comparison output on two confusion matrices that are close enough in utility.</span>
<span id="cb4-684"><a href="#cb4-684"></a></span>
<span id="cb4-685"><a href="#cb4-685"></a>Given $\epsilon, \epsilon_{\Omega} \geq 0$ and a metric $\phi$ satisfying our assumptions, @alg-lpm or @alg-lfpm finds an approximate maximizer/minimizer and supporting hyperplane. Additionally, the value of $\phi$ at that point is within $O\left(\sqrt{\epsilon_{\Omega}} + \epsilon\right)$ of the optimum, and the number of queries is $O\left(\log \frac{1}{\epsilon}\right)$. Let $\mathbf{m}^{*}$ be the true performance metric. Given $\epsilon &gt; 0$, LPM elicitation outputs a performance metric $\hat{\mathbf{m}}$, such that $\left\|\mathbf{m}^{*} - \hat{\mathbf{m}}\right\|_{\infty} \leq \sqrt{2} \epsilon + \frac{2}{k_{0}} \sqrt{2 k_{1} \epsilon_{\Omega}}$. These results ensure that @alg-lpm and @alg-lfpm find an appropriate maximizer and minimizer in the search space, within a certain range of accuracy that depends on oracle and sample noise, and within a certain number of queries. Both of these statements are guaranteed by the binary search approach.</span>
<span id="cb4-686"><a href="#cb4-686"></a></span>
<span id="cb4-687"><a href="#cb4-687"></a>Let $h_{\theta}$ and $\hat{h}_{\theta}$ be two classifiers estimated using $\eta$ and $\hat{\eta}$, respectively. Further, let $\bar{\theta}$ be such that $h_{\bar{\theta}} = \arg \max _{\theta} \phi\left(h_{\theta}\right)$. Then $\|C(\hat{h}_{\bar{\theta}}) - C\left(h_{\bar{\theta}}\right)\|_{\infty} = O\left(\left\|\hat{\eta}_{n} - \eta\right\|_{\infty}\right)$. This result indicates that the drop in elicited metric quality caused by using a dataset of samples rather than population confusion matrices is bounded by the drop in performance of the decision boundary $\eta$. These three guarantees together ensure that oracle noise and sample noise do not amplify drops in performance when using metric elicitation; rather, these drops in performance are bounded by the drops that would typically occur when using the standard machine learning paradigm of training a decision boundary and using a pre-established metric. For further interesting exploration of the types of problems that can be solved using the framework of metric elicitation, we refer the reader to <span class="co">[</span><span class="ot">@nips</span><span class="co">]</span>, which performs metric elicitation to determine the oracle's ideal tradeoff between the classifier's overall performance and the discrepancy between its performance on certain protected groups.</span>
<span id="cb4-688"><a href="#cb4-688"></a></span>
<span id="cb4-689"><a href="#cb4-689"></a><span class="fu">### Multiclass Performance Metric Elicitation</span></span>
<span id="cb4-690"><a href="#cb4-690"></a></span>
<span id="cb4-691"><a href="#cb4-691"></a>Although the previous section only described metric elicitation for binary classification problems, the general framework can still be applied to multiclass classification problems<span class="co">[</span><span class="ot">@NEURIPS2019_1fd09c5f</span><span class="co">]</span>. Consider the case of classifying subtypes of leukemia <span class="co">[</span><span class="ot">@YangNaiman+2014+477+496</span><span class="co">]</span>. We can train a neural network to predict conditional probability of a certain leukemia subtype given certain gene expressions. However, it may not be appropriate to classify the subtype purely based on whichever one has the highest confidence. For instance, a treatment for leukemia subtype C1 may be perfect for cases of C1, but it may be ineffective or harmful for certain other subtypes. Therefore, the final response from the classifier may not be as simple as as choosing the class with the highest conditional probability, just like how the threshold for binary classification may not always be 50%. With multiclass metric elicitation, we can show confusion matrices to an oracle (like the doctor in the leukemia example) to determine which classifier has the best tradeoffs. In <span class="co">[</span><span class="ot">@NEURIPS2019_1fd09c5f</span><span class="co">]</span>, the authors focus on eliciting linear performance metrics, which is what we will describe in this chapter. Most of the notation from Binary Metric Elicitation still persists, just modified to provide categorical responses. $X \in \mathcal{X}$ is the input random variable. $Y \in <span class="co">[</span><span class="ot">k</span><span class="co">]</span>$ is the output random variable, where $<span class="co">[</span><span class="ot">k</span><span class="co">]</span>$ is the index set $<span class="sc">\{</span>1, 2, \dots, k<span class="sc">\}</span>$.</span>
<span id="cb4-692"><a href="#cb4-692"></a></span>
<span id="cb4-693"><a href="#cb4-693"></a>The dataset of size $n$ is denoted by $<span class="sc">\{</span>(\vec{x}, y)<span class="sc">\}</span>_{i=1}^n$ generated independently and identically from $\mathbb{P}(X, Y)$. $\eta_i(\vec{x}) = \mathbb{P}(Y=i | X=\vec{x})$ gives the conditional probability of class $i \in <span class="co">[</span><span class="ot">k</span><span class="co">]</span>$ given an observation. $\xi_i = \mathbb{P}(Y=i)$ is the marginal probability of class $i \in <span class="co">[</span><span class="ot">k</span><span class="co">]</span>$. The set of all classifiers is $\mathcal{H} = <span class="sc">\{</span>h : \mathcal{X} \rightarrow \Delta_k<span class="sc">\}</span>$, where $\Delta_k$ is (k-1) dimensional simplex. In this case, the outputs of classifiers are 1-hot vectors of size $k$ where the only index with value 1 is the predicted class and all other positions have a value of 0. The confusion matrix for a classifier, $h$, is $C(h, \mathbb{P}) \in \mathbb{R}^{k \times k}$, where:</span>
<span id="cb4-694"><a href="#cb4-694"></a></span>
<span id="cb4-695"><a href="#cb4-695"></a>$$C_{ij}(h, \mathbb{P}) = \mathbb{P}(Y=i, h=j) \text{\qquad for } i, j \in <span class="co">[</span><span class="ot">k</span><span class="co">]</span>$$  {#eq-eq3.59}</span>
<span id="cb4-696"><a href="#cb4-696"></a></span>
<span id="cb4-697"><a href="#cb4-697"></a>Note that the confusion matrices are $k\times k$ and store the joint probabilities of each type of classification for each possible class. This means that the sum of row $i$ in the confusion matrix equals $\xi_i$, because this is equivalent to adding over all possible classifications. Since we know the sums of each row, all diagonal elements can be reconstructed from just the off-diagonal elements, so a confusion matrix $C(h, \mathbb{P})$ can be expressed as a vector of off-diagonal elements, $\vec{c}(h, \mathbb{P}) = \textit{off-diag}(C(h, \mathbb{P}))$, and $\vec{c} \in \mathbb{R}^q$ where $q := k^2 - k$. The vector $\vec{c}$ is called the vector of *'off-diagonal confusions.'* The space of off-diagonal confusions is $\mathcal{C} = <span class="sc">\{</span>\vec{c}(h, \mathbb{P}) : h \in \mathcal{H}<span class="sc">\}</span>$.</span>
<span id="cb4-698"><a href="#cb4-698"></a></span>
<span id="cb4-699"><a href="#cb4-699"></a>In cases where the oracle would care about the exact type of misclassification (i.e. misclassifying and object from class 1 as class 2), this off-diagonal confusion matrix is necessary. However, there are many cases where the performance of a classifier is determined by just the probability of correct prediction for each class, which just requires the diagonal elements. In these cases, we can define the vector of *'diagonal confusions'* as $\vec{d}(h, \mathbb{P}) = \textit{diag}(C(h, \mathbb{P})) \in \mathbb{R}^k$. The space of diagonal confusions is $\mathcal{D} = <span class="sc">\{</span>\vec{d}(h, \mathbb{P}) : h \in \mathcal{H}<span class="sc">\}</span>$.</span>
<span id="cb4-700"><a href="#cb4-700"></a></span>
<span id="cb4-701"><a href="#cb4-701"></a>Finally, the setup for metric elicitation is identical to the one examined in the previous chapter. We still assume access to an oracle that can choose between two classifiers or confusion matrices, using notation $\Gamma$ for comparing two classifiers and $\Omega$ for comparing confusion matrices, which returns 1 if the first classifier is better and 0 otherwise. We still assume that the oracle behaves according to some unknown performance metric, and we wish to recover this metric up to some small error tolerance (based on a suitable norm). The two different types of confusion vectors result in different algorithms for metric elicitation, which we will explore in later sections.</span>
<span id="cb4-702"><a href="#cb4-702"></a></span>
<span id="cb4-703"><a href="#cb4-703"></a>A Diagonal Linear Performance Metric (DLPM) is a performance metric that only considers the diagonal elements in the confusion matrix. The metric is defined as $\psi(\vec{d}) = \langle \vec{a}, \vec{d} \rangle$, where $\vec{a} \in \mathbb{R}^k$ such that $||\vec{a}||_1 = 1$. It is also called weighted accuracy [@pmlr-v37-narasimhanb15]. The family of DLPMs is denoted as $\varphi_{DLPM}$. Since these only consider the diagonal elements, which we want to maximize, we can focus on only eliciting monotonically increasing DLPMs, meaning that all elements in $\vec{a}$ are non-negative.</span>
<span id="cb4-704"><a href="#cb4-704"></a></span>
<span id="cb4-705"><a href="#cb4-705"></a>Consider the trivial classifiers that only predict a single class at all times. The diagonal confusions when only predicting class $i$ are $\vec{v}_i \in \mathbb{R}^k$ with $\xi_i$ at index $i$ and zero elsewhere. Note that this is the maximum possible value in index $i$, because this represents perfectly classifying all points that have a true class of $i$. We can consider the space of diagonal confusions, visualized in @fig-diag_geom (taken from <span class="co">[</span><span class="ot">@NEURIPS2019_1fd09c5f</span><span class="co">]</span>). The space of $\mathcal{D}$ is strictly convex, closed, and contained in the box $<span class="co">[</span><span class="ot">0, \xi_1</span><span class="co">]</span> \times \dots \times <span class="co">[</span><span class="ot">0, \xi_k</span><span class="co">]</span>$. We also know that the only vertices are $\vec{v}_i$ for each $i \in <span class="co">[</span><span class="ot">k</span><span class="co">]</span>^{(k-1)}$.</span>
<span id="cb4-706"><a href="#cb4-706"></a></span>
<span id="cb4-707"><a href="#cb4-707"></a><span class="al">![(a) Geometry of space of diagonal confusions for $k=3$. This is a convex region with three flat areas representing confusions when restricted to only two classes. (b) Geometry of diagonal confusions when restricted to classes $k_1$ and $k_2$. Notice how this is identical to the space of confusion matrices examined in the previous chapter.](Figures/diag_geometry.png)</span>{#fig-diag_geom width="<span class="sc">\\</span>textwidth"}</span>
<span id="cb4-708"><a href="#cb4-708"></a></span>
<span id="cb4-709"><a href="#cb4-709"></a>We know that this is strictly convex under the assumption that an object from any class can be misclassified as any other class. Mathematically, the assumption is that $g_{ij}(r) = \mathbb{P} \left<span class="co">[</span><span class="ot">\frac{\eta_i(X)}{\eta_j(X)} \geq r \right</span><span class="co">]</span>$ $\forall i, j \in <span class="co">[</span><span class="ot">k</span><span class="co">]</span>$ are continuous and strictly decreasing for $r \in [0, \infty)$.</span>
<span id="cb4-710"><a href="#cb4-710"></a></span>
<span id="cb4-711"><a href="#cb4-711"></a>We can also define the space of binary classification confusion matrices confined to classes $k_1$ and $k_2$, which is the 2-D $(k_1, k_2)$ axis-aligned face of $\mathcal{D}$, denoted as $\mathcal{D}_{k_1, k_2}$. Note that this is strictly convex, since $\mathcal{D}$ itself is strictly convex, and it has the same geometry as the space of binary confusion matrices examined in the previous chapter. Therefore, we can construct an RBO classifier for $\psi \in \varphi_{DLPM}$, parameterized by $\vec{a}$, as follows: </span>
<span id="cb4-712"><a href="#cb4-712"></a>$$\begin{aligned}</span>
<span id="cb4-713"><a href="#cb4-713"></a>\bar{h}_{k_1, k_2}(\vec{x})= \left<span class="sc">\{</span></span>
<span id="cb4-714"><a href="#cb4-714"></a>\begin{array}{ll}</span>
<span id="cb4-715"><a href="#cb4-715"></a>      k_1, \text{ if } a_{k_1} \eta_{k_1}(\vec{x}) \geq a_{k_2} \eta_{k_2}(\vec{x})<span class="sc">\\</span></span>
<span id="cb4-716"><a href="#cb4-716"></a>k_2, \text{ o.w.}</span>
<span id="cb4-717"><a href="#cb4-717"></a>\end{array}</span>
<span id="cb4-718"><a href="#cb4-718"></a>\right<span class="sc">\}</span>.</span>
<span id="cb4-719"><a href="#cb4-719"></a>\end{aligned}$$ {#eq-rbo_eq}</span>
<span id="cb4-720"><a href="#cb4-720"></a></span>
<span id="cb4-721"><a href="#cb4-721"></a>We can parameterize the upper boundary of $\mathcal{D}_{k_1, k_2}$,</span>
<span id="cb4-722"><a href="#cb4-722"></a>denoted as $\partial \mathcal{D}^{+}_{k_1, k_2}$, using a single</span>
<span id="cb4-723"><a href="#cb4-723"></a>parameter $m \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$. Specifically, we can construct a DLPM by</span>
<span id="cb4-724"><a href="#cb4-724"></a>setting $a_{k_1} = m$, $a_{k_2} = 1 - m$, and all others to 0. Using</span>
<span id="cb4-725"><a href="#cb4-725"></a><span class="co">[</span><span class="ot">@eq-rbo_eq</span><span class="co">]</span>, we can get the diagonal confusions, so varying $m$ parameterizes</span>
<span id="cb4-726"><a href="#cb4-726"></a>$\partial \mathcal{D}^{+}_{k_1, k_2}$. The parameterization is denoted</span>
<span id="cb4-727"><a href="#cb4-727"></a>as $\nu(m; k_1, k_2)$.</span>
<span id="cb4-728"><a href="#cb4-728"></a></span>
<span id="cb4-729"><a href="#cb4-729"></a><span class="fu">#### Diagonal Linear Performance Metric Elicitation</span></span>
<span id="cb4-730"><a href="#cb4-730"></a></span>
<span id="cb4-731"><a href="#cb4-731"></a>Suppose the oracle follows a true metric, $\psi$, that is linear and</span>
<span id="cb4-732"><a href="#cb4-732"></a>monotone increasing across all axes. If we consider the composition</span>
<span id="cb4-733"><a href="#cb4-733"></a>$\psi \circ \nu(m; k_1, k_2): <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span> \rightarrow \mathbb{R}$, we know it</span>
<span id="cb4-734"><a href="#cb4-734"></a>must be concave and unimodal, because $\mathcal{D}_{k_1, k_2}$ is a</span>
<span id="cb4-735"><a href="#cb4-735"></a>convex set. Therefore, we can find the value of $m$ that maximizes</span>
<span id="cb4-736"><a href="#cb4-736"></a>$\psi \circ \nu(m; k_1, k_2)$ for any given $k_1$ and $k_2$ using a</span>
<span id="cb4-737"><a href="#cb4-737"></a>binary search procedure.</span>
<span id="cb4-738"><a href="#cb4-738"></a></span>
<span id="cb4-739"><a href="#cb4-739"></a>Since the RBO classifier for classes $k_1$ and $k_2$ only rely on the</span>
<span id="cb4-740"><a href="#cb4-740"></a>relative weights of the classes in the DLPM (see <span class="co">[</span><span class="ot">@eq-rbo_eq</span><span class="co">]</span>), finding</span>
<span id="cb4-741"><a href="#cb4-741"></a>the value of $m$ that maximizes $\psi \circ \nu(m; k_1, k_2)$ gives us</span>
<span id="cb4-742"><a href="#cb4-742"></a>the true relative ratio between $a_{k_1}$ and $a_{k_2}$. Specifically,</span>
<span id="cb4-743"><a href="#cb4-743"></a>from the definition of $\nu$, we know that</span>
<span id="cb4-744"><a href="#cb4-744"></a>$\frac{a_{k_2}}{a_{k_1}} = \frac{1-m}{m}$. We can therefore simply</span>
<span id="cb4-745"><a href="#cb4-745"></a>calculate the ratio between $a_1$ and all other weights to reconstruct</span>
<span id="cb4-746"><a href="#cb4-746"></a>an estimate for the true metric. A python implementation of this</span>
<span id="cb4-747"><a href="#cb4-747"></a>algorithm is provided below.</span>
<span id="cb4-748"><a href="#cb4-748"></a></span>
<span id="cb4-749"><a href="#cb4-749"></a>::: {.callout-note title="code"}</span>
<span id="cb4-752"><a href="#cb4-752"></a><span class="in">```{python}</span></span>
<span id="cb4-753"><a href="#cb4-753"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-754"><a href="#cb4-754"></a></span>
<span id="cb4-755"><a href="#cb4-755"></a><span class="kw">def</span> rbo_dlpm(m, k1, k2, k):</span>
<span id="cb4-756"><a href="#cb4-756"></a>    <span class="co">"""</span></span>
<span id="cb4-757"><a href="#cb4-757"></a><span class="co">    This constructs DLPM weights for the upper boundary of the</span></span>
<span id="cb4-758"><a href="#cb4-758"></a><span class="co">    restricted diagonal confusions, given a parameter m.</span></span>
<span id="cb4-759"><a href="#cb4-759"></a><span class="co">    This is equivalent to </span><span class="ch">\n</span><span class="co">u(m; k1, k2)</span></span>
<span id="cb4-760"><a href="#cb4-760"></a><span class="co">    </span></span>
<span id="cb4-761"><a href="#cb4-761"></a><span class="co">    Inputs:</span></span>
<span id="cb4-762"><a href="#cb4-762"></a><span class="co">    - m: parameter (between 0 and 1) for the upper boundary</span></span>
<span id="cb4-763"><a href="#cb4-763"></a><span class="co">    - k1: first axis for this  face</span></span>
<span id="cb4-764"><a href="#cb4-764"></a><span class="co">    - k2: second axis for this face</span></span>
<span id="cb4-765"><a href="#cb4-765"></a><span class="co">    - k: number of classes</span></span>
<span id="cb4-766"><a href="#cb4-766"></a><span class="co">    Outputs:</span></span>
<span id="cb4-767"><a href="#cb4-767"></a><span class="co">    - DLPM weights for this point on the upper boundary</span></span>
<span id="cb4-768"><a href="#cb4-768"></a><span class="co">    """</span></span>
<span id="cb4-769"><a href="#cb4-769"></a>    new_a <span class="op">=</span> np.zeros(k)</span>
<span id="cb4-770"><a href="#cb4-770"></a>    new_a[k1] <span class="op">=</span> m</span>
<span id="cb4-771"><a href="#cb4-771"></a>    new_a[k2] <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> m</span>
<span id="cb4-772"><a href="#cb4-772"></a>    <span class="cf">return</span> new_a</span>
<span id="cb4-773"><a href="#cb4-773"></a></span>
<span id="cb4-774"><a href="#cb4-774"></a><span class="kw">def</span> dlpm_elicitation(epsilon, oracle, get_d, k):</span>
<span id="cb4-775"><a href="#cb4-775"></a>    <span class="co">"""</span></span>
<span id="cb4-776"><a href="#cb4-776"></a><span class="co">    Inputs:</span></span>
<span id="cb4-777"><a href="#cb4-777"></a><span class="co">    - epsilon: some epsilon &gt; 0 representing threshold of error</span></span>
<span id="cb4-778"><a href="#cb4-778"></a><span class="co">    - oracle: some function that accepts 2 confusion matrices and</span></span>
<span id="cb4-779"><a href="#cb4-779"></a><span class="co">        returns true if the first is preferred and false otherwise</span></span>
<span id="cb4-780"><a href="#cb4-780"></a><span class="co">    - get_d: some function that accepts dlpm weights and returns </span></span>
<span id="cb4-781"><a href="#cb4-781"></a><span class="co">        diagonal confusions</span></span>
<span id="cb4-782"><a href="#cb4-782"></a><span class="co">    - k: number of classes</span></span>
<span id="cb4-783"><a href="#cb4-783"></a><span class="co">    Outputs:</span></span>
<span id="cb4-784"><a href="#cb4-784"></a><span class="co">    - estimate for true DLPM weights</span></span>
<span id="cb4-785"><a href="#cb4-785"></a><span class="co">    """</span></span>
<span id="cb4-786"><a href="#cb4-786"></a>    a_hat <span class="op">=</span> np.zeros(k)</span>
<span id="cb4-787"><a href="#cb4-787"></a>    a_hat[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb4-788"><a href="#cb4-788"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, k):</span>
<span id="cb4-789"><a href="#cb4-789"></a>        <span class="co"># iterate over each axis to find appropriate ratio</span></span>
<span id="cb4-790"><a href="#cb4-790"></a>        a <span class="op">=</span> <span class="dv">0</span>  <span class="co"># lower bound of binary search</span></span>
<span id="cb4-791"><a href="#cb4-791"></a>        b <span class="op">=</span> <span class="dv">1</span>  <span class="co"># upper bound of binary search</span></span>
<span id="cb4-792"><a href="#cb4-792"></a></span>
<span id="cb4-793"><a href="#cb4-793"></a>        <span class="cf">while</span> (b <span class="op">-</span> a <span class="op">&gt;</span> epsilon):</span>
<span id="cb4-794"><a href="#cb4-794"></a>            c <span class="op">=</span> (<span class="dv">3</span> <span class="op">*</span> a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb4-795"><a href="#cb4-795"></a>            d <span class="op">=</span> (a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb4-796"><a href="#cb4-796"></a>            e <span class="op">=</span> (a <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb4-797"><a href="#cb4-797"></a></span>
<span id="cb4-798"><a href="#cb4-798"></a>            <span class="co"># get diagonal confusions for each point</span></span>
<span id="cb4-799"><a href="#cb4-799"></a>            d_a, d_c, d_d, d_e, d_b <span class="op">=</span> (get_d(rbo_dlpm(x, <span class="dv">0</span>, i, k)) </span>
<span id="cb4-800"><a href="#cb4-800"></a>                <span class="cf">for</span> x <span class="kw">in</span> [a, c, d, e, b])</span>
<span id="cb4-801"><a href="#cb4-801"></a></span>
<span id="cb4-802"><a href="#cb4-802"></a>            <span class="co"># query oracle for each pair</span></span>
<span id="cb4-803"><a href="#cb4-803"></a>            response_ac <span class="op">=</span> oracle(d_a, d_c)</span>
<span id="cb4-804"><a href="#cb4-804"></a>            response_cd <span class="op">=</span> oracle(d_c, d_d)</span>
<span id="cb4-805"><a href="#cb4-805"></a>            response_de <span class="op">=</span> oracle(d_d, d_e)</span>
<span id="cb4-806"><a href="#cb4-806"></a>            response_eb <span class="op">=</span> oracle(d_e, d_b)</span>
<span id="cb4-807"><a href="#cb4-807"></a></span>
<span id="cb4-808"><a href="#cb4-808"></a>            <span class="co"># update ranges to keep the peak</span></span>
<span id="cb4-809"><a href="#cb4-809"></a>            <span class="cf">if</span> response_ac:</span>
<span id="cb4-810"><a href="#cb4-810"></a>                b <span class="op">=</span> d</span>
<span id="cb4-811"><a href="#cb4-811"></a>            <span class="cf">elif</span> response_cd:</span>
<span id="cb4-812"><a href="#cb4-812"></a>                b <span class="op">=</span> d</span>
<span id="cb4-813"><a href="#cb4-813"></a>            <span class="cf">elif</span> response_de:</span>
<span id="cb4-814"><a href="#cb4-814"></a>                a <span class="op">=</span> c</span>
<span id="cb4-815"><a href="#cb4-815"></a>                b <span class="op">=</span> e</span>
<span id="cb4-816"><a href="#cb4-816"></a>            <span class="cf">elif</span> response_eb:</span>
<span id="cb4-817"><a href="#cb4-817"></a>                a <span class="op">=</span> d</span>
<span id="cb4-818"><a href="#cb4-818"></a>            <span class="cf">else</span>:</span>
<span id="cb4-819"><a href="#cb4-819"></a>                a <span class="op">=</span> d</span>
<span id="cb4-820"><a href="#cb4-820"></a></span>
<span id="cb4-821"><a href="#cb4-821"></a>        midpt <span class="op">=</span> (a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb4-822"><a href="#cb4-822"></a>        a_hat[i] <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> midpt) <span class="op">/</span> midpt</span>
<span id="cb4-823"><a href="#cb4-823"></a>    <span class="cf">return</span> a_hat <span class="op">/</span> np.<span class="bu">sum</span>(a_hat)</span>
<span id="cb4-824"><a href="#cb4-824"></a><span class="in">```</span></span>
<span id="cb4-825"><a href="#cb4-825"></a>:::</span>
<span id="cb4-826"><a href="#cb4-826"></a></span>
<span id="cb4-827"><a href="#cb4-827"></a>To use this algorithm for metric elicitation on a real dataset, we need</span>
<span id="cb4-828"><a href="#cb4-828"></a>to supply the "oracle" and "get_d" functions. The oracle function is an</span>
<span id="cb4-829"><a href="#cb4-829"></a>interface to an expert who judges which of two confusion matrices is</span>
<span id="cb4-830"><a href="#cb4-830"></a>better. The get_d function will need to construct a classifier given the</span>
<span id="cb4-831"><a href="#cb4-831"></a>DLPM weights, following the principles of the RBO classifier from <span class="co">[</span><span class="ot">@eq-rbo_eq</span><span class="co">]</span>,</span>
<span id="cb4-832"><a href="#cb4-832"></a>and calculate the confusion matrix from a validation set.</span>
<span id="cb4-833"><a href="#cb4-833"></a></span>
<span id="cb4-834"><a href="#cb4-834"></a>Using the same oracle feedback noise model from the binary metric elicitation, we can make the following guarantees:</span>
<span id="cb4-835"><a href="#cb4-835"></a></span>
<span id="cb4-836"><a href="#cb4-836"></a>::: {.callout-note title="proposition"}</span>
<span id="cb4-837"><a href="#cb4-837"></a>::: {#prop-prop_dlpm}</span>
<span id="cb4-838"><a href="#cb4-838"></a>Given $\epsilon, \epsilon_\Omega \geq 0$, and a 1-Lipschitz DLPM</span>
<span id="cb4-839"><a href="#cb4-839"></a>$\varphi^*$ parameterized by $\vec{a}^*$. Then the output $\hat{a}$ of</span>
<span id="cb4-840"><a href="#cb4-840"></a>the DLPM elicitation algorithm after $O((k-1)\log\frac{1}{\epsilon})$</span>
<span id="cb4-841"><a href="#cb4-841"></a>queries to the oracle satisfies</span>
<span id="cb4-842"><a href="#cb4-842"></a>$||\vec{a}^* - \hat{a}||_\infty \leq O(\epsilon + \sqrt{\epsilon_\Omega})$,</span>
<span id="cb4-843"><a href="#cb4-843"></a>which is equivalent to</span>
<span id="cb4-844"><a href="#cb4-844"></a>$||\vec{a}^* - \hat{a}||_2 \leq O(\sqrt{k}(\epsilon + \sqrt{\epsilon_\Omega}))$.</span>
<span id="cb4-845"><a href="#cb4-845"></a>:::</span>
<span id="cb4-846"><a href="#cb4-846"></a>:::</span>
<span id="cb4-847"><a href="#cb4-847"></a></span>
<span id="cb4-848"><a href="#cb4-848"></a>In other words, the maximum difference between the estimate and true value along any component (indicated by the L-infinity norm) is linearly bounded by the sum of the epsilon specified by the algorithm and the square root of the oracle's correctness guarantee ($\epsilon_\Omega$).</span>
<span id="cb4-849"><a href="#cb4-849"></a></span>
<span id="cb4-850"><a href="#cb4-850"></a><span class="fu">## Case Study 3: Active Preference Learning in Robotics</span></span>
<span id="cb4-851"><a href="#cb4-851"></a></span>
<span id="cb4-852"><a href="#cb4-852"></a>How exactly do robots learn human preferences from just the pairwise comparisons, if they need to learn how to act in the environment itself? The comparisons in turn help robots learn the reward function of the human, which allows them to further take actions in real settings. Let's say there are two trajectories $\xi_A$ and $\xi_B$ that might be taken as the next course of action in any context, like choosing the next turn, or choosing the next chatGPT response. The robot is offering both to a human for comparison. To answer which of them is better, the human would ask themselves if $R(\xi_A)$ or $R(\xi_B)$ is bigger, with $R(\xi) = w * \phi(\xi)$ being the reward function. In this equation $w$ and $\phi(\xi)$ are vectors of weights and features of the trajectory, so alternatively, we can express this as:</span>
<span id="cb4-853"><a href="#cb4-853"></a></span>
<span id="cb4-854"><a href="#cb4-854"></a>$$R(\xi) = \begin{bmatrix} w_1 <span class="sc">\\</span> w_2 <span class="sc">\\</span> ... <span class="sc">\\</span> w_N \end{bmatrix} \cdot \begin{bmatrix} \phi_1(\xi) <span class="sc">\\</span> \phi_2(\xi) <span class="sc">\\</span> ... <span class="sc">\\</span> \phi_N(\xi) \end{bmatrix}$$ {#eq-reward_eq}</span>
<span id="cb4-855"><a href="#cb4-855"></a></span>
<span id="cb4-856"><a href="#cb4-856"></a>If one says that they preferred $\xi_2$ less than $\xi_1$ then it means $\xi_2 &lt; \xi_1 \implies R(\xi_2) &lt; R(\xi_1) \implies w * \phi(\xi_2) &lt; w * \phi(\xi_1) \implies 0 &lt; w * (\phi(\xi_1) - \phi(\xi_2)) \implies 0 &lt; w * \Phi$. Alternatively, if one preferred $\xi_2$ more than $\xi_1$, the signs would be flipped, resulting in $0 &gt; w * \Phi$. The two results can be represented in the N-dimensional space, where when it is split by the decision boundary, it creates half-spaces indicating preferences for each of the sides. For example we can see how a query between two items can split the plain into two halves, indicating preference towards one of the items. Such an image can be extended into bigger dimensions, where a line would become a separating hyperplane. If one is to truly believe the answers of one person, they would remove everything from the other side of the hyperplane that does not agree with the received human preference. But since humans are noisy, that approach is not optimal, thus most applications up-weight the indicated side of the plane to emphasize that points on that side are better, and down-weight the other side as they do not agree with the provided comparison.</span>
<span id="cb4-857"><a href="#cb4-857"></a></span>
<span id="cb4-858"><a href="#cb4-858"></a>How should someone choose which queries to conduct, otherwise, what is the most informative query sequence? After completing one query, the next query should be orthogonal to the previous one so that the potential space consistent with the preferences decreases in half. The intuition behind that is the potential space has all of the reward functions that agree with the provided answers, so to find a specific reward function for a human, decreasing the space narrows down the possible options. The original query created the blue space, and a new one created a red space, resulting in a purple intersection of the two which is still consistent with both of the queries's results. The image shows that the purple portion is exactly half of the blue portion.</span>
<span id="cb4-859"><a href="#cb4-859"></a></span>
<span id="cb4-860"><a href="#cb4-860"></a><span class="al">![Creating further comparisons limits the space that agrees with answers to all of them. The blue area demonstrates a preference for object 1 over object 2. The red area demonstrates a preference for object 3 over object 4. Combination (purple area) shows the space that is consistent with both of those preferences.](Figures/2D-space.jpg)</span>{#fig-2dspace width="40%"}</span>
<span id="cb4-861"><a href="#cb4-861"></a></span>
<span id="cb4-862"><a href="#cb4-862"></a>Mathematically, from <span class="co">[</span><span class="ot">@pmlr-v87-biyik18a</span><span class="co">]</span> this can be expressed as set $F$ of potential queries $\phi$, where $F = <span class="sc">\{</span>\phi: \phi = \Phi(\xi_A) - \Phi(\xi_B), \xi_A, \xi_B \in \Xi<span class="sc">\}</span>$ (defining that a query is the difference between the features of two trajectories). Using that, the authors define a human update function $f_{\phi}(w) = \min(1, \exp(I^T\phi))$ that accounts for how much of the space will still be consistent with the preferences. Finally, for a specific query, they define the minimum volume removed as $\min<span class="sc">\{</span>\mathbb{E}<span class="co">[</span><span class="ot">1 - f_{\phi}(w)</span><span class="co">]</span>, \mathbb{E}<span class="co">[</span><span class="ot">1 - f_{-\phi}(w)</span><span class="co">]</span><span class="sc">\}</span>$ (expected size of the two sides of the remaining space after it is split by a query - purple area in @fig-2dspace), and the final goal is to maximize that amount over all possible queries since it is optimal to get rid of as much space as possible to narrow down the options for the reward function: $\max_{\phi} \min<span class="sc">\{</span> \mathbb{E}<span class="co">[</span><span class="ot">1 - f_{\phi}(w)</span><span class="co">]</span>, \mathbb{E}<span class="co">[</span><span class="ot">1 - f_{-\phi}(w)</span><span class="co">]</span><span class="sc">\}</span>$. Effectively this is finding such $\phi$ that maximizes the information one can get by asking the next comparison query. While this approach uses minimum volume removed, there can be other metrics inside the $\max$ function. Some applications like movie recommendations do not require extra constraints, however in robotics one might want to add more constraints that satisfy certain rules, so that the resulting query follows the dynamics of the physical world.</span>
<span id="cb4-863"><a href="#cb4-863"></a></span>
<span id="cb4-864"><a href="#cb4-864"></a>The first real example of learning reward functions from pairwise comparisons is a 2D driving simulator from <span class="co">[</span><span class="ot">@pmlr-v87-biyik18a</span><span class="co">]</span>. In @fig-car_direct you can see the setting of a 3-lane road with the orange car being controlled by the computer. The queries conducted for this problem are two different trajectories presented to the human, and they are asked to evaluate which one of them is better. For the features that contribute to the reward function, it is important to consider that robots might not find some of the information as informative for the learning process as a human would. For this example, the underlying features included the distance between lane boundaries, distance to other cars, and the heading and speed of the controlled car. The weights toward the last feature were weighted the highest according to the authors, since it takes a lot of effort for the car to change or correct its direction.</span>
<span id="cb4-865"><a href="#cb4-865"></a></span>
<span id="cb4-866"><a href="#cb4-866"></a>At the start of the learning process, the car had no direction learned and was moving all over the road. In the middle of learning after 30 queries, the simulator learned to follow the direction of the road and go straight but still experienced collisions. After 70 queries, the simulator learned to avoid collisions, as well as keep the car within the lane without swerving.</span>
<span id="cb4-867"><a href="#cb4-867"></a></span>
<span id="cb4-868"><a href="#cb4-868"></a><span class="fu">#### Active Learning for Pairwise Comparisons</span></span>
<span id="cb4-869"><a href="#cb4-869"></a></span>
<span id="cb4-870"><a href="#cb4-870"></a>We have discussed that pairwise comparisons should be selected to maximize the minimum volume of remaining options removed. The question that can come out of the driving example is does it really matter to follow that goal or does random choice of queries performs as well? It turns out that indeed most AL algorithms (purposefully selecting queries) over time converge with the performance of the random query selection, so in long term the performance is similar. However, what is different is that AL achieves better performance earlier, which in time-sensitive tasks can be a critical factor. One example of such a setting can be exoskeletons for humans as part of the rehabilitation after surgery <span class="co">[</span><span class="ot">@Li_2021</span><span class="co">]</span>. Different people have significantly different walking patterns as well as rehabilitation requirements, so the exoskeleton needs to adapt to the human as soon as possible for a more successful rehabilitation. Figure @fig-robotics demonstrates the difference in the time needed between the two approaches. In general, in robotics, the time differences that might seem small to a human might be detrimental to the final performance.</span>
<span id="cb4-871"><a href="#cb4-871"></a></span>
<span id="cb4-872"><a href="#cb4-872"></a>!<span class="co">[</span><span class="ot">Performance of AL and random query selection algorithms in the task of exoskeleton learning with human preferences. [@Li_2021]</span><span class="co">](Figures/robo_graph.png)</span>{#fig-robotics width="60%"}</span>
<span id="cb4-873"><a href="#cb4-873"></a></span>
<span id="cb4-874"><a href="#cb4-874"></a>In conclusion, pairwise comparisons show to be a great way of learning linear reward functions, but at times present challenges or incapabilities that can be further improved with additional incorporations of approaches like AL. That improves many applications in terms of time spent getting to the result in case of exoskeleton adjustments, as well as getting to a middle ground between polar behaviors in applications like negotiations.</span>
<span id="cb4-875"><a href="#cb4-875"></a></span>
<span id="cb4-876"><a href="#cb4-876"></a><span class="fu">### Application: Guiding Human Demonstrations in Robotics</span></span>
<span id="cb4-877"><a href="#cb4-877"></a></span>
<span id="cb4-878"><a href="#cb4-878"></a>A strong approach to learning policies for robotic manipulation is imitation learning, the technique of learning behaviors from human demonstrations. In particular, interactive imitation learning allows a group of humans to contribute their own demonstrations for a task, allowing for scalable learning. However, not all groups of demonstrators are equally helpful for interactive imitation learning.</span>
<span id="cb4-879"><a href="#cb4-879"></a></span>
<span id="cb4-880"><a href="#cb4-880"></a>The ideal set of demonstrations for imitation learning would follow a single, optimal method for performing the task, which a robot could learn to mimic. Conversely, *multimodality*, the presence of multiple optimal methods in the demonstration set, is challenging for imitation learning since it has to learn from contradicting information for how to accomplish a task. A common reason for multimodality is the fact that different people often subconsciously choose different paths for execution, as illustrated in @fig-multimodalexecution.</span>
<span id="cb4-881"><a href="#cb4-881"></a></span>
<span id="cb4-882"><a href="#cb4-882"></a>!<span class="co">[</span><span class="ot">Examples of two different ways to insert a nut onto a round peg. The orange demonstration picks up the nut from the hole while the blue demonstration picks up the nut from the side [@gandhi2022eliciting]</span><span class="co">](Figures/multimodal_peg.png)</span>{#fig-multimodalexecution width="50%"}</span>
<span id="cb4-883"><a href="#cb4-883"></a></span>
<span id="cb4-884"><a href="#cb4-884"></a>Gandhi et al. <span class="co">[</span><span class="ot">@gandhi2022eliciting</span><span class="co">]</span> identifies whether demonstrations are compatible with one another and offer an active elicitation interface to guide humans to provide better demonstrations in interactive imitation learning. Their key motivation is to allow multiple users to contribute demonstrations over the course of data collection by guiding users towards compatible demonstrations. To identify whether a demonstration is "compatible" with a base policy trained with prior demonstrations, the researchers measure the *likelihood* of demonstrated actions under the base policy, and the *novelty* of the visited states. Intuitively, low likelihood and low novelty demonstrations should be excluded since they represent conflicting modes of behavior on states that the robot can already handle, and are therefore incompatible. This concept of compatibility is used for filtering a new set of demonstrations and actively eliciting compatible demonstrations. In the following subsections, we describe the process of estimating compatibility and active elicitation in more detal.</span>
<span id="cb4-885"><a href="#cb4-885"></a></span>
<span id="cb4-886"><a href="#cb4-886"></a><span class="fu">#### Estimating Compatiblity</span></span>
<span id="cb4-887"><a href="#cb4-887"></a></span>
<span id="cb4-888"><a href="#cb4-888"></a>We want to define a compatibility measure $\mathcal{M}$, that estimates the performance of policy $\pi_{base}$ that is retrained on a union of $\mathcal{D}_{base}$, the known base dataset, and $\mathcal{D}_{new}$, the newly collected dataset. To define this compatibility measure in a way that is easy to compute, we can use two interpretable metrics: likelihood and novelty. The likelihood of actions $a_{new}$ in $\mathcal{D}_{new}$ is measured as the negative mean squared error between actions predicted by the base policy and this proposed action:</span>
<span id="cb4-889"><a href="#cb4-889"></a></span>
<span id="cb4-890"><a href="#cb4-890"></a>$$likelihood(s_{new}, a_{new}) = -\mathbb{E}<span class="co">[</span><span class="ot">|| \pi_{base}(s_{new}) - a_{new} ||^2_2</span><span class="co">]</span>.$$  {#eq-eq3.61}</span>
<span id="cb4-891"><a href="#cb4-891"></a></span>
<span id="cb4-892"><a href="#cb4-892"></a>The novelty of the state $s_{new}$ in $\mathcal{D}_{new}$ is the standard deviation in the predicted actions under base policy:</span>
<span id="cb4-893"><a href="#cb4-893"></a></span>
<span id="cb4-894"><a href="#cb4-894"></a>$$novelty(s_{new}) = \mathrm{Var}<span class="co">[</span><span class="ot">\pi_{base}(s_{new})</span><span class="co">]</span>.$$  {#eq-eq3.62}</span>
<span id="cb4-895"><a href="#cb4-895"></a></span>
<span id="cb4-896"><a href="#cb4-896"></a>We can plot likelihood and novelty on a 2D plane, as shown in @fig-likelihood_novelty, and identify thresholds on</span>
<span id="cb4-897"><a href="#cb4-897"></a>likelihood and novelty, denoted as $\lambda$ and $\eta$ respectively.</span>
<span id="cb4-898"><a href="#cb4-898"></a>Intuitively, demonstrations with low likelihood in low novelty states</span>
<span id="cb4-899"><a href="#cb4-899"></a>should be excluded, because this indicates that there is a conflict</span>
<span id="cb4-900"><a href="#cb4-900"></a>between the base behavior and the new demonstration due to</span>
<span id="cb4-901"><a href="#cb4-901"></a>multimodality. Note that in high novelty states, the likelihood should</span>
<span id="cb4-902"><a href="#cb4-902"></a>be disregarded because the base policy does not have a concrete idea for</span>
<span id="cb4-903"><a href="#cb4-903"></a>how to handle these states anyways so more data is needed.</span>
<span id="cb4-904"><a href="#cb4-904"></a></span>
<span id="cb4-905"><a href="#cb4-905"></a>![Examples of plots of likelihood and novelty for compatible and</span>
<span id="cb4-906"><a href="#cb4-906"></a>incompatible operators</span>
<span id="cb4-907"><a href="#cb4-907"></a><span class="co">[</span><span class="ot">@gandhi2022eliciting</span><span class="co">]</span>](Figures/likelihood_novelty.png){#fig-likelihood_novelty</span>
<span id="cb4-908"><a href="#cb4-908"></a>width="80%"}</span>
<span id="cb4-909"><a href="#cb4-909"></a></span>
<span id="cb4-910"><a href="#cb4-910"></a>The final compatibility metric, parameterized by the likelihood and</span>
<span id="cb4-911"><a href="#cb4-911"></a>novelty thresholds $\lambda$ and $\eta$, is</span>
<span id="cb4-912"><a href="#cb4-912"></a>$\mathcal{M}(\mathcal{D}_{base}, (s_{new}, a_{new})) \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$,</span>
<span id="cb4-913"><a href="#cb4-913"></a>defined as:</span>
<span id="cb4-914"><a href="#cb4-914"></a></span>
<span id="cb4-915"><a href="#cb4-915"></a>$$\begin{aligned}</span>
<span id="cb4-916"><a href="#cb4-916"></a>    \mathcal{M} = \begin{cases} </span>
<span id="cb4-917"><a href="#cb4-917"></a>        1 - \min(\frac{\mathbb{E}<span class="co">[</span><span class="ot">|| \pi_{base}(s_{new}) - a_{new} ||^2_2</span><span class="co">]</span>}{\lambda}, 1) &amp; \text{ if } \text{novelty}(s_{new}) &lt; \eta <span class="sc">\\</span></span>
<span id="cb4-918"><a href="#cb4-918"></a>        1 &amp; \text{ otherwise }</span>
<span id="cb4-919"><a href="#cb4-919"></a>       \end{cases}.</span>
<span id="cb4-920"><a href="#cb4-920"></a>\end{aligned}$$  {#eq-eq3.63}</span>
<span id="cb4-921"><a href="#cb4-921"></a></span>
<span id="cb4-922"><a href="#cb4-922"></a>Note that $\lambda$ and $\eta$ need to be specified by hand. This is</span>
<span id="cb4-923"><a href="#cb4-923"></a>accomplished by assuming the ability to collect *a priori incompatible*</span>
<span id="cb4-924"><a href="#cb4-924"></a>demonstrations to identify reasonable thresholds that remove the most</span>
<span id="cb4-925"><a href="#cb4-925"></a>datapoints in the incompatible demonstrations while keeping the most</span>
<span id="cb4-926"><a href="#cb4-926"></a>datapoints in the compatible demonstrations.</span>
<span id="cb4-927"><a href="#cb4-927"></a></span>
<span id="cb4-928"><a href="#cb4-928"></a><span class="fu">#### Case Studies with Fixed Sets</span></span>
<span id="cb4-929"><a href="#cb4-929"></a></span>
<span id="cb4-930"><a href="#cb4-930"></a>The researchers evaluate the utility of the compatibility metric on three tasks: placing a square nut on a square peg, placing a round nut on a round peg, and opening a drawer and placing a hammer inside. For each task, they train a base policy using a "proficient" operator's demonstration while sampling trajectories from other operators for the new set. The naive baseline is to use all datapoints while the $\mathcal{M}$-Filtered demonstrations use the compatibility metric to filter out incompatible demonstrations. The results are presented in @tbl-m_filter_table. As you can see, M-filtering results in equal or greater performance despite using less data than the naive baseline, demonstrating the effectiveness of compatibility-based filtering.</span>
<span id="cb4-931"><a href="#cb4-931"></a></span>
<span id="cb4-932"><a href="#cb4-932"></a>::: {#tbl-m_filter_table}</span>
<span id="cb4-933"><a href="#cb4-933"></a>  --------------- ---------------- ------------------------ --------------- ------------------------ ---------------------- ------------------------</span>
<span id="cb4-934"><a href="#cb4-934"></a><span class="in">                   Square Nut                                    Round Nut                             Hammer Placement  </span></span>
<span id="cb4-935"><a href="#cb4-935"></a>  Operator         Naive            $\mathcal{M}$-Filtered       Naive       $\mathcal{M}$-Filtered    Naive                 $\mathcal{M}$-Filtered</span>
<span id="cb4-936"><a href="#cb4-936"></a>  Base Operator      38.7 (2.1)               <span class="sc">\-</span>              13.3 (2.3)               <span class="sc">\-</span>                  24.7 (6.1)                  <span class="sc">\-</span></span>
<span id="cb4-937"><a href="#cb4-937"></a>  Operator 1         54.3 (1.5)           61.0 (4.4)          26.7 (11.7)         32.0 (12.2)              38.0 (2.0)              39.7 (4.6)</span>
<span id="cb4-938"><a href="#cb4-938"></a>  Operator 2         40.3 (5.1)           42.0 (2.0)          22.0 (7.2)           26.7 (5.0)              33.3 (3.1)              32.7 (6.4)</span>
<span id="cb4-939"><a href="#cb4-939"></a>  Operator 3         37.3 (2.1)           42.7 (0.6)          17.3 (4.6)          18.0 (13.9)              8.0 (0.0)               12.0 (0.0)</span>
<span id="cb4-940"><a href="#cb4-940"></a>  Operator 4         27.3 (3.5)           37.3 (2.1)           7.3 (4.6)           13.3 (1.2)              4.0 (0.0)               4.0 (0.0)</span>
<span id="cb4-941"><a href="#cb4-941"></a>  --------------- ---------------- ------------------------ --------------- ------------------------ ---------------------- ------------------------</span>
<span id="cb4-942"><a href="#cb4-942"></a></span>
<span id="cb4-943"><a href="#cb4-943"></a>  : Success rates (mean/std across 3 training runs) for policies trained</span>
<span id="cb4-944"><a href="#cb4-944"></a>  on $\mathcal{D}_{new}$ by using all the data (Naive) or filtering by</span>
<span id="cb4-945"><a href="#cb4-945"></a>  compatibility ($\mathcal{M}$-Filtered) <span class="co">[</span><span class="ot">@gandhi2022eliciting</span><span class="co">]</span></span>
<span id="cb4-946"><a href="#cb4-946"></a>:::</span>
<span id="cb4-947"><a href="#cb4-947"></a></span>
<span id="cb4-948"><a href="#cb4-948"></a>![The phases of the active elicitation interface: (a) initial prompting,</span>
<span id="cb4-949"><a href="#cb4-949"></a>(b) demonstrations with live feedback, and (c) corrective feedback</span>
<span id="cb4-950"><a href="#cb4-950"></a><span class="co">[</span><span class="ot">@gandhi2022eliciting</span><span class="co">]</span>](Figures/active_elicitation.png){#fig-active_elicitation</span>
<span id="cb4-951"><a href="#cb4-951"></a>width="80%"}</span>
<span id="cb4-952"><a href="#cb4-952"></a></span>
<span id="cb4-953"><a href="#cb4-953"></a><span class="fu">#### Actively Eliciting Compatible Demonstrations</span></span>
<span id="cb4-954"><a href="#cb4-954"></a></span>
<span id="cb4-955"><a href="#cb4-955"></a>In the previous section, we assume access to a dataset that has already</span>
<span id="cb4-956"><a href="#cb4-956"></a>been collected, and we see how filtering out incompatible demonstrations</span>
<span id="cb4-957"><a href="#cb4-957"></a>helps improve performance. However, when collecting a new dataset, it</span>
<span id="cb4-958"><a href="#cb4-958"></a>would be better to ensure that operators collect compatible</span>
<span id="cb4-959"><a href="#cb4-959"></a>demonstrations from the start, allowing us to retain as much data as</span>
<span id="cb4-960"><a href="#cb4-960"></a>possible for training.</span>
<span id="cb4-961"><a href="#cb4-961"></a></span>
<span id="cb4-962"><a href="#cb4-962"></a>To actively elicit compatible demonstrations, the researchers set up a</span>
<span id="cb4-963"><a href="#cb4-963"></a>pipeline for live feedback and examples. At the start, operators are</span>
<span id="cb4-964"><a href="#cb4-964"></a>given a task specification and some episodes to practice using the</span>
<span id="cb4-965"><a href="#cb4-965"></a>robot. Then, the active elicitation process begins, as shown in @fig-active_elicitation. Each operator is shown some</span>
<span id="cb4-966"><a href="#cb4-966"></a>rollouts of the base policy to understand the style of the base</span>
<span id="cb4-967"><a href="#cb4-967"></a>operator. Next, the operator provides a demonstration similar to the</span>
<span id="cb4-968"><a href="#cb4-968"></a>ones they were shown. As they record their demonstrations, the interface</span>
<span id="cb4-969"><a href="#cb4-969"></a>provides online feedback, with green indicating compatible actions and</span>
<span id="cb4-970"><a href="#cb4-970"></a>red indicating incompatible actions. If the number of incompatible</span>
<span id="cb4-971"><a href="#cb4-971"></a>state-action pairs (ones where $\mathcal{M}$ is zero) exceeds 5% of the</span>
<span id="cb4-972"><a href="#cb4-972"></a>demonstration length, the demonstration is rejected. However, to provide</span>
<span id="cb4-973"><a href="#cb4-973"></a>corrective feedback, the interface shows the areas of the demonstration</span>
<span id="cb4-974"><a href="#cb4-974"></a>with the highest average incompatibility and also provides an expert</span>
<span id="cb4-975"><a href="#cb4-975"></a>demo that shows what should actually be done. Demonstrators can use this</span>
<span id="cb4-976"><a href="#cb4-976"></a>feedback to provide more compatible demonstrations moving forward.</span>
<span id="cb4-977"><a href="#cb4-977"></a></span>
<span id="cb4-978"><a href="#cb4-978"></a>This process helps improve the demonstration quality in both simulation</span>
<span id="cb4-979"><a href="#cb4-979"></a>and real experiments, as show in @tbl-active_elicitation_results. Specifically, on the real</span>
<span id="cb4-980"><a href="#cb4-980"></a>results, active elicitation outperformed the base policy by 25% and</span>
<span id="cb4-981"><a href="#cb4-981"></a>naive data collection by 55%. Overall, active elicitation is a powerful</span>
<span id="cb4-982"><a href="#cb4-982"></a>tool to ensure that data collected for imitation learning improves the</span>
<span id="cb4-983"><a href="#cb4-983"></a>quality of the learned policy.</span>
<span id="cb4-984"><a href="#cb4-984"></a></span>
<span id="cb4-985"><a href="#cb4-985"></a>::: {#tbl-active_elicitation_results}</span>
<span id="cb4-986"><a href="#cb4-986"></a>  Task                                            Base     Naive    Naive + Filtered   Informed</span>
<span id="cb4-987"><a href="#cb4-987"></a>  ------------------------------------------------- ------------ ------------- ---------------------- --------------</span>
<span id="cb4-988"><a href="#cb4-988"></a>  Round Nut                                      13.3 (2.3)    9.6 (4.6)         9.7 (4.2)          15.7 (6.0)</span>
<span id="cb4-989"><a href="#cb4-989"></a>  Hammer Placement                               24.7 (6.1)   20.8 (15.7)       22.0 (15.5)        31.8 (16.3)</span>
<span id="cb4-990"><a href="#cb4-990"></a>  $\left<span class="co">[</span><span class="ot"> \textup{Real} \right</span><span class="co">]</span>$ Food Plating       60.0      30.0 (17.3)            <span class="sc">\-</span>             85.0 (9.6)</span>
<span id="cb4-991"><a href="#cb4-991"></a></span>
<span id="cb4-992"><a href="#cb4-992"></a>  : Success rates (mean/std across users) for policies trained on</span>
<span id="cb4-993"><a href="#cb4-993"></a>  $\mathcal{D}_{new}$ by using all the data (Naive), filtering by</span>
<span id="cb4-994"><a href="#cb4-994"></a>  compatibility ($\mathcal{M}$-Filtered), or using informed</span>
<span id="cb4-995"><a href="#cb4-995"></a>  demonstration collection <span class="co">[</span><span class="ot">@gandhi2022eliciting</span><span class="co">]</span></span>
<span id="cb4-996"><a href="#cb4-996"></a>:::</span>
<span id="cb4-997"><a href="#cb4-997"></a></span>
<span id="cb4-998"><a href="#cb4-998"></a>A fundamental limitation of eliciting compatible demonstrations is the</span>
<span id="cb4-999"><a href="#cb4-999"></a>fact that the "base" demonstrator is considered the ground truth. When</span>
<span id="cb4-1000"><a href="#cb4-1000"></a>the base demonstrator specifies a preference, all other demonstrators</span>
<span id="cb4-1001"><a href="#cb4-1001"></a>must abide by it, even if they have strong preferences against it. For</span>
<span id="cb4-1002"><a href="#cb4-1002"></a>instance, when pouring milk and cereal into a bowl, different people</span>
<span id="cb4-1003"><a href="#cb4-1003"></a>have different preferences for what is the correct order, but active</span>
<span id="cb4-1004"><a href="#cb4-1004"></a>elicitation forces all demonstrators to follow the initial preference of</span>
<span id="cb4-1005"><a href="#cb4-1005"></a>the base operator. The researchers hope that future work can enable</span>
<span id="cb4-1006"><a href="#cb4-1006"></a>users to override the default demonstration set and follow a base</span>
<span id="cb4-1007"><a href="#cb4-1007"></a>behavior that better aligns with their preferences. This could enable</span>
<span id="cb4-1008"><a href="#cb4-1008"></a>multiple modes of behavior to be collected in data while only following</span>
<span id="cb4-1009"><a href="#cb4-1009"></a>a user's specified preference instead of attempting to collapse all</span>
<span id="cb4-1010"><a href="#cb4-1010"></a>modes into a single policy.</span>
<span id="cb4-1011"><a href="#cb4-1011"></a></span>
<span id="cb4-1012"><a href="#cb4-1012"></a>Looking forward, active elicitation provides a foundation for allowing</span>
<span id="cb4-1013"><a href="#cb4-1013"></a>robots to query humans about the type of data needed, enabling more</span>
<span id="cb4-1014"><a href="#cb4-1014"></a>efficient data collection through transparency.</span>
<span id="cb4-1015"><a href="#cb4-1015"></a></span>
<span id="cb4-1016"><a href="#cb4-1016"></a>In summary, this chapter has explored the complexities and innovations</span>
<span id="cb4-1017"><a href="#cb4-1017"></a>in interAL as applied to large models within robotics. It</span>
<span id="cb4-1018"><a href="#cb4-1018"></a>begins by investigating pairwise comparisons and their role in</span>
<span id="cb4-1019"><a href="#cb4-1019"></a>efficiently learning linear reward functions from large datasets,</span>
<span id="cb4-1020"><a href="#cb4-1020"></a>overcoming limitations in supervised learning. When combined with active</span>
<span id="cb4-1021"><a href="#cb4-1021"></a>learning techniques, these comparisons supply timely, targeted, and</span>
<span id="cb4-1022"><a href="#cb4-1022"></a>context-appropriate feedback, enhancing performance in time-critical</span>
<span id="cb4-1023"><a href="#cb4-1023"></a>applications like exoskeleton adjustments during rehabilitation.</span>
<span id="cb4-1024"><a href="#cb4-1024"></a></span>
<span id="cb4-1025"><a href="#cb4-1025"></a>We then shift to imitation learning or inverse reward learning from</span>
<span id="cb4-1026"><a href="#cb4-1026"></a>demonstrations, emphasizing the difficulties introduced by multimodal</span>
<span id="cb4-1027"><a href="#cb4-1027"></a>demonstration sets. active elicitation approaches to compile compatible</span>
<span id="cb4-1028"><a href="#cb4-1028"></a>demonstrations, streamlining the learning process by guiding users to</span>
<span id="cb4-1029"><a href="#cb4-1029"></a>provide more valuable, steady examples are incredibly promising,</span>
<span id="cb4-1030"><a href="#cb4-1030"></a>however, to tackling this issue. This method shows promise in refining</span>
<span id="cb4-1031"><a href="#cb4-1031"></a>the interactive imitation learning data collection pipeline, enabling</span>
<span id="cb4-1032"><a href="#cb4-1032"></a>more capable and effective robotic training.</span>
<span id="cb4-1033"><a href="#cb4-1033"></a></span>
<span id="cb4-1034"><a href="#cb4-1034"></a>Additionally, the chapter examines the integration of foundation models</span>
<span id="cb4-1035"><a href="#cb4-1035"></a>into robotics, highlighting the transformative innovations of R3M and</span>
<span id="cb4-1036"><a href="#cb4-1036"></a>Voltron. R3M's pre-training on diverse human activities dramatically</span>
<span id="cb4-1037"><a href="#cb4-1037"></a>improves robotic manipulation with minimal supervision. Meanwhile,</span>
<span id="cb4-1038"><a href="#cb4-1038"></a>Voltron builds on these capabilities by incorporating language-driven</span>
<span id="cb4-1039"><a href="#cb4-1039"></a>representation learning for remarkably adaptable and nuanced robotic</span>
<span id="cb4-1040"><a href="#cb4-1040"></a>task performance. These models represent significant leaps in robotics</span>
<span id="cb4-1041"><a href="#cb4-1041"></a>while opening new frontiers for future research and applications.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
    <footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/stair-lab/mlhp/blob/main/src/chap4.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/stair-lab/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer><script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  
<script type="module">
/**
 * Factory function to create different types of cells based on options.
 * @param {Object} cellData - JSON object containing code, id, and options.
 * @returns {BaseCell} Instance of the appropriate cell class.
 */
globalThis.qpyodideCreateCell = function(cellData) {
    switch (cellData.options.context) {
        case 'interactive':
            return new InteractiveCell(cellData);
        case 'output':
            return new OutputCell(cellData);
        case 'setup':
            return new SetupCell(cellData);
        default:
            return new InteractiveCell(cellData);
            // throw new Error('Invalid cell type specified in options.');
    }
}  

/**
 * CellContainer class for managing a collection of cells.
 * @class
 */
class CellContainer {
    /**
     * Constructor for CellContainer.
     * Initializes an empty array to store cells.
     * @constructor
     */
    constructor() {
        this.cells = [];
    }

    /**
     * Add a cell to the container.
     * @param {BaseCell} cell - Instance of a cell (BaseCell or its subclasses).
     */
    addCell(cell) {
        this.cells.push(cell);
    }

    /**
     * Execute all cells in the container.
     */
    async executeAllCells() {
        for (const cell of this.cells) {
            await cell.executeCode();
        }
    }

    /**
     * Execute all cells in the container.
     */
    async autoRunExecuteAllCells() {
        for (const cell of this.cells) {
            await cell.autoRunExecuteCode();
        }
    }
}
  

/**
 * BaseCell class for handling code execution using Pyodide.
 * @class
 */
class BaseCell {
    /**
     * Constructor for BaseCell.
     * @constructor
     * @param {Object} cellData - JSON object containing code, id, and options.
     */
    constructor(cellData) {
        this.code = cellData.code;
        this.id = cellData.id;
        this.options = cellData.options;
        this.insertionLocation = document.getElementById(`qpyodide-insertion-location-${this.id}`);
        this.executionLock = false;
    }

    cellOptions() {
        // Subclass this? 
        console.log(this.options);
        return this.options;
    }

    /**
     * Execute the Python code using Pyodide.
     * @returns {*} Result of the code execution.
     */
    async executeCode() {
        // Execute code using Pyodide
        const result = getPyodide().runPython(this.code);
        return result;
    }
};

/**
 * InteractiveCell class for creating editable code editor with Monaco Editor.
 * @class
 * @extends BaseCell
 */
class InteractiveCell extends BaseCell {

    /**
     * Constructor for InteractiveCell.
     * @constructor
     * @param {Object} cellData - JSON object containing code, id, and options.
     */
    constructor(cellData) {
        super(cellData);
        this.editor = null;
        this.setupElement();
        this.setupMonacoEditor();
    }

    /**
     * Set up the interactive cell elements
     */
    setupElement() {

        // Create main div element
        var mainDiv = document.createElement('div');
        mainDiv.id = `qpyodide-interactive-area-${this.id}`;
        mainDiv.className = `qpyodide-interactive-area`;
        if (this.options.classes) {
            mainDiv.className += " " + this.options.classes
        }

        // Add a unique cell identifier that users can customize
        if (this.options.label) {
            mainDiv.setAttribute('data-id', this.options.label);
        }

        // Create toolbar div
        var toolbarDiv = document.createElement('div');
        toolbarDiv.className = 'qpyodide-editor-toolbar';
        toolbarDiv.id = `qpyodide-editor-toolbar-${this.id}`;

        // Create a div to hold the left buttons
        var leftButtonsDiv = document.createElement('div');
        leftButtonsDiv.className = 'qpyodide-editor-toolbar-left-buttons';

        // Create a div to hold the right buttons
        var rightButtonsDiv = document.createElement('div');
        rightButtonsDiv.className = 'qpyodide-editor-toolbar-right-buttons';

        // Create Run Code button
        var runCodeButton = document.createElement('button');
        runCodeButton.className = 'btn btn-default qpyodide-button qpyodide-button-run';
        runCodeButton.disabled = true;
        runCodeButton.type = 'button';
        runCodeButton.id = `qpyodide-button-run-${this.id}`;
        runCodeButton.textContent = '🟡 Loading Pyodide...';
        runCodeButton.title = `Run code (Shift + Enter)`;

        // Append buttons to the leftButtonsDiv
        leftButtonsDiv.appendChild(runCodeButton);

        // Create Reset button
        var resetButton = document.createElement('button');
        resetButton.className = 'btn btn-light btn-xs qpyodide-button qpyodide-button-reset';
        resetButton.type = 'button';
        resetButton.id = `qpyodide-button-reset-${this.id}`;
        resetButton.title = 'Start over';
        resetButton.innerHTML = '<i class="fa-solid fa-arrows-rotate"></i>';

        // Create Copy button
        var copyButton = document.createElement('button');
        copyButton.className = 'btn btn-light btn-xs qpyodide-button qpyodide-button-copy';
        copyButton.type = 'button';
        copyButton.id = `qpyodide-button-copy-${this.id}`;
        copyButton.title = 'Copy code';
        copyButton.innerHTML = '<i class="fa-regular fa-copy"></i>';

        // Append buttons to the rightButtonsDiv
        rightButtonsDiv.appendChild(resetButton);
        rightButtonsDiv.appendChild(copyButton);

        // Create console area div
        var consoleAreaDiv = document.createElement('div');
        consoleAreaDiv.id = `qpyodide-console-area-${this.id}`;
        consoleAreaDiv.className = 'qpyodide-console-area';

        // Create editor div
        var editorDiv = document.createElement('div');
        editorDiv.id = `qpyodide-editor-${this.id}`;
        editorDiv.className = 'qpyodide-editor';

        // Create output code area div
        var outputCodeAreaDiv = document.createElement('div');
        outputCodeAreaDiv.id = `qpyodide-output-code-area-${this.id}`;
        outputCodeAreaDiv.className = 'qpyodide-output-code-area';
        outputCodeAreaDiv.setAttribute('aria-live', 'assertive');

        // Create pre element inside output code area
        var preElement = document.createElement('pre');
        preElement.style.visibility = 'hidden';
        outputCodeAreaDiv.appendChild(preElement);

        // Create output graph area div
        var outputGraphAreaDiv = document.createElement('div');
        outputGraphAreaDiv.id = `qpyodide-output-graph-area-${this.id}`;
        outputGraphAreaDiv.className = 'qpyodide-output-graph-area';

        // Append buttons to the toolbar
        toolbarDiv.appendChild(leftButtonsDiv);
        toolbarDiv.appendChild(rightButtonsDiv);

        // Append all elements to the main div
        mainDiv.appendChild(toolbarDiv);
        consoleAreaDiv.appendChild(editorDiv);
        consoleAreaDiv.appendChild(outputCodeAreaDiv);
        mainDiv.appendChild(consoleAreaDiv);
        mainDiv.appendChild(outputGraphAreaDiv);

        // Insert the dynamically generated object at the document location.
        this.insertionLocation.appendChild(mainDiv);
    }

    /**
     * Set up Monaco Editor for code editing.
     */
    setupMonacoEditor() {

        // Retrieve the previously created document elements
        this.runButton = document.getElementById(`qpyodide-button-run-${this.id}`);
        this.resetButton = document.getElementById(`qpyodide-button-reset-${this.id}`);
        this.copyButton = document.getElementById(`qpyodide-button-copy-${this.id}`);
        this.editorDiv = document.getElementById(`qpyodide-editor-${this.id}`);
        this.outputCodeDiv = document.getElementById(`qpyodide-output-code-area-${this.id}`);
        this.outputGraphDiv = document.getElementById(`qpyodide-output-graph-area-${this.id}`);
        
        // Store reference to the object
        var thiz = this;

        // Load the Monaco Editor and create an instance
        require(['vs/editor/editor.main'], function () {
            thiz.editor = monaco.editor.create(
                thiz.editorDiv, {
                    value: thiz.code,
                    language: 'python',
                    theme: 'vs-light',
                    automaticLayout: true,           // Works wonderfully with RevealJS
                    scrollBeyondLastLine: false,
                    minimap: {
                        enabled: false
                    },
                    fontSize: '17.5pt',              // Bootstrap is 1 rem
                    renderLineHighlight: "none",     // Disable current line highlighting
                    hideCursorInOverviewRuler: true,  // Remove cursor indictor in right hand side scroll bar
                    readOnly: thiz.options['read-only'] ?? false
                }
            );
        
            // Store the official counter ID to be used in keyboard shortcuts
            thiz.editor.__qpyodideCounter = thiz.id;
        
            // Store the official div container ID
            thiz.editor.__qpyodideEditorId = `qpyodide-editor-${thiz.id}`;
        
            // Store the initial code value and options
            thiz.editor.__qpyodideinitialCode = thiz.code;
            thiz.editor.__qpyodideOptions = thiz.options;
        
            // Set at the model level the preferred end of line (EOL) character to LF.
            // This prevent `\r\n` from being given to the Pyodide engine if the user is on Windows.
            // See details in: https://github.com/coatless/quarto-Pyodide/issues/94
            // Associated error text: 
            // Error: <text>:1:7 unexpected input
        
            // Retrieve the underlying model
            const model = thiz.editor.getModel();
            // Set EOL for the model
            model.setEOL(monaco.editor.EndOfLineSequence.LF);
        
            // Dynamically modify the height of the editor window if new lines are added.
            let ignoreEvent = false;
            const updateHeight = () => {
            const contentHeight = thiz.editor.getContentHeight();
            // We're avoiding a width change
            //editorDiv.style.width = `${width}px`;
            thiz.editorDiv.style.height = `${contentHeight}px`;
                try {
                    ignoreEvent = true;
            
                    // The key to resizing is this call
                    thiz.editor.layout();
                } finally {
                    ignoreEvent = false;
                }
            };
        
            // Helper function to check if selected text is empty
            function isEmptyCodeText(selectedCodeText) {
                return (selectedCodeText === null || selectedCodeText === undefined || selectedCodeText === "");
            }
        
            // Registry of keyboard shortcuts that should be re-added to each editor window
            // when focus changes.
            const addPyodideKeyboardShortCutCommands = () => {
            // Add a keydown event listener for Shift+Enter to run all code in cell
            thiz.editor.addCommand(monaco.KeyMod.Shift | monaco.KeyCode.Enter, () => {
                // Retrieve all text inside the editor
                thiz.runCode(thiz.editor.getValue());
            });
        
            // Add a keydown event listener for CMD/Ctrl+Enter to run selected code
            thiz.editor.addCommand(monaco.KeyMod.CtrlCmd | monaco.KeyCode.Enter, () => {
                    // Get the selected text from the editor
                    const selectedText = thiz.editor.getModel().getValueInRange(thiz.editor.getSelection());
                    // Check if no code is selected
                    if (isEmptyCodeText(selectedText)) {
                        // Obtain the current cursor position
                        let currentPosition = thiz.editor.getPosition();
                        // Retrieve the current line content
                        let currentLine = thiz.editor.getModel().getLineContent(currentPosition.lineNumber);
                
                        // Propose a new position to move the cursor to
                        let newPosition = new monaco.Position(currentPosition.lineNumber + 1, 1);
                
                        // Check if the new position is beyond the last line of the editor
                        if (newPosition.lineNumber > thiz.editor.getModel().getLineCount()) {
                            // Add a new line at the end of the editor
                            thiz.editor.executeEdits("addNewLine", [{
                            range: new monaco.Range(newPosition.lineNumber, 1, newPosition.lineNumber, 1),
                            text: "\n", 
                            forceMoveMarkers: true,
                            }]);
                        }
                        
                        // Run the entire line of code.
                        thiz.runCode(currentLine);
                
                        // Move cursor to new position
                        thiz.editor.setPosition(newPosition);
                    } else {
                        // Code to run when Ctrl+Enter is pressed with selected code
                        thiz.runCode(selectedText);
                    }
                });
            }
        
            // Register an on focus event handler for when a code cell is selected to update
            // what keyboard shortcut commands should work.
            // This is a workaround to fix a regression that happened with multiple
            // editor windows since Monaco 0.32.0 
            // https://github.com/microsoft/monaco-editor/issues/2947
            thiz.editor.onDidFocusEditorText(addPyodideKeyboardShortCutCommands);
        
            // Register an on change event for when new code is added to the editor window
            thiz.editor.onDidContentSizeChange(updateHeight);
        
            // Manually re-update height to account for the content we inserted into the call
            updateHeight();
                
        });

        
        // Add a click event listener to the run button
        thiz.runButton.onclick = function () {
            thiz.runCode(
                thiz.editor.getValue()
            );
        };
        
        // Add a click event listener to the reset button
        thiz.copyButton.onclick = function () {
            // Retrieve current code data
            const data = thiz.editor.getValue();
            
            // Write code data onto the clipboard.
            navigator.clipboard.writeText(data || "");
        };
        
        // Add a click event listener to the copy button
        thiz.resetButton.onclick = function () {
            thiz.editor.setValue(thiz.editor.__qpyodideinitialCode);
        };
    }

    disableInteractiveCells() {
        // Enable locking of execution for the cell
        this.executionLock = true;

        // Disallowing execution of other code cells
        document.querySelectorAll(".qpyodide-button-run").forEach((btn) => {
            btn.disabled = true;
        });
    }

    enableInteractiveCells() {
        // Remove locking of execution for the cell
        this.executionLock = false;

        // All execution of other code cells
        document.querySelectorAll(".qpyodide-button-run").forEach((btn) => {
            btn.disabled = false;
        });
    }

    /**
     * Execute the Python code inside the editor.
     */
    async runCode(code) {
        
        // Check if we have an execution lock
        if (this.executeLock) return; 
        
        this.disableInteractiveCells();

        // Force wait procedure
        await mainPyodide;

        // Clear the output stock
        qpyodideResetOutputArray();

        // Generate a new canvas element, avoid attaching until the end
        let graphFigure = document.createElement("figure");
        document.pyodideMplTarget = graphFigure;

        console.log("Running code!");
        // Obtain results from the base class
        try {
            // Always check to see if the user adds new packages
            await mainPyodide.loadPackagesFromImports(code);

            // Process result
            const output = await mainPyodide.runPythonAsync(code);

            // Add output
            qpyodideAddToOutputArray(output, "stdout");
        } catch (err) {
            // Add error message
            qpyodideAddToOutputArray(err, "stderr");
            // TODO: There has to be a way to remove the Pyodide portion of the errors... 
        }

        const result = qpyodideRetrieveOutput();

        // Nullify the output area of content
        this.outputCodeDiv.innerHTML = "";
        this.outputGraphDiv.innerHTML = "";        

        // Design an output object for messages
        const pre = document.createElement("pre");
        if (/\S/.test(result)) {
            // Display results as HTML elements to retain output styling
            const div = document.createElement("div");
            div.innerHTML = result;
            pre.appendChild(div);
        } else {
            // If nothing is present, hide the element.
            pre.style.visibility = "hidden";
        }

        // Add output under interactive div
        this.outputCodeDiv.appendChild(pre);

        // Place the graphics onto the page
        if (graphFigure) {

            if (this.options['fig-cap']) {
                // Create figcaption element
                const figcaptionElement = document.createElement('figcaption');
                figcaptionElement.innerText = this.options['fig-cap'];
                // Append figcaption to figure
                graphFigure.appendChild(figcaptionElement);    
            }

            this.outputGraphDiv.appendChild(graphFigure);
        }

        // Re-enable execution
        this.enableInteractiveCells();
    }
};

/**
 * OutputCell class for customizing and displaying output.
 * @class
 * @extends BaseCell
 */
class OutputCell extends BaseCell {
    /**
     * Constructor for OutputCell.
     * @constructor
     * @param {Object} cellData - JSON object containing code, id, and options.
     */
    constructor(cellData) {
      super(cellData);
    }
  
    /**
     * Display customized output on the page.
     * @param {*} output - Result to be displayed.
     */
    displayOutput(output) {
        const results = this.executeCode();
        return results;
    }
  }

/**
 * SetupCell class for suppressed output.
 * @class
 * @extends BaseCell
 */
class SetupCell extends BaseCell {
    /**
     * Constructor for SetupCell.
     * @constructor
     * @param {Object} cellData - JSON object containing code, id, and options.
     */
    constructor(cellData) {
        super(cellData);
    }

    /**
     * Execute the Python code without displaying the results.
     */
    runSetupCode() {
        // Execute code without displaying output
        this.executeCode();
    }
};
</script>
<script type="module">
// Handle cell initialization initialization
qpyodideCellDetails.map(
    (entry) => {
      // Handle the creation of the element
      qpyodideCreateCell(entry);
    }
  );
</script>




</body></html>