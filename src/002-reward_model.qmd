# Reward Model {#ch-reward-models}

## Parameterization and Learning of Utility Functions {#sec-learning}
The attributes representing a choice $z_i$ are crucial in defining the human preference model, as they provide the context for capturing human behavior when choice $i$ is made. 

With an understanding of the various techniques we can use to model
human preferences, we can now create robust models which utilize context
attributes about the options an individual has in front of them and
model their choices. However, these models on their own are powerless;
their parameters are initialized randomly and we must fit the models to
the actual human choice data!

Each of the models we have studied contain distinct parameters which aim
to capture human preferences; for example $\beta$ is a parameter vector
containing variables which represent a linear function to compute
utility given a choice's attributes. We can also choose to represent
stochastic utility functions or embedding functions for Ideal Point
Models as neural networks. But how can we compute the optimal values of
these parameters?

In this section, we give the reader an overview of the different methods
available to tune human preference model parameters using given data. We
refer the reader to [@book_estimation_casella; @book_estimation_bock]
for first-principle derivations of these methods and a deeper dive into
their theoretical properties (convergence, generalization,
data-hungriness, etc.).

A common and powerful approach for computing the parameters of a model
is maximum likelihood estimation
[@book_estimation_casella; @book_estimation_bock]. The likelihood of a
model is the probability of the observed data given the model
parameters; intuitively we wish to maximize this likelihood, as that
would mean that our model associates observed human preferences in the
data with high probability. We can formally define the likelihood for a
model with parameters $\beta$ and a given data point $(z_i, y_i)$ as:
$$\mathcal{L}(z_i, y_i; \beta) = \mathbb{P}(y = y_i | z_i; \beta)$$

Assuming our data is independent and identically distributed (iid), the
likelihood over the entire dataset is the joint probability of all
observed data as defined by the model:
$$\mathcal{L}(z, Y; \beta) = \prod_{i = 1}^J \mathbb{P}(y = y_i | z_i; \beta)$$

In our very first example of binary choice with logistic noise, this was
simply the model's probability of the observed preference value:
$$\mathcal{L}(z_i, y_i; \beta) = \frac{1}{1 + \exp^{-u_{i,j}^*}}$$

In the same case with noise following a standard normal distribution,
this took the form: $$\mathcal{L}(z_i, y_i; \beta) = \Phi(u_{i,j}^*)$$

Fortunately, in these cases, there are straightforward methods for
parameter estimation: logistic regression and probit regression (binary
or multinomial, depending on the model), respectively. We can use
ordinal regression to estimate the model's parameters for our ordered
preference model.

Generally, the objective function commonly found in parameter learning
can be optimized with stochastic gradient descent (SGD)
[@gradient_descent]. We can define an objective function as the
likelihood to maximize this objective. Since SGD minimizes a given
objective, we must negate the likelihood, which ensures that a converged
solution maximizes the likelihood. SGD operates by computing the
gradient of the objective with respect to the parameters of the model,
which provides a signal of the direction in which the parameters must
move to *maximize* the objective. Then, SGD makes an update step by
subtracting this gradient from the parameters (most often with a scale
factor called a *learning rate*), to move the parameters in a direction
which *minimizes* the objective. When the objective is the negative
likelihood (or sometimes negative log-likelihood for convenience or
tractability), the result is an increase in the overall likelihood.

In the case of logistic and Gaussian models, SGD may yield a challenging
optimization problem as its stochasticity can lead to noisy updates, for
example, if certain examples or batches of examples are biased.
Mitigations include batched SGD, in which multiple samples are randomly
sampled from the dataset at each iteration, learning rates, which reduce
the impact of noisy gradient updates, and momentum and higher-order
optimizers which reduce noise by using movering averages of gradients or
provide better estimates of the best direction in which to update the
gradients. Some models, such as those that use neural networks, may, in
fact, be intractable to estimate without a method such as SGD (or its
momentum-based derivatives). For example, neural networks with many
layers, non-linearities, and parameters can only be efficiently computed
with gradient-based methods.

### Reward Learning with Large Language Models

Taking a step away from explicitly modeling human bias and preference,
we consider applying a deep learning approach to state-of-the-art
language models. We begin by introducing the concepts of *foundation
models* and *alignment*. A foundation model [@Liang2021] in machine
learning typically refers to a large and pre-trained neural network
model that serves as the basis for various downstream tasks. In natural
language processing, models like GPT-3, Llama, and BERT are considered
foundation models. They are pre-trained on a massive corpus of text
data, learning to understand language and context, and are capable of
various language-related tasks such as text classification, language
generation, and question answering. Foundation models are important
because they alleviate the need to train massive neural networks from
scratch, a compute and data expensive endeavor. However, a raw
foundation model, trained on a pretraining objective such as a language
modeling objective, is not useful on its own. It must be aligned to
respond correctly based on human preferences.

In short, alignment for foundation models is the process by which model
behavior is aligned with human values, ethics, and societal norms. Large
Language Models (LLMs) are a foundation model for natural language
processing. They are trained using a next-word prediction objective,
allowing them to generate coherent language. A simple way to align a
Large Language Model is to train it to follow instructions in a
supervised way, using instruction-response pairs curated by hand.
However, this limits the upper limit of LLM performance to the
performance of the annotators' writing abilities. This type of
annotation is also expensive.

An alternative, more promising approach is to train LLMs using
reinforcement learning, potentially enabling them to surpass human-level
performance. The main challenge with this method lies in defining an
explicit reward function for generating free-form text. To address this,
a reward model (RM) can be trained based on human preferences, providing
a mechanism to score the quality of the generated text. This approach,
known as Reinforcement Learning from Human Feedback (RLHF), leverages
human feedback to guide model training, allowing LLMs to better align
with human expectations while continuously improving performance.

![Overall architecture of a reward model based on LLM](Figures/arch.png){#fig-rm-arch}

The Llama2 reward model [@2307.09288] is initialized from the pretrained
Llama2 LLM. In the LLM, the last layer is a mapping
$L: \mathbb{R}^D \rightarrow \mathbb{R}^V$, where $D$ is the embedding dimension
from the transformer decoder stack and $V$ is the vocabulary size. To
get the RM, we replace that last layer with a randomly initialized
scalar head that maps $L: \mathbb{R}^D \rightarrow \mathbb{R}^1$. It's important to
initialize the RM from the LLM it's meant to evaluate. This is because:

1.  The RM will have the same "knowledge" as the LLM. This is
    particularly useful if evaluating things like "does the LLM know
    when it doesn't know?". However, in cases where the RM is simply
    evaluating helpfulness or factuality, it may be useful to have the
    RM know more.

2.  The RM is on distribution for the LLM - it is initialized in a way
    where it semantically understands the LLM's outputs.

An RM is trained with paired preferences, following the format:
$$\begin{aligned}
    \langle prompt\_history, response\_accepted, response\_rejected \rangle
\end{aligned}$$ Prompt_history is a multiturn history of user prompts
and model generations, response_accepted is the preferred final model
generation by an annotator, and response_rejected is the unpreferred
response. The RM is trained with a binary ranking loss with an optional
margin term m(r), shown in equation (7). There is also often a small
regularization term added to center the score distribution on 0.
$$\mathcal{L}_{\text{ranking}} = -\log(\sigma(r_\theta(x,y_c) - r_\theta(x,y_r) - m(r)))$$
The margin term increases the distance in scores specifically for
preference pairs annotators rate as easier to separate.

::: {#tbl-margin_nums}
  -------------- --------------- -------- ---------- -----------------
                  Significantly   Better   Slightly     Negligibly
                     Better                 Better    Better / Unsure
  Margin Small          1          2/3       1/3             0
  Margin Large          3           2         1              0
  -------------- --------------- -------- ---------- -----------------

  : Two variants of preference rating based margin with different magnitude.
:::

![Reward model score distribution shift caused by incorporating
preference rating based margin in ranking loss. With the margin term,
we observe a binary split pattern in reward distribution, especially for
a larger margin.](Figures/margin-2.png){#fig-margin-2
width="\\linewidth"}

It may seem confusing how the margins were chosen. It's primarily
because the sigmoid function, which is used to normalize the raw reward
model score, flattens out beyond the range of $[-4, 4]$. Thus, the
maximum possible margin is eight.

When training or using a reward model, watching for the following is
important:

1.  **LLM Distribution Shift**: With each finetune of the LLM, the RM
    should be updated through a collection of fresh human preferences
    using generations from the new LLM. This ensures that the RM stays
    aligned with the current distribution of the LLM and avoids drifting
    off-distribution.

2.  **RM and LLM are coupled**: An RM is generally optimized to
    distinguish human preferences more efficiently within the specific
    distribution of the LLM to be optimized. However, this
    specialization poses a challenge: such an RM will underperform when
    dealing with generations not aligned with this specific LLM
    distribution, such as generations from a completely different LLM.

3.  **Training Sensitivities of RMs**: Training RMs can be unstable and
    prone to overfitting, especially with multiple training epochs. It's
    generally advisable to limit the number of epochs during RM training
    to avoid this issue.

The industry has centered around optimizing for two primary qualities in
LLMs: helpfulness and harmlessness (safety). There are also other axes
such as factuality, reasoning, tool use, code, multilingual, and more,
but these are out of scope for us. In the Llama2 paper, preference data
was collected from humans for each quality, with separate guidelines.
This presents a challenge for co-optimizing the final LLM towards both
goals.

Two main approaches can be taken for Reinforcement Learning from Human
Feedback (RLHF) in this context:

1.  Train a unified reward model that integrates both datasets.

2.  Train two separate reward models, one for each quality, and optimize
    the LLM toward both.

Option 1 is difficult because of the tension between helpfulness and
harmlessness. They trade off against each other, confusing an RM trained
on both. The chosen solution was option 2, where two RMs are used to
train the LLM in a piecewise fashion. The helpfulness RM is used as the
primary optimization term, while the harmlessness RM acts as a penalty
term, driving the behavior of the LLM away from unsafe territory only
when the LLM veers beyond a certain threshold. This is formalized as
follows, where $R_s$, $R_h$, and $R_c$ are the safety, helpfulness, and
combined reward, respectively. $g$ and $p$ are the model generation and
the user prompt: $$\begin{aligned}
    R_c(g \mid p) =
    \begin{cases}
        R_s(g \mid p) & \text{if } \text{is\_safety}(p) \text{ or } R_s(g \mid p) < 0.15 \\
        R_h(g \mid p) & \text{otherwise}
    \end{cases}
\end{aligned}$$

There are several open issues with reward models alluded to in the
paper. For example, how best to collect human feedback? Training
annotators and making sure they do the correct thing is hard. What
should the guidelines be? Another question is whether RMs can be made
robust to adversarial prompts. Last but not least, do RMs have
well-calibrated scores? This matters for RLHF - pure preference accuracy
isn't enough.

### Reward Learning in Robotics

To help set up our basic reward learning problem, consider a user and a
robot. The user's preferences or goals can be represented by an internal
reward function, R($\xi$), which the robot needs to learn. Since the
reward function isn't explicit, there are a variety of ways that the
robot can learn this reward function, which we will discuss in the next
section. An example method of learning a reward function from human data
is using pairwise comparison. Consider the robot example from section
one, but now, the robot shows the human two possible trajectories
$\xi_A$ and $\xi_B$ as depicted in the diagram below.

![Two different trajectories taken by a robot to prompt
user ranking.](Figures/robots.png){#fig-reward-robot-1 width="70%"}

The user is show both the trajectories above and asked to rank which one
is better. Based on iterations of multiple trajectories and ranking, the
robot is able to learn the user's internal reward function. There quite
a lot of ways that models can learn a reward function from human data.
Here's a list [@myers2021learning] of some of them:

1.  Pairwise comparison: This is the method that we saw illustrated in
    the previous example. The robot is able to learn based on a
    comparison ranking provided by the user.

2.  Expert demonstrations: Experts perform the task and the robot learns
    the optimal reward function from these demonstrations.

3.  Sub-optimal demonstrations: The robot is provided with
    demonstrations that are not quite as good as the expert
    demonstrations but it is still able to learn a noisy reward function
    from the demonstrations.

4.  Physical Corrections: While the robot is performing the task, at
    each point in its trajectory (or at an arbitrary point in its
    trajectory) its arm is corrected to a more suitable position. Based
    on these corrections, the robot is able to learn the reward
    function.

5.  Ranking: This method is similar to pairwise comparison but involves
    more trajectories than 2. All the trajectories may have subtle
    differences from each other, but these differences help provide
    insight to the model.

6.  Trajectory Assessment: Given a single trajectory, the user rates how
    close it is to optimal, typically using a ranking scale.

    Each of these methods allows the robot to refine its understanding
    of the user's reward function, but their effectiveness can vary
    depending on the application. For instance, expert demonstrations
    tend to produce more reliable results but may not always be feasible
    in everyday tasks. Pairwise comparison and ranking methods offer
    more flexibility but might require a higher number of iterations.

### Reward Learning with Meta Learning

Learning a reward function from human preferences is an intricate and
complicated task. At its core, this task is about designing algorithms
that can capture what humans value based on their elicited preferences.
However, due to the nuanced and multifaceted nature of human desires,
learning reward functions from human can be a difficult task. Therefore,
meta-learning rewards may be considered to facilitate the reward
learning processes. Meta-learning, often referred to as "learning to
learn," aims to design models that can adapt to new tasks with minimal
additional efforts. We discuss paper [@hejna2023few] in @sec-few-shot showing how meta-learning can be leveraged for
few-shot preference learning, where a system can quickly adapt to a new
task after only a few queries to pairwise preferences from human.

Moving beyond the concept of learning from pairwise preferences, in @sec-watch we discuss a different approach where
meta-learning intersects with both demonstrations and
rewards [@zhou2019watch]. This paper considers the use of both
demonstrations and rewards elicited from human that guide the learning
process.

In the regular learning setting, a model is fitted to a dataset with
certain learning algorithm. The learning algorithm, for example, can be
the minimization of a loss function. To formulate the "regular" learning
procedure, let's denote the training dataset as $D$, and the test
dataset as $S$. Given a model parameterized by $\theta$; training loss
function $L(\theta, D)$; and test loss function $L(\theta, S)$, we can
formulate a process of "regular" machine learning process as
$$\begin{aligned}
    \theta^\star = \arg\min_\theta\quad L(\theta, D).
\end{aligned}$$ Note that the minimization of the training loss function
is essentially *one* possible learning algorithm. For example, instead
of minimizing the loss function, one may do gradient descent with model
regularization on the loss function, where the final solution may not be
the one that actually minimizes the loss function. As a result, we may
want to be more general and more abstract for the moment, and denote the
learning algorithm as $\mathcal{A}$. Thus, we can write
$$\begin{aligned}
    \theta^\star = \mathcal{A}(D),
\end{aligned}$$ i.e., the learning algorithm $\mathcal{A}$ takes in a
training dataset and outputs a model parameter $\theta^\star$. Then, the
performance of the model is evaluated by the test loss
$L(\mathcal{A}(D), S)$. As we can see, in the regime of "regular"
learning, the learning algorithm $\mathcal{A}$ is pre-defined and fixed.

Meta-learning, or learning-to-learn, essentially asks the question of
whether one can *learn* the learning algorithm $\mathcal{A}$ from prior
tasks, such that the modal can adapt to a new task more
quickly/proficiently. For example, different human languages share
similar ideas, and therefore a human expert who has learned many
languages should be able to learn a new language easier than an average
person. In other words, the human expert should have learned how to
learn new languages more quickly based on their past experiences on
learning languages.

To mathematically formulate meta-learning, we consider a family of
learning algorithms $\mathcal{A}_\omega$ parameterized by $\omega$. The
"prior" tasks are represented by a set of meta-training datasets
$\{(D_i, S_i)\}_{i=1}^N$ consists of $N$ pairs of training dataset $D_i$
and test dataset $S_i$. As we noted before, a learning algorithm
$\mathcal{A}_\omega$ takes in a training dataset, and outputs a model,
i.e., $$\begin{aligned}
    \forall i: \quad \theta^\star_i=\mathcal{A}_\omega(D_i).
\end{aligned}$$

Therefore, the **meta-learning objective** is $$\begin{aligned}
    \min_\omega \quad \sum_{i}\ L(\mathcal{A}_\omega(D_i), S_i).
\end{aligned}$$ The above optimization problem gives a solution
$\omega^\star$ which we use as the meta-parameter. Then, when a new task
comes with a new training dataset $D_{new}$, we can simply apply
$\theta^\star_{new}=\mathcal{A}_{\omega^\star}(D_{new})$ to obtain the
adapted model $\theta^\star_{new}$. Note that we usually assume the
meta-training datasets $D_i, S_i$ and the new dataset $D_{new}$ share
the same underlying structure, or they come from the same distribution
of datasets.

One of the most popular meta-learning method is Model-Agnosic
Meta-Learning (MAML) [@finn2017model]. In MAML, the meta-parameter
$\omega$ shares the same space as the model parameter $\theta$. At its
core, in MAML the learning algorithm is defined to be $$\begin{aligned}
    \mathcal{A}_\omega(D_i)=\omega-\alpha \nabla_\omega L(\omega, D_i),
\end{aligned}$$ where $\alpha$ is the step size. As we can see, in fact
$\omega$ is defined as the initialization of fine-tuning $\theta$. With
a good $\omega$ learned, the model can adapt to a new task very quickly.
In general, meta-learning can be summarized as follows: Given data from
prior tasks, learn to solve a new task more quickly/proficiently. Given
the general nature of meta-learning, one may be curious about whether
preference learning can be benefited from meta-learning, which we
discuss in the following section.

#### Few-Shot Preference Learning for Reinforcement Learning {#sec-few-shot}

Reinforcement learning (RL) in robotics often stumbles when it comes to
devising reward functions aligning with human intentions.
Preference-based RL algorithms aim to solve this by learning from human
feedback, but this often demands a *highly impractical number of
queries* or leads to oversimplified reward functions that don't hold up
in real-world tasks.

To address the impractical requirement of human queries, as we discussed
in the previous section, one may apply meta-learning so that the RL
agent can adapt to new tasks with fewer human queries. [@hejna2023few]
proposes to pre-training models on previous tasks with the meta-learning
method MAML [@finn2017model], and then the meta-trained model can adapt
to new tasks with fewer queries.

We consider Reinforcement Learning (RL) settings where a state is
denoted as $s\in S$, and action is denoted as $a\in A$, for state space
$S$ and action space $A$. The reward function $r:S\times A \to \mathbb{R}$ is
unknown and need to be learned from eliciting human preferences. There
are multiple tasks, where each task has its own reward function and
transition probabilities. The reward model is parameterized by $\psi$.
We denote $\hat{r}_\psi(s,a)$ to be a learned estimate of an unknown
ground-truth reward function $r(s,a)$, parameterized by $\psi$.
Accordingly, a reward model determines a RL policy $\phi$ by maximizing
the accumulated rewards. The preferences is learned via pairwise
comparison of trajectory segments $$\begin{aligned}
    \sigma = (s_t, a_t, s_{t+1}, a_{t+1}, ..., s_{t+k-1}, s_{t+k-1})
\end{aligned}$$ of $k$ states and actions.

For each pre-training task, there is a dataset $D$ consists of labeled
queries $(\sigma_1, \sigma_2, y)$ where $y\in \{0, 1\}$ is the label
representing which trajectory is preferred. Therefore, a loss function
$L(\psi, D)$ captures how well the reward model characterizes the
preferences in dataset $D$. In [@hejna2023few] they the preference
predictor over segments using the Bradley-Terry model of paired
comparisons [@bradley1952rank], i.e., $$\begin{aligned}
    P[\sigma_1 \succ \sigma_2 ] = \frac{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1})}{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1}) + \exp \sum_t \hat{r}_\psi(s_t^{2}, a_t^{2})}.
\end{aligned}$$ Then, the loss function is essentially a binary
cross-entropy which the reward model $\psi$ aims to minimize, i.e.,
$$\begin{aligned}
    {L}(\psi,  {D}) = - \mathbb{E}_{(\sigma^1, \sigma^2, y) \sim {D}} \left[ y(1) \log (P[\sigma_1 \succ \sigma_2 ]) + y(2)\log(1 - P[\sigma_1 \succ \sigma_2 ]) \right].
\end{aligned}$$

##### Method Component 1: Pre-Training with Meta Learning {#method-component-1-pre-training-with-meta-learning}

To efficiently approximate the reward function $r_\text{new}$ for a new
task with minimal queries, as described in [@hejna2023few], we aim to
utilize a pre-trained reward function $\hat{r}_\psi$ that can be quickly
fine-tuned using just a few preference comparisons. By pre-training on
data from prior tasks, we can leverage the common structure across tasks
to speed up the adaptation process. Although any meta-learning method is
compatible, [@hejna2023few] opt for Model Agnostic Meta-Learning (MAML)
due to its simplicity. Therefore, the pre-training update for the reward
model $\psi$ is $$\begin{aligned}
    \psi \xleftarrow{} \psi - \beta \nabla_\psi \sum_{i = 1}^N {L} (\psi - \alpha \nabla_\psi {L}(\psi, {D}_i), {D}_i),
\end{aligned}$$ where $\alpha, \beta$ are the inner and outer learning
rate, respectively. We note that data $\{D_i\}_i$ of labeled preferences
queries for prior tasks can come from offline datasets, simulated
policies, or actual humans.

##### Method Component 2: Few-Shot Adaptation {#method-component-2-few-shot-adaptation}

With the aforementioned pre-training with meta learning, the
meta-learned reward model can then be used for few-shot preference based
RL during an online adaptation phase. The core procedure of the few-shot
adaption is descibed as below

1.  Given a pre-trained reward model $\psi$

2.  For time step $t=1, 2, \dots$

    1.  Find pairs of trajectories $(\sigma_1, \sigma_2)$ with
        preference uncertainty based on $\psi$.

    2.  Query human preference $y$ and forms a new dataset $D_{new}$

    3.  Update the reward model by
        $\psi'\leftarrow \psi - \alpha \nabla_\psi L(\psi, D_{new})$

    4.  Update the policy with the new reward model $\psi'$

As mentioned in [@hejna2023few], uncertain queries are selected using
the disagreement of an ensemble of reward functions over the preference
predictors. Specifically, comparisons that maximize
$\texttt{std}(P[\sigma_1 \succ \sigma_2])$ are selected each time
feedback is collected.

The whole pipeline of the method is outlined in @fig-few-1.

![An overview of the proposed method in [@hejna2023few]. **Pre-training
(left):** In the pre-training phase, trajectory segment comparisons are
generated using data from previously learned tasks. Then, they are used
to train a reward model. **Online-Adaptation (Right)**: After
pre-training the reward model, it is adapted to new data from human
feedback. The adapted reward model is then used to train a policy for a
new task in a closed loop manner.](Figures/overview-few.png){#fig-few-1
width="\\linewidth"}

We present one set of experiment from the paper, as it illustrates the
effectiveness of the proposed method in a straightforward way. The
experiment test the propoesed method on the Meta-World
benchmark [@yu2020meta]. Three baselines are compared with the proposed
method:

1.  SAC: The Soft-Actor Critic RL algorithm trained from ground truth
    rewards. This represents the standard best possible method given the
    ground-truth reward.

2.  PEBBLE: The PEBBLE algorithm [@lee2021pebble]. It does not use
    information from pripor tasks.

3.  Init: This method initialize the reward model with the pretained
    weights from meta learning. However, instead of adapting the reward
    model to the new task, it performs standard updates as in PEBBLE.

The results are shown in @fig-few-exp, where we can see that the proposed methord
outperforms all of the baselines.

![Results on MetaWorld tasks. The title of each subplot indicates the
task and number of artificial feedback queries used in training. Results
for each method are shown across five seeds.
](Figures/few-exp.png){#fig-few-exp width="\\linewidth"}

This paper [@hejna2023few] shows that meta reward learning indeed reduce
the number of queries of human preferences. However, as mentioned in the
paper, there are still some drawbacks, as shown in the following.

Many of the queries the model pick for human preference elicitation are
actually almost identical to human. After all, the model would pick the
most uncertain pair of trajectories for human preference queries, and
similar trajectories are for sure having high uncertainty in their
preference. This suggest the need of new ways for designing the query
selection strategy.

Moreover, despite the improved query complexity, it still needs an
impractical amount of queries. As shown in @fig-few-exp, the "sweep into" task still needs 2500 human
queries for it to work properly, which is still not ideal for what we
want them to be.

In addition, it is mentioned in the paper that the proposed method may
be even worse than training from scratch, if the new task is too
out-of-distribution. Certainly, since meta-learning assumes
in-distribution tasks, we cannot expect the proposed method to be good
for out-of-distribution task. It is thus an interesting future direction
to investigate whether one can design a method that automatically
balance between using the prior information or training from scratch.

#### Watch Try Learn {#sec-watch}

Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards
[@zhou2019watch] asks the question "How can we efficiently learn both
from expert demonstrations and from trials where we only get **binary**
feedback from a human\". Why do we care about this question? In the
context of robotics, a very compelling answer is the *cost of
data-collection*. In a hypothetical world in which we have a vast number
of **expert demonstrations** of robots accomplishing a large number of
diverse tasks, we don't necessarily need to worry about learning from
trials or from humans. We could simply learn a very capable imitation
agent to perform any task. Natural Language Processing could be seen as
living in this world, because internet-scale data is available.
**Robots, however, are expensive**, so people generally don't have
access to them, and therefore cannot use them to produce information to
imitate. Similarly, **human time is expensive**, so even for large
organizations that do have access to a lot of robots, it's still hard to
collect a lot of expert demonstrations.

The largest available collection of robotics datasets today is Open
X-Embodiment ([@padalkar2023open]), which consists of around 1M episodes
from more than 300 different scenes. Even such large datastes are not
enough to learn generally-capable robotic policies from imitation
learning alone.

![Visualization of the Open X-Embodiment dataset collection. Even this
large-scale dataset for robot learning is not yet enough to learn
generally-capable robotic
policies.](Figures/open_x_embodiment.png){#fig-open-x-embodiment}

**Main insight:** binary feedback is much cheaper to obtain than expert
demonstrations! Instead of hiring people to act as robot operators to
tell the robot exactly what to do, if there was a way of having many
robots trying things in parallel, we can have humans watch videos of
what the robots did and then give a success classification of whether
the robot accomplished the goal. This is a much cheaper form of human
supervision because the human labels don't necessarily need to be given
in real time, so one human labeler can label many trajectories in
parallel, and the human doesn't need to be a skilled robot operator.

Concretely, this paper seeks to learn new tasks with the following
general problem setting:

1.  We only get 1 expert demonstration of the target task

2.  After seeing the expert demonstration, we have robots try to solve
    the task 1 or more times.

3.  The user (or some pre-defined reward function) annotates each trial
    as success/failure.

4.  The agent learns from both the demos and the annotated trials to
    perform well on the target task.

Note that this work falls under the **meta-learning** umbrella, because
we are learning an algorithm for quickly learning new tasks given new
observations (demos, trials, and success labels.)

The **main contribution** of this paper is a meta-learning algorithm for
incorporating demonstrations and binary feedback from trials to solve
new tasks.

Meta-Learning deals with efficient learning of new tasks. In the context
of robotics or reinforcement learning in general, **how do we define
tasks**? We will use the Markov decision process (**MDP**) formalism. A
task $T_i$ is described with the tuple $\{S, A, r_i, P_i\}$.

1.  $S$ represents the *state-space* of the task, or all possible states
    the agent could find itself in. This work uses image-observations,
    so $S$ is the space of all possible RGB images.

2.  $A$ is the action space, meaning the set of all possible actions the
    agent could take. In robotics there are many ways of representing
    action spaces, and this work considers end-effector positions,
    rotations, and opening.

3.  $r_i$ is the reward function for the task, with function signature
    $r_i : S \times A \to \mathbb{R}$. This work assumes all reward functions
    are binary.

4.  $P_i$ is the transition dynamics function. It's a function that maps
    state-action pairs to probability distributions over next states.

Notice that $S$ and $A$ are shared across tasks. Transition dynamics
functions are normally also shared between tasks because they represent
the laws of physics. However, this work considers environments with
different objects, so they don't share the dynamics function. Given this
definition for tasks, they assume that the tasks from the data that they
get come from some unknown task-generating distribution $p(T)$.

Let's give a more precise definition of the problem statement considered
by **Watch, Try, Learn**. As the paper name suggests, there are 3 phases
for the problem statement.

**Watch:** During the *watch* phase, we give the agent $K$
demonstrations of the target tasks. This paper considers the case where
$K$ always equals 1, and all demonstrations are successful. That is,
each demonstration consists of a trajectory
$\{(s_0, a_0), \ldots, (s_H, a_H)\}$ where $H$ is the task horizon, and
the final state is always successful, that is
$r_i(s_H, a_H) = 1, r_i(s_j, a_j) = 0$ for every $j \neq H$.

Importantly, these demonstrations alone might not be sufficient for
**full task specification**. As an example, consider a demonstration in
which an apple is moved to the right, next to a pan. Seeing this
demonstration alone, the task could be always moving the apple to the
right, or it could be always moving the apple next to the pan,
irrespective of where the pan is. The expected output after the Watch
phase is a policy capable of gathering information about a task, given
demonstrations.

**Try:** In the Try phase, we use the agent learned during the Watch
phase to attempt the task for $L$ trials. As specified earlier, this
paper considers the casae where $L$ always equals 1. After the agent
completes the trials, humans (or pre-programmed reward functions)
provide one binary reward for each trial, indicating whether the trial
was successful. The expected output of this phase is $L$ trajectories
and corresponding feedback that hopefully *disambiguate* the task.

**Learn:** After completing the trials, the agent must learn from both
the original expert demonstrations and the trials, and become capable of
solving the target task.

**Given Data:** To train agents that can Watch, Try, and Learn, we are
given a dataset of expert demonstrations containing multiple demos for
each task, and the dataset contains hundreds of tasks. Importantly, **no
online interaction** is needed for training, and this method trains only
with **supervised learning** and no reinforcement learning.

This section describes exactly how this paper trains an agent from the
given expert demonstrations, and how to incorporate the trials and human
feedback into the loop.

**Training to Watch:** We now describe the algorithm to obtain an agent
conditioned on the given expert demonstration. In particular, what we
want to obtain out of the Watch phase is a policy conditioned on a set
of expert demonstrations. Formally, we want to obtain
$\pi_\theta^{\text{watch}}(a | s, \{d_{i,k}\})$.

The way we can obtain this policy is through **meta-imitation
learning**. Given the demonstrations $\{\textbf{d}_{i,k}\}$ for task
$i$, we sample another *different* demonstration coming from the same
task $\textbf{d}_i^{\text{test}}$. The key insight here is that
$\textbf{d}_i^{\text{test}}$ is an example of **optimal behavior** given
the demonstrations. Therefore, to obtain
$\pi_\theta^{\text{watch}}(a | s, \{d_{i,k}\})$, we simply regress the
policy to imitate actions taken on $\textbf{d}_i^{\text{test}}$.
Concretely, we train policy parameters $\theta$ to minimize the
following loss:

$\mathcal{L}^\text{watch}(\theta, \mathcal{D}_i^*) = \mathbb{E}_{\{d_{i,k}\} \sim \mathcal{D}_i^*} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^*  \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[ 
- \log \pi_\theta^{\text{watch}} (a_t | s_t, \{d_{i,k}\}) \big]$

This corresponds to doing imitation learning by minimizing the negative
log-likelihood of the test trajectory actions, conditioning the policy
on the entire demo set. However, how is the conditioning on the demo set
achieved?

![Vision-based policy architecture that conditions on a set of
demonstrations.](Figures/watch-try-learn-architecture.png){#fig-watch-try-learn-arch}

@fig-watch-try-learn-arch visualizes how Watch Try Learn
deals with conditioning on demonstrations. In addition to using features
obtained from the images of the current state, the architecture uses
features from frames sampled (in order) from the demonstration episodes,
which are concatenated together.

**Trying:** On the **Try** phase, when the agent is given a set of
demonstrations $\{\textbf{d}_{i,k}\}$, we deploy the policy
$\pi_\theta^{\text{watch}}(a | s, \{\textbf{d}_{i,k}\})$ to collect $L$
trials. There is no training involved in the Try phase, we simply
condition the policy on the given demonstrations

**Training to Learn:** During the Watch phase the objective was to train
a policy conditioned on demonstrations
$\pi_\theta^{\text{watch}}(a | s, \{\textbf{d}_{i,k}\})$. The authors of
Watch, Try, Learn use a similar strategy as the Watch phase for the
Learn phase. We now want to train a policy that is conditioned on the
demonstrations, as well as the trials and binary feedback. That is, we
want to learn
$\pi_\phi^{\text{watch}}(a | s, \{\textbf{d}_{i,k}\}, \{\mathbf{\tau}_{i, l}\})$.
To train the policy, we again use meta-imitation learning where we
additionally sample yet another trajectory from the same task.
Concretely, we train policy parameters $\phi$ to minimize the following
loss:

$\mathcal{L}^{\text{learn}}(\phi, \mathcal{D}_i, \mathcal{D}_i^*) = \mathbb{E}_{(\{d_{i,k}\}, \{\mathbf{\tau}_{i,l}\}) \sim \mathcal{D}_i} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^* \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[ 
- \log \pi_\theta^{\text{learn}} (a_t | s_t, \{d_{i,k}\}, \{\tau_{i,l}\}) \big]$

The conditioning on both the demo episodes and the trial episodes is
achieved in the exact same way as in the Watch phase, and is visualized
in @fig-watch-try-learn-arch. The architecture is simply
adjusted to be able to take in more images fro mthe trial episodes.

In this section, we describe the evaluation suite for the paper,
including the simulation benchmark used, the baselines considered, and
the results.

**Gripper environment setup:**

![Visualization of different tasks from the simulated benchmark for
Watch Try Learn.](Figures/watch-try-learn-envs.png){#fig-envs}

@fig-envs illustrates the different task families considered in the simulated
Gripper environment. Button Pressing, Grasping, Pushing, and Pick and
Place. For each task family, the environment supports hundreds of
different tasks by changing the objects in the scene and the objectives
(e.g. which object to pick and where to place). For each task in each
task family, a handful of expert demonstrations are given in a
demonstrations dataset. As mentioned previously, the environment gives
the agent image observations, and take in actions as end-effector
(gripper) positions, angles, and opening.

**Baselines:** The following three baselines are considered:

1.  **Behavior Cloning**: simple imitation learning based on maximum
    log-likelihood training using data from all tasks.

2.  **Meta-imitation learning**: This baseline corresponds to simply
    running the policy from the Watch step, without using any trial
    data. That is, we only condition on the set of expert
    demonstrations, but no online trials.

3.  **Behavior Cloning + SAC**: Pre-train a policy with Behavior Cloning
    on all data, and follow that with Reinforcement Learning fine-tuning
    for the specific target task, using the maximum-entropy algorithm
    SAC ([@haarnoja2018soft]).

![Results for Watch Try Learn on the gripper control environment, and
comparisons with
baselines.](Figures/watch-try-learn-results.png){#fig-watch-try-learn-results
width="50%"}

::: {#tbl-watch-try-learn-table}
  **METHOD**                     **SUCCESS RATE**
  ----------------------------- ------------------
  BC                              .09 $\pm$ .01
  MIL                             .30 $\pm$ .02
  WTL, 1 TRIAL (OURS)             .42 $\pm$ .02
  **RL FINE-TUNING WITH SAC**   
  BC + SAC, 1500 TRIALS           .11 $\pm$ .07
  BC + SAC, 2000 TRIALS           .29 $\pm$ .10
  BC + SAC, 2500 TRIALS           .39 $\pm$ .11

  : Average success rates over all tasks.
:::

@fig-watch-try-learn-results shows average success rates for
Watch Try Learn compared to baselines. Watch Try Learn significantly
outperforms baselines on every task family. In particular, it is far
superior to Behavior Cloning, which is a very weak baseline, and it
significantly surpasses Meta-Imitation Learning on 3 out of 4 task
families. @tbl-watch-try-learn-table includes comparison with BC
fine-tuned with Reinforcement Learning. Even after 2500 online trials,
SAC is not able to obtain the success rate that Watch Try Learn achieves
after only 1 trial. Overall, Watch Try Learn exhibits very significant
performance gains over prior methods.

### Direct Preference Optimization

A modern method for estimating the parameters of a human preference
model is direct preference optimization [@rafailov2023direct], which is
used in the context of aligning language models to human preferences. A
recent approach [@christiano2023deep] first trains a reward model that
captures human preferences and then uses proximal policy optimization to
train a language model-based policy to reflect those learned
preferences. Direct Preference Optimization (DPO), on the other hand,
removes the need for a reward model by directly using the model
likelihood of two outcomes (a preferred or highly-ranked sequence and an
unpreferred or low-ranked sequence) to capture the preference
represented in the data. DPO provides a simpler framework than its
reinforcement learning approach and results in comparable performance
with improved stability. Furthermore, it obviates the need to train a
reward model, instead using a language model policy and human preference
dataset to align the policy directly to human preferences.

### Model Design Consideration

When designing models and learning their parameters, one must account
for important tradeoffs when designing and optimizing a model to learn
human preferences.

**Bias vs. Variance Trade-off.** In modeling human preferences, we aim
to ensure that predicted utilities accurately reflect overall human
preferences. One key challenge is managing the bias and variance
trade-off.

Bias refers to assumptions made during model design and training that
can skew predictions. For example, in Ideal Point Models, we make the
assumption that the representations we use for individuals and choices
are aligned in the embedding space, and that this representation is
sufficient to capture human preferences using distance metrics. However,
there are myriad cases in which this may break down, for example if the
two sets of vectors follow different distributions each with their own
unique biases. If the representations do not come from the same domain,
one may have little visibility into how a distance metric computes the
final utility value for a choice for a given individual. Some ways to
mitigate bias in human preference models include increasing the number
of parameters in a model (allowing for better learning of patterns in
the data) or removing inductive biases based on our assumptions of the
underlying data.

On the other hand, variance refers to the model's sensitivity to small
changes in the input, which leads to significant changes in the outp ut.
This phenomenon is often termed 'overfitting' or 'overparameterization.'
This behavior can occur in models that have many parameters, and learn
correlations in the data that do not contribute to learning human
preferences, but are artifacts of noise in the dataset that one should
ultimately ignore. One can address variance in models by reducing the
number of parameters or incorporating biases in the model based on
factors we can assume about the data.

**Model Scope.** One important consideration unique to human preference
models is that we wish to model individual preferences, and we may
choose to do so at arbitrary granularity. For example, we can fit models
to a specific individual or even multiple models for an individual, each
for different purposes or contexts. On the other end of the spectrum, we
may create a model to capture human preferences across large populations
or the world.

Individual models may certainly prove to be more powerful, as they do
not need to generalize across multiple individuals and can dedicate all
of their parameters to learning the preferences of a single user. In the
context of human behavior, this can be a significant advantage as any
two individuals can be arbitrarily different or even opposite in their
preferences. On the other hand, models fit only one person can
tremendously overfit to the training distribution and capture noise in
the data, which is not truly representative of human preferences.

On the end of the spectrum, models fit to the entire world may be
inadequate to model human preferences for arbitrary individuals,
especially those whose data it has not been fit to. As such, models may
underfit the given training distribution. These models aim to generalize
to many people but may fail to capture the nuances of individual
preferences, especially for those whose data is not represented in the
training set. As a result, they may not perform well for arbitrary
individuals within the target population

Choosing the appropriate scope for a model is crucial. ne must balance
the trade-off between overfitting to noise in highly granular models and
underfitting in broader models that may not capture individual nuances.

## Multimodal Preferences

One of the core assumptions about learning a reward function is that it
is unimodal, meaning that it consists of data from one person with a
certain set of preferences or a group of people with similar
preferences. However, the model of unimodality often oversimplifies
human preferences and their often conflicting nature. To accurately
capture all the nuances of human preference, we examine a multi-modal
distribution with some baseline assumptions. Consider a scenario where
we, as regular drivers, make a left-hand turn at an intersection
[@myers2021learning]. What would we do if we saw a car speeding down the
road approaching us? The figure below describes some options. Following
a timid driving pattern, some vehicles would stop to let the other car
go, preventing a collision. Other vehicles would be more aggressive and
try to make the turn before colliding with the oncoming vehicle. Given
the data of one of these driving patterns, our model (our autonomous
vehicle) can make an appropriate decision. However, what if our model
was given data from both aggressive and timid drivers, and we don't know
which data corresponds to which type of driver? If we applied standard
learning based on comparison techniques, we see, as illustrated by the
figure below, that the car would have an accident trying to find a
policy close enough to both driving patterns.

![[@myers2021learning] shows the possibilities of 2 different driving
patterns when a car is taking a left-hand turn at an intersection and
sees another car approaching
head-on.](Figures/driving-patt.png){#fig-driving-patt}

![The figure [@myers2021learning] depicts the resultant collision when we
try to find a policy close enough to both the driving
patterns.](Figures/driving-coll.png){#fig-driving-coll}

As illustrated by the driving example, we see that multi-modality for
our reward function is extremely important and, in some cases, if it is
not considered, can lead to fatal decisions [@myers2021learning]. But why
can't we label the groups, which would be the timid and aggressive
drivers in the driving case, and then learn separate reward functions
for each driver? The first problem with this approach is that it is
inefficient and time-consuming to separate the data into groups because
we would have to cluster and label the data. Secondly, it would not be
accurate just to split the data because a more timid driver can be
aggressive when they are in a hurry.

To formulate this problem of learning reward functions and mixing
coefficients from ranking queries in a fully observable deterministic
dynamical system, we begin by describing the system as a trajectory
$\xi = (s_0, a_0, ..., s_T, a_T)$, where the sequence of states and
actions represents the system's evolution over time. Assume there are
$M$ different reward functions, each representing an expert's
preferences. Using the linearity assumption in reward learning, we model
each expert's reward function as a linear combination of features in a
known, fixed feature space $\phi(\xi)$. The reward for the $m$-th expert
is given by: $$R_m(\xi) = \omega^T_m \phi(\xi),$$ where $\omega_m$ is a
vector of parameters corresponding to the $m$-th expert's preferences.
There exists an unknown distribution over the reward parameters and we
can represent this distribution with mixing coefficients $\alpha_m$ such
that $\sum_M^{m = 1} \alpha_m = 1$. Our goal is to learn reward
functions and mixing coefficients using ranking queries.

To define our problem, let's consider a robot who performs the following
trajectories and asks a user to rank all the trajectories.

![The figure [@myers2022learning] depicts a few different trajectories
for an example multi-modal ranking
scenario.](Figures/robot-traj.png){#fig-robot-traj}

The robot will be given back a set of trajectory rankings, coming from M
humans and the objective is to learn the underlying reward function. We
can represent the response of the ranking query as
$x = (\xi_{a_1},\ ...\ ,\xi_{a_K})$ where $a_1$ is the index of the
expert's top choice, $a_2$ is the index of the expert's second choice,
\... and so on. With the response $x$, we generate a probability
distribution with the softmax rule [@myers2022learning]:
$Pr(x_1 = \xi_{a_1} | R = R_m) = \frac{e^R_m(\xi_{a_1})}{\sum_{j=1}^Ke^R_m(\xi_{a_j})}$.
where $R_m(\xi_{a_i})$ denotes the reward assigned by the $m$-th expert
to trajectory $\xi_{a_i}$. Then, we randomly sample our probability
distribution to pick our top choice. From the remaining trajectories, we
noisily choose from our distribution to rank our second-best option. We
repeat this process until we have ranked all our trajectories. This
follows what is known as the Plackett-Luce Ranking Model.

Given knowledge of the true reward function weights $\omega_m$ and
mixing coefficients $\alpha_m$, we have the following joint mass over
observations x from a query Q:
$Pr(x\ |\ Q) = \sum_{m = 1}^M \alpha_m\prod_{i = 1}^K\frac{e^{\omega_m^T \Phi(\xi_{a_i})}}{\sum_{j = i}^K e^{\omega_m^T \Phi(\xi_{a_j})}}$.

With the above formulation of the joint mass distribution over
observation and queries, we can now formulate an objective.
Specifically, it is to present users with the best set of queries that
learn reward weights, $\omega$, and mixing coefficient, $\alpha$, based
upon user rankings of preferred query responses. By learning these
parameters, we can have an accurate estimation of the joint mass
distribution of the observations.

To learn these parameters, we use a Bayesian learning framework. The
goal will be to learn the reward weights, $\omega_m$, and all mixing
coefficients $\alpha_m$. Thus, define the parameters to be
$\theta = \{\omega, \alpha\}$. We start by simplifying the posterior
over the parameters.

$$\begin{aligned}
\Pr(\Theta | Q^{(1)}, x^{(1)}, Q^{(2)}, x^{(2)}, \ldots) & \propto \Pr(\Theta) \Pr(Q^{(1)} | x^{(1)}, Q^{(2)}, x^{(2)}, \ldots | \Theta) \\
& = \Pr(\Theta) \prod_t \Pr(x^{(t)} | Q^{(t)}, \Theta, Q^{(1)}, x^{(1)}, \ldots, Q^{(t-1)}, x^{(t-1)}) \\
& \propto \Pr(\Theta) \prod_t \Pr(x^{(t)} | \Theta, Q^{(t)})
\end{aligned}$$

Note that the first proportionality term is directly from Bayes rule
(removing normalization constant). The first equation comes directly
from the assumption that the queries at timestamp $t$ are conditionally
independent of the parameters given previous queries & rankings. This
assumption is reasonable because the previous queries & rankings ideally
give all the information to inform the choice of the next set of. The
last proportionality term comes from the assumption that the ranked
queries are conditionally independent given the parameters

The prior distribution is dependent on use case. For example, in the
user studies conducted by the authors to verify this method, they use a
standard Gaussian for the reward weights and the mixing coefficients to
be uniform on a $M - 1$ simplex to ensure that they add up to 1. Then we
can use maximum likelihood estimation to compute the parameters with the
simplified posterior.


<!--{{< include psets/pset1.qmd >}}-->

<!--
Through our exploration of human preference models, we will ground ourselves in
building a health coaching system that can provide meal recommendations aligned with a user's dietary needs and preferences. Examples of scenarios which can benefit from a model of how humans make choices include:

1.  **Health coaching:** Humans express their preferences every time
    they pick lunch for consumption. Humans may have several goals
    related to nutrition, such as weight loss and improving
    concentration. We can learn how a given individual or set of
    individuals prefer to eat to provide personalized recommendations to
    help them attain their goals. This chapter will use this use case to
    ground human preference modeling in a real-life application.

2.  **Social media:** Platforms have a far greater amount of content
    than one can consume in a lifetime, yet such products must aim to
    maximize user engagement. To accomplish this, we can learn what
    specific things people like to see in their feeds to optimize the
    value they gain out of their time on social media. For example, the
    video feed social media platform [TikTok](https://www.tiktok.com/)
    has had viral adoption due to its notorious ability to personalize a
    feed for its users based on their preferences.

3.  **Shopping:** Retail corporations largely aim to maximize revenue by
    making it easy for people to make purchases. Recommendation systems
    on online shopping platforms provide a mechanism for curating
    specific items based on an individual's previous purchases (or even
    browsing history) to make shoppers aware of items they may like and,
    therefore, purchase.

Two key models used in pairwise sampling are the Thurstonian and Bradley-Terry models [@cattelan2012]. The Thurstonian model assumes each item $i$ has a true score $u_i$ following a normal distribution. The difference $d_{ij} = u_i - u_j$ is also normally distributed. The probability that item $i$ is preferred over item $j$ is given by $P(i \succ j) = \Phi \left( \frac{u_i - u_j}{\sqrt{2\sigma^2}} \right)$, where $\Phi$ is the cumulative normal distribution function. The denominator $\sqrt{2\sigma^2}$ is the standard deviation of the difference $d_{ij} = u_i - u_j$ when $u_i$ and $u_j$ are normally distributed with variance $\sigma^2$[@cattelan2012]. The Bradley-Terry model defines the probability of preference based on latent scores $\beta_i$ and $\beta_j$. The probability that item $i$ is preferred over item $j$ is $P(i \succ j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}$. This model is used to estimate relative strengths or preferences based on latent scores. [@cattelan2012].

::: {#tbl-philosophy}
  -----------------------------------------------------------------------
  Application                         Human Preference
  ----------------------------------- -----------------------------------
  Computer vision: train a neural     This is how humans process images
  network to predict bounding boxes   by identifying the position and
  delineating all instances of dogs   geometry of the things we see in
  in an image                         them

  Natural language processing: train  Coherent text is itself a
  a model to generate coherent text   human-created and defined concept,
                                      and we prefer that any
                                      synthetically generated text
                                      matches that of humans

  Computer vision: train a diffusion  Humans prefer that images
  model to generate realistic images  accurately capture the world as
  of nature                           observed by humans, and this
                                      generative model should reflect the
                                      details that comprise that
                                      preference
  -----------------------------------------------------------------------

  : Examples of machine learning tasks and their interpretation as
  modeling human preferences.
:::
-->

<!--
Game theory provides a mathematical framework for analyzing strategic
interactions among rational agents. These models help in understanding
and predicting human behavior by considering multiple criteria and the
associated trade-offs. They enhance the understanding of preferences
across multiple criteria and allow for richer and more accurate feedback
through structured comparisons. Game-theory framings capture the
complexity of preferences and interactions in decision-making processes
[@bhatia2020preference].

The most popular form of preference elicitation involves pairwise
comparisons. Users are asked to choose between two options, such as
product A or product B. This method is used in various applications like
search engines, recommender systems, and interactive robotics. Key
concepts include the Von Neumann Winner and the Blackwell Winner. The
Von Neumann Winner refers to a distribution over objects that beats or
ties every other object in the collection under the expected utility
assumption. The Blackwell Winner generalizes the Von Neumann Winner for
multi-criteria problems using a target set for acceptable payoff vectors
[@bhatia2020preference].

Game-theory framings provide a framework for preference learning along
multiple criteria. These models use tools from vector-valued payoffs in
game theory, with Blackwell's approach being a key concept. This
approach allows for a more comprehensive understanding of preferences by
considering multiple criteria simultaneously [@bhatia2020preference].

In game-theory framings, pairwise preferences are modeled as random
variables. Comparisons between objects along different criteria are
captured in a preference tensor $P$. This tensor models the probability
that one object is preferred over another along a specific criterion,
allowing for a detailed understanding of preferences across multiple
dimensions [@bhatia2020preference].

The preference tensor $P$ captures object comparisons along different
criteria. It is defined as:
$$P(i_1, i_2; j) = P(i_1 \succ i_2 \text{ along criterion } j)$$ where
$P(i_2, i_1; j) = 1 - P(i_1, i_2; j)$. These values are aggregated to
form an overall preference matrix $P_{ov}$ [@bhatia2020preference].

The Blackwell Winner is defined using a target set $S$ of acceptable
score vectors. The goal is to find a distribution $\pi^*$ such that
$P(\pi^*, \pi) \in S$ for all $\pi$. This method minimizes the maximum
distance to the target set, providing a robust solution to
multi-criteria preference problems [@bhatia2020preference].

The optimization problem for finding the Blackwell Winner is defined as:
$$\pi(P, S, \|\cdot\|) = \arg \min_{\pi \in \Delta_d} \left[ \max_{\pi' \in \Delta_d} \rho(P(\pi, \pi'), S) \right]$$
where $\rho(u, v) = \|u - v\|$. This measures the distance to the target
set, ensuring that the selected distribution is as close as possible to
the ideal preference vector [@bhatia2020preference].
-->
