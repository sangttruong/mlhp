---
title: Aggregation of Preferences
format: html
filters:
  - pyodide
execute:
  engine: pyodide
  pyodide:
    auto: true
---

## Social Choice Theory {#sec-choices-aggregation}
In many applications, human preferences must be aggregated across multiple individuals to determine a collective decision or ranking. This process is central to social choice theory, which provides a mathematical foundation for preference aggregation. Unlike individual preference modeling, which focuses on understanding how a single person makes decisions, social choice theory addresses the challenge of combining multiple preference profiles into a single, coherent outcome. One of the most widely used approaches to aggregating preferences is voting. A voting rule is a function that maps a set of individual preference rankings to a collective decision. The outcome of a vote is determined by a social choice function (SCF), which selects a winner based on the aggregated preferences. Several voting rules exist, each with different properties:

- Plurality Rule: Each voter assigns one point to their top choice, and the alternative with the most points wins.
- Borda Count: Voters rank all alternatives, and points are assigned based on the position in each ranking.
- Single Transferable Vote (STV): Voters rank choices, and rounds of elimination occur until a candidate has a majority.
- Condorcet Methods: The Condorcet winner is the item that would win in all pairwise comparisons against other alternatives (if one exists).

However, preference aggregation is not always straightforward. The Condorcet Paradox illustrates that no single alternative may be a clear winner due to cycles in majority preferences, violating transitivity. Additionally, different voting rules can yield different winners, highlighting the importance of selecting an appropriate aggregation method. A fundamental result in social choice theory is Arrow’s Impossibility Theorem, which states that when there are three or more alternatives, no voting system can simultaneously satisfy the following fairness criteria:

1. Unanimity (Pareto efficiency): If all individuals prefer one item over another, the group ranking should reflect this.
2. Independence of Irrelevant Alternatives: The relative ranking of two items should not be influenced by another unrelated item.
3. Non-dictatorship: No single individual's preference should always determine the group's outcome.

Arrow’s theorem suggests that every fair aggregation method must compromise on at least one of these desirable properties. Additionally, the Gibbard-Satterthwaite Theorem proves that any deterministic voting rule that selects a single winner is either dictatorial (one person always determines the result) or manipulable (voters can strategically misrepresent their preferences to achieve a better outcome). While manipulation is theoretically possible, certain voting rules, such as STV, introduce computational complexity that makes strategic voting impractical in real-world scenarios.

Preference aggregation is also critical in RLHF, where human judgments guide model training. Aggregating human preferences in RLHF faces challenges similar to traditional voting, such as inconsistencies in preferences and strategic bias. Several approaches address these challenges. For example, Majority Voting simply aggregates by selecting the most preferred response. Weighted Voting adjusts vote weights based on expertise or trustworthiness. Jury Learning is a method that integrates dissenting opinions, ensuring that minority perspectives are not entirely disregarded. Lastly, Social Choice in AI Alignment incorporates diverse human feedback to align AI behavior with a broad spectrum of human values. These approaches highlight the interplay between human preference modeling and machine learning. Designing aggregation mechanisms that reflect collective human values is an ongoing research challenge. While traditional social choice methods focus on aggregation, recent work in pluralistic alignment suggests alternative frameworks that preserve the diversity of human preferences rather than collapsing them into a single decision. Pluralistic AI systems aim to:

1. Present a spectrum of reasonable responses instead of forcing a single choice.
2. Allow steering towards specific perspectives while maintaining fairness.
3. Ensure distributional pluralism, calibrating AI systems to diverse human viewpoints.

This perspective is particularly relevant in generative AI, where models trained on aggregated preferences may fail to capture the nuances of diverse human values. Aggregating human preferences is a complex task influenced by mathematical constraints and strategic considerations. Voting-based methods provide well-studied mechanisms for aggregation, but they face fundamental limitations, as Arrow’s and Gibbard-Satterthwaite’s theorems outlined. Beyond traditional aggregation, emerging approaches in RL and AI alignment seek to balance fairness, robustness, and pluralism. As machine learning systems increasingly interact with human preferences, designing aggregation frameworks that capture the richness of human decision-making remains an active and critical area of research.

## Auction Theory {#single-item-auctions}

The first problem within auction theory we will consider is the *single-item auction*. The premise of this problem is that there is a single item to sell, $n$ bidders (with unknown private valuations of the item $v_1$, \..., $v_n$). The bidder's individual objective is to maximize utility: the value $v_i$ of the item subtracted by the price paid for the item. The auction procedure is standard in the sense that bids are solicited and the highest bid will win the auction. While the objective of the individual bidder is clear, there could be a plethora of different objectives for the auction as a whole. One option could be to maximize social surplus, meaning the goal is to maximize the value of the winner. Another objective could be to maximize seller profit which is the payment of the winner. For simplicity, we can focus on the first objective where the goal is to maximize social surplus. If we want to maximize social surplus it turns out that a great way to do this is the "second-price auction".

In the second-price auction, we will operate under slightly different conditions. In the second-price auction we (1) solicit sealed bids, (2) have the winner be the highest bidder, and (3) charger winner the second-highest bid price. As an example, if the solicited bids are $b = (2, 6, 4, 1)$ the winner will be that who bid $6$, but will pay a price of $4$. From here, we can do some equilibrium analysis to try and learn what the optimal bidding strategy is for each bidder. Let the amount bidder $i$ bids to be $b_i$, so we have bids $b_1, b_2, ..., b_n$. How much should bidder $i$ bid? To analyze this, let us define $t_i = max_{j \neq i} b_j$ which represents the max of the bids that is not from bidder $i$. There are now two cases to consider: if $b_i$ \> $t_i$ and if $b_i$ \< $t_i$. In the first case the bidder $i$ wins, and if the bidder bid $b_i = v_i$, they are guaranteed to have a positive return on bid. In the other case, they lose the bid and the net loss is 0 because they don't have to pay. From this we can conclude that bidder $i$'s dominant strategy is to just bid $b_i = v_i$. Rigorously proving this is a little bit trickier, but it was shown from Vickrey in 1961 \[cite\] that truthful bidding is the dominant strategy in second-price auctions. A corollary of this is that we are maximizing social surplus since bids are values and the winner is the bidder with highest valuation.

If we want to look at things from the perspective of a seller trying to maximize their profit we need to treat the bidder's bids as uniform random variables. Consider the example scenario where we have two bidders each bidding uniformly between 0 and 1. What is the seller's expected profit? (in this case profit and revenue for the seller are the same because we assume the seller throws away the item if it doesn't sell/has no valuation for it). From there the question now becomes, can we get more expected profit from the seller's perspective? It turns out there is a design where we can add a reserve price of $r$ to the second-price auction. The way this works is we can (1) Insert seller-bid at $r$, (2) solicit bids, (3) pick the highest bidder, and 3) charge the 2nd-highest bid. In effect, this is just the second-price auction but with a bid from the seller as well, at a price of $r$. A lemma, that we won't prove here, is that the second-price auction with reserve price $r$ still has a dominant strategy of just being truthful. Let's now consider what the profit of a second-price auction would be with two bidders that uniformly bid between 0 and 1 -- but this time we have a reserve price of $1/2$. To calculate the expected profit we break down the situation into 3 cases:

-   Case 1: $1/2 > v_1 > v_2 \rightarrow 1/4 \text{ probability} \rightarrow  \mathbb{E}[\text{profit}] = 0$

-   Case 2: $v_1 > v_2 > 1/2 \rightarrow 1/4 \text{probability} \rightarrow \mathbb{E}[v_2 | \text{case 2}] = 2/3$

-   Case 3: $v_1 > 1/2 > v_2 \rightarrow 1/2 \text{ probability} \rightarrow 1/2$

Why is $E[v2 | case 2] = 2/3$? If $v_1$ and $v_2$ are greater than $1/2$, they are evenly spread across the interval, meaning the expectation will be 1/2 + 1/6 = 2/3. Adding up all these cases we get $E[profit] = 5/12$. It turns out that second-price auctions with reserve actually maximize profit in general (for symmetric bidders)! In the previous section we conclude that second-price auctions with reserve maximize profit for the seller. In order to prove this, we now move to the more general topic of asking how should a monopolist divide good across separate markets. We can make the assumption that the demand model is a concave revenue $R(q)$ in quantity $q$. Under this assumption, we can just divide supply into $q = q_a + q_b$ such that $R'_a(q_a) = R'_b(q_b)$. The idea from here is a theorem from Myerson in 1981 that states an optimal action maximizes "marginal revenue". Consider an example where we have two bidders bidding a uniform value between 0 and 1. Our revenue curve can now be derived from the offering price $V(q) = 1 - q$ like so: $R(q) = qV(q) = q - q^2$. Taking the derivative gives us the marginal revenue $R'(q) = 1-2q$. This means two things: 1) we want to sell to bidder $i$ with the highest $R'(q_i)$ and 2) we want to sell to bidder $i$ with value at least $1/2$ (if we want a positive $R'(q_i)$. But this is just a second-price auction with reserve $1/2$! This means that for symmetric bidders, a second price with reserve is the optimal auction.

An interesting topic to discuss is what benefits auctions bring to the table as opposed to just standard pricing. Online auctions used to be a lot more popular in the early 2000s and have been completely replaced by standard online pricing, even on sites like e-bay. While auctions are slower and have added inherent complexities, they are actually optimal on paper. Standard pricing on the other is non-optimal; although it is fast and simpler for buyers. There is actually a way to quantify this: for pricing $k$ units, the loss is at most $1 / \sqrt{2\pi k}$ of optimal profit. Let's consider applications in duopoly platform design. We know that the optimal auction is second-price with reserve, but what happens when we introduce competition between two auction platforms? Some important details related to the revenue of a second-price auction is that a second-price auction with no reserve and n bidders leads to larger revenue having an optimal reserve and n - 1 bidders [@bulow-klemperer1996]. Additionally, with an entry cost, no reserve is the optimal strategy for maximizing revenue  [@mcafee-87]. Let's consider an example of a competing auction system which is Google ads vs Bing ads. How should an advertiser divide the budget between Google and Bing? They should give the same budget to both companies. What happens if Bing raises their prices? Then, the advertising company moves more of its budget to Google from Bing.

The Bulow-Klemperer theorem demonstrates that increased competition can be more valuable than perfect knowledge of bidders' valuation distributions. This result provides insight into the potential of simple, prior-independent auctions to approach the performance of optimal auctions. The theorem states that for a single-item auction with bidders' valuations drawn independently from a regular distribution $F$. Let $F$ be a regular distribution and $n$ a positive integer. Then:
$$E_{v_1,\ldots,v_{n+1} \sim F}[\text{Rev(VA)}(n+1 \text{ bidders})] \geq E_{v_1,\ldots,v_n \sim F}[\text{Rev(OPT}_F)(n \text{ bidders})]$$  {#eq-eq3.64}
where VA denotes the Vickrey auction and $\text{OPT}_F$ denotes the
optimal auction for $F$. This shows that running a simple Vickrey auction with one extra bidder outperforms the revenue-optimal auction that requires precise knowledge of the distribution. It suggests that in practice, effort spent on recruiting additional bidders may be more fruitful than fine-tuning auction parameters.

The VCG mechanism is a cornerstone of mechanism design, providing a general solution for welfare maximization in multi-parameter environments. The key result is that in every general mechanism design environment, there is a dominant-strategy incentive-compatible (DSIC) welfare-maximizing mechanism. According to VCG, given bids $b_1, \ldots, b_n$, where each $b_i$ is indexed by the outcome set $\Omega$, the allocation rule is $x(b) = \arg \max_{\omega \in \Omega} \sum_{i=1}^n b_i(\omega)$. The payment rule for each agent $i$ is $p_i(b) = \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega) - \sum_{j \neq i} b_j(\omega^*)$ where $\omega^* = x(b)$ is the chosen outcome. The key insight is to charge each agent its "externality" - the welfare loss inflicted on other agents by its presence. This payment rule, coupled with the welfare-maximizing allocation rule, yields a DSIC mechanism. The VCG mechanism can be interpreted as having each agent pay its bid minus a "rebate" equal to the increase in welfare attributable to its presence:

$$p_i(b) = b_i(\omega^*) - \left[ \sum_{j=1}^n b_j(\omega^*) - \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega) \right]$$  {#eq-eq3.67}

While the VCG mechanism provides a theoretical solution for DSIC welfare-maximization in general environments, it can be challenging to implement in practice due to computational and communication complexities.

Combinatorial auctions are an important class of multi-parameter mechanism design problems, with applications ranging from spectrum auctions to airport slot allocation. In a combinatorial auction, there are $n$ bidders and a set $M$ of $m$ items. The outcome set $\Omega$ consists of allocations $(S_1, \ldots, S_n)$, where $S_i$ is the bundle allocated to bidder $i$. Each bidder $i$ has a private valuation $v_i(S)$ for each bundle $S \subseteq M$. While the VCG mechanism theoretically solves the welfare-maximization problem, combinatorial auctions face several major challenges in practice. First, each bidder has $2^m - 1$ private parameters, making direct revelation infeasible for even moderate numbers of items. This necessitates the use of indirect mechanisms that elicit information on a "need-to-know" basis. In addition, even when preference elicitation is not an issue, welfare-maximization can be an intractable problem. In practice, approximations are often used, hoping to achieve reasonably good welfare. The VCG mechanism can exhibit bad revenue and incentive properties in combinatorial settings. For example, adding bidders can sometimes decrease revenue to zero, and the mechanism can be vulnerable to collusion and false-name bids. Last but not least, strategic Behavior in Iterative Auctions: Most practical combinatorial auctions are iterative, comprising multiple rounds. This introduces new opportunities for strategic behavior, such as using bids to signal intentions to other bidders. These challenges make combinatorial auctions a rich and complex area of study, requiring careful design to balance theoretical guarantees with practical considerations.

Spectrum auctions represent a complex application of combinatorial auction theory. With n bidders and m non-identical items, each bidder has a private valuation for every possible bundle of items, making it impractical to directly elicit all preferences. This necessitates the use of indirect, iterative mechanisms that query bidders for valuation information on a "need-to-know" basis, sacrificing some of the desirable properties of direct mechanisms like dominant strategy incentive compatibility (DSIC) and full welfare maximization. The fundamental challenge in spectrum auctions lies in the nature of the items being sold. There is a dichotomy between items that are substitutes (where $v(AB) \leq v(A) + v(B))$ and those that are complements (where $v(AB) > v(A) + v(B))$. Substitute items, such as licenses for the same area with equal-sized frequency ranges, are generally easier to handle. When items are substitutes, welfare maximization is computationally tractable, and the VCG mechanism avoids many undesirable properties. However, complementary items, which arise naturally in spectrum auctions when bidders want adjacent licenses, present significant challenges. Early attempts at spectrum auctions revealed the pitfalls of naive approaches. Sequential auctions, where items are sold one after another, proved problematic as demonstrated by a Swiss auction in 2000. Bidders struggled to bid intelligently without knowing future prices, leading to unpredictable outcomes and potential revenue loss. Similarly, simultaneous sealed-bid auctions, as used in New Zealand in 1990, created difficulties for bidders in coordinating their bids across multiple items, resulting in severely suboptimal outcomes.

The Simultaneous Ascending Auction (SAA) emerged as a solution to these issues and has formed the basis of most spectrum auctions over the past two decades. In an SAA, multiple items are auctioned simultaneously in rounds, with bidders placing bids on any subset of items subject to an activity rule. This format facilitates price discovery, allowing bidders to adjust their strategies as they learn about others' valuations. It also allows bidders to determine valuations on a need-to-know basis, reducing the cognitive burden compared to direct-revelation auctions. Despite its advantages, the SAA is not without vulnerabilities. Demand reduction, where bidders strategically reduce their demand to lower prices, can lead to inefficient outcomes even when items are substitutes. The exposure problem arises with complementary items, where bidders risk winning only a subset of desired items at unfavorable prices. These issues highlight the ongoing challenges in designing effective spectrum auctions, balancing theoretical guarantees with practical considerations.

Case study: Classroom Peer Grading. This section discusses work by Jason Hartline, Yingkai Li, Liren Shan, and Yifan Wu at Northwestern University, where researchers examined mechanism design for the classroom, specifically in terms of the optimization of scoring rules. They explored peer grading in the classroom and how to construct a peer grading system that optimizes the objectives for each stakeholder in the system, including those being graded, the peer graders, the TAs of the class, and the professor. Firstly, let's think of the classroom like a computer. We can think of students as local optimizers; their incentive is to minimize the amount of work they need to do and maximize the grades that they receive. The graders are imprecise operators, which means that there is some uncertainty in their ability to grade the work completed by the students. The syllabus can be thought of as the rules that map the actions of the students to the grade they end up receiving in the class. Our overall goals for this classroom based on these definitions is to minimize work, maximize learning, and fairly assess the students for the work that they do [@jasonH2020]. One basic question that we can examine, is what is the best syllabus that maximizes our objectives for our classroom design. Some components of this could include grading randomized exams, grading with partial credit, group projects, and finally, peer grading, which is the component that we will be taking a deeper dive into. The general situation of the peer grading problem is that proper scoring rules make peer grades horrible [@jasonH2020]. So we want to be able to optimize scoring rules and make sure that we are optimizing each component of the peer grading pipeline.

The main algorithms focused on in this peer grading design paper were matching peers and TAs to submissions and the grading of those submissions from the TAs and the peer reviews  [@jasonH2020]. There are quite a number of advantages to peer grading including that peers are able to learn from reviewing other people's work, it reduces the work for the teacher, and improves the turnaround time for assignment feedback (which are all part of our overarching goals for our mechanism design for the classroom). But, it is also important to acknowledge the potential disadvantages of the peer grading system: it is possible that the peer graders present inaccurate grades and there is student unrest. This presents us with a challenge: being able to incentivize accurate peer reviews.

One problem that we run into, when we use the proper scoring rule to score peer reviews, if the peer graders use the lazy peer strategy, which means that they always report 80$\%$ for their peer reviews, they get graded very well using the proper scoring rule algorithm. In fact, the proper scoring rule says that their peer review is 96$\%$ accurate [@jasonH2023]. So how do we incentivize effort in reviews from peer graders? We use a scoring rule that maximizes the difference in score between effort or no effort reviews as indicated by the peer reviewers [@jasonH2023]. So overall, the analysis of datasets leads to decision optimizations and, eventually, payoff from those decisions.

In conclusion, scoring rules are essential in being able to understand and analyze data thoroughly, and optimal scoring rules for binary effort allow us to understand the setting independent of the dataset [@jasonH2023].

## Incentive-Compatible Online Learning

To address this problem, we seek to create a model. We first outline the key criteria that our model must achieve. The model revolves around repeated interactions between a planner (the system) and multiple agents (the users). Each agent, upon arrival in the system, is presented with a set of available options to choose from. These options could vary widely depending on the application of the model, such as routes in a transportation network, a selection of hotels in a travel booking system, or even entertainment choices in a streaming service. The *interaction process* is straightforward but crucial: agents arrive, select an action from the provided options, and then report feedback based on their experience. This feedback is vital as it forms the basis upon which the planner improves and evolves its recommendations. The agents in this model are considered strategic; they aim to maximize their reward based on the information available to them. This aspect of the model acknowledges the real-world scenario where users are typically self-interested and seek to optimize their own outcomes. The *planner*, on the other hand, has a broader objective. It aims to learn which alternatives are best in a given context and works to maximize the overall welfare of all agents. This involves a complex balancing act: the planner must accurately interpret feedback from a diverse set of agents, each with their own preferences and biases, and use this information to refine and improve the set of options available. The ultimate goal of the planner is to create a dynamic, responsive system that not only caters to the immediate needs of individual agents but also enhances the collective experience over time, leading to a continually improving recommendation ecosystem.

Here, we seek to address the inherent limitations faced by the planner, particularly in scenarios where monetary transfers are not an option, and the only tool at its disposal is the control over the flow of information between agents. This inquiry aims to understand the extent to which these limitations impact the planner's ability to effectively guide and influence agent behavior. A critical question is whether the planner can successfully induce exploration among agents, especially in the absence of financial incentives. This involves investigating strategies to encourage users to try less obvious or popular options, thus broadening the scope of feedback and enhancing the system's ability to learn and identify the best alternatives. Another question is understanding the rate at which the planner learns from agent interactions. This encompasses examining how different agent incentives, their willingness to explore, and their feedback impact the speed and efficiency with which the planner can identify optimal recommendations.

The model can be extended in several directions, each raising its own set of questions.
    
    1.  Multiple Agents with Interconnected Payoffs: When multiple agents arrive simultaneously, their choices and payoffs become interconnected, resembling a game. The research question here focuses on how these interdependencies affect individual and collective decision-making.

    2.  Planner with Arbitrary Objective Function: Investigating scenarios where the planner operates under an arbitrary objective function, which might not align with maximizing overall welfare or learning the best alternative.

    3.  Observed Heterogeneity Among Agents: This involves situations where differences among agents are observable and known, akin to contextual bandits in machine learning. The research question revolves around how these observable traits can be used to tailor recommendations more effectively.

    4.  Unobserved Heterogeneity Among Agents: This aspect delves into scenarios where differences among agents are not directly observable, necessitating the use of causal inference techniques to understand and cater to diverse user needs.

In our setup, there is a "planner," which aims to increase exploration, and many independent "agents," which will act selfishly (in a way that they believe will maximize their individual reward) [@mansour2019bayesianincentivecompatiblebanditexploration; @mansour2021bayesianexplorationincentivizingexploration]. Under our model shown in Figure [1.1](#fig-planner-agent){reference-type="ref" reference="fig-planner-agent"}, there are $K$ possible actions that all users can take, and each action has some mean reward $\mu_i \in [0, 1]$. In addition, there is a common prior belief on each $\mu_i$ across all users.. The $T$ agents, or users, will arrive sequentially. As the $t$'th user arrives, they are recommended an action $I_t$ by the planner, which they are free to follow or not follow. After taking whichever action they choose, the user experiences some realized reward $r_i \in [0, 1]$, which is stochastic i.i.d. with mean $\mu_i$, and reports this reward back to the planner.

So far, the model we have defined is equivalent to a multi-armed bandit model, which we have seen earlier in this chapter ([1](#4optim){reference-type="ref" reference="4optim"}). Under this model, well-known results in economics, operations research and computer science show that $O(\sqrt{T})$ regret is achievable [@russo2015informationtheoreticanalysisthompsonsampling; @auer_cesa-bianchi_fischer_2002; @LAI19854] with algorithms such as Thompson sampling and UCB. However, our agents are strategic and aim to maximize their own rewards. If they observe the rewards gained from actions taken by other previous users, they will simply take the action they believe will yield the highest reward given the previous actions; they would prefer to benefit from exploration done by other users rather than take the risk of exploring themselves. Therefore, exploration on an individual level, which the planner would like to facilitate, is not guaranteed under this paradigm.

In light of this, we also require that our model satisfy **incentive compatibility**, or that taking the action recommended by the planner has an expected utility that is as high as any other action the agent could take. Formally, $\forall i : \, E[\mu_i | I_t = i] \geq E[\mu_{i'} | I_t = i].$ Note that this incentivizes the agents to actually take the actions recommended by the planner; if incentive compatibility is not satisfied, agents would simply ignore the planner and take whatever action they think will lead to the highest reward.

At a high level, the key to achieving incentive compatibility while still creating a policy for the planner that facilitates exploration is information asymmetry. Under this paradigm, the users only have access to their previous recommendations, actions, and rewards, and not to the recommendations, actions, and rewards of other users. Therefore, they are unsure of whether, after other users take certain actions and receive certain rewards, arms that they might have initially considered worse in practice outperform arms that they initially considered better. Only the planner has access to the previous actions and rewards of all users; the user only has access to their own recommendations and overall knowledge of the planner's policy. The main question we aim to answer for the rest of this section is, given this new constraint of incentive compatibility, is $O(\sqrt{T})$ regret still achievable? We illustrate such an algorithm in the following.

The main result here is a **black-box reduction** algorithm to turn any bandit algorithm into an *incentive compatible* one, with only a constant increase in Bayesian regret. Since, as mentioned earlier, there are bandit algorithms with $O(\sqrt{T})$ Bayesian regret, black-box reduction will also allow us to get incentive-compatible algorithms with $O(\sqrt{T})$ regret. The idea of black-box reduction will be to simulate $T$ steps of any bandit algorithm in an incentive-compatible way in $c T$ steps. This allows us to design incentive-compatible recommendation systems by using any bandit algorithm and then adapting it. Consider the following setting: there are two possible actions, $A_1$ and $A_2$. Assume the setting of **deterministic rewards**, where action 1 has reward $\mu_1$ with prior $U[1/3, 1]$ and mean $\mathbb{E}[\mu_1] = 2/3$, and action 2 has reward $\mu_2$ with prior $U[0, 1]$ and mean $\mathbb{E}[\mu_2] = 1/2$. Without the planner intervention and with full observability, users would simply always pick $A_1$, so how can the planner *incentivize* users to play $A_2$?

The key insight is going to be to *hide exploration in a pool of exploitation*. The users are only going to receive a recommendation from the planner, and no other observations. After deterministically recommending the action with the highest expected reward ($A_1$), the planner will pick one **guinea pig** to recommend the exploratory action of $A_2$. The users don't know whether they are the guinea pig, so intuitively, as long as the planner picks guinea pigs uniformly at random and at low enough frequencies, the optimal decision for the users is still to follow the planner's recommendation, even if it might go against their interest. The planner will pick the user who will be recommended the exploratory action uniformly at random from the $L$ users that come after the first one (which deterministically gets recommended the exploitation action). Under this setting (illustrated in Figure [1.2](#fig-deterministic-guinea-pig){reference-type="ref" reference="fig-deterministic-guinea-pig"}), it is optimal for users to always follow the option that is recommended for them. More formally, if $I_t$ is the recommendation that a user receives at time $t$, then we have that:

$$
\begin{split}
    \mathbb{E}[\mu_1 - \mu_2 | I_t = 2] Pr[I_t = 2] &= \frac{1}{L} (\mu_1 - \mu_2) \quad \text{(Gains if you are the unlucky guinea pig)}\\
    &+ (1 - \frac{1}{L}) \mathbb{E}[\mu_1 - \mu_2 | \mu_1 < \mu_2] \times p[\mu_1 < \mu_2] \quad \text{(Loss if you are not and $\mu_1 < \mu_2$)}\\
    &\leq 0
\end{split}
$$

This holds when $L \geq 12$. It means that the gains from not taking the recommended action are *negative*, which implies that users should always take the recommendation. So far we have considered the case where rewards are deterministic, but what about **stochastic rewards**? We are now going to consider the case where rewards are independent and identically distributed from some distribution, and where each action $A_i$ has some reward distribution $r_i^t \sim D_i, \mathbb{E}[r_i^t] = \mu_i$. Back to the case where there are only two actions, we are going to adapt the prior algorithm of guinea pig-picking to the stochastic reward setting. Since one reward observation is not enough to fully know $\mu_1$ anymore, we'll instead observe the outcome of the first action $M$ times to form a strong posterior $\mathbb{E}[\mu_1 | r_1^1, \ldots r_1^M]$. We can use with stochastic rewards when there are two actions. Similarly, as before, we pick one guinea pig uniformly at random from the next $L$ users and use the reward we get as the exploratory signal.\ In a very similar manner, we can generalize this algorithm from always having two actions to the general multi-armed bandit problem. Now suppose we have a general multi-armed bandit algorithm $A$. We will wrap this algorithm around our black box reduction algorithm to make it incentive-compatible. We wrap every decision that $A$ would make by exactly $L-1$ recommendations of the action believed to be the best so far. This guarantees that the expected rewards for the users that are not chosen as guinea pigs are at least as good as $A$'s reward at phase $n$.

## Mutual Information Paradigm {#mutual-information-paradigm}
In this section we discuss an influential new framework for designing peer prediction mechanisms, the Mutual Information Paradigm (MIP) introduced by Kong and Schoenebeck [@kongschoenebeck2019]. Traditional peer prediction approaches typically rely on scoring rules and correlation between agents' signals. However, these methods often struggle with issues like uninformed equilibria, where agents can coordinate on uninformative strategies that yield higher payoffs than truth-telling. The core idea is to reward agents based on the mutual information between their report and the reports of other agents. We consider a setting with $n$ agents, each possessing a private signal $\Psi_i$ drawn from some set $\Sigma$. The mechanism asks each agent to report their signal, which we denote as $\hat{\Psi}_i$. For each agent $i$, the mechanism randomly selects a reference agent $j \neq i$. Agent $i$'s payment is then calculated as $MI(\hat{\Psi}_i; \hat{\Psi}_j)$ where $MI$ is an information-monotone mutual information measure. An information-monotone $MI$ measure must satisfy the following properties:

-   **Symmetry**: $MI(X; Y) = MI(Y; X)$.

-   **Non-negativity**: $MI(X; Y) \geq 0$, with equality if and only if $X$ and $Y$ are independent.

-   **Data processing inequality**: For any transition probability $M$, if $Y$ is independent of $M(X)$ conditioned on $X$, then $MI(M(X); Y) \leq MI(X; Y)$.

Two important families of mutual information measures that satisfy these properties are $f$-mutual information and Bregman mutual information. The $f$-mutual information is defined as $MI_f(X; Y) = D_f(U_{X,Y}, V_{X,Y})$, where $D_f$ is an $f$-divergence, $U_{X,Y}$ is the joint distribution of $X$ and $Y$, and $V_{X,Y}$ is the product of their marginal distributions. The Bregman mutual information is defined as: $BMI_{PS}(X; Y) = \mathbb{E}_{X} [D{PS}(U_{Y|X}, U_Y)]$, where $D_{PS}$ is a Bregman divergence based on a proper scoring rule $PS$, $U_{Y|X}$ is the conditional distribution of $Y$ given $X$, and $U_Y$ is the marginal distribution of $Y$. The MIP framework can be applied in both single-question and multi-question settings. In the multi-question setting, the mechanism can estimate the mutual information empirically from multiple questions. In the single-question setting, additional techniques like asking for predictions about other agents' reports are used to estimate the mutual information. A key theoretical result of the MIP framework is that when the chosen mutual information measure is strictly information-monotone with respect to agents' priors, the resulting mechanism is both dominantly truthful and strongly truthful. This means that truth-telling is a dominant strategy for each agent and that the truth-telling equilibrium yields strictly higher payoffs than any other non-permutation strategy profile. As research continues to address practical implementation challenges of designing truthful mechanisms, MIP-based approaches have significant potential to improve preference elicitation and aggregation in real-world applications lacking verifiable ground truth.
