---
title: Reward Model
format: html
filters:
  - pyodide
execute:
  engine: pyodide
  pyodide:
    auto: true

---

Designing a good *utility function* (or *reward function*) by hand for a complex AI or robotics task is notoriously difficult and error-prone ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=Today%2C%20in%20practice%2C%20reward%20functions,a%20simple%20autonomous%20driving%20task)) ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=The%20approach%20of%20hand,to%20need%20a%20better%20approach)). Instead of manually specifying what is “good” behavior, we can **learn a utility function from human preferences**. In this chapter, we explore how an agent can infer a human’s underlying utility function (their preferences or reward criteria) from various forms of feedback. We discuss both *supervised learning* and *Bayesian* approaches to utility learning, and examine techniques motivated by robotics—learning from demonstrations, physical corrections, trajectory evaluations, and pairwise comparisons. Throughout, we include mathematical formulations and code examples to illustrate the learning process.

## Estimating Utility Functions from Human Feedback

At the heart of learning from human preferences lies a **latent utility function** — a function that assigns numerical value to states, trajectories, or outcomes according to a human’s (possibly unspoken) preferences. The goal of a learning algorithm is to infer this function from observed feedback, which may come in the form of demonstrations, ratings, rankings, or pairwise comparisons. But how exactly do we represent and update our belief about this hidden utility function?

Two major paradigms in statistical learning provide different answers:

### Point Estimation: Fitting a Single Utility Function

In **point estimation**, we seek a single "best guess" for the utility function — typically a function $u_\theta(x)$ from a parameterized family (e.g. linear models, neural nets), with parameters $\theta \in \mathbb{R}^d$. Given data $\mathcal{D}$ from human feedback (e.g. preferences), we choose the parameter $\hat{\theta}$ that best explains the observed behavior. Formally:

$$
\hat{\theta} = \arg\max_\theta \; p(\mathcal{D} \mid \theta)
$$

This is **maximum likelihood estimation (MLE)**: we pick the parameters that make the observed data most probable under our model. Once $\hat{\theta}$ is selected, we treat $u_{\hat{\theta}}$ as the agent’s utility function, and optimize or sample behavior accordingly.

This approach is straightforward and computationally efficient. It is the foundation of most supervised learning methods (like logistic regression or deep learning), and it provides a natural interpretation: we’re directly finding the utility function that agrees with the human feedback. However, it discards **uncertainty**: it assumes the data is sufficient to pin down a single utility function, which may not be true in practice.

### Posterior Estimation: Maintaining a Distribution over Utility Functions

In contrast, **posterior estimation** takes a fully Bayesian view. Instead of committing to one estimate, we maintain a **distribution over utility functions**. That is, we place a prior $p(\theta)$ over parameters (or over functions $u$ more generally), and update this to a posterior after observing data $\mathcal{D}$:

$$
p(\theta \mid \mathcal{D}) \;=\; \frac{p(\mathcal{D} \mid \theta)\, p(\theta)}{p(\mathcal{D})}
$$

This posterior expresses our **uncertainty** over which utility functions are compatible with the human feedback. From this distribution, we can make predictions (e.g., using the posterior mean utility), quantify confidence, or even actively select new queries to reduce uncertainty (active learning).

For instance, if we model utilities with a Gaussian Process (GP), then the posterior over $u(x)$ is also a GP after observing comparisons or evaluations. If we use a neural network for $u_\theta(x)$, we can approximate the posterior with ensembles, variational inference, or MCMC.

Posterior estimation is especially valuable when human feedback is sparse, noisy, or ambiguous — as is often the case in real-world preference learning. It allows the agent to reason about *what it doesn’t know* and to take cautious or exploratory actions accordingly.


The next two sections instantiate these two perspectives. In Section 4.1, we explore **point estimation** via supervised learning — treating preference data as labeled examples and fitting a utility model. In Section 4.2, we shift to **posterior estimation** with Bayesian methods like Gaussian processes and Bayesian neural networks, which model both our current estimate and the uncertainty around it.

## Supervised Learning of Utility from Preferences

Supervised learning approaches treat human feedback as labeled data to directly fit a utility function. The core idea is to assume there exists a true utility function $u^*(x)$ (over states, outcomes, or trajectories $x$) that explains a human’s choices. We then choose a parameterized model $u_\theta(x)$ and adjust $\theta$ so that $u_\theta$ agrees with the human-provided preferences.

### Learning from Pairwise Preference Comparisons

A common feedback format is **pairwise comparisons**: the human is shown two options (outcomes or trajectories) $A$ and $B$ and indicates which is preferred. We can model the probability that the human prefers $A$ over $B$ using a logistic or Bradley–Terry model ([What is Direct Preference Optimization (DPO)?](https://www.analyticsvidhya.com/blog/2024/01/dpo-andrew-ngs-perspective-on-the-next-big-thing-in-ai/#:~:text=,function%20of%20the%20policy%20directly)):

$$
P(A \succ B \mid u_\theta) \;=\; \sigma\!\Big(u_\theta(A) - u_\theta(B)\Big)\,,
$$

where $\sigma(z)=\frac{1}{1+e^{-z}}$ is the sigmoid function. This implies the human is more likely to prefer $A$ if $u_\theta(A)$ is much larger than $u_\theta(B)$. Given a dataset of comparisons $D=\{(A_i, B_i, y_i)\}$ (with $y_i=1$ if $A_i$ was preferred and $0$ if $B_i$ was preferred), we can fit $\theta$ by maximizing the likelihood of the human’s choices. Equivalently, we minimize a binary cross-entropy loss:

$$
\mathcal{L}(\theta) = -\sum_{i} \Big[\,y_i \log \sigma(u_\theta(A_i)\!-\!u_\theta(B_i)) + (1-y_i)\log(1-\sigma(u_\theta(A_i)\!-\!u_\theta(B_i)))\Big]\,,
$$

often with a regularization term to prevent overfitting. This is a straightforward supervised learning problem – essentially logistic regression – on pairwise difference features.

**Example:** Suppose a human’s utility for an outcome can be described by a quadratic function (unknown to the learning algorithm). We collect some pairwise preferences and then train a utility model $u_\theta(x)$ to predict those preferences. The code below simulates this scenario:

```{python}
import numpy as np

# True utility function (unknown to learner), e.g. u*(x) = -(x-5)^2 + constant 
def true_utility(x):
    return -(x-5)**2  # (peak at x=5)

# Generate synthetic pairwise preference data
np.random.seed(42)
n_pairs = 20
X1 = np.random.uniform(0, 10, size=n_pairs)  # 20 random x-values
X2 = np.random.uniform(0, 10, size=n_pairs)  # 20 more random x-values
# Determine preferences according to true utility
prefs = (true_utility(X1) > true_utility(X2)).astype(int)  # 1 if X1 preferred, else 0

# Parametric model for utility: u_theta(x) = w0 + w1*x + w2*x^2  (quadratic form)
# Initialize weights
w = np.zeros(3)
lr = 0.01       # learning rate
reg = 1e-3      # L2 regularization strength
for epoch in range(1000):
    # Compute predictions via logistic model
    util_diff = (w[0] + w[1]*X1 + w[2]*X1**2) - (w[0] + w[1]*X2 + w[2]*X2**2)
    pred = 1 / (1 + np.exp(-util_diff))      # σ(w·(phi(X1)-phi(X2)))
    # Gradient of cross-entropy loss
    grad = np.array([0.0, 0.0, 0.0])
    error = pred - prefs  # (sigma - y)
    # Features for X1 and X2
    phi1 = np.vstack([np.ones(n_pairs), X1, X1**2]).T
    phi2 = np.vstack([np.ones(n_pairs), X2, X2**2]).T
    phi_diff = phi1 - phi2
    # Gradient: derivative of loss w.rt w = (sigma - y)*φ_diff (averaged) + reg
    grad = phi_diff.T.dot(error) / n_pairs + reg * w
    # Update weights
    w -= lr * grad

print("Learned weights:", w)
```

After training, we can compare the learned utility function $u_\theta(x)$ to the true utility $u^*(x)$. Below we plot the two functions:

```{python}
import matplotlib.pyplot as plt

# Plot true vs learned utility curves
xs = np.linspace(0, 10, 200)
true_vals = true_utility(xs)
learned_vals = w[0] + w[1]*xs + w[2]*xs**2

plt.figure(figsize=(6,4))
plt.plot(xs, true_vals, label="True Utility", linewidth=3)
plt.plot(xs, learned_vals, label="Learned Utility", linestyle="--", linewidth=3)
plt.xlabel("State x")
plt.ylabel("Utility")
plt.title("True vs. Learned Utility Function")
plt.legend()
plt.show()
```

 ([image]()) *Comparison of the true utility function (solid blue) and the learned utility function (dashed orange) inferred from pairwise preferences. The learned curve closely matches the true utility up to an arbitrary scaling factor (utility is only defined up to affine transform when inferred from comparisons).* 

In this example, the algorithm successfully recovered a utility function that orders states almost the same as the true utility $u^*(x)$. In general, learning from comparisons can infer the *relative* utility of options (which item is preferred), although the absolute scale of $u_\theta$ is unidentifiable without further assumptions. Supervised learning on preferences has been widely used for ranking problems and preference-based reward learning ([What is Direct Preference Optimization (DPO)?](https://www.analyticsvidhya.com/blog/2024/01/dpo-andrew-ngs-perspective-on-the-next-big-thing-in-ai/#:~:text=,function%20of%20the%20policy%20directly)).

### Direct Preference Optimization (DPO)

While the above approach learns a utility and could then use it for decision-making, an alternative in some domains is to **directly optimize the policy based on preferences**, bypassing an explicit reward model. **Direct Preference Optimization (DPO)** is a recent technique introduced for aligning language models to human preferences ([[2305.18290] Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290#:~:text=estimated%20reward%20without%20drifting%20too,tuning)) ([What is Direct Preference Optimization (DPO)?](https://www.analyticsvidhya.com/blog/2024/01/dpo-andrew-ngs-perspective-on-the-next-big-thing-in-ai/#:~:text=,function%20of%20the%20policy%20directly)), but it illustrates a general principle applicable to utility learning. The insight of DPO is that we can fold the reward model into the policy itself, and then update the policy via a simple supervised objective on preference data ([What is Direct Preference Optimization (DPO)?](https://www.analyticsvidhya.com/blog/2024/01/dpo-andrew-ngs-perspective-on-the-next-big-thing-in-ai/#:~:text=,or%20performing%20significant%20hyperparameter%20tuning)).

In the context of DPO for language models, one has a base policy $\pi_{\text{ref}}$ (e.g. the pre-trained model) and a new policy $\pi_\theta$ we want to fine-tune. Given a human preference between two model outputs (responses) $y_+$ (preferred) vs $y_-$ for the same prompt $x$, DPO adjusts $\pi_\theta$ to increase the log-probability of $y_+$ and decrease that of $y_-$. Formally, the *DPO loss* for a single preference is:

$$
\mathcal{L}_{\text{DPO}}(\theta) \;=\; -\log \sigma\!\Big(\underbrace{\log \pi_\theta(y_+|x) - \log \pi_\theta(y_-|x)}_{\text{policy preference score}}\Big)\,.
$$

Intuitively, this is the same logistic loss as before, now directly in terms of the policy’s log-likelihood scores for each option. The overall objective is the average of this loss over a dataset of human preference comparisons. Optimizing this loss via gradient descent on $\pi_\theta$ effectively *bakes in* a reward model: $\log \pi_\theta(y|x) - \log \pi_{\text{ref}}(y|x)$ plays the role of a learned reward ([[2305.18290] Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290#:~:text=estimated%20reward%20without%20drifting%20too,tuning)). In fact, one can show that DPO is implicitly fitting a reward function that $\pi_\theta$ is maximizing ([What is Direct Preference Optimization (DPO)?](https://www.analyticsvidhya.com/blog/2024/01/dpo-andrew-ngs-perspective-on-the-next-big-thing-in-ai/#:~:text=,function%20of%20the%20policy%20directly)). Crucially, DPO does *not* require a separate reinforcement learning loop; it treats preference data as direct supervision for the policy, using a simple binary classification objective ([What is Direct Preference Optimization (DPO)?](https://www.analyticsvidhya.com/blog/2024/01/dpo-andrew-ngs-perspective-on-the-next-big-thing-in-ai/#:~:text=,process%20is%20more%20computationally%20intensive)).

**Why DPO?** In practice, DPO has been shown to achieve similar or better alignment performance compared to reinforcement learning from human feedback (RLHF) while being more stable and easier to implement ([What is Direct Preference Optimization (DPO)?](https://www.analyticsvidhya.com/blog/2024/01/dpo-andrew-ngs-perspective-on-the-next-big-thing-in-ai/#:~:text=DPO%20is%20shown%20to%20fine,compared%20to%20traditional%20RLHF%20methods)) ([What is Direct Preference Optimization (DPO)?](https://www.analyticsvidhya.com/blog/2024/01/dpo-andrew-ngs-perspective-on-the-next-big-thing-in-ai/#:~:text=,process%20is%20more%20computationally%20intensive)). It avoids the need to sample from the model during training or tune delicate hyperparameters of RL. Conceptually, DPO demonstrates that if we structure our utility model cleverly (here, as the log-ratio of policy and reference), we can extract an optimal policy in closed-form and learn utilities via supervised learning.

**Mathematical connection:** DPO’s formulation is related to the Bradley–Terry model for preferences and to the idea of *maximum entropy* policies. If we imagine a reward function $R_\theta(y|x) = \log \pi_\theta(y|x) - \log \pi_{\text{ref}}(y|x)$, DPO’s loss is pushing $R_\theta(y_+) > R_\theta(y_-)$ for preferred pairs. The optimal $\pi_\theta$ under this criterion turns out to be $\pi_\theta(y|x) \propto \pi_{\text{ref}}(y|x)\exp\{R_\theta(y|x)\}$ ([[2305.18290] Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290#:~:text=estimated%20reward%20without%20drifting%20too,tuning)). In other words, $\pi_\theta$ is proportional to the reference policy re-weighted by an exponentiated reward – a result consistent with the principle of maximum entropy (softmax) optimization. DPO thus blurs the line between learning a utility function and learning a policy; it directly outputs a policy that (implicitly) encodes the learned utility. 

### Other Supervised Approaches and Variants

The pairwise logistic approach can be extended to other feedback types. If humans provide numeric *ratings* or *scores* for options, one can treat utility learning as a regression problem: fit $u_\theta(x)$ to predict those scores (perhaps with a suitable bounded output or ordinal regression if scores are ordinal). If humans rank multiple options at once, algorithms like *RankNet* or *RankSVM* generalize the pairwise approach to listwise ranking losses. All these methods boil down to defining a loss that penalizes disagreements between the predicted utility order and the human-provided preferences, then optimizing $\theta$ to minimize that loss.

Supervised learning of utility is powerful due to its simplicity, but it typically provides point estimates of $u_\theta$. Next, we consider Bayesian approaches that maintain uncertainty over the utility function.

## Bayesian Methods for Utility Learning

When feedback data is sparse, as is common in preference learning, it can be advantageous to model uncertainty over the utility function. **Bayesian approaches** place a prior on the utility function and update a posterior as human feedback is observed. This yields not only a best-guess utility function but also a measure of confidence or uncertainty, which is valuable for active learning (deciding which queries to ask next) and for safety (knowing when the learned reward might be wrong).

### Utility Learning with Gaussian Processes

A popular Bayesian approach assumes that the human’s utility function can be modeled as a **Gaussian Process (GP)** – a distribution over functions. A GP prior is defined by a mean function (often taken to be 0 for convenience) and a kernel (covariance function) $k(x,x')$ which encodes assumptions about the smoothness or structure of the utility function. For example, one might assume $u(x)$ is a smooth function of state, and choose a radial basis function (RBF) kernel $k(x,x') = \sigma_f^2 \exp(-\|x-x'\|^2/(2\ell^2))$ with some length-scale $\ell$.

After observing some preference data, Bayes’ rule gives a posterior over the function $u(x)$. In the case of **pairwise comparisons**, the likelihood of a comparison $(A \succ B)$ given an underlying utility function $u$ can be modeled via the same logistic function: $P(A \succ B \mid u) = \sigma(u(A)-u(B))$. Combining this likelihood with the GP prior is analytically intractable (due to the non-Gaussian logistic likelihood), but one can use approximation techniques (Laplace approximation or MCMC) to obtain a posterior GP ([A tutorial on learning from preferences and choices with Gaussian ...](https://arxiv.org/html/2403.11782v1#:~:text=A%20tutorial%20on%20learning%20from,2005%29%20and%20by)). The result is a *Gaussian process preference model* that can predict the utility of any new option with an uncertainty interval.

If we have direct evaluations of utility (e.g., the human provides a numeric reward for some states), the GP inference is simpler – it reduces to standard GP regression. However, in many real-world scenarios, humans are better at making relative judgments than assigning absolute utility values. This change in feedback type transforms the inference problem fundamentally. Instead of having a Gaussian likelihood (as in standard GP regression), we now have a **non-Gaussian likelihood**, typically modeled using a **probit** or **logistic** function. The observed data no longer provide direct samples of the latent utility function, but instead impose constraints on the *relative* ordering of latent values.

Due to this non-Gaussian likelihood, **exact Bayesian inference is no longer tractable**: the posterior over the latent utility function given the pairwise data does not have a closed-form expression. The GP prior is still Gaussian, but the posterior becomes **non-Gaussian and multi-modal**, particularly as the number of comparisons grows.

To address this, we must turn to **approximate inference methods**. One common and computationally efficient choice is the **Laplace approximation**, which approximates the true posterior with a Gaussian centered at the **maximum a posteriori (MAP)** estimate. This involves:
1. Finding the mode of the posterior (i.e., the most probable utility values given the data),
2. Approximating the curvature of the log-posterior around this mode using the Hessian (second derivative),
3. Using this local curvature to construct a Gaussian approximation.

While not exact, this method works well in practice, especially when the posterior is unimodal and the number of comparison pairs is moderate. Other alternatives such as variational inference or sampling-based methods (e.g., Hamiltonian Monte Carlo) can yield more accurate results but often require more complex implementation and computational resources.

```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
from scipy.optimize import minimize

# --- True latent utility function ---
def true_u(x):
    return np.sin(x) + 0.1 * x

# --- RBF Kernel function ---
def rbf_kernel(x1, x2, length_scale=0.8, sigma_f=1.0):
    x1, x2 = np.atleast_2d(x1).T, np.atleast_2d(x2).T
    sqdist = (x1 - x2.T) ** 2
    return sigma_f**2 * np.exp(-0.5 * sqdist / length_scale**2)

# --- Generate synthetic preference data ---
np.random.seed(42)
num_pairs = 10
X_candidates = np.linspace(0, 10, 100)
true_utilities = true_u(X_candidates)

# Sample preference pairs
idx_pairs = np.random.choice(len(X_candidates), size=(num_pairs, 2), replace=True)
X_pref_pairs = []
for i, j in idx_pairs:
    xi, xj = X_candidates[i], X_candidates[j]
    if true_utilities[i] > true_utilities[j]:
        X_pref_pairs.append((xi, xj))
    else:
        X_pref_pairs.append((xj, xi))
X_pref_pairs = np.array(X_pref_pairs)

# --- Unique x values and indexing ---
X_all = np.unique(X_pref_pairs.flatten())
n = len(X_all)
x_to_idx = {x: i for i, x in enumerate(X_all)}

# --- GP prior kernel matrix ---
length_scale = 0.8
sigma_f = 1.0
sigma_noise = 1e-6
K = rbf_kernel(X_all, X_all, length_scale, sigma_f) + sigma_noise * np.eye(n)

# --- Negative log-posterior function ---
def neg_log_posterior(f):
    prior_term = 0.5 * f.T @ np.linalg.solve(K, f)
    lik_term = 0.0
    for xi, xj in X_pref_pairs:
        fi, fj = f[x_to_idx[xi]], f[x_to_idx[xj]]
        delta = (fi - fj) / np.sqrt(2)
        lik_term -= np.log(norm.cdf(delta) + 1e-6)
    return prior_term + lik_term

# --- MAP estimation of latent utilities ---
f_init = np.zeros(n)
res = minimize(neg_log_posterior, f_init, method="L-BFGS-B")
f_map = res.x

# --- Laplace approximation: compute W (Hessian of neg log likelihood) ---
W = np.zeros((n, n))
for xi, xj in X_pref_pairs:
    i, j = x_to_idx[xi], x_to_idx[xj]
    fi, fj = f_map[i], f_map[j]
    delta = (fi - fj) / np.sqrt(2)
    phi = norm.pdf(delta)
    Phi = norm.cdf(delta) + 1e-6
    w = (phi / Phi)**2 + delta * phi / Phi
    w /= 2  # adjust for sqrt(2)
    W[i, i] += w
    W[j, j] += w
    W[i, j] -= w
    W[j, i] -= w

# --- Posterior covariance approximation ---
L = np.linalg.cholesky(K)
K_inv = np.linalg.solve(L.T, np.linalg.solve(L, np.eye(n)))
H = K_inv + W
H_inv = np.linalg.inv(H)

# --- Prediction at test points ---
X_test = np.linspace(0, 10, 200)
K_s = rbf_kernel(X_all, X_test, length_scale, sigma_f)
K_ss_diag = np.diag(rbf_kernel(X_test, X_test, length_scale, sigma_f))

# Posterior mean and variance
posterior_mean = K_s.T @ K_inv @ f_map
temp = np.linalg.solve(H, K_s)
posterior_var = K_ss_diag - np.sum(K_s * temp, axis=0)
posterior_std = np.sqrt(np.maximum(posterior_var, 0))

# --- Visualization ---
plt.figure(figsize=(8, 4))
plt.plot(X_test, true_u(X_test), 'k--', label="True utility")
plt.plot(X_test, posterior_mean, 'b-', label="Posterior mean")
plt.fill_between(X_test,
                 posterior_mean - 1.96 * posterior_std,
                 posterior_mean + 1.96 * posterior_std,
                 color='blue', alpha=0.2, label="95% CI")
plt.scatter(X_all, [true_u(x) for x in X_all], c='red', marker='x', label="Observed x")
plt.title("GP Preference Learning (Laplace Approximation, 100 Pairs)")
plt.xlabel("x")
plt.ylabel("Utility")
plt.legend()
plt.tight_layout()
plt.show()
```

 ([image]()) *Gaussian Process posterior for a utility function (blue mean with 95% confidence band) after observing 5 points of noisy utility data (red ×). The true utility function (black dashed) is non-trivial. The GP correctly captures the function’s value around observed regions and expresses high uncertainty in the unobserved middle region. In practice, this uncertainty could guide an algorithm to query more feedback in the region $x\approx [4,7]$ to reduce ambiguity.* 

Gaussian processes are a flexible way to learn utility functions. They naturally handle irregular data and provide principled uncertainty estimates. GP-based preference learning has been applied to tasks like interactive Bayesian optimization, where an algorithm seeks to find the maximum of $u(x)$ by iteratively querying a human which of two options is better ([Learning from human preferences | OpenAI](https://openai.com/index/learning-from-human-preferences/#:~:text=Our%20AI%20agent%20starts%20by,refines%20its%20understanding%20of%20the%C2%A0goal)). The GP’s uncertainty can be used to select comparison queries that are maximally informative (e.g. focusing on regions where the model is most unsure which option is better).

### Bayesian Neural Networks and Other Models

Instead of GPs, one can use **Bayesian neural networks** or ensemble methods to model uncertainty in $u_\theta(x)$. For instance, a neural network can be trained on preference data, and techniques like Monte Carlo dropout or deep ensembles can provide uncertainty estimates for its predictions. These approaches scale to high-dimensional inputs (where GPs may be less practical) while still capturing epistemic uncertainty about the utility.

One principled way to capture uncertainty in Bayesian neural networks is via **Markov Chain Monte Carlo (MCMC)** methods, which seek to approximate the **posterior distribution** over model parameters given the data. In this setting, we place a prior over the neural network weights, $p(\theta)$, and define a likelihood function based on observed preferences—typically using a probabilistic choice model such as the **Bradley-Terry** or **probit** model. Given a dataset $\mathcal{D} = \{(x_i, x_j) : x_i \succ x_j\}$, the posterior is defined as

$$
p(\theta \mid \mathcal{D}) \propto p(\mathcal{D} \mid \theta) \cdot p(\theta),
$$

where $p(\mathcal{D} \mid \theta)$ is the likelihood of observing the pairwise comparisons under the utility function $u_\theta(x)$, and $p(\theta)$ is the prior over the parameters.

Unlike Gaussian processes, for which posterior inference is tractable in closed form under Gaussian likelihoods, inference in BNNs with non-Gaussian likelihoods is generally **intractable**. This is due to the non-conjugate nature of the neural network likelihood and the high-dimensional, nonlinear structure of the weight space. As a result, **approximate inference** methods are required.

MCMC provides a general-purpose approach to approximate sampling from the posterior. The key idea is to construct a Markov chain whose stationary distribution is the target posterior. One of the most widely used algorithms is the **Metropolis-Hastings (MH)** algorithm. Given a current state $\theta_t$, a new proposal $\theta'$ is generated from a proposal distribution $q(\theta' \mid \theta_t)$, and accepted with probability

$$
A = \min\left(1, \frac{p(\mathcal{D} \mid \theta') \, p(\theta') \, q(\theta_t \mid \theta')}{p(\mathcal{D} \mid \theta_t) \, p(\theta_t) \, q(\theta' \mid \theta_t)}\right).
$$

When the proposal distribution is **symmetric**, i.e., $q(\theta' \mid \theta_t) = q(\theta_t \mid \theta')$, the acceptance probability simplifies to a ratio of posterior densities. Over time, the chain yields samples $\theta^{(1)}, \dots, \theta^{(T)} \sim p(\theta \mid \mathcal{D})$, which can be used to compute posterior predictive estimates for the utility function:

$$
\mathbb{E}[u(x)] \approx \frac{1}{T} \sum_{t=1}^T u_{\theta^{(t)}}(x),
$$

with corresponding uncertainty estimates captured via the variance of the predictions across samples.

MCMC methods are particularly appealing for preference learning because they directly quantify epistemic uncertainty in the utility function, which is crucial for downstream tasks such as decision-making, active learning, and safe exploration. Furthermore, MCMC makes no restrictive assumptions on the form of the posterior and can be used with **non-convex** and **multi-modal** distributions that arise from complex neural network architectures.

However, MCMC also faces significant **computational challenges** in practice. First, the convergence of the Markov chain can be slow, especially in high-dimensional parameter spaces. Second, naive random-walk proposals (as in the basic Metropolis-Hastings algorithm) may suffer from low acceptance rates and poor mixing. More advanced MCMC methods such as **Hamiltonian Monte Carlo (HMC)** and **No-U-Turn Sampling (NUTS)** can help address these issues by using gradient information to propose more efficient moves through the parameter space.

Despite these limitations, MCMC remains a valuable tool for **principled Bayesian inference** in preference modeling, particularly in settings where uncertainty quantification is critical and computational cost is acceptable. In lower-dimensional settings or as a pedagogical tool, even simple MH-based approaches can offer intuitive and effective approximations to the posterior over preference functions.

```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# --- True latent utility function ---
def true_u(x):
    return np.sin(x) + 0.1 * x

# --- Generate synthetic preference data ---
np.random.seed(0)
X = np.linspace(0, 10, 40)
y_true = true_u(X)

# Create pairwise comparisons
pairs = []
for _ in range(50):
    i, j = np.random.choice(len(X), 2, replace=False)
    if y_true[i] > y_true[j]:
        pairs.append((X[i], X[j], 1))  # x_i preferred over x_j
    else:
        pairs.append((X[j], X[i], 1))

# --- Define a deep neural network: 3 hidden layers ---
def init_deep_params(hidden_dims=[10, 10, 10]):
    params = {}
    layer_dims = [1] + hidden_dims + [1]
    for i in range(len(layer_dims) - 1):
        W_key = f"W{i+1}"
        b_key = f"b{i+1}"
        params[W_key] = np.random.randn(layer_dims[i+1], layer_dims[i]) * 0.1
        params[b_key] = np.zeros((layer_dims[i+1], 1))
    return params

def deep_forward(x, params):
    x = x.reshape(1, 1)
    num_layers = len(params) // 2
    h = x
    for i in range(1, num_layers):
        h = np.tanh(params[f"W{i}"] @ h + params[f"b{i}"])
    out = params[f"W{num_layers}"] @ h + params[f"b{num_layers}"]
    return out.squeeze()

def deep_utility(x, params):
    return np.array([deep_forward(np.array([xi]), params) for xi in x])

# --- Log likelihood (Bradley-Terry) ---
def deep_log_likelihood(params, pairs):
    ll = 0.0
    for xi, xj, _ in pairs:
        ui = deep_forward(np.array([xi]), params)
        uj = deep_forward(np.array([xj]), params)
        ll += np.log(norm.cdf((ui - uj) / np.sqrt(2)) + 1e-6)
    return ll

# --- Gaussian prior on weights ---
def deep_log_prior(params):
    lp = 0.0
    for v in params.values():
        lp -= 0.5 * np.sum(v**2)
    return lp

# --- Proposal distribution ---
def deep_propose(params, sigma=0.05):
    new_params = {}
    for k, v in params.items():
        new_params[k] = v + np.random.randn(*v.shape) * sigma
    return new_params

# --- Metropolis-Hastings sampling ---
def deep_mh(init_params, pairs, num_iters=2000, burn_in=500):
    samples = []
    current = init_params
    current_lp = deep_log_likelihood(current, pairs) + deep_log_prior(current)

    for i in range(num_iters):
        proposal = deep_propose(current)
        proposal_lp = deep_log_likelihood(proposal, pairs) + deep_log_prior(proposal)
        accept_prob = np.exp(proposal_lp - current_lp)
        if np.random.rand() < accept_prob:
            current = proposal
            current_lp = proposal_lp
        if i >= burn_in:
            samples.append(current)

    return samples

# --- Run MCMC ---
deep_samples = deep_mh(init_deep_params(), pairs, num_iters=2000, burn_in=200)

# --- Posterior predictions ---
X_test = np.linspace(0, 10, 200)
deep_preds = np.array([deep_utility(X_test, s) for s in deep_samples])
deep_mean = deep_preds.mean(axis=0)
deep_std = deep_preds.std(axis=0)

# --- Plot results ---
plt.figure(figsize=(8, 4))
plt.plot(X_test, true_u(X_test), 'k--', label='True utility')
plt.plot(X_test, deep_mean, 'b-', label='BNN (3-layer) mean')
plt.fill_between(X_test, deep_mean - 1.96 * deep_std, deep_mean + 1.96 * deep_std,
                 color='blue', alpha=0.2, label='95% CI')
plt.title("3-layer Bayesian Neural Network via MCMC on Preference Data")
plt.xlabel("x")
plt.ylabel("Utility")
plt.legend()
plt.tight_layout()
plt.show()
```

Another Bayesian approach is **Bayesian Inverse Reinforcement Learning (IRL)**, where a prior is placed on the parameters of a reward function and Bayes’ rule is used to update this distribution given demonstrations or preferences ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=derive%20a%20new%20framework%20for,and%20dampening%20their%20respective%20drawbacks)) ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=Our%20simulated%20experimental%20results%20show,that%20doesn%E2%80%99t%20use%20any%20demonstrations)). Early work like Ramachandran & Amir (2007) treated IRL as Bayesian inference, using MCMC to sample likely reward functions consistent with demonstrations. Such methods yield a posterior over reward functions, reflecting ambiguity when multiple rewards explain the human’s behavior.

In summary, Bayesian utility learning methods acknowledge that with limited human feedback, many possible utility functions might be compatible with the data. They keep track of this ambiguity, which is crucial for making cautious decisions and for actively gathering more feedback.

## Learning from Human Feedback in Robotics

Thus far, we discussed preference learning in general terms. We now focus on **robotics**, where an agent must learn a *reward/utility function* that captures the human’s objectives for a *sequential decision-making* task. Robotics brings additional challenges: the utility often depends on a trajectory of states and actions, and feedback can come in multiple forms. We outline several key forms of human feedback for robot learning and how to learn from them:

- **Learning from demonstrations** – inferring utility from expert demonstrations of the task.
- **Learning from physical corrections** – updating utility when a human physically intervenes in the robot’s behavior.
- **Learning from trajectory evaluations** – using human-provided scores or critiques of full trajectories.
- **Learning from pairwise trajectory comparisons** – inferring reward from which of two trajectories a human prefers.

These are not mutually exclusive; in practice, combinations can be very powerful ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=derive%20a%20new%20framework%20for,and%20dampening%20their%20respective%20drawbacks)). We describe each mode and how utility functions can be derived.

### Learning from Demonstrations (Inverse Reinforcement Learning)

In **Learning from Demonstrations**, also known as Inverse Reinforcement Learning, the human provides examples of desired behavior (e.g. teleoperating a robot to show how to perform a task). The assumption is that the demonstrator is approximately optimizing some latent reward function $R^*(s,a)$ (or utility for trajectories). IRL algorithms then search for a reward function $R_\theta$ under which the given demonstrations $\tau_{demo}$ have high expected return ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=Inverse%20Reinforcement%20Learning,the%20robot%20using%20a%20controller)). 

One classic approach is *Maximum Margin IRL*, which finds a reward function that makes the return of the demonstration trajectories higher than that of any other trajectories by a large margin. Another is *Maximum Entropy IRL*, which models the demonstrator as noisily optimal (Boltzmann-rational) ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=Inverse%20Reinforcement%20Learning,the%20robot%20using%20a%20controller)). In MaxEnt IRL, the probability of a trajectory $\tau$ under reward parameters $\theta$ is modeled as:

$$
P(\tau \mid \theta) = \frac{\exp\{R_\theta(\tau)\}}{\displaystyle \sum_{\tau'} \exp\{R_\theta(\tau')\}} \,,
$$

where $R_\theta(\tau) = \sum_{t} r_\theta(s_t, a_t)$ is the cumulative reward of $\tau$. The IRL algorithm then adjusts $\theta$ to maximize the likelihood of the human demonstrations (while often using techniques to approximate the denominator, since summing over all trajectories is intractable). The end result is a reward function $R_\theta(s,a)$ that rationalizes the demonstrations.

*Key challenge:* unless demonstrations are *optimal* and cover the space well, IRL might recover an ambiguous or incorrect reward. In robotics, humans often have difficulty providing flawless demonstrations (due to hard-to-use interfaces or limited expertise) ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=IRL%20can%20work%20remarkably%20well,in%20the%20marked%20blue%20line)) ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=The%20above%20user%20had%20extensive,users%20remarked%20about%20the%20system)). For example, users teleoperating a robot arm might move jerkily or only accomplish part of the task ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=Image)). This makes sole reliance on demonstrations problematic. Nonetheless, demonstration data can provide a strong prior: it shows at least one way to succeed (or partial preferences for certain behaviors).

### Learning from Preferences and Rankings of Trajectories

When high-quality demonstrations are hard to obtain, **preference queries on trajectories** are a viable alternative ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=Preference,discussed%20above%2C%20the%20user%20may)). In preference-based learning for robotics, the robot (or algorithm) presents two (or more) trajectories of the task outcome, and the human chooses which one is better. Each such comparison provides a bit of information about the true underlying reward. By asking many queries, the algorithm can home in on the reward function that explains the human’s choices ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=While%20neither%20trajectory%20perfectly%20accomplishes,reward%20function%20for%20the%20robot)).

A concrete example is an agent learning to do a backflip in simulation ([Learning from human preferences | OpenAI](https://openai.com/index/learning-from-human-preferences/#:~:text=the%20complex%20goal%20a%20bit,two%20proposed%20behaviors%20is%20better)) ([Learning from human preferences | OpenAI](https://openai.com/index/learning-from-human-preferences/#:~:text=Our%20AI%20agent%20starts%20by,refines%20its%20understanding%20of%20the%C2%A0goal)). The agent initially performs random flails. The system then repeatedly shows the human *two video clips* of the agent’s behavior and asks which is closer to a proper backflip. From these comparisons, a reward model is learned that assigns higher value to behaviors more like backflips ([Learning from human preferences | OpenAI](https://openai.com/index/learning-from-human-preferences/#:~:text=Our%20AI%20agent%20starts%20by,refines%20its%20understanding%20of%20the%C2%A0goal)). The agent then uses reinforcement learning to optimize this learned reward, gradually performing better backflips. This process continues, with the human being asked comparisons on trajectories where the algorithm is most uncertain (to maximally inform the reward model) ([Learning from human preferences | OpenAI](https://openai.com/index/learning-from-human-preferences/#:~:text=Our%20AI%20agent%20starts%20by,refines%20its%20understanding%20of%20the%C2%A0goal)).

 ([Learning from human preferences | OpenAI](https://openai.com/index/learning-from-human-preferences/)) *Framework for learning from human preferences in robotics: a reward predictor (utility function) is learned from human feedback on trajectory comparisons, and an RL algorithm uses this learned reward to improve the policy ([Learning from human preferences | OpenAI](https://openai.com/index/learning-from-human-preferences/#:~:text=Our%20AI%20agent%20starts%20by,refines%20its%20understanding%20of%20the%C2%A0goal)). The loop is iterative: as the policy improves, new queries focus on areas of uncertainty to refine the reward model.* 

Such preference-based reward learning has enabled complex skills without explicitly programmed rewards ([Learning from human preferences | OpenAI](https://openai.com/index/learning-from-human-preferences/#:~:text=the%20complex%20goal%20a%20bit,two%20proposed%20behaviors%20is%20better)). Notably, Christiano *et al.* (2017) showed that an agent can learn Atari game policies and robotic manipulations from a few hundred comparison queries, achieving goals that are hard to specify but easy to judge ([Learning from human preferences | OpenAI](https://openai.com/index/learning-from-human-preferences/#:~:text=solve%20modern%20RL%20environments,simple%20to%20judge%20but%C2%A0challenging%20%E2%81%A0%C2%A0to%C2%A0specify)). Preferences are often easier for humans than demonstrations: choosing between options is simpler than generating one from scratch ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=While%20neither%20trajectory%20perfectly%20accomplishes,reward%20function%20for%20the%20robot)). However, preference learning can be slow if each query only yields one bit of information. Active learning and combining preferences with other feedback can greatly improve efficiency ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=Our%20simulated%20experimental%20results%20show,that%20doesn%E2%80%99t%20use%20any%20demonstrations)).

### Learning from Trajectory Evaluations (Critiques and Ratings)

Sometimes humans provide feedback in the form of *evaluative scores* or critiques on full trajectories (or partial trajectories). For example, after a robot finishes an attempt at a task, the human might give a reward signal (e.g. +1/-1, or a rating 1–5 stars, or say “too slow” vs “good job”). This is the premise of the **TAMER framework** (Training an Agent via Evaluative Reinforcement) and related approaches, where a human’s scalar reward signals are directly treated as the reward function for the agent in reinforcement learning.

From a utility learning perspective, such feedback can be used to directly fit a utility model $u_\theta$ that predicts the human’s rating for a given trajectory. For instance, if a human provides a score $H(\tau)$ for trajectory $\tau$, one can treat it as a training target for $u_\theta(\tau)$ (possibly under a regression loss). However, because humans are inconsistent and may not precisely quantify their preferences, it’s often useful to model $H(\tau)$ as a noisy realization of the underlying utility, rather than a perfect label. A Bayesian approach could treat $H(\tau)$ as a noisy observation of $u(\tau)$ and update a posterior for $u$. Alternatively, classification approaches can be used (e.g. treat trajectories into “liked” vs “disliked” based on thresholded ratings).

A challenge with trajectory-level feedback is *credit assignment*: the human’s single score must be attributed to the entire sequence of actions. Algorithms like COACH (Continuous cOaching of Automated Control Handlers) address this by allowing humans to give feedback at intermediate steps, thereby guiding the agent which specific part of the behavior was good or bad. In either case, learning from trajectory evaluations turns the human into a *reward function provider*, and the learning algorithm’s job is to infer the latent reward function that the human’s evaluations are trying to convey.

### Learning from Physical Corrections

Robots that physically collaborate with humans can receive **physical corrections**: the human may push the robot or otherwise intervene to adjust its behavior. Such corrections provide insight into the human’s desired utility. For example, if a household robot is carrying a fragile object too recklessly and the human physically slows it down or re-routes it, that indicates the human’s reward favors safety over speed at that moment.

Learning from physical corrections can be formalized in different ways. One approach is to treat a correction as a demonstration on a small segment: the human’s intervention suggests a better action or trajectory than what the robot was doing. This can be converted into a comparison: “the trajectory after correction is preferred over the original trajectory” for that time segment. The robot can then update its reward function $\theta$ to satisfy $R_\theta(\text{human-corrected behavior}) > R_\theta(\text{robot’s initial behavior})$. Repeated corrections yield a dataset of such pairwise preferences, focused on the states where the robot was wrong ([Learning from Physical Human Corrections, One Feature at a Time](https://www.researchgate.net/publication/323526338_Learning_from_Physical_Human_Corrections_One_Feature_at_a_Time#:~:text=We%20focus%20on%20learning%20robot,the%20robot%20is%20acting)) ([Unified Learning from Demonstrations, Corrections, and ...](https://dl.acm.org/doi/10.1145/3623384#:~:text=Unified%20Learning%20from%20Demonstrations%2C%20Corrections%2C,Gleb%20Shevchuk%2C%20and%20Dorsa%20Sadigh)).

Another approach is to infer the human’s intent through the sequence of corrections. Research by Losey *et al.* (2021) formalized learning from *sequences* of physical corrections, noting that each correction is not independent: a series of pushes might only make sense in aggregate ([[2104.00078] Learning Human Objectives from Sequences of Physical Corrections](https://arxiv.org/abs/2104.00078#:~:text=,by%20all%20of%20the%20human%27s)) ([[2104.00078] Learning Human Objectives from Sequences of Physical Corrections](https://arxiv.org/abs/2104.00078#:~:text=In%20this%20paper%20we%20formalize,over%20their%20sequence%20of%20corrections)). By analyzing the cumulative effect of multiple interventions, the algorithm can deduce the underlying objective more accurately (e.g. the human consistently steers the robot away from the table edges, implying a high negative reward for collisions). Their algorithm introduced an auxiliary reward term to capture the human’s trade-off: they will correct the robot if the immediate mistake is worth fixing relative to long-term performance ([[2104.00078] Learning Human Objectives from Sequences of Physical Corrections](https://arxiv.org/abs/2104.00078#:~:text=corrections,over%20their%20sequence%20of%20corrections)). The conclusion was that reasoning over the sequence of corrections improved learning of the human’s objective ([[2104.00078] Learning Human Objectives from Sequences of Physical Corrections](https://arxiv.org/abs/2104.00078#:~:text=In%20this%20paper%20we%20formalize,over%20their%20sequence%20of%20corrections)).

Physical corrections are intuitive for humans – we often instinctively guide others or objects when they err. For the robot, interpreting this guidance requires converting it into constraints or examples for the utility function. It is a powerful signal because it is *active*: the human is not just telling preferences but directly imparting the desired direction of change.

### Combining Multiple Feedback Types

Each feedback modality has strengths and weaknesses. Demonstrations provide a lot of information but can be hard to perform; preferences are easy for humans but yield information slowly; corrections are very informative locally but require physical interaction; trajectory evaluations are straightforward but coarse. **Combining these modes** can lead to faster and more robust reward learning ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=derive%20a%20new%20framework%20for,and%20dampening%20their%20respective%20drawbacks)). For example, the DemPref algorithm ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=derive%20a%20new%20framework%20for,and%20dampening%20their%20respective%20drawbacks)) first uses demonstrations to get an initial rough reward model, then uses preference queries to refine it quickly ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=In%20DemPref%2C%20we%20learn%20the,used%20to%20construct%20a%20probabilistic)). In user studies, such combined approaches learned better rewards with fewer queries than using either alone ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=Our%20simulated%20experimental%20results%20show,that%20doesn%E2%80%99t%20use%20any%20demonstrations)) ([Learning Reward Functions by Integrating Human Demonstrations and Preferences – Stanford ILIAD](https://iliad.stanford.edu/blog/2019/06/24/learning-reward-functions-by-integrating-human-demonstrations-and-preferences/#:~:text=quality%20demonstrations%20received%20from%20the,quality)).

In practical robot learning systems, one might start by asking for a demonstration. If the demo is suboptimal, the system can then ask preference questions on alternative attempts to clarify the true goal. During actual execution, if the human intervenes, the robot updates its reward function on the fly to avoid repeating the mistake. This *interactive reward learning* loop continues until the robot’s behavior aligns with human intent.

## Summary

Learning utility functions from human preferences enables **value alignment**: aligning an AI system’s objectives with what humans actually want, rather than what we *think* we want in abstract. We covered how supervised learning can extract utilities from comparisons or scores, and how Bayesian methods like Gaussian processes and Bayesian neural nets can capture uncertainty in our inferences. In robotics, we saw that feedback can come in many forms – demonstrations, comparisons, corrections, evaluations – each providing a unique window into the human’s utility function. By intelligently combining these signals, robots can efficiently learn complex reward functions that would be extremely difficult to hand-code.

Key takeaways and best practices include:

- *Use the right feedback for the problem:* If optimal examples are available, demonstrations jump-start learning. If not, pairwise preferences or scalar critiques might be easier to obtain.
- *Model uncertainty:* Knowing what the system doesn’t know (via a Bayesian model) allows for smart query selection and avoids overconfidently optimizing the wrong objective.
- *Iterate with the human:* Preference learning is fundamentally an interactive process. An agent can query a human in ambiguous cases and continuously refine the utility estimate ([Learning from human preferences | OpenAI](https://openai.com/index/learning-from-human-preferences/#:~:text=Our%20AI%20agent%20starts%20by,refines%20its%20understanding%20of%20the%C2%A0goal)).
- *Validate the learned utility:* Once a reward is learned, testing the robot’s policy and having humans verify or correct it is crucial. Even a few manual corrections can reveal if the learned reward misses a key aspect, allowing further refinement.
- *Be aware of scaling and bias:* Human feedback can be noisy or biased. Techniques like DPO suggest ways to simplify learning and avoid instability, but one should monitor for issues like reward hacking or unintended solutions, intervening with additional feedback as needed.

Learning from human preferences is a rich area of ongoing research. It lies at the intersection of machine learning, human-computer interaction, and ethics. As AI systems become more advanced, the importance of teaching them *our* utility functions (and not mis-specified proxies) grows. The methods discussed in this chapter are building blocks toward AI that truly understands and pursues what humans value, acquired through learning *with* humans in the loop rather than in isolation. By mastering these techniques, we move closer to AI and robots that can be trusted to make decisions aligned with human preferences and well-being. 

<!--
### Reward Learning in Robotics

To help set up our basic reward learning problem, consider a user and a
robot. The user's preferences or goals can be represented by an internal
reward function, R($\xi$), which the robot needs to learn. Since the
reward function isn't explicit, there are a variety of ways that the
robot can learn this reward function, which we will discuss in the next
section. An example method of learning a reward function from human data
is using pairwise comparison. Consider the robot example from section
one, but now, the robot shows the human two possible trajectories
$\xi_A$ and $\xi_B$ as depicted in the diagram below.

![Two different trajectories taken by a robot to prompt
user ranking.](Figures/robots.png){#fig-reward-robot-1 width="70%"}

The user is show both the trajectories above and asked to rank which one
is better. Based on iterations of multiple trajectories and ranking, the
robot is able to learn the user's internal reward function. There quite
a lot of ways that models can learn a reward function from human data.
Here's a list [@myers2021learning] of some of them:

1.  Pairwise comparison: This is the method that we saw illustrated in
    the previous example. The robot is able to learn based on a
    comparison ranking provided by the user.

2.  Expert demonstrations: Experts perform the task and the robot learns
    the optimal reward function from these demonstrations.

3.  Sub-optimal demonstrations: The robot is provided with
    demonstrations that are not quite as good as the expert
    demonstrations but it is still able to learn a noisy reward function
    from the demonstrations.

4.  Physical Corrections: While the robot is performing the task, at
    each point in its trajectory (or at an arbitrary point in its
    trajectory) its arm is corrected to a more suitable position. Based
    on these corrections, the robot is able to learn the reward
    function.

5.  Ranking: This method is similar to pairwise comparison but involves
    more trajectories than 2. All the trajectories may have subtle
    differences from each other, but these differences help provide
    insight to the model.

6.  Trajectory Assessment: Given a single trajectory, the user rates how
    close it is to optimal, typically using a ranking scale.

    Each of these methods allows the robot to refine its understanding
    of the user's reward function, but their effectiveness can vary
    depending on the application. For instance, expert demonstrations
    tend to produce more reliable results but may not always be feasible
    in everyday tasks. Pairwise comparison and ranking methods offer
    more flexibility but might require a higher number of iterations.

### Direct Preference Optimization

A modern method for estimating the parameters of a human preference
model is direct preference optimization [@rafailov2023direct], which is
used in the context of aligning language models to human preferences. A
recent approach [@christiano2023deep] first trains a reward model that
captures human preferences and then uses proximal policy optimization to
train a language model-based policy to reflect those learned
preferences. Direct Preference Optimization (DPO), on the other hand,
removes the need for a reward model by directly using the model
likelihood of two outcomes (a preferred or highly-ranked sequence and an
unpreferred or low-ranked sequence) to capture the preference
represented in the data. DPO provides a simpler framework than its
reinforcement learning approach and results in comparable performance
with improved stability. Furthermore, it obviates the need to train a
reward model, instead using a language model policy and human preference
dataset to align the policy directly to human preferences.

<!--
Through our exploration of human preference models, we will ground ourselves in
building a health coaching system that can provide meal recommendations aligned with a user's dietary needs and preferences. Examples of scenarios which can benefit from a model of how humans make choices include:

1.  **Health coaching:** Humans express their preferences every time
    they pick lunch for consumption. Humans may have several goals
    related to nutrition, such as weight loss and improving
    concentration. We can learn how a given individual or set of
    individuals prefer to eat to provide personalized recommendations to
    help them attain their goals. This chapter will use this use case to
    ground human preference modeling in a real-life application.

2.  **Social media:** Platforms have a far greater amount of content
    than one can consume in a lifetime, yet such products must aim to
    maximize user engagement. To accomplish this, we can learn what
    specific things people like to see in their feeds to optimize the
    value they gain out of their time on social media. For example, the
    video feed social media platform [TikTok](https://www.tiktok.com/)
    has had viral adoption due to its notorious ability to personalize a
    feed for its users based on their preferences.

3.  **Shopping:** Retail corporations largely aim to maximize revenue by
    making it easy for people to make purchases. Recommendation systems
    on online shopping platforms provide a mechanism for curating
    specific items based on an individual's previous purchases (or even
    browsing history) to make shoppers aware of items they may like and,
    therefore, purchase.

Two key models used in pairwise sampling are the Thurstonian and Bradley-Terry models [@cattelan2012]. The Thurstonian model assumes each item $i$ has a true score $u_i$ following a normal distribution. The difference $d_{ij} = u_i - u_j$ is also normally distributed. The probability that item $i$ is preferred over item $j$ is given by $P(i \succ j) = \Phi \left( \frac{u_i - u_j}{\sqrt{2\sigma^2}} \right)$, where $\Phi$ is the cumulative normal distribution function. The denominator $\sqrt{2\sigma^2}$ is the standard deviation of the difference $d_{ij} = u_i - u_j$ when $u_i$ and $u_j$ are normally distributed with variance $\sigma^2$[@cattelan2012]. The Bradley-Terry model defines the probability of preference based on latent scores $\beta_i$ and $\beta_j$. The probability that item $i$ is preferred over item $j$ is $P(i \succ j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}$. This model is used to estimate relative strengths or preferences based on latent scores. [@cattelan2012].

::: {#tbl-philosophy}
  -----------------------------------------------------------------------
  Application                         Human Preference
  ----------------------------------- -----------------------------------
  Computer vision: train a neural     This is how humans process images
  network to predict bounding boxes   by identifying the position and
  delineating all instances of dogs   geometry of the things we see in
  in an image                         them

  Natural language processing: train  Coherent text is itself a
  a model to generate coherent text   human-created and defined concept,
                                      and we prefer that any
                                      synthetically generated text
                                      matches that of humans

  Computer vision: train a diffusion  Humans prefer that images
  model to generate realistic images  accurately capture the world as
  of nature                           observed by humans, and this
                                      generative model should reflect the
                                      details that comprise that
                                      preference
  -----------------------------------------------------------------------

  : Examples of machine learning tasks and their interpretation as
  modeling human preferences.
:::
-->

<!--
Game theory provides a mathematical framework for analyzing strategic
interactions among rational agents. These models help in understanding
and predicting human behavior by considering multiple criteria and the
associated trade-offs. They enhance the understanding of preferences
across multiple criteria and allow for richer and more accurate feedback
through structured comparisons. Game-theory framings capture the
complexity of preferences and interactions in decision-making processes
[@bhatia2020preference].

The most popular form of preference elicitation involves pairwise
comparisons. Users are asked to choose between two options, such as
product A or product B. This method is used in various applications like
search engines, recommender systems, and interactive robotics. Key
concepts include the Von Neumann Winner and the Blackwell Winner. The
Von Neumann Winner refers to a distribution over objects that beats or
ties every other object in the collection under the expected utility
assumption. The Blackwell Winner generalizes the Von Neumann Winner for
multi-criteria problems using a target set for acceptable payoff vectors
[@bhatia2020preference].

Game-theory framings provide a framework for preference learning along
multiple criteria. These models use tools from vector-valued payoffs in
game theory, with Blackwell's approach being a key concept. This
approach allows for a more comprehensive understanding of preferences by
considering multiple criteria simultaneously [@bhatia2020preference].

In game-theory framings, pairwise preferences are modeled as random
variables. Comparisons between objects along different criteria are
captured in a preference tensor $P$. This tensor models the probability
that one object is preferred over another along a specific criterion,
allowing for a detailed understanding of preferences across multiple
dimensions [@bhatia2020preference].

The preference tensor $P$ captures object comparisons along different
criteria. It is defined as:
$$P(i_1, i_2; j) = P(i_1 \succ i_2 \text{ along criterion } j)$$ where
$P(i_2, i_1; j) = 1 - P(i_1, i_2; j)$. These values are aggregated to
form an overall preference matrix $P_{ov}$ [@bhatia2020preference].

The Blackwell Winner is defined using a target set $S$ of acceptable
score vectors. The goal is to find a distribution $\pi^*$ such that
$P(\pi^*, \pi) \in S$ for all $\pi$. This method minimizes the maximum
distance to the target set, providing a robust solution to
multi-criteria preference problems [@bhatia2020preference].

The optimization problem for finding the Blackwell Winner is defined as:
$$\pi(P, S, \|\cdot\|) = \arg \min_{\pi \in \Delta_d} \left[ \max_{\pi' \in \Delta_d} \rho(P(\pi, \pi'), S) \right]$$
where $\rho(u, v) = \|u - v\|$. This measures the distance to the target
set, ensuring that the selected distribution is as close as possible to
the ideal preference vector [@bhatia2020preference].
-->
