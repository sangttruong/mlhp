---
title: Models of Preferences and Decisions
format: html
filters:
  - pyodide
execute:
  engine: pyodide
  pyodide:
    auto: true

---

::: {.content-visible when-format="html"}

<iframe
  src="https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/"
  style="width:45%; height:225px;"
></iframe>
<iframe
  src="https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/"
  style="width:45%; height:225px;"
></iframe>
[Fullscreen Part 1](https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/){.btn .btn-outline-primary .btn role="button"}
[Fullscreen Part 2](https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/){.btn .btn-outline-primary .btn role="button"}

:::

Human preference modeling aims to capture humans' decision making processes in a probabilistic framework. Many problems would benefit from a quantitative perspective, enabling an understanding of how humans engage with the world. In this chapter, we will explore how one can model human preferences, including different formulations of such models, how one can optimize these models given data, and considerations one should understand to create such systems.

## The Construction of Preference {#sec-foundations}

### Axiom 1. Construction of Choices Set: Luce’s Choice Axiom (Luce, 1959) {#axiom-1-preference-models-model-choice}
Preference models model the preferred choices amongst a set of items. Preference models must enumerate the set of all possible choices included in a human decision. As such, we must ensure that the choices we enumerate capture the entire domain (collectively exhaustive) but are distinct (mutually exclusive) choices. A discrete set of choices is a constraint we canonically impose to ensure we can tractably model preferences and aptly estimate the parameters of preference models. We assume that if a new item is added to the choice set, the relative probabilities of choosing between the original items remain unchanged. This is known as the Independence of Irrelevant Alternatives (IIA) property from Luce's axiom of choices [@Luce1977].

### Axiom 2. Preference Centers around Utility: Reciprocity (Block & Marschak, 1960) {#axiom-2-preference-centers-around-reward}
Preference models are centered around the notion of reward, a scalar quantity representing the benefit or value an individual attains from selecting a given choice. We assume that the underlying reward mechanism of a human preference model captures the final decision output from a human. We use the notation $u_{i,j}$ as the reward of person $i$ choosing item $j$. The reward is a random variable, decomposing into true reward $u_{i,j}^*$ and a random noise $\epsilon_{i,j}$: $u_{i,j} = u_{i,j}^* + \epsilon_{i,j}$. @mcfadden_conditional_1974 posits that reward can further be decomposed into user-specific reward $\theta_i$ and item-specific reward $z_j$: $u_{i,j}^* = \theta_i + z_j$. This decomposition indicates that for a single user, only the relative difference in reward matters to predict the choice among items, and the scale of rewards is important when comparing across users. 

### Axiom 3. Preference captures decision-making: Wins as a Sufficient Statistic (Bühlmann & Huber, 1963) {#axiom-3-preference-captures-decision-making}
Human preferences are classified into two categories: revealed preferences and stated preferences. Revealed preferences are those one can observe retroactively from existing data. The implicit decision-making knowledge can be captured via learnable parameters and their usage in models that represent relationships between input decision attributes that may have little interpretability but enable powerful models of human preference. Such data may be easier to acquire and can reflect real-world outcomes (since they are, at least theoretically, inherently based on human preferences). However, if we fail to capture sufficient context in such data, human preference models may not sufficiently capture human preferences. Stated preferences are those individuals explicitly indicate in potentially experimental conditions. The explicit knowledge may be leveraged by including inductive biases during modeling (for example, the context used in a model), which are reasonable assumptions for how a human would consider a set of items. This may include controlled experiments or studies. This may be harder to obtain and somewhat biased, as they can be hypothetical or only accurately reflect a piece of the overall context of a decision. However, they enable greater control of the decision-making process.

### Axiom 4. Rationality: The Transitivity of odds (Good, 1955) {#human-rationality}
The preference model assumes that humans are rational. Perfect rationality posits that individuals make decisions that maximize their reward, assuming they have complete information and the cognitive ability to process this information to make optimal choices. Numerous studies have shown that this assumption frequently fails to describe actual human behavior. Bounded rationality acknowledges that individuals operate within the limits of their information and cognitive capabilities [@simon1972theories]. Here, decisions are influenced by noise, resulting in probabilistic choice behavior: while individuals aim to maximize their reward, noise can lead to deviations from perfectly rational choices [@miljkovic2005rational]. Instead of deterministic reward maximization, the decision maker will choose an item with probability proportional to the reward they receive for that item. This probabilistic model can be operationalized with Boltzmann distribution. Utility of person $i$ on item $j$ is computed by a function $f_i: e_j \rightarrow \mathbb{R}$, where $e_j \in \mathbb{R}^d$ is an embedding of item $j$. The probability of item $j$ being preferred by person $i$ over all other alternatives in the choice set $\mathcal{C}$ is

$$
p_{ij} =  p_i(j \succ j': j' \neq j \forall j' \in \mathcal{C}) = Z_i^{-1} \exp \circ f_i(e_j) \text{ where } Z_i = \sum_{j' \in \mathcal{C}} \exp \circ f_i(e_{j'})
$$

One can extend the above model in various ways. For example, the above model does not account for similar actions. Consider the following example when choosing a mode of transportation: car and train, with no particular preference for either choice. The preferred probability is 50% for either item. However, if we have 99 cars and one train in the choice set, we would have a 99% probability of choosing a car. To address this issue, various extensions have been proposed. For example, we can introduce a similarity metric to cluster items. We want a metric that acts more as a distance in the feature space with the following properties: Identity (an item is most similar to itself), symmetric (the similarity of item $j$ to $j'$ is the same as that of $j'$ to $j$), and positive semidefinite (similarity metric is non-negative). Under this extension, the probablity of item $j$ being preferred over all other alternatives by person $i$ is $p_{ij} / w_j, \text{ where } w_j = \sum_{j' \in \mathcal{C}} s(e_j, e_{j'})$. This de-weights similar items, which is the desired effect for human decision-making. 

## Models of Preferences and Decisions {#preference-model}

Next, we explore ways humans can express their preferences, including accept-reject sampling, pairwise sampling, rank-order sampling, rating-scale sampling, best-worst scaling, and multiple-choice samples. We will understand the process of collecting data through simulation and, when appropriate, discuss the real-world application of these models. Each item $i$ is represented by a $d=2$ dimensional vector $x^i$. There is only one user in the simulation, and they have a latent reward function $f$ that they use to compute the latent reward of an item from the features. Here, the latent reward function is the Ackley function \cite{ackley1987}.

::: {.callout-note title="code"}
```{pyodide-python}
import numpy as np
np.random.seed(0)

def ackley(X, a=20, b=0.2, c=2*np.pi):
    """
    Compute the Ackley function.
    Parameters:
      X: A NumPy array of shape (n, d) where each row is a d-dimensional point.
      a, b, c: Parameters of the Ackley function.
    Returns:
      A NumPy array of function values.
    """
    X = np.atleast_2d(X)
    d = X.shape[1]
    sum_sq = np.sum(X ** 2, axis=1)
    term1 = -a * np.exp(-b * np.sqrt(sum_sq / d))
    term2 = -np.exp(np.sum(np.cos(c * X), axis=1) / d)
    return term1 + term2 + a + np.e
```
:::

```{python}
import numpy as np
np.random.seed(0)

def ackley(X, a=20, b=0.2, c=2*np.pi):
    """
    Compute the Ackley function.
    Parameters:
      X: A NumPy array of shape (n, d) where each row is a d-dimensional point.
      a, b, c: Parameters of the Ackley function.
    Returns:
      A NumPy array of function values.
    """
    X = np.atleast_2d(X)
    d = X.shape[1]
    sum_sq = np.sum(X ** 2, axis=1)
    term1 = -a * np.exp(-b * np.sqrt(sum_sq / d))
    term2 = -np.exp(np.sum(np.cos(c * X), axis=1) / d)
    return term1 + term2 + a + np.e
```

We next define a function to visualize the surface:

::: {.callout-note title="code"}
```{pyodide-python}
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
ccmap = LinearSegmentedColormap.from_list("ackley", ["#f76a05", "#FFF2C9"])
plt.rcParams.update({
    "font.size": 14,
    "axes.labelsize": 16,
    "xtick.labelsize": 14,
    "ytick.labelsize": 14,
    "legend.fontsize": 14,
    "axes.titlesize": 16,
})

def draw_surface():
    inps = np.linspace(-2, 2, 100)
    X, Y = np.meshgrid(inps, inps)
    grid = np.column_stack([X.ravel(), Y.ravel()])
    Z = ackley(grid).reshape(X.shape)
    
    plt.figure(figsize=(6, 5))
    contour = plt.contourf(X, Y, Z, 50, cmap=ccmap)
    plt.contour(X, Y, Z, levels=15, colors='black', linewidths=0.5, alpha=0.6)
    plt.colorbar(contour, label=r'$f(x)$', ticks=[0, 3, 6])
    plt.xlim(-2, 2)
    plt.ylim(-2, 2)
    plt.xticks([-2, 0, 2])
    plt.yticks([-2, 0, 2])
    plt.xlabel(r'$x_1$')
    plt.ylabel(r'$x_2$')
```
:::
```{python}
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
ccmap = LinearSegmentedColormap.from_list("ackley", ["#f76a05", "#FFF2C9"])
plt.rcParams.update({
    "font.size": 14,
    "axes.labelsize": 16,
    "xtick.labelsize": 14,
    "ytick.labelsize": 14,
    "legend.fontsize": 14,
    "axes.titlesize": 16,
})
plt.rcParams['text.usetex'] = True

def draw_surface():
    inps = np.linspace(-2, 2, 100)
    X, Y = np.meshgrid(inps, inps)
    grid = np.column_stack([X.ravel(), Y.ravel()])
    Z = ackley(grid).reshape(X.shape)
    
    plt.figure(figsize=(6, 5))
    contour = plt.contourf(X, Y, Z, 50, cmap=ccmap)
    plt.contour(X, Y, Z, levels=15, colors='black', linewidths=0.5, alpha=0.6)
    plt.colorbar(contour, label=r'$f(x)$', ticks=[0, 3, 6])
    plt.xlim(-2, 2)
    plt.ylim(-2, 2)
    plt.xticks([-2, 0, 2])
    plt.yticks([-2, 0, 2])
    plt.xlabel(r'$x_1$')
    plt.ylabel(r'$x_2$')
```

### Item-wise Model {#item-wise-model}
One method for data collection is accept-reject sampling, where the user considers one item at a time and decides if they like it. Below is an example survey using accept-reject sampling:

::: {.content-visible when-format="html"}
<iframe
  src="https://app.opinionx.co/5f30e903-c7b2-42e3-821a-e65271144bd9"
  style="width:100%; height:450px;"
></iframe>
:::

We will use a simulation to familiarize ourselves with accept-reject sampling. On the surface below, blue and red points correspond to accept or reject points.

::: {.callout-note title="code"}
```{pyodide-python}
d = 2
n_items = 800
items = np.random.randn(n_items, d)*0.5 + np.ones((n_items, d))*0.5
rewards = ackley(items)
y = (rewards > rewards.mean())
draw_surface()
plt.scatter(items[:, 0], items[:, 1], c=y, cmap='coolwarm', alpha=0.5)
plt.show()
```
:::

```{python}
d = 2
n_items = 800
items = np.random.randn(n_items, d)*0.5 + np.ones((n_items, d))*0.5
rewards = ackley(items)
y = (rewards > rewards.mean())
draw_surface()
plt.scatter(items[:, 0], items[:, 1], c=y, cmap='coolwarm', alpha=0.5)
plt.show()
```

The binary choice model centers around one item. The model predicts, for that item, after observing user choices in the past, whether that item will be chosen. We use binary variable $y \in \{0, 1\}$ to represent whether the user will pick that choice in the next selection phase. We denote $P = p(y = 1)$. We can formally model $y$ as a function of the reward of the positive choice: $y = \mathbb{I}[U>0]$. We explore two cases based on the noise distribution. $\psi$ is the logistic function or the standard normal cumulative distribution function if noise follows logistic distribution and the standard normal distribution, respectively:
$$
p(u_{i,j} > 0) = p(u_{i,j}^* + \epsilon > 0) = 1 - p( \epsilon < -u_{i,j}^*) = \psi(u_{i,j}^*).
$$

A generalization of accept-reject sampling is rating-scale sampling. Rating-scale sampling, such as the Likert scale, is a method in which participants rate items on a fixed-point scale (e.g., 1 to 5, "Strongly Disagree" to "Strongly Agree") to measure levels of preference towards items [@harpe2015]. Participants can also mark a point on a continuous rating scale to indicate their preference or attitude. Commonly used in surveys, product reviews, and psychological assessments, this method provides a more nuanced measure than discrete scales. Rating-scale sampling is simple for participants to understand and use, provides rich data on the intensity of preferences, and is flexible enough for various measurements (e.g., agreement, satisfaction). However, rating-scale sampling methods also have limitations. Ratings can be influenced by personal biases and interpretations of scales, leading to subjectivity. There is a central tendency bias, where participants may avoid extreme ratings, resulting in clustering responses around the middle. Different participants might interpret scale points differently, and fixed-point scales may not capture the full nuance of participants' preferences or attitudes.

::: {.callout-note title="code"}
```{pyodide-python}
from matplotlib.colors import LinearSegmentedColormap
likert_cmap = LinearSegmentedColormap.from_list("likert_scale", ["red", "blue"], N=5)
normalized = (rewards - rewards.min()) / (rewards.max() - rewards.min())
ratings = np.round(normalized * 4).squeeze()

draw_surface()
scatter = plt.scatter(items[:, 0], items[:, 1], c=ratings, cmap=likert_cmap, alpha=0.5)
plt.show()
```
:::

```{python}
from matplotlib.colors import LinearSegmentedColormap
likert_cmap = LinearSegmentedColormap.from_list("likert_scale", ["red", "blue"], N=5)
normalized = (rewards - rewards.min()) / (rewards.max() - rewards.min())
ratings = np.round(normalized * 4).squeeze()

draw_surface()
scatter = plt.scatter(items[:, 0], items[:, 1], c=ratings, cmap=likert_cmap, alpha=0.5)
plt.show()
```

Suppose we have a single example with attributes $z_i$ and wish to know which of $J$ rating scales an individual will choose from. We can define $J - 1$ parameters, which act as thresholds on the reward computed by $u_i = u_{i,j}^*$ to classify the predicted choice between these items. For example, if there are three predefined items, we can define parameters $a, b \in \mathbb{R}$ such that
$$
y_i =
\begin{cases} 
    1 & u < a \\
    2 & a \le u < b \\
    3 & \text{else}
\end{cases}
$$

By assuming the noise distribution to be either logistic or standard normal, we have 
$$
\begin{split}
    p(y_i = 1) & = p(u < a) = p(u_{i,j}^* + \epsilon < a) = \psi(a-u_{i,j}^*) \\
    p(y_i = 2) & = p(a \le u < b) = p(a - u_{i,j}^* \le \epsilon < b - u_{i,j}^*) = \psi(b-u_{i,j}^*)  - \psi(u_{i,j}^*-a) \\
    p(y_i = 3) & = p(u > b) = p(u_{i,j}^* + \epsilon > b ) = p( \epsilon > b - u_{i,j}^*) = \psi(b-u_{i,j}^*)
\end{split}
$$

Having the model, we next explore the estimation of model parameters. A common approach for parameter estimation is maximum likelihood [@book_estimation_casella; @book_estimation_bock]. The likelihood of a model is the probability of the observed data given the model parameters; intuitively, we wish to maximize this likelihood, as that would mean that our model associates observed human preferences with high probability. Assuming our data is independent and identically distributed (iid), the likelihood over the entire dataset is the joint probability of all observed data as defined by the binary choice model with logistic noise is

$$\mathcal{L}(z, Y; \beta) = \prod_{i = 1}^J p(y = y_i | z_i; \beta) = \prod_{i = 1}^J \frac{1}{1 + \exp^{-u_{i,j}^*}}$$ 

This objective can be optimized with a gradient-based method, such as gradient descent [@gradient_descent]. Gradient descent operates by computing the gradient of the objective with respect to the parameters of the model, which provides a signal of the direction in which the parameters must move to minimize the objective. Then, SGD makes an update step by subtracting this gradient from the parameters (most often with a scale factor called a learning rate) to move the parameters in a direction that minimizes the objective. In the case of logistic and Gaussian models, SGD may yield a challenging optimization problem as its stochasticity can lead to noisy updates, for example, if certain examples or batches of examples are biased. Mitigations include batched SGD, in which multiple samples are randomly sampled from the dataset at each iteration; learning rates, which reduce the impact of noisy gradient updates, and momentum and higher-order optimizers, which reduce noise by using moving averages of gradients or provide better estimates of the best direction in which to update the gradients.

::: {.callout-note title="code"}
```{pyodide-python}
import numpy as np
from scipy.optimize import minimize
from sklearn.metrics import roc_auc_score
from tqdm import tqdm

# Set random seed for reproducibility (optional)
np.random.seed(42)

# Number of users and items
num_users = 50
num_items = 100

# Generate user-specific and item-specific rewards
theta_true = np.random.randn(num_users)
z_true = np.random.randn(num_items)

# Define the logistic (sigmoid) function
def sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))

# Generate observed choices using the logistic function
# Compute probability matrix: shape (num_users, num_items)
probs = sigmoid(theta_true[:, None] - z_true[None, :])
# Sample binary responses (0 or 1) from a Bernoulli distribution
data = np.random.binomial(1, probs)

# Mask out a fraction of the response matrix (80% observed, 20% missing)
mask = np.random.rand(num_users, num_items) > 0.2  # boolean mask
# Create a version of the data with missing values (not needed for optimization, but for reference)
data_masked = data.copy().astype(float)
data_masked[~mask] = np.nan

# Count of observed entries (used for averaging)
observed_count = np.sum(mask)

# We will optimize over parameters theta and z.
# Initialize estimates (random starting points)
theta_init = np.random.randn(num_users)
z_init = np.random.randn(num_items)

# Pack parameters into a single vector for the optimizer.
# First num_users elements are theta_est, next num_items are z_est.
params_init = np.concatenate([theta_init, z_init])

def objective(params):
    """
    Computes the loss and gradient for the current parameters.
    Loss is defined as the negative log likelihood (averaged over observed entries).
    """
    # Unpack parameters
    theta = params[:num_users]
    z = params[num_users:]
    
    # Compute difference and estimated probabilities
    diff = theta[:, None] - z[None, :]  # shape: (num_users, num_items)
    sigma = sigmoid(diff)
    
    # To avoid log(0), clip probabilities a little bit
    eps = 1e-8
    sigma = np.clip(sigma, eps, 1 - eps)
    
    # Compute negative log likelihood only on observed entries
    # For each observed entry: if data == 1 then -log(sigma) else -log(1-sigma)
    log_likelihood = data * np.log(sigma) + (1 - data) * np.log(1 - sigma)
    loss = -np.sum(mask * log_likelihood) / observed_count
    
    # Compute gradient with respect to the difference x = theta_i - z_j
    # d(loss)/d(x) = sigma - data  (for observed entries, zero otherwise)
    diff_grad = (sigma - data) * mask  # shape: (num_users, num_items)
    
    # Gradients for theta: sum over items (axis 1)
    grad_theta = np.sum(diff_grad, axis=1) / observed_count
    # Gradients for z: negative sum over users (axis 0)
    grad_z = -np.sum(diff_grad, axis=0) / observed_count
    
    # Pack gradients back into a single vector
    grad = np.concatenate([grad_theta, grad_z])
    return loss, grad

# Callback to track progress (optional)
iteration_progress = tqdm()

def callback(xk):
    iteration_progress.update(1)

# Optimize using L-BFGS-B
result = minimize(
    fun=lambda params: objective(params),
    x0=params_init,
    method="L-BFGS-B",
    jac=True,
    callback=callback,
    options={"maxiter": 100, "disp": True}
)
iteration_progress.close()

# Extract the estimated parameters
theta_est = result.x[:num_users]
z_est = result.x[num_users:]

# Compute final estimated probabilities
probs_final = sigmoid(theta_est[:, None] - z_est[None, :])

# Compute AUC ROC on observed (training) and missing (test) entries
train_probs = probs_final[mask]
test_probs = probs_final[~mask]
train_labels = data[mask]
test_labels = data[~mask]

auc_train = roc_auc_score(train_labels, train_probs)
auc_test = roc_auc_score(test_labels, test_probs)

print(f"Train AUC: {auc_train:.4f}")
print(f"Test AUC: {auc_test:.4f}")

```
:::

```{python}
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Bernoulli
from tqdm import tqdm

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Number of users and items
num_users = 50
num_items = 100

# Generate user-specific and item-specific rewards
theta = torch.randn(num_users, device=device, requires_grad=True)
z = torch.randn(num_items, device=device, requires_grad=True)

# Generate observed choices using logistic function
probs = torch.sigmoid(theta[:, None] - z[None, :])
data = Bernoulli(probs=probs).sample()

# Mask out a fraction of the response matrix
mask = torch.rand_like(data) > 0.2  # 80% observed, 20% missing
data_masked = data.clone()
data_masked[~mask] = float('nan')

# Initialize parameters for EM algorithm
theta_est = torch.randn(num_users, device=device, requires_grad=True)
z_est = torch.randn(num_items, device=device, requires_grad=True)

# Optimizer
optimizer = optim.LBFGS([theta_est, z_est], lr=0.1, max_iter=20, history_size=10, line_search_fn="strong_wolfe")

def closure():
    optimizer.zero_grad()
    probs_est = torch.sigmoid(theta_est[:, None] - z_est[None, :])
    loss = -(Bernoulli(probs=probs_est).log_prob(data) * mask).mean()
    loss.backward()
    return loss

# EM Algorithm
pbar = tqdm(range(100))
for iteration in pbar:
    if iteration > 0:
        previous_theta = theta_est.clone()
        previous_z = z_est.clone()
        previous_loss = loss.clone()
    
    loss = optimizer.step(closure)
    
    if iteration > 0:
        d_loss = (previous_loss - loss).item()
        d_theta = torch.norm(previous_theta - theta_est, p=2).item()
        d_z = torch.norm(previous_z - z_est, p=2).item()
        grad_norm = torch.norm(optimizer.param_groups[0]["params"][0].grad, p=2).item()
        grad_norm += torch.norm(optimizer.param_groups[0]["params"][1].grad, p=2).item()
        pbar.set_postfix({"grad_norm": grad_norm, "d_theta": d_theta, "d_z": d_z, "d_loss": d_loss})
        if d_loss < 1e-5 and d_theta < 1e-5 and d_z < 1e-5 and grad_norm < 1e-5:
            break

# Compute AUC ROC on observed and inferred data
from torchmetrics import AUROC
auroc = AUROC(task="binary")
probs_final = torch.sigmoid(theta_est[:, None] - z_est[None, :])
train_probs = probs_final[mask]
test_probs = probs_final[~mask]
train_labels = data[mask]
test_labels = data[~mask]
auc_train = auroc(train_probs, train_labels)
auc_test = auroc(test_probs, test_labels)
print(f"train auc: {auc_train}")
print(f"test auc: {auc_test}")
```

### Pairwise Model {#pairwise-model}

In *pairwise sampling*, participants compare two items to determine which is preferred. One of the major advantages of this method is the low cognitive demand for raters. Its disadvantage is the limited amount of information content elicited by a sample. Below is a survey based on pairwise sampling:

::: {.content-visible when-format="html"}
<iframe
  src="https://app.opinionx.co/6bef4ca1-82f5-4c1d-8c5a-2274509f22e2"
  style="width:100%; height:450px;"
></iframe>
:::

::: {.callout-note title="code"}
```{pyodide-python}
n_pairs = 10000
pair_indices = np.random.randint(0, n_items, size=(n_pairs, 2))
# Exclude pairs where both indices are the same
mask = pair_indices[:, 0] != pair_indices[:, 1]
pair_indices = pair_indices[mask]

scores = np.zeros(n_items, dtype=int)
wins = rewards[pair_indices[:, 0]] > rewards[pair_indices[:, 1]]

# For pairs where the first item wins:
#   - Increase score for the first item by 1
#   - Decrease score for the second item by 1
np.add.at(scores, pair_indices[wins, 0], 1)
np.add.at(scores, pair_indices[wins, 1], -1)

# For pairs where the second item wins or it's a tie:
#   - Decrease score for the first item by 1
#   - Increase score for the second item by 1
np.add.at(scores, pair_indices[~wins, 0], -1)
np.add.at(scores, pair_indices[~wins, 1], 1)

# Determine preferred and non-preferred items based on scores
preferred = scores > 0
non_preferred = scores < 0

draw_surface()
plt.scatter(items[preferred, 0], items[preferred, 1], c='blue', label='Preferred', alpha=0.5)
plt.scatter(items[non_preferred, 0], items[non_preferred, 1], c='purple', label='Non-preferred', alpha=0.5)
plt.legend()
plt.show()
```
:::

```{python}
n_pairs = 10000
pair_indices = np.random.randint(0, n_items, size=(n_pairs, 2))
# Exclude pairs where both indices are the same
mask = pair_indices[:, 0] != pair_indices[:, 1]
pair_indices = pair_indices[mask]

scores = np.zeros(n_items, dtype=int)
wins = rewards[pair_indices[:, 0]] > rewards[pair_indices[:, 1]]

# For pairs where the first item wins:
#   - Increase score for the first item by 1
#   - Decrease score for the second item by 1
np.add.at(scores, pair_indices[wins, 0], 1)
np.add.at(scores, pair_indices[wins, 1], -1)

# For pairs where the second item wins or it's a tie:
#   - Decrease score for the first item by 1
#   - Increase score for the second item by 1
np.add.at(scores, pair_indices[~wins, 0], -1)
np.add.at(scores, pair_indices[~wins, 1], 1)

# Determine preferred and non-preferred items based on scores
preferred = scores > 0
non_preferred = scores < 0

draw_surface()
plt.scatter(items[preferred, 0], items[preferred, 1], c='blue', label='Preferred', alpha=0.5)
plt.scatter(items[non_preferred, 0], items[non_preferred, 1], c='purple', label='Non-preferred', alpha=0.5)
plt.legend()
plt.show()
```

The Bradley-Terry model compares the reward of choice over all others [@bradley-terry-model] in the set of $J$ choices $i \in \{1, 2, \dots, J\}$. Each choice can also have its unique random noise variable representing the unobserved factor. However, we can also choose to have all choices' unobserved factors follow the same distribution (e.g., independent and identically distributed, IID). The noise is represented as an extreme value distribution, although we can choose alternatives such as a multivariate Gaussian distribution: $\epsilon \sim \mathcal{N}(0, \Sigma)$. If $\Sigma$ is not a diagonal matrix, we effectively model correlations in the noise across choices, enabling us to avoid the IID assumption. In the case of the extreme value distribution, we model the probability of a user preferring choice $i$, which we denote as $P_i = Z^{-1}\exp(u_{i,j}^*)$ where $Z = \sum_{j = 1}^{J} \exp(u_{i,j}^*)$.

We can model an open-ended ranking of the available items with the Plackett-Luce model, in which we jointly model the full sequence of choice ordering [@plackett_luce]. The general form models the joint distribution as the product of conditional probabilities, where each is conditioned on the preceding ranking terms. Given an ordering of $J$ choices $\{y_1, \dots, y_J\}$, we factorize the joint probability into conditionals. Each conditional follows the Bradley-Terry model:
$$
p(y_1, \dots, y_J) = p(y_1) p(y_2 | y_1) ... p(y_J | y_{1:{J - 1}}) = \prod_{i = 1}^J \frac{\exp(u_{i,j}^*)}{\sum_{j \ge i} \exp(u_{i,j}^*)}
$$

Pairwise sampling has proven useful in aligning large language models (LLM) with human preference. An LLM, such as GPT-4, Llama 3.2, and BERT, typically refers to a large and pre-trained neural network that serves as the basis for various downstream tasks. They are pre-trained on a massive corpus of text data, learning to understand language and context. They are capable of multiple language-related tasks such as text classification, language generation, and question answering. A LLM should be aligned to respond correctly based on human preferences. A promising approach is to train LLMs using reinforcement learning (RL) with the reward model (RM) learned from human preference data, providing a mechanism to score the quality of the generated text. This approach, known as RL from human feedback (RLHF), leverages human feedback to guide model training, allowing LLMs to better align with human expectations while continuously improving performance.

We discuss the reward model used in the Llama2 model. The Llama2 RM [@2307.09288] is initialized from the pretrained Llama2 LLM. In the LLM, the last layer is a mapping $L: \mathbb{R}^D \rightarrow \mathbb{R}^V$, where $D$ is the embedding dimension from the transformer decoder stack and $V$ is the vocabulary size. To get the RM, we replace that last layer with a randomly initialized scalar head that maps $L: \mathbb{R}^D \rightarrow \mathbb{R}^1$. It's important to initialize the RM from the LLM it's meant to evaluate. The RM will have the same "knowledge" as the LLM. This is particularly useful for evaluation objectives such as "Does the LLM know when it doesn't know?". However, in cases where the RM is simply evaluating helpfulness or factuality, it may be helpful to have the RM know more. In addition, the RM is on distribution for the LLM - it is initialized in a way where it semantically understands the LLM's outputs. An RM is trained with paired preferences (prompt history, accepted response, rejected response). Prompt history is a multiturn history of user prompts and model generations; the accepted response is the preferred final model generation by an annotator, and the rejected response is the unpreferred response. The RM is trained with maximum likelihood under the Bradley-Terry model with an optional margin term m(r):

$$p(y_c \succ y_r | x) = \sigma(r_\theta(x,y_c) - r_\theta(x,y_r) - m(r))$$ 

The margin term increases the distance in scores specifically for preference pairs annotators rate as easier to separate. Margins were designed primarily based on the sigmoid function, which is used to normalize the raw reward model score flattens out beyond the range of $[-4, 4]$. Thus, the maximum possible margin is eight. A small regularization term is often added to center the score distribution on 0. We consider two variants of preference rating-based margin. When the preference rating-based margin is small, outcomes are rated as "Significantly Better" (1), "Better" (2 out of 3), and "Slightly Better" (1 out of 3), and "Negligibly Better or Unsure" (0 out of 3). In contrast, when the margin is large, outcomes are rated as "Significantly Better" (3), "Better" (2), and "Slightly Better" (1), and "Negligibly Better or Unsure" (0 out of 3).

### List-wise Model {#list-wise-model}
*Multiple-choice sampling* involves participants selecting one item from a set of alternatives. Multiple-choice sampling is simple for participants to understand and reflect on realistic decision-making scenarios where individuals choose one item from many. It is beneficial in complex choice scenarios, such as modes of transportation, where choices are not independent [@bolt2009]. Multiple-choice sampling often relies on simplistic assumptions such as the independence of irrelevant alternatives (IIA), which may not always hold true. This method may also fail to capture the variation in preferences among different individuals, as it typically records only the most preferred choice without accounting for the relative importance of other items. In *rank-order sampling*, participants rank items from most to least preferred. Used in voting, market research, and psychology, it provides rich preference data but is more complex and cognitively demanding than pairwise comparisons, especially for large item sets. Participants may also rank inconsistently [@ragain2019]. *In Best-worst scaling* (BWS), participants are presented with items and asked to identify the most and least preferred items. The primary objective of BWS is to discern the relative importance or preference of items, making it widely applicable in various fields such as market research, health economics, and social sciences [@campbell2015]. BWS provides rich data on the relative importance of items, helps clarify preferences, reduces biases found in traditional rating scales, and results in rewards that are easy to interpret. However, BWS also has limitations, including potential scale interpretation differences among participants and design challenges to avoid biases, such as the order effect or the context in which items are presented.

## The Utility Function Class {#function-class}

### Parametric and Nonparametric Function Class
The reward of the item can take parametric form, such as $z_j = f_{\theta}(x_j)$. It can also take the nonparametric form, which is commonly used in the ideal point model, where the reward of an item $j$ is calculated by the distance from the item to the human in some embedding space[@huber1976ideal]. Given vector representation $e_i$ of choice $i$ and a vector $v_n$ representing an individual $n$, we can use a distance function $K$ to model a stochastic reward function with the unobserved factors following a specified distribution: $u_{n, i} = K(e_i, v_n) + \epsilon_{n, i}$. The intuition is that vectors exist in a shared $n$-dimensional space, and as such, we can use geometry to match choices whose representations are closest to that of a given individual [@ideal_point; @tatli2022distancepreferences] when equipped with a distance metric. Certain distance metrics, such as Euclidian distance or inner product, can easily be biased by the scale of vectors. A distance measure such as cosine similarity, which compensates for scale by normalizing the inner product of two vectors by the product of their magnitudes, can mitigate this bias yet may discard valuable information encoded by the length of the vectors. Beyond the distance metric alone, this model places a strong inductive bias that the individual and choice representations share a common embedding space. In some contexts, this can be a robust bias to add to the model [@idealpoints], but it is a key factor one must consider before employing such a model, and it is a key design choice for modeling.

### Unimodal and Multimodal Function Class
So far, we have considered learning from data from one person with a particular set of preferences or a group with similar preferences, but this is not always the case. Consider a scenario where a user turns left at an intersection [@myers2021learning]. What would they do if they saw a car speeding down the road approaching them? Following a timid driving pattern, some vehicles would stop to let the other car go, preventing a collision. Other vehicles would be more aggressive and try to make the turn before colliding with the oncoming vehicle. Given the data of one of these driving patterns, the model can make an appropriate decision. However, what if the model was given data from both aggressive and timid drivers and does not know which data corresponds to which type of driver? A naive preference learning approach would result in a model trying to find a policy close enough to both driving patterns. The group label is often unobserved because it is expensive to obtain or a data point cannot be cleanly separated into any group (e.g., a more timid driver can be aggressive when they are in a hurry).

@myers2022learning formulates this problem as learning a mixture of $M$ linear reward functions on the embedding space, where $M$ is given. The reward of item $j$ given by the expert $i$ is given by: $f_i(e_j) = w^\top_i e_j,$ where $w_m$ is a vector of parameters corresponding to the $m$-th expert's preferences. An unknown distribution over the reward parameters exists, and we can represent this distribution with convex mixing coefficients $\alpha = [\alpha_1, ..., \alpha_M]$. Consider a robot that performs the following trajectories and asks a user to rank all the trajectories. The robot will be given back a set of trajectory rankings from M humans, and the objective is to learn the underlying reward function. Given the ranking $(j_1 \succ ... \succ j_K | m)$ of expert $m$ and define $\theta = \{w_{1:M}, \alpha_{1:M}\}$, the probability of item $j$ being preferred by $m$ over all other alternatives is 

$$p(j_1 \succ ... \succ j_K | \theta) = \sum_{i = 1}^M \alpha_i \prod_{j = 1}^K  p_{ij}$$

Then the parameters posterior is $p(\theta | Q_{1:T}, x_{1:T}) \propto p(\theta) \prod_t p(x_t | Q_{\leq t}, \theta) = p(\theta) \prod_t p(x_t | \theta, Q_t)$. The first proportionality is from the Bayes rule and the assumption that the queries at timestamp $t$ are conditionally independent of the parameters given history. This assumption is reasonable because the previous queries & rankings ideally give all the information to inform the choice of the next set. The last proportionality term comes from the assumption that the ranked queries are conditionally independent given the parameters. The prior distribution is dependent on the use case. For example, in the user studies conducted by the authors to verify this method, they use a standard Gaussian for the reward weights and the mixing coefficients to be uniform on a $M - 1$ simplex to ensure that they add up to 1. Then, we can use maximum likelihood estimation to compute the parameters with the simplified posterior.

Another example setting multimodal preference is negotiations [@kwon2021targeted]. Let's say there are some shared items and two people with different utilities and desires for items, where each person only knows their utility. In a specific case of @fig-negotiation, Bob as a proposing agent and Alice as a controlled agent who has many different ways of responding to Bob's proposals. Different methods can be used to design Alice as an AI agent. The first idea is reinforcement learning, where multiple rounds of negotiations are done, the model simulates game theory and sees how Bob reacts. Authors of this setting [@kwon2021targeted] show that over time the model learns to ask for the same thing over and over again, as Alice is not trained to be human-like or negotiable, and just tries to maximize Alice's utility. The second approach is supervised learning, where the model can be trained on some dataset, learning the history of negotiations. This results in Alice being very agreeable, which demonstrates two polar results of the two approaches, and it would be ideal to find a middle ground and combine both of them. The authors proposed the Targeted acquisition approach, which is based on active learning ideas. The model asks diverse questions at different cases and stages of negotiations like humans, determining which questions are more valuable to be asked throughout learning. Such an approach ended up in more fair and optimal results than supervised or reinforcement learning [@kwon2021targeted].

### Single Objective and Multi-Objective Utility
The industry has centered around optimizing for two primary reward signals: helpfulness and harmlessness (safety). There are also other axes, such as factuality, reasoning, tool use, code, and multilingualism, but these are out of scope for us. The Llama2 paper collected preference data from humans for each quality, with separate guidelines. This presents a challenge for co-optimizing the final LLM towards both goals. Two main approaches can be taken for RLHF in this context. Train a unified reward model that integrates both datasets or train two separate reward models, one for each quality, and optimize the LLM toward both. Option 1 is difficult because of the tension between helpfulness and harmlessness. They trade off against each other, confusing an RM trained in both. The chosen solution was item 2, where two RMs are used to train the LLM piecewise. The helpfulness RM is used as the primary optimization term, while the harmlessness RM acts as a penalty term, driving the behavior of the LLM away from unsafe territory only when the LLM veers beyond a certain threshold. This is formalized as follows, where $R_s$, $R_h$, and $R_c$ are the safety, helpfulness, and combined reward, respectively. $g$ and $p$ are the model generation and the user prompt:

$$
\begin{aligned}
    R_c(g \mid p) =
    \begin{cases}
        R_s(g \mid p) & \text{if } \text{is\_safety}(p) \text{ or } R_s(g \mid p) < 0.15 \\
        R_h(g \mid p) & \text{otherwise}
    \end{cases}
\end{aligned}
$$

### Pretraining
<!--
In the context of robotics, a very compelling answer is the *cost of data collection*. In a hypothetical world where we have a vast number of expert demonstrations of robots accomplishing many diverse tasks, we don't necessarily need to worry about learning from trials or from humans. We could simply learn a competent imitation agent to perform any task. Natural Language Processing could be seen as living in this world because internet-scale data is available. Robots, however, are expensive, so people generally don't have access to them and, therefore cannot use them to produce information to imitate. Similarly, human time is expensive, so even for large organizations with access to many robots, it's still hard to collect a lot of expert demonstrations. The most extensive available collection of robotics datasets today is the Open X-Embodiment [@padalkar2023open], which consists of around 1M episodes from more than 300 different scenes. Even such large datasets are not enough to learn generally capable robotic policies from imitation learning alone.
-->

RL often stumbles when it comes to devising reward functions aligning with human intentions. Preference-based RL aims to solve this by learning from human feedback, but this often demands a *highly impractical number of queries* or leads to oversimplified reward functions that don't hold up in real-world tasks. As discussed in the previous section, one may apply meta-learning so that the RL agent can adapt to new tasks with fewer human queries to address the impractical requirement of human queries. [@hejna2023few] proposes pre-training models on previous tasks with the meta-learning method MAML [@finn2017model], and then the meta-trained model can adapt to new tasks with fewer queries. We consider settings where a state is denoted as $s\in S$, and action is denoted as $a\in A$, for state space $S$ and action space $A$. The reward function $r: S\times A \to \mathbb{R}$ is unknown and needs to be learned from eliciting human preferences. There are multiple tasks, each with its own reward function and transition probabilities. The reward model is parameterized by $\psi$. We denote $\hat{r}_\psi(s, a)$ to be a learned estimate of an unknown ground-truth reward function $r(s, a)$, parameterized by $\psi$. Accordingly, a reward model determines an RL policy $\phi$ by maximizing the accumulated rewards. The preferences is learned via pair. For each pre-training task, there is a dataset $D$ consists of binary preference between pair of trajectory. Bradley-Terry model is used to predict the preferred trajectory. 

To efficiently approximate the reward function $r_\text{new}$ for a new task with minimal queries, @hejna2023few utilizes a pre-trained reward function $\hat{r}_\psi$ that can be quickly fine-tuned using just a few preference comparisons by leveraging the common structure across tasks by pre-training on data from prior tasks. Although any meta-learning method is compatible, [@hejna2023few] opts for Model Agnostic Meta-Learning (MAML) due to its simplicity. With the aforementioned pre-training with meta learning, the meta-learned reward model can then be used for few-shot preference-based RL during an online adaptation phase. Given a pre-trained reward model $\psi$, the the active few-shot adaption iterates between finding informative pair of trajectory to query human and update reward model and corresponding policy with new data. Informative pair is selected using the disagreement of an ensemble of reward functions over the preference predictors. Specifically, comparisons that maximize $\mathbb{V}(p(e_j \succ e_{j'}))$ are selected each time feedback is collected.

The experiment tests the proposed method on the Meta-World benchmark [@yu2020meta]. Three baselines compared with the proposed method are (1) Soft-Actor Critic (SAC) trained from ground truth rewards, representing performance upper bound, PEBBLE [@lee2021pebble], which does not use information from prior tasks, and (3) Init, which initializes the reward model with the pretrained weights from meta learning but instead of adapting the reward model to the new task, it performs standard updates as in PEBBLE. The results show that the proposed method outperforms all of the baseline methods. There are still some drawbacks. For example, many of the queries the model picks to elicit human preference are almost identical. Moreover, despite the improved query complexity, an impractical number of queries still need to be made. In addition, it is mentioned in the paper that the proposed method may be even worse than training from scratch if the new task is too out-of-distribution. Designing a method that automatically balances between using the prior information or training from scratch is an important future direction.

@zhou2019watch studies a related problem by asking the question, "How can we efficiently learn both from expert demonstrations and from trials where we only get binary feedback from a human?" This paper seeks to learn new tasks with the following general problem setting: We only get one expert demonstration of the target task; after seeing the expert demonstration, robots try to solve the task 1 or more times; then the user (or some pre-defined reward function) annotates each trial as a success/failure; the agent learns from both the demos and the annotated trials to perform well on the target task. A task $i$ is described by the tuple $\{S, A, r_i, P_i\}$. $S$ and $A$ represents all possible states and action, respectively. $r_i$ is the reward function $r_i : S \times A \to \mathbb{R}$, and $P_i$ is the transition dynamics function. $S$ and $A$ are shared across tasks. Learning occurs in 3 phases. During the watch phase, we give the agent $K=1$ demonstrations of the target tasks and all demonstrations are successful. In the Try phase, we use the agent learned during the Watch phase to attempt the task for $L$ trials. After the agent completes the trials, humans (or pre-programmed reward functions) provide one binary reward for each trial, indicating whether the trial was successful. The expected output of this phase is $L$ trajectories and corresponding feedback. After completing the trials, the agent must learn from both the original expert demonstrations and the trials to solve the target task.

First, we are given a dataset of expert demonstrations containing multiple demos for each task and the dataset contains hundreds of tasks. Importantly, no online interaction is needed for training, and this method trains only with supervised learning. This section describes how this paper trains an agent from the given expert demonstrations, and how to incorporate the trials and human feedback into the loop. What we want to obtain out of the Watch phase is a policy conditioned on a set of expert demonstrations via meta-imitation learning. Given the demonstrations $\{d_{i,k}\}$ for task $i$, we sample another different demonstration coming from the same task $d_i^{\text{test}}$, where $d_i^{\text{test}}$ is an example of optimal behavior given the demonstrations. The policy is obtained by imitating actions taken on $d_i^{\text{test}}$ via maximum likelihood:

$$\mathcal{L}^\text{watch}(\theta, \mathcal{D}_i^*) = \mathbb{E}_{\{d_{i,k}\} \sim \mathcal{D}_i^*} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^*  \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \log \pi_\theta^{\text{watch}} (a_t | s_t, \{d_{i,k}\})$$

This corresponds to imitation learning by minimizing the negative log-likelihood of the test trajectory actions, conditioning the policy on the entire demo set. However, how is the conditioning on the demo set achieved? In addition to using features obtained from the images of the current state, the architecture uses features from frames sampled (in order) from the demonstration episodes, which are concatenated together. On the Try phase when the agent is given a set of demonstrations $\{d_{i,k}\}$, we deploy the policy $\pi_\theta^{\text{watch}}(a | s, \{d_{i,k}\})$ to collect $L$ trials. There is no training involved in the Try phase; we simply condition the policy on the given demonstrations. During the Watch phase, the objective was to train a policy conditioned on demonstrations $\pi_\theta^{\text{watch}}(a | s, \{d_{i,k}\})$. The authors of Watch, Try, Learn uses a similar strategy as the Watch phase for the Learn phase. We now want to train a policy that is conditioned on the demonstrations, as well as the trials and binary feedback. We want to learn $\pi_\phi^{\text{watch}}(a | s, \{d_{i,k}\}, \{\mathbf{\tau}_{i, l}\})$. To train the policy, we again use meta-imitation learning, where we additionally sample yet another trajectory from the same task. Concretely, we train policy parameters $\phi$ to minimize the following loss:
$$\mathcal{L}^{\text{learn}}(\phi, \mathcal{D}_i, \mathcal{D}_i^*) = \mathbb{E}_{(\{d_{i,k}\}, \{\mathbf{\tau}_{i,l}\}) \sim \mathcal{D}_i} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^* \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[- \log \pi_\theta^{\text{learn}} (a_t | s_t, \{d_{i,k}\}, \{\tau_{i,l}\}) \big]$$

Three baselines are considered: (1) behavior cloning is simple imitation learning based on maximum log-likelihood training using data from all tasks, (2) meta-imitation learning corresponds to simply running the policy from the Watch step without using any trial data. We only condition on the set of expert demonstrations, but no online trials, and (3) behavior cloning + SAC pretrains a policy with behavior cloning on all data, and follow that with RL fine-tuning for the specific target task, using the maximum-entropy algorithm SAC [@haarnoja2018soft]. The proposed approach significantly outperforms baselines on every task family: it is far superior to behavior cloning and it significantly surpasses Meta-Imitation Learning on 3 out of 4 task families.

### Others Consideration
One key challenge is managing the bias and variance trade-off. Bias refers to assumptions made during model design and training that can skew predictions. For example, in Ideal Point Models, we make the assumption that the representations we use for individuals and choices are aligned in the embedding space and that this representation is sufficient to capture human preferences using distance metrics. However, there are myriad cases in which this may break down, for example, if the two sets of vectors follow different distributions, each with their own unique biases. If the representations do not come from the same domain, one may have little visibility into how a distance metric computes the final reward value for a choice for a given individual. Some ways to mitigate bias in human preference models include increasing the number of parameters in a model (allowing for better learning of patterns in the data) or removing inductive biases based on our assumptions of the underlying data. On the other hand, variance refers to the model's sensitivity to small changes in the input, which leads to significant changes in the output. This phenomenon is often termed 'overfitting' or 'overparameterization.' This behavior can occur in models that have many parameters and learn correlations in the data that do not contribute to learning human preferences but are artifacts of noise in the dataset that one should ultimately ignore. One can address variance in models by reducing the number of parameters or incorporating biases in the model based on factors we can assume about the data.

Another important consideration unique to human preference models is that we wish to model individual preferences, and we may choose to do so at arbitrary granularity. For example, we can fit models to a specific individual or even multiple models for an individual, each for different purposes or contexts. On the other end of the spectrum, we may create a model to capture human preferences across large populations or the world. Individual models may prove to be more powerful, as they do not need to generalize across multiple individuals and can dedicate all of their parameters to learning the preferences of a single user. In the context of human behavior, this can be a significant advantage as any two individuals can be arbitrarily different or even opposite in their preferences. On the other hand, models that fit only one person can tremendously overfit the training distribution and capture noise in the data, which is not truly representative of human preferences. On the end of the spectrum, models fit to the entire world may be inadequate to model human preferences for arbitrary individuals, especially those whose data it has not been fit to. As such, models may underfit the given training distribution. These models aim to generalize to many people but may fail to capture the nuances of individual preferences, especially for those whose data is not represented in the training set. As a result, they may not perform well for arbitrary individuals within the target population. Choosing the appropriate scope for a model is crucial. It must balance the trade-off between overfitting to noise in highly granular models and underfitting in broader models that may not capture individual nuances.

When training or using a reward model, LLM Distribution Shift is an important factor to consider. With each finetune of the LLM, the RM should be updated through a collection of fresh human preferences using generations from the new LLM. This ensures that the RM stays aligned with the current distribution of the LLM and avoids drifting off-distribution. In addition, RM and LLM are coupled: An RM is generally optimized to distinguish human preferences more efficiently within the specific distribution of the LLM to be optimized. However, this specialization poses a challenge: such an RM will underperform when dealing with generations not aligned with this specific LLM distribution, such as generations from a completely different LLM. Last but not least, training RMs can be unstable and prone to overfitting, especially with multiple training epochs. It's generally advisable to limit the number of epochs during RM training to avoid this issue.

## Exercises
### Question 1: Choice Modeling (15 points) {#question-1-choice-modeling-15-points .unnumbered}

We discussed discrete choice modeling in the context of reward being a linear function. Suppose we are deciding between $N$ choices and that the reward for each choice is given by $U_i=\beta_i\mathbf{x}+\epsilon_i$ for $i=1, 2, \cdots, N$. We view $\mathbf{x}$ as the data point that is being conditioned on for deciding which choice to select, and $\beta_i$ as the weights driving the linear reward model. The noise $\epsilon_i$ is i.i.d. sampled from a type of extreme value distribution called the *Gumbel* distribution. The standard Gumbel distribution is given by the density function $f(x)=e^{-(x+e^{-x})}$ and cumulative distribution function $F(x)=e^{-e^{-x}}.$ Fix $i$. Our objective is to calculate $p(U_i\,\, \text{has max reward})$.

(a) **(Written, 2 points)**. Set $U_i=t$ and compute $p(U_j<t)$ for $j\neq i$ in terms of $F$. Use this probability to derive an integral for $p(U_i\,\,  \text{has max reward})$ over $t$ in terms of $f$ and $F$. Example of solution environment.

(b) **(Written, 4 points)**. Compute the integral derived in part (a) with the appropriate $u$-substitution. You should arrive at multi-class logistic regression!

Next, you will implement logistic regression to predict preferred completions. We will use the preference dataset from [RewardBench](<https://huggingface.co/datasets/allenai/reward-bench>). Notice the provided `data/chosen_embeddings.pt` and `data/rejected_embeddings.pt` files. These files were constructed by feeding the prompt alongside the chosen/rejected responses through Llama3-8B-Instruct and selecting the last token's final hidden embedding. Let $e_1$ and $e_2$ be two hidden embeddings with $e_1\succ e_2$. We assume reward is a linear function of embedding $u_j=w^\top e_j$ and use the Bradley-Terry model to predict the preferred item. We can view maximum likelihood across the preference dataset with this model as logistic regression on $e_1-e_2$ and all labels being $1$. Here, we are given a dataset $X$ with $N$ rows of datapoints and $D$ features per datapoint. The weights of the model are parametrized by $w$, a $d$-dimensional column vector. Given binary labels $y$ of shape $N$ by $1$, the negative log likelihood function and the corresponding gradient is 

$$p(y, X| w)=-\frac{1}{N}(y^\top \log(\sigma(X^\top w)) + (1-y)^\tau \log(1-\sigma(X^\top w))), \quad \nabla_w p(y, X | w)=\frac{1}{N}X^T(\sigma(X^\top w)-y),$$

where $\sigma$ is the sigmoid function and is applied element-wise along with $\log$. As usual, we use maximum likelihood to learn the parameter.

1.  **(Coding, 5 points)**. Implement the functions `train` and the `predict_probs` in `LogisticRegression` class. The starter code is provided below.

::: {.callout-note title="code"}
```{pyodide-python}
from sklearn.model_selection import train_test_split
import torch

class LogisticRegression:
    def __init__(self):
        self.weights = None  # Initialized during training

    def train(self, X, y, learning_rate, num_iterations):
        """
        Train the logistic regression model using full batch gradient-based optimization.

        Parameters:
        - X (torch.Tensor): Training data of shape (n_samples, n_features).
        - y (torch.Tensor): Target labels of shape (n_samples,).
        """
        n_samples, n_features = X.shape

        # Initialize weights without the bias term
        self.weights = torch.zeros(n_features)

        for i in range(num_iterations):
            # YOUR CODE HERE (~4-5 lines)
                pass
            # END OF YOUR CODE

    def predict_probs(self, X):
        """
        Predict probabilities for samples in X.

        Parameters:
        - X (torch.Tensor): Input data of shape (n_samples, n_features).

        Returns:
        - y_probs (torch.Tensor): Predicted probabilities.
        """
        y_probs = None

        # YOUR CODE HERE (~2-3 lines)
        pass
        # END OF YOUR CODE

        return y_probs


if __name__ == "__main__":
    # Load in Llama3 embeddings of prompt + completions on RewardBench
    chosen_embeddings = torch.load('data/chosen_embeddings.pt')
    rejected_embeddings = torch.load('data/rejected_embeddings.pt')

    # Subtract the embeddings according to the Bradley-Terry reward model setup presented in the problem 
    X = (chosen_embeddings - rejected_embeddings).to(torch.float)
    y = torch.ones(X.shape[0])
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)  

    # Tune the learning_rate and num_iterations
    learning_rate = None
    num_iterations = None
    model = LogisticRegression()
    model.train(X_train, y_train, learning_rate=learning_rate, num_iterations=num_iterations)
    print(f"Expected Train Accuracy: {model.predict_probs(X_train).mean()}")
    print(f"Expected Validation Accuracy: {del.predict_probs(X_val).mean()}") # Should reach at least 90%
```
:::

3.  **(Written, 4 points)**. Open the notebook `rewardbench_preferences.ipynb` and run all the cells. Make sure to tune the `learning_rate` and `num_iterations`. Report your final expected accuracy on the training and validation sets. How close are the two expected accuracies? You should be able to achieve $\approx 90\%$ expected accuracy on validation. You may add loss reporting to the `train` function to verify your model is improving over time.

### Question 2: Revealed and Stated Preferences (20 points) {#question-2-revealed-and-stated-preferences-20-points .unnumbered}

Alice and Bob are running for president. For $R$ voters, we can access their revealed candidate preferences through some means (e.g., social media, blogs, event history). Assume there is an unknown probability $z$ of voting for Alice among the population. The aim of this question is to estimate $z$ through *maximum likelihood estimation* by also incorporating stated preferences. In this scenario, we collect stated preferences through surveys. When surveyed, voters tend to be more likely to vote for Alice with probability $\frac{z+1}{2}$ for reasons of "political correctness."

(a) **(Written, 5 points)**. Suppose there are $R_A$ revealed preferences for Alice, $R_B$ revealed preferences for Bob, $S_A$ stated preferences for Alice, and $S_B$ stated preferences for Bob. Note $R=R_A+R_B$. Compute the log-likelihood of observing such preferences in terms of $z, R_A, R_B, S_A, S_B$.

(b) **(Coding, 1 point)**. Implement the short function `stated_prob` in the file `voting/simulation.py`.

(c) **(Coding, 5 points)**. Implement the class `VotingSimulation`.

(d) **(Coding, 7 points)**. Implement your derived expression from part (a) in the `log_likelihoods` function.

(e) **(Written, 2 points)**. Finally, implement the `average_mae_mle` method that will allow us to visualize the mean absolute error (MAE) of our maximum likelihood estimate $\hat{z}$ (i.e., $|\hat{z}-z|$) as the number of voters surveyed increases. Open `voting/visualize_sim.ipynb` and run the cells to get a plot of MAE vs. voters surveyed averaged across $100$ simulations. Attach the plot to this question and briefly explain what you notice.

::: {.callout-note title="code"}
```{pyodide-python}
import torch
import random
import matplotlib.pyplot as plt
from tqdm import tqdm
random.seed(42)
torch.manual_seed(42)

def stated_prob(z_values):
    """
    Computes the probability of stated preferences based on z values.
    
    Args:
        z_values (torch.Tensor): The z value(s), where z represents the true probability of voting for Alice.

    Returns:
        torch.Tensor: Probability for stated preferences, derived from z values.
    """
    # YOUR CODE HERE (~1 line)
    # END OF YOUR CODE

class VotingSimulation:
    """
    A class to simulate the voting process where revealed and stated preferences are generated.
    
    Attributes:
        R (int): Number of revealed preferences.
        z (float): The true probability of voting for Alice.
        revealed_preferences (torch.Tensor): Simulated revealed preferences of R voters using Bernoulli distribution.
                                             Takes on 1 for Alice, and 0 for Bob.
        stated_preferences (torch.Tensor): Simulated stated preferences, initialized as an empty tensor.
                                           Takes on 1 for Alice, and 0 for Bob.
    """
    def __init__(self, R, z):
        self.R = R
        self.z = z
        self.revealed_preferences = None # YOUR CODE HERE (~1 line)
        self.stated_preferences = torch.tensor([])

    def add_survey(self):
        """
        Simulates an additional stated preference based on stated_prob and adds it to the list.
        This updates the self.stated_preferences tensor by concatenating on a new simulated survey result.
        """
        # YOUR CODE HERE (~3 lines)
        # END OF YOUR CODE

def log_likelihoods(revealed_preferences, stated_preferences, z_values):
    """
    Computes the log likelihoods across both revealed and stated preferences.
    Use your answer in part (a) to help.
    
    Args:
        revealed_preferences (torch.Tensor): Tensor containing revealed preferences (0 or 1).
        stated_preferences (torch.Tensor): Tensor containing stated preferences (0 or 1).
        z_values (torch.Tensor): Tensor of underlying z values to calculate likelihood for.

    Returns:
        torch.Tensor: Log likelihood for each z value.
    """
    # YOUR CODE HERE (~10-16 lines)
    pass
    # END OF YOUR CODE 

def average_mae_mle(R, z, survey_count, num_sims, z_sweep):
    """
    Runs multiple simulations to compute the average mean absolute error (MAE) of Maximum Likelihood Estimation (MLE) 
    for z after increasing number of surveys.
    
    Args:
        R (int): Number of revealed preferences.
        z (float): The true probability of voting for Alice.
        survey_count (int): Number of additional surveys to perform.
        num_sims (int): Number of simulation runs to average over.
        z_sweep (torch.Tensor): Range of z values to consider for maximum likelihood estimation.

    Returns:
        torch.Tensor: Tensor of mean absolute errors averaged over simulations.
                      Should have shape (survey_count, )
    """
    all_errors = []
    for _ in tqdm(range(num_sims)):
        errors = []
        vote_simulator = VotingSimulation(R=R, z=z)

        for _ in range(survey_count):
            revealed_preferences = vote_simulator.revealed_preferences
            stated_preferences = vote_simulator.stated_preferences

            # YOUR CODE HERE (~6-8 lines)
            pass # Compute log_likelihoods across z_sweep. Argmax to find MLE for z. 
                 # Append the absolute error to errors and add a survey to the simulator.
            # END OF YOUR CODE

        errors_tensor = torch.stack(errors) 
        all_errors.append(errors_tensor)

    # Calculate the average error across simulations 
    mean_errors = torch.stack(all_errors).mean(dim=0)
    return mean_errors

if __name__ == "__main__":
    # DO NOT CHANGE!
    max_surveys = 2000
    z = 0.5
    R = 10
    num_sims = 100
    z_sweep = torch.linspace(0.01, 0.99, 981)

    # Compute and plot the errors. Attach this plot to part (d).
    mean_errors = average_mae_mle(R, z, max_surveys, num_sims, z_sweep)
    plt.plot(mean_errors)

    plt.xlabel('Surveys Conducted')
    plt.ylabel('Average Error')
    plt.title(f'MLE MAE Error (z={z}, {num_sims} simulations)')
    plt.show()
```
:::

### Question 3: Probabilistic Multi-modal Preferences (25 points) {#question-3-probabilistic-multi-modal-preferences-25-points .unnumbered}

Suppose you are part of the ML team on the movie streaming site CardinalStreams. After taking CS329H, you collect a movie preferences dataset with $30000$ examples of the form $(m_1, m_2, \text{user id})$ where $m_1$ and $m_2$ are movies with $m_1\succ m_2$. The preferences come from $600$ distinct users with $50$ examples per user. Each movie has a $10$-dimensional feature vector $m$, and each user has a $10$-dimensional weight vector $u$. Given movie features $m_1, m_2$ and user weights $u$, the user's preference between the movies is given by a Bradley-Terry reward model:
$$P(m_1\succ m_2)=\frac{e^{u\cdot m_1}}{e^{u\cdot m_1} + e^{u\cdot m_2}}=\frac{1}{1+e^{u\cdot (m_2-m_1)}}=\sigma(u\cdot (m_1-m_2)).$$

You realize that trying to estimate the weights for each user with only $50$ examples will not work due to the lack of data. Instead, you choose to drop the user IDs column and shuffle the dataset in order to take a *multi-modal preferences* approach. For simplicity, you assume a model where a proportion $p$ of the users have weights $w_1$ and the other $1-p$ have weights $w_2$. In this setting, each user belongs to one of two groups: users with weights $w_1$ are part of Group 1, and users with weights $w_2$ are part of Group 2.

(a) **(Written, 3 points)**. For a datapoint $(m_1, m_2)$ with label $m_1\succ m_2$, compute the data likelihood $P(m_1\succ m_2 | p, w_1, w_2)$ assuming $p, w_1, w_2$ are given.

(b) **(Written, 3 points)**. As a follow up, use the likelihood to simplify the posterior distribution of $p, w_1, w_2$ after updating on $(m_1, m_2)$ leaving terms for the priors unchanged.

(c) **(Written, 4 points)**. Assume priors $p\sim B(1, 1)$, $w_1\sim\mathcal{N}(0, \mathbf{I})$, and $w_2\sim\mathcal{N}(0, \mathbf{I})$ where $B$ represents the Beta distribution and $\mathcal{N}$ represents the normal distribution. You will notice that the posterior from part (b) has no simple closed-form. As a result, we must resort to *Markov Chain Monte Carlo (MCMC)* approaches to sample from the posterior. These approaches allow sampling from highly complex distributions by constructing a Markov chain $\{x_t\}_{t=1}^\infty$ so that $\lim_{t\to\infty}x_t$ act as desired samples from the target distribution. You can think of a Markov chain as a sequence with the special property that $x_{t+1}$ only depends on $x_t$ for all $t\ge 1$.

The most basic version of MCMC is known as Metropolis-Hastings. Assume $\pi$ is the target distribution we wish to sample from where $\pi(z)$ represents the probability density at point $z$. Metropolis-Hastings constructs the approximating Markov chain $x_t$ as follows: a proposal $P$ for $x_{t+1}$ is made via sampling from a chosen distribution $Q(\,\cdot\,| x_t)$ (e.g., adding Gaussian noise). The acceptance probability of the proposal is given by

$$A= \min \left( 1, \frac{\pi(P)Q(x_t | P)}{\pi(x_t)Q(P | x_t)} \right). \text{ That is } x_{t+1}=\begin{cases} P & \text{with probability } A, \\ x_t & \text{with probability } 1 - A. \end{cases}$$ To extract our samples from $\pi$, we run the Markov chain for $N$ timesteps and disregard the first $T<N$ timesteps in what is called the *burn-in or mixing time* (i.e., our final samples are $x_{T+1}, x_{T+2},\cdots, x_{N}$). The mixing time is needed to ensure that the Markov chain elements are representative of the distribution $\pi$ -- initial elements of the chain will not be a good approximation of $\pi$ and depend more on the choice of initialization $x_1$. To build some intuition, suppose we have a biased coin that turns heads with probability $p_{\text{heads}}$. We observe $12$ coin flips to have $9$ heads (H) and $3$ tails (T). If our prior for $p_{\text{H}}$ was $B(1, 1)$, then our posterior will be $B(1+9, 1+3)=B(10, 4)$. The Bayesian update is given by

$$p(p_{\text{H}}|9\text{H}, 3\text{T}) = \frac{p(9\text{H}, 3\text{T} | p_{\text{H}})B(1, 1)(p_{\text{H}})}{\int_0^1 P(9\text{H}, 3\text{T} | p_{\text{H}})B(1, 1)(p_{\text{H}}) dp_{\text{H}}} =\frac{p(9\text{H}, 3\text{T} | p_{\text{H}})}{\int_0^1 p(9\text{H}, 3\text{T} | p_{\text{H}})  dp_{\text{H}}}.$$

**Find the acceptance probablity** $A$ in the setting of the biased coin assuming the proposal distribution $Q(\cdot|x_t)=x_t+N(0,\sigma)$ for given $\sigma$. Notice that this choice of $Q$ is symmetric, i.e., $Q(x_t|P)=Q(P|x_t)$. In addition, you will realize that is unnecessary to compute the normalizing constant of the Bayesian update (i.e., the integral in the denominator) which is why MCMC is commonly used to sample from posteriors!

(d) **(Written + Coding, 6 points)**. Implement Metropolis-Hastings to sample from the posterior distribution of the biased coin in `multimodal_preferences/biased_coin.py`. Attach a histogram of your MCMC samples overlayed on top of the true posterior $B(10, 4)$ by running `python biased_coin.py`.

::: {.callout-note title="code"}
```{pyodide-python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import beta

def likelihood(p: float) -> float:
    """
    Computes the likelihood of 9 heads and 3 tails assuming p_heads is p.

    Args:
    p (float): A value between 0 and 1 representing the probability of heads.

    Returns:
    float: The likelihood value at p_heads=p. Return 0 if p is outside the range [0, 1].
    """
    # YOUR CODE HERE (~1-3 lines)
    pass
    # END OF YOUR CODE


def propose(x_current: float, sigma: float) -> float:
    """
    Proposes a new sample from the proposal distribution Q.
    Here, Q is a normal distribution centered at x_current with standard deviation sigma.

    Args:
    x_current (float): The current value in the Markov chain.
    sigma (float): Standard deviation of the normal proposal distribution.

    Returns:
    float: The proposed new sample.
    """
    # YOUR CODE HERE (~1-3 lines)
    pass
    # END OF YOUR CODE


def acceptance_probability(x_current: float, x_proposed: float) -> float:
    """
    Computes the acceptance probability A for the proposed sample.
    Since the proposal distribution is symmetric, Q cancels out.

    Args:
    x_current (float): The current value in the Markov chain.
    x_proposed (float): The proposed new value.

    Returns:
    float: The acceptance probability
    """
    # YOUR CODE HERE (~4-6 lines)
    pass
    # END OF YOUR CODE


def metropolis_hastings(N: int, T: int, x_init: float, sigma: float) -> np.ndarray:
    """
    Runs the Metropolis-Hastings algorithm to sample from a posterior distribution.

    Args:
    N (int): Total number of iterations.
    T (int): Burn-in period (number of initial samples to discard).
    x_init (float): Initial value of the chain.
    sigma (float): Standard deviation of the proposal distribution.

    Returns:
    list: Samples collected after the burn-in period.
    """
    samples = []
    x_current = x_init

    for t in range(N):
        # YOUR CODE HERE (~7-10 lines)
        # Use the propose and acceptance_probability functions to get x_{t+1} and store it in samples after the burn-in period T
        pass
        # END OF YOUR CODE

    return samples


def plot_results(samples: np.ndarray) -> None:
    """
    Plots the histogram of MCMC samples along with the true Beta(10, 4) PDF.

    Args:
    samples (np.ndarray): Array of samples collected from the Metropolis-Hastings algorithm.

    Returns:
    None
    """
    # Histogram of the samples from the Metropolis-Hastings algorithm
    plt.hist(samples, bins=50, density=True, alpha=0.5, label="MCMC Samples")

    # True Beta(10, 4) distribution for comparison
    p = np.linspace(0, 1, 1000)
    beta_pdf = beta.pdf(p, 10, 4)
    plt.plot(p, beta_pdf, "r-", label="Beta(10, 4) PDF")

    plt.xlabel("p_heads")
    plt.ylabel("Density")
    plt.title("Metropolis-Hastings Sampling of Biased Coin Posterior")
    plt.legend()
    plt.show()


if __name__ == "__main__":
    # MCMC Parameters (DO NOT CHANGE!)
    N = 50000  # Total number of iterations
    T = 10000  # Burn-in period to discard
    x_init = 0.5  # Initial guess for p_heads
    sigma = 0.1  # Standard deviation of the proposal distribution

    # Run Metropolis-Hastings and plot the results
    samples = metropolis_hastings(N, T, x_init, sigma)
    plot_results(samples)
```
:::

(e) **(Coding, 9 points)**. Implement Metropolis-Hastings in the movie setting inside\ `multimodal_preferences/movie_metropolis.py`. The movie dataset we use for grading will not be provided. However, randomly constructed datasets can be used to test your implementation by running `python movie_metropolis.py`. You should be able to achieve a $90\%$ success rate with most `fraction_accepted` values above $0.1$. Success is measured by thresholded closeness of predicted parameters to true parameters. You may notice occasional failures that occur due to lack of convergence which we will account for in grading.

::: {.callout-note title="code"}
```{pyodide-python}
import torch
import torch.distributions as dist
import math
from tqdm import tqdm
from typing import Tuple

def make_data(
    true_p: torch.Tensor, true_weights_1: torch.Tensor, true_weights_2: torch.Tensor, num_movies: int, feature_dim: int
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Generates a synthetic movie dataset according to the CardinalStreams model.

    Args:
        true_p (torch.Tensor): Probability of coming from Group 1.
        true_weights_1 (torch.Tensor): Weights for Group 1.
        true_weights_2 (torch.Tensor): Weights for Group 2.

    Returns:
        Tuple[torch.Tensor, torch.Tensor]: A tuple containing the dataset and labels.
    """
    # Create movie features
    first_movie_features = torch.randn((num_movies, feature_dim))
    second_movie_features = torch.randn((num_movies, feature_dim))

    # Only care about difference of features for Bradley-Terry
    dataset = first_movie_features - second_movie_features

    # Get probabilities that first movie is preferred assuming Group 1 or Group 2
    weight_1_probs = torch.sigmoid(dataset @ true_weights_1)
    weight_2_probs = torch.sigmoid(dataset @ true_weights_2)

    # Probability that first movie is preferred overall can be viewed as sum of conditioning on Group 1 and Group 2
    first_movie_preferred_probs = (
        true_p * weight_1_probs + (1 - true_p) * weight_2_probs
    )
    labels = dist.Bernoulli(first_movie_preferred_probs).sample()
    return dataset, labels


def compute_likelihoods(
    dataset: torch.Tensor,
    labels: torch.Tensor,
    p: torch.Tensor,
    w_1: torch.Tensor,
    w_2: torch.Tensor,
) -> torch.Tensor:
    """
    Computes the likelihood of each datapoint. Use your calculation from part (a) to help.

    Args:
        dataset (torch.Tensor): The dataset of differences between movie features.
        labels (torch.Tensor): The labels where 1 indicates the first movie is preferred, and 0 indicates preference of the second movie.
        p (torch.Tensor): The probability of coming from Group 1.
        w_1 (torch.Tensor): Weights for Group 1.
        w_2 (torch.Tensor): Weights for Group 2.

    Returns:
        torch.Tensor: The likelihoods for each datapoint. Should have shape (dataset.shape[0], )
    """
    # YOUR CODE HERE (~6-8 lines)
    pass
    # END OF YOUR CODE

def compute_prior_density(
    p: torch.Tensor, w_1: torch.Tensor, w_2: torch.Tensor
) -> torch.Tensor:
    """
    Computes the prior density of the parameters.

    Args:
        p (torch.Tensor): The probability of preferring model 1.
        w_1 (torch.Tensor): Weights for model 1.
        w_2 (torch.Tensor): Weights for model 2.

    Returns:
        torch.Tensor: The prior densities of p, w_1, and w_2.
    """
    # Adjusts p to stay in the range [0.3, 0.7] to prevent multiple equilibria issues at p=0 and p=1
    p_prob = torch.tensor([2.5]) if 0.3 <= p <= 0.7 else torch.tensor([0.0])

    def normal_pdf(x: torch.Tensor) -> torch.Tensor:
        """Computes the PDF of the standard normal distribution at x."""
        return (1.0 / torch.sqrt(torch.tensor(2 * math.pi))) * torch.exp(-0.5 * x**2)

    weights_1_prob = normal_pdf(w_1)
    weights_2_prob = normal_pdf(w_2)

    # Concatenate the densities
    concatenated_prob = torch.cat([p_prob, weights_1_prob, weights_2_prob])
    return concatenated_prob


def metropolis_hastings(
    dataset: torch.Tensor,
    labels: torch.Tensor,
    sigma: float = 0.01,
    num_iters: int = 30000,
    burn_in: int = 20000,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]:
    """
    Performs the Metropolis-Hastings algorithm to sample from the posterior distribution.
    DO NOT CHANGE THE DEFAULT VALUES!

    Args:
        dataset (torch.Tensor): The dataset of differences between movie features.
        labels (torch.Tensor): The labels indicating which movie is preferred.
        sigma (float, optional): Standard deviation for proposal distribution.
            Defaults to 0.01.
        num_iters (int, optional): Total number of iterations. Defaults to 30000.
        burn_in (int, optional): Number of iterations to discard as burn-in.
            Defaults to 20000.

    Returns:
        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]: Samples of p,
        w_1, w_2, and the fraction of accepted proposals.
    """
    feature_dim = dataset.shape[1]

    # Initialize random starting parameters by sampling priors
    curr_p = 0.3 + 0.4 * torch.rand(1)
    curr_w_1 = torch.randn(feature_dim)
    curr_w_2 = torch.randn(feature_dim)

    # Keep track of samples and total number of accepted proposals
    p_samples = []
    w_1_samples = []
    w_2_samples = []
    accept_count = 0 

    for T in tqdm(range(num_iters)):
        # YOUR CODE HERE (~3 lines)
        pass # Sample proposals for p, w_1, w_2
        # END OF YOUR CODE

        # YOUR CODE HERE (~4-6 lines)
        pass # Compute likehoods and prior densities on both the proposed and current samples
        # END OF YOUR CODE

        # YOUR CODE HERE (~2-4 lines)
        pass # Obtain the ratios of the likelihoods and prior densities between the proposed and current samples 
        # END OF YOUR CODE 

        # YOUR CODE HERE (~1-2 lines)
        pass # Multiply all ratios (both likelihoods and prior densities) and use this to calculate the acceptance probability of the proposal
        # END OF YOUR CODE

        # YOUR CODE HERE (~4-6 lines)
        pass # Sample randomness to determine whether the proposal should be accepted to update curr_p, curr_w_1, curr_w_2, and accept_count
        # END OF YOUR CODE 

        # YOUR CODE HERE (~4-6 lines)
        pass # Update p_samples, w_1_samples, w_2_samples if we have passed the burn in period T
        # END OF YOUR CODE 

    fraction_accepted = accept_count / num_iters
    print(f"Fraction of accepted proposals: {fraction_accepted}")
    return (
        torch.stack(p_samples),
        torch.stack(w_1_samples),
        torch.stack(w_2_samples),
        fraction_accepted,
    )


def evaluate_metropolis(num_sims: int, num_movies: int, feature_dim: int) -> None:
    """
    Runs the Metropolis-Hastings algorithm N times and compare estimated parameters
    with true parameters to obtain success rate. You should attain a success rate of around 90%. 

    Note that there are two successful equilibria to converge to. They are true_weights_1 and true_weights_2 with probabilities
    p and 1-p in addition to true_weights_2 and true_weights_1 with probabilities 1-p and p. This is why even though it may appear your
    predicted parameters don't match the true parameters, they are in fact equivalent. 

    Args:
        num_sims (int): Number of simulations to run.

    Returns:
        None
    """
    
    success_count = 0
    for _ in range(num_sims):
        # Sample random ground truth parameters
        true_p = 0.3 + 0.4 * torch.rand(1)
        true_weights_1 = torch.randn(feature_dim)
        true_weights_2 = torch.randn(feature_dim)

        print("\n---- MCMC Simulation ----")
        print("True parameters:", true_p, true_weights_1, true_weights_2)

        dataset, labels = make_data(true_p, true_weights_1, true_weights_2, num_movies, feature_dim)
        p_samples, w_1_samples, w_2_samples, _ = metropolis_hastings(dataset, labels)

        p_pred = p_samples.mean(dim=0)
        w_1_pred = w_1_samples.mean(dim=0)
        w_2_pred = w_2_samples.mean(dim=0)

        print("Predicted parameters:", p_pred, w_1_pred, w_2_pred)

        # Do casework on two equilibria cases to check for success
        p_diff_case_1 = torch.abs(p_pred - true_p)
        p_diff_case_2 = torch.abs(p_pred - (1 - true_p))

        w_1_diff_case_1 = torch.max(torch.abs(w_1_pred - true_weights_1))
        w_1_diff_case_2 = torch.max(torch.abs(w_1_pred - true_weights_2))

        w_2_diff_case_1 = torch.max(torch.abs(w_2_pred - true_weights_2))
        w_2_diff_case_2 = torch.max(torch.abs(w_2_pred - true_weights_1))

        pass_case_1 = (
            p_diff_case_1 < 0.1 and w_1_diff_case_1 < 0.5 and w_2_diff_case_1 < 0.5
        )
        pass_case_2 = (
            p_diff_case_2 < 0.1 and w_1_diff_case_2 < 0.5 and w_2_diff_case_2 < 0.5
        )
        passes = pass_case_1 or pass_case_2

        print(f'Result: {"Success" if passes else "FAILED"}')
        if passes:
            success_count += 1
    print(f'Success rate: {success_count / num_sims}')


if __name__ == "__main__":
    evaluate_metropolis(num_sims=10, num_movies=30000, feature_dim=10)
```
:::
